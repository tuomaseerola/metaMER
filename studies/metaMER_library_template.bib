@{agarwal2021an,
  AUTHOR = {Agarwal, Gaurav and Om, Hari},
  JOURNAL = {IET Signal Processing},
  NOTE = {Cited by: 23},
  NUMBER = {2},
  PAGES = {98 – 121},
  TITLE = {An efficient supervised framework for music mood recognition using autoencoder-based optimised support vector regression model},
  TYPE = {Article},
  VOLUME = {15},
  YEAR = {2021},
  DOI = {10.1049/sil2.12015},
  ABSTRACT = {Music is the art of ‘language of emotions’. Recently, music mood recognition is an emerging task. An efficient supervised framework for music mood recognition using autoencoder-based optimised support vector regression (SVR) model is developed for the music emotion recognition. Our main intention is to increase the accuracy of emotion classification of music by considering text-dependent and non-text-dependent features. For the high level feature representation, stacked autoencoder is used with two hidden layers. Modified K-Medoid-based brain storm optimisation-based support vector regression (SVR_KMBSO) model is utilised for the emotion classification. Using the K-Medoid-based brain storm algorithm, the optimal parameters of the SVR are selected. The proposed framework utilises ISMIR2012 dataset and NJU_V1 dataset for English and for Hindi; online songs are also gathered and used for the music mood recognition. All the three datasets include songs based on four emotions like happy, angry, relax and sad. The experimental results are evaluated and compared with the existing classifiers including SVR, deep belief network (DBN) and Recurrent neural network (RNN). The proposed method SVR_KMBSO achieved high accuracy using three different datasets. © 2021 The Authors. IET Signal Processing published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.},
  KEYWORDS = {Character recognition; Emotion Recognition; Learning systems; Recurrent neural networks; Regression analysis; Storms; Auto encoders; Emotion classification; Emotion recognition; Feature representation; Hidden layers; High-level features; K-medoid; Music emotions; Support vector regression models; Support vector regressions; Music},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85116929889&doi = 10.1049%2fsil2.12015&partnerID = 40&md5 = 4e078ee8b1c740d21d4b7d8dbf7f6cd8},
  PARADIGM = {classification},
  NOTES_CA = {include, classification},
  NOTES_TE = {include, classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{alvarez2023ri,
  AUTHOR = {Álvarez, P. and de Quirós, J. García and Baldassarri, S.},
  JOURNAL = {International Journal of Interactive Multimedia and Artificial Intelligence},
  NOTE = {Cited by: 4; All Open Access, Gold Open Access, Green Open Access},
  NUMBER = {2},
  PAGES = {168 – 181},
  TITLE = {RIADA: A Machine-Learning Based Infrastructure for Recognising the Emotions of Spotify Songs},
  TYPE = {Article},
  VOLUME = {8},
  YEAR = {2023},
  DOI = {10.9781/ijimai.2022.04.002},
  ABSTRACT = {The music emotions can help to improve the personalization of services and contents offered by music streaming providers. Many research works based on the use of machine learning techniques have addressed the problem of recognising the music emotions during the last years. Nevertheless, the results obtained are only applied on small-size music repositories and do not consider what the users feel when they listen to the songs. These issues prevent the existing proposals to be integrated into the personalization mechanisms of the online music providers. In this paper, we present the RIADA infrastructure which is composed by a set of systems able to annotate emotionally the catalog of songs offered by Spotify based on the users’ perception. RIADA works with the Spotify playlist miner and data services to build emotion recognition models that can solve the open challenges previously mentioned. Machine learning algorithms, music information retrieval techniques, architectures for parallelization of applications and cloud computing have been combined to develop a complex result of engineering able to integrate the music emotions into the Spotify-based applications. © 2023, Universidad Internacional de la Rioja. All rights reserved.},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85147947463&doi = 10.9781%2fijimai.2022.04.002&partnerID = 40&md5 = 8f61f829c998052047f5da67adcf48c9},
  AUTHOR_KEYWORDS = {Affective Annotation; Cloud Computing; Emotion Recognition; Machine Learning; Music; Spotify},
  PARADIGM = {classification},
  NOTES_CA = {include, classification task},
  NOTES_TE = {include, classification, spotify},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{bai2017mu,
  AUTHOR = {Bai, Junjie and Luo, Kan and Peng, Jun and Shi, Jinliang and Wu, Ying and Feng, Lixiao and Li, Jianqing and Wang, Yingxu},
  JOURNAL = {International Journal of Cognitive Informatics and Natural Intelligence},
  NOTE = {Cited by: 6},
  NUMBER = {4},
  PAGES = {80 – 92},
  TITLE = {Music emotions recognition by machine learning with cognitive classification methodologies},
  TYPE = {Article},
  VOLUME = {11},
  YEAR = {2017},
  DOI = {10.4018/IJCINI.2017100105},
  ABSTRACT = {Music emotions recognition (MER) is a challenging field of studies addressed in multiple disciplines such as musicology, cognitive science, physiology, psychology, arts and affective computing. In this article, music emotions are classified into four types known as those of pleasing, angry, sad and relaxing. MER is formulated as a classification problem in cognitive computing where 548 dimensions of music features are extracted and modeled. A set of classifications and machine learning algorithms are explored and comparatively studied for MER, which includes Support Vector Machine (SVM), k-Nearest Neighbors (KNN), Neuro-Fuzzy Networks Classification (NFNC), Fuzzy KNN (FKNN), Bayes classifier and Linear Discriminant Analysis (LDA). Experimental results show that the SVM, FKNN and LDA algorithms are the most effective methodologies that obtain more than 80% accuracy for MER. © 2017 IGI Global.},
  KEYWORDS = {Artificial intelligence; Arts computing; Barium compounds; Discriminant analysis; Feature extraction; Fuzzy neural networks; Image retrieval; Learning systems; Nearest neighbor search; Pattern recognition; Psychophysiology; Support vector machines; Affective Computing; Classification methodologies; Emotion classification; K nearest neighbor (KNN); Linear discriminant analysis; Multiple disciplines; Music emotions; Neuro-fuzzy network; Learning algorithms},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85038434665&doi = 10.4018%2fIJCINI.2017100105&partnerID = 40&md5 = a2e1f1d69ab6ceb6f0cee7f2eb26985a},
  AUTHOR_KEYWORDS = {Emotion Classification; Feature Extraction; Machine Learning; Music Emotion Recognition; Pattern Recognition},
  PARADIGM = {classification},
  NOTES_CA = {include, classification},
  NOTES_TE = {include, classification, quality issues?},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{bhuvanakumar2023em,
  AUTHOR = {Bhuvana Kumar, V. and Kathiravan, M.},
  JOURNAL = {Frontiers in Computer Science},
  NOTE = {Cited by: 0; All Open Access, Gold Open Access},
  TITLE = {Emotion recognition from MIDI musical file using Enhanced Residual Gated Recurrent Unit architecture},
  TYPE = {Article},
  VOLUME = {5},
  YEAR = {2023},
  DOI = {10.3389/fcomp.2023.1305413},
  ABSTRACT = {The complex synthesis of emotions seen in music is meticulously composed using a wide range of aural components. Given the expanding soundscape and abundance of online music resources, creating a music recommendation system is significant. The area of music file emotion recognition is particularly fascinating. The RGRU (Enhanced Residual Gated Recurrent Unit), a complex architecture, is used in our study to look at MIDI (Musical Instrument and Digital Interface) compositions for detecting emotions. This involves extracting diverse features from the MIDI dataset, encompassing harmony, rhythm, dynamics, and statistical attributes. These extracted features subsequently serve as input to our emotion recognition model for emotion detection. We use an improved RGRU version to identify emotions and the Adaptive Red Fox Algorithm (ARFA) to optimize the RGRU hyperparameters. Our suggested model offers a sophisticated classification framework that effectively divides emotional content into four separate quadrants: positive-high, positive-low, negative-high, and negative-low. The Python programming environment is used to implement our suggested approach. We use the EMOPIA dataset to compare its performance to the traditional approach and assess its effectiveness experimentally. The trial results show better performance compared to traditional methods, with higher accuracy, recall, F-measure, and precision. Copyright © 2023 Kumar and Kathiravan.},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85181443074&doi = 10.3389%2ffcomp.2023.1305413&partnerID = 40&md5 = 977f4cd7e7b4a9a110b29a8f48b0e632},
  AUTHOR_KEYWORDS = {adaptive Red Fox algorithm; EMOPIA; emotion recognition; Enhanced Residual Gated Recurrent Unit; Musical Instrument Digital Interface},
  PARADIGM = {classification},
  NOTES_CA = {include, quadrant classification},
  NOTES_TE = {include, midi, classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{dufour2021us,
  AUTHOR = {Dufour, Isabelle and Tzanetakis, George},
  JOURNAL = {IEEE TRANSACTIONS ON AFFECTIVE COMPUTING},
  MONTH = {JUL-SEP},
  NUMBER = {3},
  PAGES = {666-681},
  TITLE = {Using Circular Models to Improve Music Emotion Recognition},
  VOLUME = {12},
  YEAR = {2021},
  DOI = {10.1109/TAFFC.2018.2885744},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1109/TAFFC.2018.2885744},
  ISSN = {1949-3045},
  ORCID = {Tzanetakis, George/0000-0002-6844-7912},
  RESEARCHERID = {Tzanetakis, George/I-6593-2013},
  PARADIGM = {classification},
  NOTES_CA = {include, classification},
  NOTES_TE = {include, classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{feng2024ex,
  AUTHOR = {Feng, Qiyue},
  JOURNAL = {Applied Mathematics and Nonlinear Sciences},
  NOTE = {Cited by: 0},
  NUMBER = {1},
  TITLE = {Exploration of the Integration of Traditional Music Cultural Elements in Music Informatization Teaching},
  TYPE = {Article},
  VOLUME = {9},
  YEAR = {2024},
  DOI = {10.2478/amns-2024-0973},
  ABSTRACT = {With the development of information technology, music teaching methods are getting more affluent and more prosperous. This paper proposes a model for emotion classification and assessment that integrates traditional music culture elements with information technology in music teaching. The research first combines TextCNN and BiLSTM algorithms to establish the emotion classification model of conventional music. Then it combines PYIN and DTW algorithms to establish the evaluation model of traditional music, which completes the auxiliary efficacy of music informationized teaching. In the emotion classification test of the model, the classification accuracy and F1 value of emotions of different music samples are 82.98% and 75.22%, respectively. The model’s recognition accuracy of the four voices is 86.76%, and the overall effective scoring percentage is 81.98% under different playing abnormalities. This study has had an impact on traditional music evaluation. The model in this paper can be used to classify and evaluate emotions in conventional music, providing more intelligent and high-quality technical services for integrating traditional music into music teaching. © 2023 Qiyue Feng, published by Sciendo.},
  KEYWORDS = {Quality control; BiLSTM; Classification models; DTW; Emotion classification; Informatization; Music evaluation; PYIN; Teaching methods; TextCNN; Traditional music; Classification (of information)},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85192244090&doi = 10.2478%2famns-2024-0973&partnerID = 40&md5 = f61a9cbc265402b97c5600f8b5f1d679},
  AUTHOR_KEYWORDS = {BiLSTM; DTW; Music evaluation; PYIN; TextCNN; Traditional music},
  PARADIGM = {classification},
  NOTES_CA = {include, classification task},
  NOTES_TE = {include, classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{hizlisoy2021mu,
  AUTHOR = {Hizlisoy, Serhat and Yildirim, Serdar and Tufekci, Zekeriya},
  JOURNAL = {Engineering Science and Technology, an International Journal},
  NOTE = {Cited by: 85; All Open Access, Gold Open Access},
  NUMBER = {3},
  PAGES = {760 – 767},
  TITLE = {Music emotion recognition using convolutional long short term memory deep neural networks},
  TYPE = {Article},
  VOLUME = {24},
  YEAR = {2021},
  DOI = {10.1016/j.jestch.2020.10.009},
  ABSTRACT = {In this paper, we propose an approach for music emotion recognition based on convolutional long short term memory deep neural network (CLDNN) architecture. In addition, we construct a new Turkish emotional music database composed of 124 Turkish traditional music excerpts with a duration of 30 s each and the performance of the proposed approach is evaluated on the constructed database. We utilize features obtained by feeding convolutional neural network (CNN) layers with log-mel filterbank energies and mel frequency cepstral coefficients (MFCCs) in addition to standard acoustic features. Classification results show that the best performance is obtained when the new feature set is combined with the standard features using the long short term memory (LSTM) + deep neural network (DNN) classi fier. The overall accuracy of 99.19% is obtained using the proposed system with 10 fold cross-validation. Specifically, 6.45 points improvement is achieved. Additionally, the results also show that the LSTM + DNN classifier yields 1.61, 1.61 and 3.23 points improvements in music emotion recognition accuracies compared to k-nearest neighbor (k-NN), support vector machine (SVM), and Random Forest classifiers, respectively. © 2020 Karabuk University},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85096857185&doi = 10.1016%2fj.jestch.2020.10.009&partnerID = 40&md5 = a6d9948932df1065662495509f77bfe5},
  AUTHOR_KEYWORDS = {Convolutional long short term memory deep neural networks; Music emotion recognition; Turkish emotional music database},
  PARADIGM = {classification},
  NOTES_CA = {include, classification},
  NOTES_TE = {include, classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{nag2022on,
  AUTHOR = {Nag, Sayan and Basu, Medha and Sanyal, Shankha and Banerjee, Archi and Ghosh, Dipak},
  JOURNAL = {Physica A: Statistical Mechanics and its Applications},
  NOTE = {Cited by: 19},
  TITLE = {On the application of deep learning and multifractal techniques to classify emotions and instruments using Indian Classical Music},
  TYPE = {Article},
  VOLUME = {597},
  YEAR = {2022},
  DOI = {10.1016/j.physa.2022.127261},
  ABSTRACT = {Music is often considered as the language of emotions. The way it stimulates the emotional appraisal across people from different communities, culture and demographics has long been known and hence categorizing on the basis of emotions is indeed an intriguing basic research area. Indian Classical Music (ICM) is famous for its ambiguous nature, i.e. its ability to evoke a number of mixed emotions through only a single musical narration, and hence classifying evoked emotions from ICM becomes a more challenging task. With the rapid advancements in the field of Deep Learning, this Music Emotion Recognition (MER) task is becoming more and more relevant and robust, hence can be applied to one of the most challenging test case i.e. classifying emotions elicited from ICM. In this paper we present a new dataset called JUMusEmoDB which presently has 1600 audio clips (approximately 30 s each) where 400 clips each correspond to happy, sad, calm and anxiety emotional scales. The initial annotations and emotional classification of the database was done based on an emotional rating test (5-point Likert scale) performed by 100 participants. The clips have been taken from different conventional ‘raga’ renditions played in two Indian stringed instruments – sitar and sarod by eminent maestros of ICM and digitized in 44.1 kHz frequency. The ragas, which are unique to ICM, are described as musical structures capable of inducing different moods or emotions. For supervised classification purposes, we have used Convolutional Neural Network (CNN) based architectures (resnet50, mobilenet v2.0, squeezenet v1.0 and a proposed ODE-Net) on corresponding music spectrograms of the 6400 sub-clips (where every clip was segmented into 4 sub-clips) which contain both time as well as frequency domain information. Along with emotion classification, instrument classification based response was also attempted on the same dataset using the CNN based architectures. In this context, a nonlinear technique, Multifractal Detrended Fluctuation Analysis (MFDFA) was also applied on the musical clips to classify them on the basis of complexity values extracted from the method. The initial classification accuracy obtained from the applied methods are quite inspiring and have been corroborated with ANOVA results to determine the statistical significance. This type of CNN based classification algorithm using a rich corpus of Indian Classical Music is unique even in the global perspective and can be replicated in other modalities of music also. The link to this newly developed dataset has been provided in the dataset description section of the paper. This dataset is still under development and we plan to include more data containing other emotional as well as instrumental entities into consideration. © 2022 Elsevier B.V.},
  KEYWORDS = {Classification (of information); Convolutional neural networks; Deep learning; Frequency domain analysis; Music; Network architecture; Convolutional neural network; Emotion; Emotion recognition; Indian classical music; Learning techniques; Multifractal detrended fluctuation analysis; Multifractal technique; Music emotions; Network-based architectures; Research areas; Fractals},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85127732290&doi = 10.1016%2fj.physa.2022.127261&partnerID = 40&md5 = 634cc77ee93e9ac834ca0dad8b815119},
  AUTHOR_KEYWORDS = {Classification; CNN; Emotions; Indian Classical Music; Instruments; MFDFA},
  PARADIGM = {classification},
  NOTES_CA = {include, classification},
  NOTES_TE = {include, basic emotion classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{nguyen2017an,
  AUTHOR = {Nguyen, Van Loi and Kim, Donglim and Ho, Van Phi and Lim, Younghwan},
  JOURNAL = {International Journal of Electrical and Computer Engineering},
  NOTE = {Cited by: 14},
  NUMBER = {3},
  PAGES = {1246 – 1254},
  TITLE = {A new recognition method for visualizing music emotion},
  TYPE = {Article},
  VOLUME = {7},
  YEAR = {2017},
  DOI = {10.11591/ijece.v7i3.pp1246-1254},
  ABSTRACT = {This paper proposes an emotion detection method using a combination of dimensional approach and categorical approach. Thayer's model is divided into discrete emotion sections based on the level of arousal and valence. The main objective of the method is to increase the number of detected emotions which is used for emotion visualization. To evaluate the suggested method, we conducted various experiments with supervised learning and feature selection strategies. We collected 300 music clips with emotions annotated by music experts. Two feature sets are employed to create two training models for arousal and valence dimensions of Thayer's model. Finally, 36 music emotions are detected by proposed method. The results showed that the suggested algorithm achieved the highest accuracy when using RandomForest classifier with 70% and 57.3% for arousal and valence, respectively. These rates are better than previous studies. © 2017 Institute of Advanced Engineering and Science.},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85021156960&doi = 10.11591%2fijece.v7i3.pp1246-1254&partnerID = 40&md5 = c3ac2cfa0f3a859b3232ebd8875ba59b},
  AUTHOR_KEYWORDS = {Feature extraction; Music emotion recognition algorithm; Music information retrieval; Music mood detection},
  PARADIGM = {classification},
  NOTES_CA = {include, classification},
  NOTES_TE = {include},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{panda2020no,
  AUTHOR = {Panda, Renato and Malheiro, Ricardo and Paiva, Rui Pedro},
  JOURNAL = {IEEE Transactions on Affective Computing},
  NOTE = {Cited by: 83; All Open Access, Green Open Access},
  NUMBER = {4},
  PAGES = {614 – 626},
  TITLE = {Novel Audio Features for Music Emotion Recognition},
  TYPE = {Article},
  VOLUME = {11},
  YEAR = {2020},
  DOI = {10.1109/TAFFC.2018.2820691},
  ABSTRACT = {This work advances the music emotion recognition state-of-the-art by proposing novel emotionally-relevant audio features. We reviewed the existing audio features implemented in well-known frameworks and their relationships with the eight commonly defined musical concepts. This knowledge helped uncover musical concepts lacking computational extractors, to which we propose algorithms - namely related with musical texture and expressive techniques. To evaluate our work, we created a public dataset of 900 audio clips, with subjective annotations following Russell's emotion quadrants. The existent audio features (baseline) and the proposed features (novel) were tested using 20 repetitions of 10-fold cross-validation. Adding the proposed features improved the F1-score to 76.4 percent (by 9 percent), when compared to a similar number of baseline-only features. Moreover, analysing the features relevance and results uncovered interesting relations, namely the weight of specific features and musical concepts to each emotion quadrant, and warrant promising new directions for future research in the field of music emotion recognition, interactive media, and novel music interfaces. © 2010-2012 IEEE.},
  KEYWORDS = {Feature extraction; Speech recognition; Textures; 10-fold cross-validation; Affective Computing; Audio database; Emotion recognition; Interactive media; Music information retrieval; Music interfaces; Musical concepts; Audio acoustics},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85044777756&doi = 10.1109%2fTAFFC.2018.2820691&partnerID = 40&md5 = 6ff59839d539ee19db6d2a75a843d7d6},
  AUTHOR_KEYWORDS = {Affective computing; audio databases; emotion recognition; feature extraction; music information retrieval},
  PARADIGM = {classification},
  NOTES_CA = {include, quadrant classification},
  NOTES_TE = {include, classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{sorussa2020em,
  AUTHOR = {Sorussa, Kanawat and Choksuriwong, Anant and Karnjanadecha, Montri},
  JOURNAL = {ECTI Transactions on Computer and Information Technology},
  NOTE = {Cited by: 4; All Open Access, Gold Open Access},
  NUMBER = {1},
  PAGES = {53 – 66},
  TITLE = {Emotion classi cation system for digital music with a cascaded technique},
  TYPE = {Article},
  VOLUME = {14},
  YEAR = {2020},
  DOI = {10.37936/ecti-cit.2020141.205317},
  ABSTRACT = {Music selection is diffcult without effcient organization based on metadata or tags, and one effective tag scheme is based on the emotion expressed by the music. However, manual annotation is labor intensive and unstable because the perception of music emotion varies from person to person. This paper presents an emotion classi cation system for digital music with a resolution of eight emotional classes. Russell’s emotion model was adopted as common ground for emotional annotation. The music information retrieval (MIR) toolbox was employed to extract acoustic features from audio les. The classi cation system utilized a supervised machine learning technique to recognize acoustic features and create predictive models. Four predictive models were proposed and compared. The models were composed by crossmatching two types of neural networks, the Levenberg-Marquardt (LM) and resilient backpropagation (Rprop), with two types of structures: a traditional multiclass model and the cascaded structure of a binary-class model. The performance of each model was evaluated via the MediaEval Database for Emotional Analysis (DEAM) benchmark. The best result was achieved by the model trained with the cascaded Rprop neural network (accuracy of 89.5%). In addition, correlation coeffcient analysis showed that timbre features were the most impactful for prediction. Our work offers an opportunity for a competitive advantage in music classi cation because only a few music providers currently tag music with emotional terms. © 2020, ECTI Association Sirindhon International Institute of Technology. All rights reserved.},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85084006024&doi = 10.37936%2fecti-cit.2020141.205317&partnerID = 40&md5 = 508dd81e9a9120b0c6e87d44e90a2d5a},
  AUTHOR_KEYWORDS = {Articial neural networks; Classi cation algorithms; Emotion recognition; Music information retrieval},
  PARADIGM = {classification},
  NOTES_CA = {include, classification},
  NOTES_TE = {include, classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{yang2021an,
  AUTHOR = {Yang, Jing},
  JOURNAL = {Frontiers in Psychology},
  NOTE = {Cited by: 14; All Open Access, Gold Open Access, Green Open Access},
  TITLE = {A Novel Music Emotion Recognition Model Using Neural Network Technology},
  TYPE = {Article},
  VOLUME = {12},
  YEAR = {2021},
  DOI = {10.3389/fpsyg.2021.760060},
  ABSTRACT = {Music plays an extremely important role in people’s production and life. The amount of music is growing rapidly. At the same time, the demand for music organization, classification, and retrieval is also increasing. Paying more attention to the emotional expression of creators and the psychological characteristics of music are also indispensable personalized needs of users. The existing music emotion recognition (MER) methods have the following two challenges. First, the emotional color conveyed by the first music is constantly changing with the playback of the music, and it is difficult to accurately express the ups and downs of music emotion based on the analysis of the entire music. Second, it is difficult to analyze music emotions based on the pitch, length, and intensity of the notes, which can hardly reflect the soul and connotation of music. In this paper, an improved back propagation (BP) algorithm neural network is used to analyze music data. Because the traditional BP network tends to fall into local solutions, the selection of initial weights and thresholds directly affects the training effect. This paper introduces artificial bee colony (ABC) algorithm to improve the structure of BP neural network. The output value of the ABC algorithm is used as the weight and threshold of the BP neural network. The ABC algorithm is responsible for adjusting the weights and thresholds, and feeds back the optimal weights and thresholds to the BP neural network system. BP neural network with ABC algorithm can improve the global search ability of the BP network, while reducing the probability of the BP network falling into the local optimal solution, and the convergence speed is faster. Through experiments on public music data sets, the experimental results show that compared with other comparative models, the MER method used in this paper has better recognition effect and faster recognition speed. © Copyright © 2021 Yang.},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85116855247&doi = 10.3389%2ffpsyg.2021.760060&partnerID = 40&md5 = d8babd4e6050f14375d2b4632dfdb587},
  AUTHOR_KEYWORDS = {ABC algorithm; BP neural network; emotion recognition; MediaEval Emotion in Music data set; music},
  PARADIGM = {classification},
  NOTES_CA = {include, classification},
  NOTES_TE = {include, classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{yeh2014po,
  AUTHOR = {Yeh, Chia-Hung and Tseng, Wen-Yu and Chen, Chia-Yen and Lin, Yu-Dun and Tsai, Yi-Ren and Bi, Hsuan-I and Lin, Yu-Ching and Lin, Ho-Yi},
  JOURNAL = {Multimedia Tools and Applications},
  NOTE = {Cited by: 11},
  NUMBER = {3},
  PAGES = {2103 – 2128},
  TITLE = {Popular music representation: chorus detection & emotion recognition},
  TYPE = {Article},
  VOLUME = {73},
  YEAR = {2014},
  DOI = {10.1007/s11042-013-1687-2},
  ABSTRACT = {This paper proposes a popular music representation strategy based on the song’s emotion. First, a piece of popular music is decomposed into chorus and verse segments through the proposed chorus detection algorithm. Three descriptive features: intensity, frequency band and rhythm regularity are extracted from the structured segments for emotion detection. A hierarchical Adaboost classifier is employed to recognize the emotion of a piece of popular music. The general emotion of the music is classified according to Thayer’s model into four emotions: happy, angry, depressed and relaxed. Experiments conducted on a 350-popular-music database show the average recall and precision of our proposed chorus detection are approximately 95 % and 84 %, respectively; and the average precision rate of emotion detection is 92 %. Additional tests are performed on songs with cover versions in different lyrics and languages, and the resultant precision rate is 90 %. The proposes approaches have been tested and proven by the professional online music company, KKBOX Inc. and show promising performance for effectively and efficiently identifying the emotions of a variety of popular music. © 2013, Springer Science+Business Media New York.},
  KEYWORDS = {Multimedia systems; Chorus; Emotion; MFCCs; Popular music; Rhythm; Verse; Adaptive boosting},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-84912027522&doi = 10.1007%2fs11042-013-1687-2&partnerID = 40&md5 = 6ce0fe7a0b38a6e2037403e140bd0a60},
  AUTHOR_KEYWORDS = {Adaboost; Chorus; Emotion; MFCCs; Popular music; Rhythm; Verse},
  PARADIGM = {classification},
  NOTES_CA = {include, classification},
  NOTES_TE = {include, or unclear, report classification broadly/vaguely},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{zhang2017fe,
  AUTHOR = {Zhang, Jiang Long and Huang, Xiang Lin and Yang, Li Fang and Xu, Ye and Sun, Shu Tao},
  JOURNAL = {Multimedia Systems},
  NOTE = {Cited by: 10},
  NUMBER = {2},
  PAGES = {251 – 264},
  TITLE = {Feature selection and feature learning in arousal dimension of music emotion by using shrinkage methods},
  TYPE = {Article},
  VOLUME = {23},
  YEAR = {2017},
  DOI = {10.1007/s00530-015-0489-y},
  ABSTRACT = {Music emotion recognition is an important topic in music information retrieval area. A lot of acoustic features are used to train a music classification or regression emotion model. However, these existing features may not be efficient for classification or regression task. Furthermore, most works do not explain why these features do work for classification. In our work, eight features are extracted to represent the arousal dimension of music emotion, and various commonly used statistical learning methods such as Logistic Regression, and tree-based methods are applied to interpret important features. Then the shrinkage methods are applied to feature selection and classification in music emotion recognition for the first time. Our tests show that the proposed approaches are efficient for feature selection just as entropy-based filter methods, and better than wrapper methods. The shrinkage methods can produce more continuous and low variance model than wrapper methods. Then, we discover that the most useful features are low specific loudness sensation coefficients (low-SONE), root mean square and loudness-flux. Moreover, the shrinkage methods apply in logistic regression perform better for classification than most of other methods. We get an average accuracy rate of 83.8 %. © 2015, Springer-Verlag Berlin Heidelberg.},
  KEYWORDS = {Feature extraction; Regression analysis; Shrinkage; Speech recognition; Feature selection and classification; Features learning; Features selection; Music classification; Music information retrieval; Shrinkage methods; Statistical learning; Statistical learning methods; Classification (of information)},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-84944937222&doi = 10.1007%2fs00530-015-0489-y&partnerID = 40&md5 = f12764992ee29a4976644aacf6bbb43d},
  AUTHOR_KEYWORDS = {Features learning; Features selection; Music arousal dimension classification; Shrinkage method; Statistical learning},
  PARADIGM = {classification},
  NOTES_CA = {include, classification},
  NOTES_TE = {include},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{bai2016di,
  AUTHOR = {Bai, Junjie and Feng, Lixiao and Peng, Jun and Shi, Jinliang and Luo, Kan and Li, Zuojin and Liao, Lu and Wang, Yingxu},
  JOURNAL = {INTERNATIONAL JOURNAL OF COGNITIVE INFORMATICS AND NATURAL INTELLIGENCE},
  MONTH = {OCT-DEC},
  NUMBER = {4},
  PAGES = {74-89},
  TITLE = {Dimensional Music Emotion Recognition by Machine Learning},
  VOLUME = {10},
  YEAR = {2016},
  DOI = {10.4018/IJCINI.2016100104},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.4018/IJCINI.2016100104},
  ISSN = {1557-3958},
  EISSN = {1557-3966},
  ORCID = {Wang, Yingxu/0000-0003-0445-3632 Peng, Jun/0000-0001-6800-0064 luo, kan/0000-0003-2317-6714},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{battcock2021in,
  AUTHOR = {Battcock, Aimee and Schutz, Michael},
  JOURNAL = {JOURNAL OF NEW MUSIC RESEARCH},
  MONTH = {OCT 20},
  NUMBER = {5},
  PAGES = {447-468},
  TITLE = {Individualized interpretation: Exploring structural and interpretive effects on evaluations of emotional content in Bach's Well Tempered Clavier},
  VOLUME = {50},
  YEAR = {2021},
  DOI = {10.1080/09298215.2021.1979050},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1080/09298215.2021.1979050},
  ISSN = {0929-8215},
  EISSN = {1744-5027},
  EARLYACCESSDATE = {JAN 2022},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include, commonality analysis},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{beveridge2018po,
  AUTHOR = {Beveridge, Scott and Knox, Don},
  JOURNAL = {PSYCHOLOGY OF MUSIC},
  MONTH = {MAY},
  NUMBER = {3},
  PAGES = {411-423},
  TITLE = {Popular music and the role of vocal melody in perceived emotion},
  VOLUME = {46},
  YEAR = {2018},
  DOI = {10.1177/0305735617713834},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1177/0305735617713834},
  ISSN = {0305-7356},
  EISSN = {1741-3087},
  ORCID = {Knox, Don/0000-0003-1303-1183},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include, R},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{cao2023th,
  AUTHOR = {Cao, Yujing and Park, Jinwan},
  JOURNAL = {IEEE ACCESS},
  PAGES = {141192-141204},
  TITLE = {The Analysis of Music Emotion and Visualization Fusing Long Short-Term Memory Networks Under the Internet of Things},
  VOLUME = {11},
  YEAR = {2023},
  DOI = {10.1109/ACCESS.2023.3341926},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1109/ACCESS.2023.3341926},
  ISSN = {2169-3536},
  PARADIGM = {regression},
  NOTES_CA = {include, single-modality reported},
  NOTES_TE = {include, some quality issues},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{chen2017co,
  AUTHOR = {Chen, Yu-An and Wang, Ju-Chiang and Yang, Yi-Hsuan and Chen, Homer H.},
  JOURNAL = {IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING},
  MONTH = {JUL},
  NUMBER = {7},
  PAGES = {1409-1420},
  TITLE = {Component Tying for Mixture Model Adaptation in Personalization of Music Emotion Recognition},
  VOLUME = {25},
  YEAR = {2017},
  DOI = {10.1109/TASLP.2017.2693565},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1109/TASLP.2017.2693565},
  ISSN = {2329-9290},
  EISSN = {2329-9304},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{chin2018pr,
  AUTHOR = {Chin, Yu-Hao and Wang, Jia-Ching and Wang, Ju-Chiang and Yang, Yi-Hsuan},
  JOURNAL = {IEEE TRANSACTIONS ON AFFECTIVE COMPUTING},
  MONTH = {OCT-DEC},
  NUMBER = {4},
  PAGES = {541-549},
  TITLE = {Predicting the Probability Density Function of Music Emotion Using Emotion Space Mapping},
  VOLUME = {9},
  YEAR = {2018},
  DOI = {10.1109/TAFFC.2016.2628794},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1109/TAFFC.2016.2628794},
  ISSN = {1949-3045},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include, R2},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{coutinho2017sh,
  AUTHOR = {Coutinho, Eduardo and Schuller, Bjorn},
  JOURNAL = {PLOS ONE},
  MONTH = {JUN 28},
  NUMBER = {6},
  TITLE = {Shared acoustic codes underlie emotional communication in music and speech-Evidence from deep transfer learning},
  VOLUME = {12},
  YEAR = {2017},
  DOI = {10.1371/journal.pone.0179289},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1371/journal.pone.0179289},
  ISSN = {1932-6203},
  ARTICLE = {e0179289},
  ORCID = {Coutinho, Eduardo/0000-0001-5234-1497 Schuller, Bj{\"o}rn Wolfgang/0000-0002-6478-8699},
  RESEARCHERID = {Coutinho, Eduardo/K-1391-2019 Schuller, Bj{\"o}rn Wolfgang/D-3241-2011},
  PARADIGM = {regression},
  NOTES_CA = {include, can report intradomain accuracies},
  NOTES_TE = {include},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{deng2015em,
  AUTHOR = {Deng, James J. and Leung, Clement H. C. and Milani, Alfredo and Chen, Li},
  JOURNAL = {ACM TRANSACTIONS ON INTERACTIVE INTELLIGENT SYSTEMS},
  MONTH = {MAR},
  NUMBER = {1},
  TITLE = {Emotional States Associated with Music: Classification, Prediction of Changes, and Consideration in Recommendation},
  VOLUME = {5},
  YEAR = {2015},
  DOI = {10.1145/2723575},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1145/2723575},
  ISSN = {2160-6455},
  EISSN = {2160-6463},
  ORCID = {Chen, Li/0000-0002-5842-838X MILANI, Alfredo/0000-0003-4534-1805},
  COMMENT = {discussed},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{gingras2014be,
  AUTHOR = {Gingras, Bruno and Marin, Manuela M. and Fitch, W. Tecumseh},
  JOURNAL = {Quarterly Journal of Experimental Psychology},
  NOTE = {Cited by: 32},
  NUMBER = {7},
  PAGES = {1428 – 1446},
  TITLE = {Beyond intensity: Spectral features effectively predict music-induced subjective arousal},
  TYPE = {Article},
  VOLUME = {67},
  YEAR = {2014},
  DOI = {10.1080/17470218.2013.863954},
  ABSTRACT = {Emotions in music are conveyed by a variety of acoustic cues. Notably, the positive association between sound intensity and arousal has particular biological relevance. However, although amplitude normalization is a common procedure used to control for intensity in music psychology research, direct comparisons between emotional ratings of original and amplitude-normalized musical excerpts are lacking.In this study, 30 nonmusicians retrospectively rated the subjective arousal and pleasantness induced by 84 six-second classical music excerpts, and an additional 30 nonmusicians rated the same excerpts normalized for amplitude. Following the cue-redundancy and Brunswik lens models of acoustic communication, we hypothesized that arousal and pleasantness ratings would be similar for both versions of the excerpts, and that arousal could be predicted effectively by other acoustic cues besides intensity.Although the difference in mean arousal and pleasantness ratings between original and amplitude-normalized excerpts correlated significantly with the amplitude adjustment, ratings for both sets of excerpts were highly correlated and shared a similar range of values, thus validating the use of amplitude normalization in music emotion research. Two acoustic parameters, spectral flux and spectral entropy, accounted for 65% of the variance in arousal ratings for both sets, indicating that spectral features can effectively predict arousal. Additionally, we confirmed that amplitude-normalized excerpts were adequately matched for loudness. Overall, the results corroborate our hypotheses and support the cue-redundancy and Brunswik lens models. © 2013 The Experimental Psychology Society.},
  KEYWORDS = {Acoustic Stimulation; Acoustics; Adult; Arousal; Emotions; Female; Humans; Linear Models; Loudness Perception; Male; Music; Predictive Value of Tests; Questionnaires; Recognition (Psychology); Young Adult; acoustics; adult; arousal; auditory stimulation; emotion; female; hearing; human; male; music; physiology; predictive value; psychology; questionnaire; recognition; statistical model; young adult},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-84902376903&doi = 10.1080%2f17470218.2013.863954&partnerID = 40&md5 = 021d634d0f679aaff17d6770894daad5},
  AUTHOR_KEYWORDS = {Arousal; Brunswik lens model; Emotion; Intensity; Music},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include, has R2},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{grekow2018au,
  AUTHOR = {Grekow, Jacek},
  JOURNAL = {JOURNAL OF INFORMATION AND TELECOMMUNICATION},
  MONTH = {JUL 3},
  NUMBER = {3},
  PAGES = {322-333},
  TITLE = {Audio features dedicated to the detection and tracking of arousal and valence in musical compositions},
  VOLUME = {2},
  YEAR = {2018},
  DOI = {10.1080/24751839.2018.1463749},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1080/24751839.2018.1463749},
  ISSN = {2475-1839},
  EISSN = {2475-1847},
  RESEARCHERID = {Grekow, Jacek/M-9500-2015},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{grekow2021mu,
  AUTHOR = {Grekow, Jacek},
  JOURNAL = {JOURNAL OF INTELLIGENT INFORMATION SYSTEMS},
  MONTH = {DEC},
  NUMBER = {3},
  PAGES = {531-546},
  TITLE = {Music emotion recognition using recurrent neural networks and pretrained models},
  VOLUME = {57},
  YEAR = {2021},
  DOI = {10.1007/s10844-021-00658-5},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1007/s10844-021-00658-5},
  ISSN = {0925-9902},
  EISSN = {1573-7675},
  ORCID = {Grekow, Jacek/0000-0003-2094-0107},
  RESEARCHERID = {Grekow, Jacek/M-9500-2015},
  EARLYACCESSDATE = {AUG 2021},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{griffiths2021am,
  AUTHOR = {Griffiths, Darryl and Cunningham, Stuart and Weinel, Jonathan and Picking, Richard},
  JOURNAL = {JOURNAL OF NEW MUSIC RESEARCH},
  MONTH = {AUG 8},
  NUMBER = {4},
  PAGES = {355-372},
  TITLE = {A multi-genre model for music emotion recognition using linear regressors},
  VOLUME = {50},
  YEAR = {2021},
  DOI = {10.1080/09298215.2021.1977336},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1080/09298215.2021.1977336},
  ISSN = {0929-8215},
  EISSN = {1744-5027},
  ORCID = {Cunningham, Stuart/0000-0002-5348-7700 Weinel, Jonathan/0000-0001-5347-3897},
  RESEARCHERID = {Cunningham, Stuart/HSH-5303-2023},
  EARLYACCESSDATE = {SEP 2021},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include, linear regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{hu2017cr,
  AUTHOR = {Hu, Xiao and Yang, Yi-Hsuan},
  JOURNAL = {IEEE TRANSACTIONS ON AFFECTIVE COMPUTING},
  MONTH = {APR-JUN},
  NUMBER = {2},
  PAGES = {228-240},
  TITLE = {Cross-Dataset and Cross-Cultural Music Mood Prediction: A Case on Western and Chinese Pop Songs},
  VOLUME = {8},
  YEAR = {2017},
  DOI = {10.1109/TAFFC.2016.2523503},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1109/TAFFC.2016.2523503},
  ISSN = {1949-3045},
  ORCID = {Hu, Xiao/0000-0003-3994-0385},
  RESEARCHERID = {Hu, Xiao/A-7645-2013 Hu, Xiao/AAD-8405-2020},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{hu2022de,
  AUTHOR = {Hu, Xiao and Li, Fanjie and Liu, Ruilun},
  JOURNAL = {APPLIED SCIENCES-BASEL},
  MONTH = {SEP},
  NUMBER = {18},
  TITLE = {Detecting Music-Induced Emotion Based on Acoustic Analysis and Physiological Sensing: A Multimodal Approach},
  VOLUME = {12},
  YEAR = {2022},
  DOI = {10.3390/app12189354},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.3390/app12189354},
  ARTICLE = {9354},
  EISSN = {2076-3417},
  ORCID = {Hu, Xiao/0000-0003-3994-0385 Li, Fanjie/0000-0001-7016-6354},
  RESEARCHERID = {Hu, Xiao/AAD-8405-2020},
  COMMENT = {includes audio-only model},
  PARADIGM = {regression},
  NOTES_CA = {include , single-modality reported},
  NOTES_TE = {include, includes audio only classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{koh2023me,
  AUTHOR = {Koh, En Yan and Cheuk, Kin Wai and Heung, Kwan Yee and Agres, Kat R. and Herremans, Dorien},
  JOURNAL = {SENSORS},
  MONTH = {JAN},
  NUMBER = {1},
  TITLE = {MERP: A Music Dataset with Emotion Ratings and Raters' Profile Information},
  VOLUME = {23},
  YEAR = {2023},
  DOI = {10.3390/s23010382},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.3390/s23010382},
  ARTICLE = {382},
  EISSN = {1424-8220},
  ORCID = {Herremans, Dorien/0000-0001-8607-1640 Cheuk, Kin Wai/0000-0003-3213-8242 Agres, Kathleen/0000-0001-7260-2447},
  RESEARCHERID = {Herremans, Dorien/G-9599-2018 Cheuk, Kin Wai/HZJ-8015-2023},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{malheiro2018em,
  AUTHOR = {Malheiro, Ricardo and Panda, Renato and Gomes, Paulo and Paiva, Rui Pedro},
  JOURNAL = {IEEE TRANSACTIONS ON AFFECTIVE COMPUTING},
  MONTH = {APR-JUN},
  NUMBER = {2},
  PAGES = {240-254},
  TITLE = {Emotionally-Relevant Features for Classification and Regression of Music Lyrics},
  VOLUME = {9},
  YEAR = {2018},
  DOI = {10.1109/TAFFC.2016.2598569},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1109/TAFFC.2016.2598569},
  ISSN = {1949-3045},
  ORCID = {Malheiro, Ricardo/0000-0002-3010-2732 Panda, Renato/0000-0003-2539-5590 Paiva, Rui Pedro/0000-0003-3215-3960},
  RESEARCHERID = {Malheiro, Ricardo/L-9369-2017 Panda, Renato/AAK-7581-2020 Paiva, Rui Pedro/D-9602-2018},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{markov2014mu,
  AUTHOR = {Markov, Konstantin and Matsui, Tomoko},
  JOURNAL = {IEEE Access},
  NOTE = {Cited by: 73; All Open Access, Gold Open Access},
  PAGES = {688 – 697},
  TITLE = {Music genre and emotion recognition using Gaussian processes},
  TYPE = {Article},
  VOLUME = {2},
  YEAR = {2014},
  DOI = {10.1109/ACCESS.2014.2333095},
  ABSTRACT = {Gaussian Processes (GPs) are Bayesian nonparametric models that are becoming more and more popular for their superior capabilities to capture highly nonlinear data relationships in various tasks, such as dimensionality reduction, time series analysis, novelty detection, as well as classical regression and classification tasks. In this paper, we investigate the feasibility and applicability of GP models for music genre classification and music emotion estimation. These are two of the main tasks in the music information retrieval (MIR) field. So far, the support vector machine (SVM) has been the dominant model used in MIR systems. Like SVM, GP models are based on kernel functions and Gram matrices; but, in contrast, they produce truly probabilistic outputs with an explicit degree of prediction uncertainty. In addition, there exist algorithms for GP hyperparameter learning - something the SVM framework lacks. In this paper, we built two systems, one for music genre classification and another for music emotion estimation using both SVM and GP models, and compared their performances on two databases of similar size. In all cases, the music audio signal was processed in the same way, and the effects of different feature extraction methods and their various combinations were also investigated. The evaluation experiments clearly showed that in both music genre classification and music emotion estimation tasks the GP performed consistently better than the SVM. The GP achieved a 13.6% relative genre classification error reduction and up to an 11% absolute increase of the coefficient of determination in the emotion estimation task. © 2014 IEEE.},
  KEYWORDS = {Classification (of information); Feature extraction; Gaussian distribution; Gaussian noise (electronic); Image retrieval; Support vector machines; Time series analysis; Coefficient of determination; Dimensionality reduction; Feature extraction methods; Gaussian Processes; Music emotions; Music genre classification; Music information retrieval; Prediction uncertainty; Audio acoustics},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-84923317754&doi = 10.1109%2fACCESS.2014.2333095&partnerID = 40&md5 = 91033029d14599965aafb73fa7175273},
  AUTHOR_KEYWORDS = {Gaussian processes; Music emotion estimation; Music genre classification},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include, mediaeval, R2 metrics},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{medina2020em,
  AUTHOR = {Medina, Yesid Ospitia and Beltran, Jose Ramon and Baldassarri, Sandra},
  JOURNAL = {PERSONAL AND UBIQUITOUS COMPUTING},
  MONTH = {2020 APR 15},
  TITLE = {Emotional classification of music using neural networks with the MediaEval dataset},
  YEAR = {2020},
  DOI = {10.1007/s00779-020-01393-4},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1007/s00779-020-01393-4},
  ISSN = {1617-4909},
  EISSN = {1617-4917},
  ORCID = {Ospitia, Yesid/0000-0002-5494-2787 Beltran, Jose Ramon/0000-0002-7500-4650 Baldassarri, Sandra/0000-0002-9315-6391},
  RESEARCHERID = {Ospitia, Yesid/AAD-6729-2021 Beltran, Jose Ramon/K-7693-2015 Baldassarri, Sandra/L-6033-2014},
  EARLYACCESSDATE = {APR 2020},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include, classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{orjesek2022en,
  AUTHOR = {Orjesek, Richard and Jarina, Roman and Chmulik, Michal},
  JOURNAL = {MULTIMEDIA TOOLS AND APPLICATIONS},
  MONTH = {FEB},
  NUMBER = {4},
  PAGES = {5017-5031},
  TITLE = {End-to-end music emotion variation detection using iteratively reconstructed deep features},
  VOLUME = {81},
  YEAR = {2022},
  DOI = {10.1007/s11042-021-11584-7},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1007/s11042-021-11584-7},
  ISSN = {1380-7501},
  EISSN = {1573-7721},
  ORCID = {Chmulik, Michal/0000-0002-0513-5129 Jarina, Roman/0000-0002-0478-5808},
  RESEARCHERID = {Chmulik, Michal/IQW-1183-2023 Jarina, Roman/E-2541-2018},
  EARLYACCESSDATE = {JAN 2022},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include, both R2 and classification included},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{panwar2019ar,
  AUTHOR = {Panwar, Sharaj and Rad, Paul and Choo, Kim-Kwang Raymond and Roopaei, Mehdi},
  JOURNAL = {JOURNAL OF SUPERCOMPUTING},
  MONTH = {JUN},
  NUMBER = {6, SI},
  PAGES = {2986-3009},
  TITLE = {Are you emotional or depressed? Learning about your emotional state from your music using machine learning},
  VOLUME = {75},
  YEAR = {2019},
  DOI = {10.1007/s11227-018-2499-y},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1007/s11227-018-2499-y},
  ISSN = {0920-8542},
  EISSN = {1573-0484},
  ORCID = {Choo, Kim-Kwang Raymond/0000-0001-9208-5336},
  RESEARCHERID = {Choo, Kim-Kwang Raymond/A-3634-2009 najafirad, peyman/ACB-9554-2022},
  PARADIGM = {regression},
  NOTES_CA = {include, authors add detail about VA accuracy},
  NOTES_TE = {include, DEAM},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{saizclar2022pr,
  AUTHOR = {Saiz-Clar, Elena and Angel Serrano, Miguel and Manuel Reales, Jose},
  JOURNAL = {PSYCHOLOGY OF MUSIC},
  MONTH = {JUL},
  NUMBER = {4},
  PAGES = {1107-1120},
  TITLE = {Predicting emotions in music using the onset curve},
  VOLUME = {50},
  YEAR = {2022},
  DOI = {10.1177/03057356211031658},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1177/03057356211031658},
  ISSN = {0305-7356},
  EISSN = {1741-3087},
  ORCID = {Serrano, Miguel-Angel/0000-0002-6574-4532 Serrano, Miguel-Angel/0000-0002-6574-4532 Saiz-Clar, Elena/0000-0002-8714-2305},
  RESEARCHERID = {Serrano, Miguel-Angel/ABE-7279-2021 Serrano, Miguel-Angel/AAZ-4393-2020},
  EARLYACCESSDATE = {AUG 2021},
  PARADIGM = {regression},
  NOTES_CA = {exclude, no modeling task},
  NOTES_TE = {include, R2},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{vempala2024pr,
  AUTHOR = {Vempala, Naresh and Russo, Frank and Lab, Smart},
  MONTH = {},
  TITLE = {Predicting Emotion from Music Audio Features Using Neural Networks},
  YEAR = {2024},
  DOI = {10.32920/25413184},
  ABSTRACT = {{$<$}p{$ > $}We describe our implementation of two neural networks: a static feedforward network, and an Elman network, for predicting mean valence/arousal ratings of participants for musical excerpts based on audio features. Thirteen audio features were extracted from 12 classical music excerpts (3 from each emotion quadrant). Valence/arousal ratings were collected from 45 participants for the static network, and 9 participants for the Elman network. For the Elman network, each excerpt was temporally segmented into four, sequential chunks of equal duration. Networks were trained on eight of the 12 excerpts and tested on the remaining four. The static network predicted values that closely matched mean participant ratings of valence and arousal. The Elman network did a good job of predicting the arousal trend but not the valence trend. Our study indicates that neural networks can be trained to identify statistical consistencies across audio features to predict valence/arousal values.{$<$}/p{$ > $}},
  KEYWORDS = {Audio Event Detection; Melody Extraction; Emotion Recognition; Affective Computing; Speech Emotion},
  SOURCE = {open_alex},
  URL = {https://doi.org/10.32920/25413184},
  BDSK = {https://doi.org/10.32920/25413184},
  DATE = {2024-05-13 15:05:26 +0100},
  DATE.1 = {2024-05-13 15:05:26 +0100},
  LA = {en},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include, classification of quadrant},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{wang2021ac,
  AUTHOR = {Wang, Xin and Wei, Yujia and Heng, Lena and McAdams, Stephen},
  JOURNAL = {FRONTIERS IN PSYCHOLOGY},
  MONTH = {SEP 29},
  TITLE = {A Cross-Cultural Analysis of the Influence of Timbre on Affect Perception in Western Classical Music and Chinese Music Traditions},
  VOLUME = {12},
  YEAR = {2021},
  DOI = {10.3389/fpsyg.2021.732865},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.3389/fpsyg.2021.732865},
  ISSN = {1664-1078},
  ARTICLE = {732865},
  ORCID = {McAdams, Stephen/0000-0002-6744-9035 Heng, Lena/0000-0002-4395-2576},
  RESEARCHERID = {McAdams, Stephen/GQB-0225-2022 Wei, Yujia/IAQ-8917-2023},
  PARADIGM = {regression},
  NOTES_CA = {include, acoustic model present in paper},
  NOTES_TE = {include},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{wang2022co,
  AUTHOR = {Wang, Xin and Wang, Li and Xie, Lingyun},
  JOURNAL = {Applied Sciences (Switzerland)},
  NOTE = {Cited by: 7; All Open Access, Gold Open Access},
  NUMBER = {12},
  TITLE = {Comparison and Analysis of Acoustic Features of Western and Chinese Classical Music Emotion Recognition Based on V‐A Model},
  TYPE = {Article},
  VOLUME = {12},
  YEAR = {2022},
  DOI = {10.3390/app12125787},
  ABSTRACT = {Music emotion recognition is increasingly becoming important in scientific research and practical applications. Due to the differences in musical characteristics between Western and Chinese classical music, it is necessary to investigate the distinctions in music emotional feature sets to improve the accuracy of cross‐cultural emotion recognition models. Therefore, a comparative study on emotion recognition in Chinese and Western classical music was conducted. Using the V‐A model as an emotional perception model, approximately 1000 pieces of Western and Chinese classical excerpts in total were selected, and approximately 20‐dimension feature sets for different emotional dimensions of different datasets were finally extracted. We considered different kinds of al-gorithms at each step of the training process, from pre‐processing to feature selection and regression model selection. The results reveal that the combination of MaxAbsScaler pre‐processing and the wrapper method using the recursive feature elimination algorithm based on extremely randomized trees is the optimal algorithm. The harmonic change detection function is a culturally universal fea-ture, whereas spectral flux is a culturally specific feature for Chinese classical music. It is also found that pitch features are more significant for Western classical music, whereas loudness and rhythm features are more significant for Chinese classical music. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85132193259&doi = 10.3390%2fapp12125787&partnerID = 40&md5 = 96f19b725a52bb52a769612e02f0a74c},
  AUTHOR_KEYWORDS = {classical music; extreme random tree; feature selection; music emotion recognition; V‐A model},
  PARADIGM = {regression},
  NOTES_CA = {include, though reports negative r^2 values in model comparison in table 2},
  NOTES_TE = {include},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{wang2022cr,
  AUTHOR = {Wang, Xin and Wei, Yujia and Yang, Dasheng},
  JOURNAL = {COGNITIVE COMPUTATION AND SYSTEMS},
  MONTH = {JUN},
  NUMBER = {2, SI},
  PAGES = {116-129},
  TITLE = {Cross-cultural analysis of the correlation between musical elements and emotion},
  VOLUME = {4},
  YEAR = {2022},
  DOI = {10.1049/ccs2.12032},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1049/ccs2.12032},
  EISSN = {2517-7567},
  RESEARCHERID = {Wei, Yujia/IAQ-8917-2023},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include, regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{xia2022st,
  AUTHOR = {Xia, Yu and Xu, Fumei},
  JOURNAL = {Mathematical Problems in Engineering},
  NOTE = {Cited by: 6; All Open Access, Gold Open Access},
  TITLE = {Study on Music Emotion Recognition Based on the Machine Learning Model Clustering Algorithm},
  TYPE = {Article},
  VOLUME = {2022},
  YEAR = {2022},
  DOI = {10.1155/2022/9256586},
  ABSTRACT = {In recent years, the explosive growth of online music resources makes it difficult to retrieve and manage music information. To efficiently retrieve and classify music information has become a hot research topic. Thayer's two-dimensional emotion plane is selected as the basis for establishing the music emotion database. Music is divided into five categories, the concept of continuous emotion perception is introduced, and music emotion is regarded as a point on a two-dimensional emotional plane, together with the two sentiment variables to determine its location. The artificial labeling method is used to determine the position range of the five types of emotions on the emotional plane, and the regression method is used to obtain the relationship between the VA value and the music features so that the music emotion classification problem is transformed into a regression problem. A regression-based music emotion classification system is designed and implemented, which mainly includes a training part and a testing part. In the training part, three algorithms, namely, polynomial regression, support vector regression, and k-plane piecewise regression, are used to obtain the regression model. In the test part, the input music data is regressed and predicted to obtain its VA value and then classified, and the system performance is considered by classification accuracy. Results show that the combined method of support vector regression and k-plane piecewise regression improves the accuracy by 3 to 4 percentage points compared to using one algorithm alone; compared with the traditional classification method based on a support vector machine, the accuracy improves by 6 percentage points. Music emotion is classified by algorithms such as support vector machine classification, K-neighborhood classification, fuzzy neural network classification, fuzzy K-neighborhood classification, Bayesian classification, and Fisher linear discrimination, among which the support vector machine, fuzzy K-neighborhood, and the accuracy rate of music emotion classification realized by Fisher linear discriminant algorithm are more than 80%; a new algorithm "mixed classifier"is proposed, and the music emotion recognition rate based on this algorithm reaches 84.9%. © 2022 Yu Xia and Fumei Xu.},
  KEYWORDS = {Behavioral research; Classification (of information); Clustering algorithms; Fuzzy neural networks; Information management; Regression analysis; Speech recognition; Support vector machines; Vectors; Classifieds; Emotion recognition; K neighborhoods; Music emotion classifications; Music emotions; Music information; Percentage points; Piecewise regression; Support vector regressions; Two-dimensional; Emotion Recognition},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85140847927&doi = 10.1155%2f2022%2f9256586&partnerID = 40&md5 = 95604b41d31354a5954af89be5f47193},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include, although some quality issues},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{xie2020mu,
  AUTHOR = {Xie, Baijun and Kim, Jonathan C. and Park, Chung Hyuk},
  JOURNAL = {APPLIED SCIENCES-BASEL},
  MONTH = {FEB},
  NUMBER = {3},
  TITLE = {Musical Emotion Recognition with Spectral Feature Extraction Based on a Sinusoidal Model with Model-Based and Deep-Learning Approaches},
  VOLUME = {10},
  YEAR = {2020},
  DOI = {10.3390/app10030902},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.3390/app10030902},
  ARTICLE = {902},
  EISSN = {2076-3417},
  ORCID = {Park, Chung Hyuk/0000-0003-0742-6541 Xie, Baijun/0000-0001-5080-198X},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{xu2021us,
  AUTHOR = {Xu, Liang and Sun, Zaoyi and Wen, Xin and Huang, Zhengxi and Chao, Chi-ju and Xu, Liuchang},
  JOURNAL = {PEERJ COMPUTER SCIENCE},
  MONTH = {NOV 15},
  TITLE = {Using machine learning analysis to interpret the relationship between music emotion and lyric features},
  VOLUME = {7},
  YEAR = {2021},
  DOI = {10.7717/peerj-cs.785},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.7717/peerj-cs.785},
  ARTICLE = {e785},
  EISSN = {2376-5992},
  ORCID = {Xu, Liang/0000-0003-3889-927X Sun, Zaoyi/0000-0002-4551-3606},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include, audio prediction and R2 included},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{zhang2016br,
  AUTHOR = {Zhang, JiangLong and Huang, XiangLin and Yang, Lifang and Nie, Liqiang},
  JOURNAL = {NEUROCOMPUTING},
  MONTH = {OCT 5},
  NUMBER = {SI},
  PAGES = {333-341},
  TITLE = {Bridge the semantic gap between pop music acoustic feature and emotion: Build an interpretable model},
  VOLUME = {208},
  YEAR = {2016},
  DOI = {10.1016/j.neucom.2016.01.099},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1016/j.neucom.2016.01.099},
  ISSN = {0925-2312},
  EISSN = {1872-8286},
  ORCID = {Zhang, Jianglong/0000-0002-9079-2499},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include, classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{zhang2019us,
  AUTHOR = {Zhang, Le-kai and Sun, Shou-qian and Xing, Bai-xi and Luo, Rui-ming and Zhang, Ke-jun},
  JOURNAL = {FRONTIERS OF INFORMATION TECHNOLOGY \& ELECTRONIC ENGINEERING},
  MONTH = {JUL},
  NUMBER = {7},
  PAGES = {964-974},
  TITLE = {Using psychophysiological measures to recognize personal music emotional experience},
  VOLUME = {20},
  YEAR = {2019},
  DOI = {10.1631/FITEE.1800101},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1631/FITEE.1800101},
  ISSN = {2095-9184},
  EISSN = {2095-9230},
  RESEARCHERID = {zhang, ke/AAH-8217-2019},
  PARADIGM = {regression},
  NOTES_CA = {include, single-modality reported},
  NOTES_TE = {include, use a subset of the results},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{zhang2023mo,
  AUTHOR = {Zhang, Meixian and Zhu, Yonghua and Zhang, Wenjun and Zhu, Yunwen and Feng, Tianyu},
  JOURNAL = {MULTIMEDIA TOOLS AND APPLICATIONS},
  MONTH = {FEB},
  NUMBER = {5},
  PAGES = {7319-7341},
  TITLE = {Modularized composite attention network for continuous music emotion recognition},
  VOLUME = {82},
  YEAR = {2023},
  DOI = {10.1007/s11042-022-13577-6},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1007/s11042-022-13577-6},
  ISSN = {1380-7501},
  EISSN = {1573-7721},
  ORCID = {Zhang, Meixian/0000-0002-6696-2814},
  RESEARCHERID = {Zhu, Yonghua/HSI-1360-2023 Feng, Tian-Yu/AAX-3892-2020},
  EARLYACCESSDATE = {AUG 2022},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include, R2},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}


@{zhang2024ap,
  AUTHOR = {Zhang, Yao and Cai, Delin and Zhang, Dongmei},
  JOURNAL = {Environment and Social Psychology},
  NOTE = {Cited by: 0; All Open Access, Gold Open Access},
  NUMBER = {4},
  TITLE = {Application and algorithm optimization of music emotion recognition in piano performance evaluation},
  TYPE = {Article},
  VOLUME = {9},
  YEAR = {2024},
  DOI = {10.54517/esp.v9i4.2344},
  ABSTRACT = {In the current research, we integrate distinct learning modalities—Curriculum Learning (CL) and Reinforcement Learning (RL)—in an attempt to develop and optimize Music Emotion Recognition (MER) in piano performance. Classical approaches have never been successful when applied in the field of determining the degree of emotion in the music of the piano, owing to the substantial complexity required. Addressing this particular issue is the primary motivation for the present endeavour. In an approach that’s comparable to how human beings acquire information, it trains the RL agent CL in phases; such an approach improves the student’s learning model in understanding the diverse emotions expressed by musical compositions. A higher rating of performance can be achieved after learning the model to recognize emotions more effectively and precisely. A set of piano melodies with emotional content notes has been included in the EMOPIA repository for use when conducting the process of evaluation. In order to benchmark the proposed approach with different models, parameters including R2, Mean Absolute Error (MAE), and Root Mean Square Error (RMSE) were deployed. Studies indicate that the recommended approach accurately recognizes the emotions expressed by piano-playing music. In challenging tasks like MER, the significance of implementing the CL paradigm with the RL has been emphasized using the outcomes mentioned earlier. © 2024 by author(s).},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85184876855&doi = 10.54517%2fesp.v9i4.2344&partnerID = 40&md5 = f2f5f142bc51490d199e335a63b73647},
  AUTHOR_KEYWORDS = {Curriculum Learning; Machine Learning; MBE; Music Emotion Recognition; piano music; Reinforcement Learning; RMSE},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include, classification of quadrants},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { }
}
