<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tuomas Eerola">
<meta name="author" content="Cameron J. Anderson">
<meta name="keywords" content="music, emotion, recognition, meta-analysis">

<title>A Meta-Analysis of Music Emotion Recognition Studies – metaMER</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../manuscript/datasets.html" rel="next">
<link href="../analysis/analysis.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-b6254045a21d07dd46cfe160e8c2e3db.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">metaMER</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../index.html" aria-current="page"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/tuomaseerola/metaMER/"> <i class="bi bi-github" role="img" aria-label="GitHub">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../manuscript/manuscript.html">Manuscript</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../preregistration/preregistration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Plan</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Analysis</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../studies/search_syntax.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Search Syntax</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../studies/extraction_details.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Extraction Details</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../studies/pass3_comparison.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Pass 3 Comparison</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../studies/library_parser.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Library Parser</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../analysis/analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Analysis</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../manuscript/manuscript.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Manuscript</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../manuscript/datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Common datasets</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a>
  <ul class="collapse">
  <li><a href="#a-brief-history-of-mer" id="toc-a-brief-history-of-mer" class="nav-link" data-scroll-target="#a-brief-history-of-mer">A brief history of MER</a></li>
  <li><a href="#classification-and-regression-approaches" id="toc-classification-and-regression-approaches" class="nav-link" data-scroll-target="#classification-and-regression-approaches">Classification and regression approaches</a></li>
  <li><a href="#dataset-size-and-scope" id="toc-dataset-size-and-scope" class="nav-link" data-scroll-target="#dataset-size-and-scope">Dataset size and scope</a></li>
  <li><a href="#the-current-benchmarks-of-mer" id="toc-the-current-benchmarks-of-mer" class="nav-link" data-scroll-target="#the-current-benchmarks-of-mer">The current benchmarks of MER</a></li>
  <li><a href="#aims" id="toc-aims" class="nav-link" data-scroll-target="#aims">Aims</a></li>
  </ul></li>
  <li><a href="#methods" id="toc-methods" class="nav-link" data-scroll-target="#methods">Methods</a>
  <ul class="collapse">
  <li><a href="#study-identification" id="toc-study-identification" class="nav-link" data-scroll-target="#study-identification">Study identification</a></li>
  <li><a href="#quality-control" id="toc-quality-control" class="nav-link" data-scroll-target="#quality-control">Quality control</a></li>
  <li><a href="#study-encoding" id="toc-study-encoding" class="nav-link" data-scroll-target="#study-encoding">Study encoding</a></li>
  </ul></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a>
  <ul class="collapse">
  <li><a href="#results-for-regression-studies" id="toc-results-for-regression-studies" class="nav-link" data-scroll-target="#results-for-regression-studies">Results for regression studies</a>
  <ul class="collapse">
  <li><a href="#results-for-valence" id="toc-results-for-valence" class="nav-link" data-scroll-target="#results-for-valence">Results for valence</a></li>
  <li><a href="#results-for-arousal" id="toc-results-for-arousal" class="nav-link" data-scroll-target="#results-for-arousal">Results for arousal</a></li>
  </ul></li>
  <li><a href="#results-for-classification-studies" id="toc-results-for-classification-studies" class="nav-link" data-scroll-target="#results-for-classification-studies">Results for classification studies</a>
  <ul class="collapse">
  <li><a href="#quantifying-study-heterogeneity-2" id="toc-quantifying-study-heterogeneity-2" class="nav-link" data-scroll-target="#quantifying-study-heterogeneity-2">Quantifying study heterogeneity</a></li>
  <li><a href="#reporting-splits-2" id="toc-reporting-splits-2" class="nav-link" data-scroll-target="#reporting-splits-2">Reporting splits</a></li>
  <li><a href="#model-success-across-concepts-model-types-and-feature-counts" id="toc-model-success-across-concepts-model-types-and-feature-counts" class="nav-link" data-scroll-target="#model-success-across-concepts-model-types-and-feature-counts">Model success across concepts, model types and feature counts</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#discussion-and-conclusions" id="toc-discussion-and-conclusions" class="nav-link" data-scroll-target="#discussion-and-conclusions">Discussion and conclusions</a>
  <ul class="collapse">
  <li><a href="#improvements-to-predictive-accuracy" id="toc-improvements-to-predictive-accuracy" class="nav-link" data-scroll-target="#improvements-to-predictive-accuracy">Improvements to predictive accuracy</a></li>
  <li><a href="#recommendations-for-future-mer-studies" id="toc-recommendations-for-future-mer-studies" class="nav-link" data-scroll-target="#recommendations-for-future-mer-studies">Recommendations for future MER studies</a>
  <ul class="collapse">
  <li><a href="#funding-statement" id="toc-funding-statement" class="nav-link" data-scroll-target="#funding-statement">Funding statement</a></li>
  <li><a href="#competing-interests-statement" id="toc-competing-interests-statement" class="nav-link" data-scroll-target="#competing-interests-statement">Competing interests statement</a></li>
  <li><a href="#open-practices-statement" id="toc-open-practices-statement" class="nav-link" data-scroll-target="#open-practices-statement">Open practices statement</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/tuomaseerola/metaMER/blob/main/manuscript/manuscript.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/tuomaseerola/metaMER/edit/main/manuscript/manuscript.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/tuomaseerola/metaMER/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">A Meta-Analysis of Music Emotion Recognition Studies</h1>
</div>


<div class="quarto-title-meta-author column-body">
  <div class="quarto-title-meta-heading">Authors</div>
  <div class="quarto-title-meta-heading">Affiliations</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Tuomas Eerola <a href="mailto:tuomas.eerola@durham.ac.uk" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            1
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Cameron J. Anderson <a href="mailto:andersoc@mcmaster.ca" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            2
          </p>
      </div>
  </div>

<div class="quarto-title-meta column-body">

      
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="block-title">Abstract</div>
    <p>This meta-analysis examines music emotion recognition (MER) models published between 2014 and 2024, focusing on predictions of valence, arousal, and categorical emotions. A total of 553 studies were identified, of which 96 full-text articles were assessed, resulting in a final review of 34 studies. These studies reported 204 models, including 86 for emotion classification and 204 for regression. Using the best-performing model from each study, we found that valence and arousal were predicted with reasonable accuracy (r = 0.67 and r = 0.81, respectively), while classification models achieved an accuracy of 0.87 as measured with Matthews correlation coefficient. Across modelling approaches, linear and tree-based methods generally outperformed neural networks in regression tasks, whereas neural networks and support vector machines (SVMs) showed highest performance in classification tasks. We highlight key recommendations for future MER research, emphasizing the need for greater transparency, feature validation, and standardized reporting to improve comparability across studies.</p>
  </div>
</div>

<div>
  <div class="keywords">
    <div class="block-title">Keywords</div>
    <p>music, emotion, recognition, meta-analysis</p>
  </div>
</div>

</header>


<!-- ACM Computing Surveys: Shorter survey papers should be self-contained and not exceed 15 pages in length including references. -->
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Emotional engagement is a key reason why people engage with music in their everyday activities, and it is also why music is increasingly being used in various health applications <span class="citation" data-cites="juslin2022emotions agres2021music">(<a href="#ref-agres2021music" role="doc-biblioref">Agres et al., 2021</a>; <a href="#ref-juslin2022emotions" role="doc-biblioref">Juslin et al., 2022</a>)</span>. In recent years, significant advances have been made in music information retrieval (MIR), particularly in music emotion recognition (MER) tasks <span class="citation" data-cites="gomez2021 panda2020audio">(<a href="#ref-gomez2021" role="doc-biblioref">Gómez-Cañón et al., 2021</a>; <a href="#ref-panda2020audio" role="doc-biblioref">Panda et al., 2023</a>)</span>. Music Emotion Recognition (MER) is an interdisciplinary field that combines computer science, psychology, and musicology to identify the emotions conveyed by music. Research in this area involves developing computational models capable of recognizing emotions from musical content. The emotional attributions of music are based on various theoretical frameworks for emotion and require annotated datasets to build and train these models. Improvements to modelling techniques, datasets, and available features have created new opportunities to improve the accuracy and reliability of MER systems developed to predict emotion labels or ratings in music using audio features. Over the past 25 years, these studies have established the types of emotions that listeners perceive and recognize in music. In the last 15 years, research has increasingly focused on tracing these recognized emotions back to specific musical components, such as expressive features <span class="citation" data-cites="lindstrom2003expressivity">(<a href="#ref-lindstrom2003expressivity" role="doc-biblioref">Lindström et al., 2003</a>)</span>, structural aspects of music <span class="citation" data-cites="eerola_friberg_bresin_2013 anderson2022ex grimaud_eerola_2022">(<a href="#ref-anderson2022ex" role="doc-biblioref">Anderson &amp; Schutz, 2022</a>; <a href="#ref-eerola_friberg_bresin_2013" role="doc-biblioref">Eerola et al., 2013</a>; <a href="#ref-grimaud_eerola_2022" role="doc-biblioref">Grimaud &amp; Eerola, 2022</a>)</span>, acoustic features <span class="citation" data-cites="yang2008 panda2013multi saari_et_al_2015 eerola2011c panda2020audio">(<a href="#ref-eerola2011c" role="doc-biblioref">Eerola, 2011</a>; <a href="#ref-panda2020audio" role="doc-biblioref">Panda et al., 2023</a>, <a href="#ref-panda2013multi" role="doc-biblioref">2013</a>; <a href="#ref-saari_et_al_2015" role="doc-biblioref">Saari et al., 2015</a>; <a href="#ref-yang2008" role="doc-biblioref">Y. H. Yang et al., 2008</a>)</span>, or emergent properties revealed through deep learning techniques <span class="citation" data-cites="er2019music sarkar2020recognition">(<a href="#ref-er2019music" role="doc-biblioref">Er &amp; Aydilek, 2019</a>; <a href="#ref-sarkar2020recognition" role="doc-biblioref">Sarkar et al., 2020</a>)</span>.</p>
<!-- added a MER definition to 2nd sentence above -->
<p>Despite increased interest in MER studies, there is no consensus on the extent to which emotions can accurately be recognized by computational models. The current literature presents a diverse and mixed picture regarding the success of models in predicting emotions within the affective circumplex – valence and arousal– <span class="citation" data-cites="yang2011">(<a href="#ref-yang2011" role="doc-biblioref">Y.-H. Yang &amp; Chen, 2011</a>)</span> and in classifying distinct emotion categories <span class="citation" data-cites="fu2010survey">(<a href="#ref-fu2010survey" role="doc-biblioref">Fu et al., 2010</a>)</span>.</p>
<section id="a-brief-history-of-mer" class="level2">
<h2 class="anchored" data-anchor-id="a-brief-history-of-mer">A brief history of MER</h2>
<p>Music’s capacity to convey emotions has been widely discussed since the earliest artificial intelligence (AI) applications in the 1950s. Whereas early discourse largely focused on generative composition using computers <span class="citation" data-cites="zaripov1969">(<a href="#ref-zaripov1969" role="doc-biblioref">Zaripov &amp; Russell, 1969</a>)</span>, attention later shifted to creating methods to predict emotion using music’s structural cues. Novel techniques for information retrieval emerged in the 1950s and 1960s <span class="citation" data-cites="fairthorne1968">(<a href="#ref-fairthorne1968" role="doc-biblioref">Fairthorne, 1968</a>)</span>, inspiring analogous developments for automated music analysis <span class="citation" data-cites="kassler1966toward mendel1969some">(<a href="#ref-kassler1966toward" role="doc-biblioref">Kassler, 1966</a>; <a href="#ref-mendel1969some" role="doc-biblioref">Mendel, 1969</a>)</span>. These developments would set the stage for early work in music emotion recognition (MER). <span class="citation" data-cites="katayose_sentiment_1988">Katayose et al. (<a href="#ref-katayose_sentiment_1988" role="doc-biblioref">1988</a>)</span> conducted the first study of this nature, creating an algorithm that associated emotions with analyzed chords to generate descriptions like “there is [a] hopeful mood on chord[s] 69 to 97.” <span class="citation" data-cites="katayose_sentiment_1988">(<a href="#ref-katayose_sentiment_1988" role="doc-biblioref">Katayose et al., 1988, p. 1087</a>)</span>.</p>
</section>
<section id="classification-and-regression-approaches" class="level2">
<h2 class="anchored" data-anchor-id="classification-and-regression-approaches">Classification and regression approaches</h2>
<p>In the early 2000s, several research groups conducted studies using regression <span class="citation" data-cites="friberg_automatic_2002 liu_automatic_2003">(<a href="#ref-friberg_automatic_2002" role="doc-biblioref">Friberg et al., 2002</a>; <a href="#ref-liu_automatic_2003" role="doc-biblioref">Liu et al., 2003</a>)</span> and classification <span class="citation" data-cites="lu_automatic_2005 feng_popular_2003 mandel_support_2006">(<a href="#ref-feng_popular_2003" role="doc-biblioref">Feng et al., 2003</a>; <a href="#ref-lu_automatic_2005" role="doc-biblioref">Lu et al., 2005</a>; <a href="#ref-mandel_support_2006" role="doc-biblioref">M. I. Mandel et al., 2006</a>)</span> techniques to predict emotion in music audio or MIDI. Citing “MIR researchers’ growing interest in classifying music by moods” <span class="citation" data-cites="downie_music_2008">(<a href="#ref-downie_music_2008" role="doc-biblioref">Downie, 2008, p. 1</a>)</span>, the Music Information Retrieval EXchange (MIREX) introduced Audio Mood Classification (AMC) to their rotation of tasks in 2007. In the first year, nine systems classified mood labels in a common data set, reaching 52.65% in classification accuracy (SD = 11.19%). These annual events, along with growing interest in the burgeoning field of affective computing <span class="citation" data-cites="picard_affective_1997">(<a href="#ref-picard_affective_1997" role="doc-biblioref">Picard, 1997</a>)</span>, would lead to an explosion of interest in MER research.</p>
<!--Paragraph on MER as regression task-->
<p>In the tenth annual AMC task, the highest performing model reached 69.83% accuracy <span class="citation" data-cites="park2017representation">(<a href="#ref-park2017representation" role="doc-biblioref">Park et al., 2017</a>)</span>. In parallel, research groups began independently evaluating MER using regression algorithms. The first study to popularize this approach predicted valence (i.e., the negative—positive emotional quality) and arousal (i.e., the calm—exciting quality) in 195 Chinese pop songs <span class="citation" data-cites="yang2008">(<a href="#ref-yang2008" role="doc-biblioref">Y. H. Yang et al., 2008</a>)</span> using audio-extracted features represented in 114 dimensions. Applying support vector regression, the study achieved 58.3% accuracy in predicting arousal and 28.1% in predicting valence. This difference in prediction accuracy between dimensions has reappeared in several subsequent studies <span class="citation" data-cites="bai2016dimensional coutinho2013psychoacoustic">(e.g., <a href="#ref-bai2016dimensional" role="doc-biblioref">Bai et al., 2016</a>; <a href="#ref-coutinho2013psychoacoustic" role="doc-biblioref">Coutinho &amp; Dibben, 2013</a>)</span>, with some research suggesting this challenge reflects fewer well-established predictors and more individual differences for valence than arousal <span class="citation" data-cites="yang2007music eerola2011c">(<a href="#ref-eerola2011c" role="doc-biblioref">Eerola, 2011</a>; <a href="#ref-yang2007music" role="doc-biblioref">Y.-H. Yang et al., 2007</a>)</span>.</p>
<!--paragraph on models used across regession & classification tasks-->
<p>Across regression and classification paradigms, a wide range of models have been employed, ranging from multiple linear regression (MLR) <span class="citation" data-cites="saizclar2022pr griffiths2021am yang2008">(<a href="#ref-griffiths2021am" role="doc-biblioref">Griffiths et al., 2021</a>; <a href="#ref-saizclar2022pr" role="doc-biblioref">Saiz-Clar et al., 2022</a>; <a href="#ref-yang2008" role="doc-biblioref">Y. H. Yang et al., 2008</a>)</span> to deep neural networks <span class="citation" data-cites="orjesek2022en hizlisoy2021mu">(<a href="#ref-hizlisoy2021mu" role="doc-biblioref">Hizlisoy et al., 2021</a>; <a href="#ref-orjesek2022en" role="doc-biblioref">Orjesek et al., 2022</a>)</span>. In classification tasks, early studies commonly employed Gaussian mixture models <span class="citation" data-cites="lu_automatic_2005 liu_automatic_2003">(<a href="#ref-liu_automatic_2003" role="doc-biblioref">Liu et al., 2003</a>; <a href="#ref-lu_automatic_2005" role="doc-biblioref">Lu et al., 2005</a>)</span> and support vector machines <span class="citation" data-cites="mandel2007labrosa lin2009eeg tzanetakis2007marsyas">(<a href="#ref-lin2009eeg" role="doc-biblioref">Lin et al., 2009</a>; <a href="#ref-mandel2007labrosa" role="doc-biblioref">M. Mandel &amp; Ellis, 2007</a>; <a href="#ref-tzanetakis2007marsyas" role="doc-biblioref">Tzanetakis, 2007</a>)</span>, whereas convolutional, recurrent, and fully-connected neural networks are increasingly popular in recent years <span class="citation" data-cites="grekow2021music song2018audio coutinho2017sh">(<a href="#ref-coutinho2017sh" role="doc-biblioref">Coutinho &amp; Schuller, 2017</a>; <a href="#ref-grekow2021music" role="doc-biblioref">Grekow, 2021</a>; <a href="#ref-song2018audio" role="doc-biblioref">Song et al., 2018</a>)</span>. In regression tasks, a wide range of algorithms have been tested, including partial least squares (PLS) <span class="citation" data-cites="gingras2014be wang2021ac">(<a href="#ref-gingras2014be" role="doc-biblioref">Gingras et al., 2014</a>; <a href="#ref-wang2021ac" role="doc-biblioref">Wang et al., 2021</a>)</span>, support vector machines (SVMs) <span class="citation" data-cites="grekow2018au hu2017cr agarwal2021an">(<a href="#ref-agarwal2021an" role="doc-biblioref">Agarwal &amp; Om, 2021</a>; <a href="#ref-grekow2018au" role="doc-biblioref">Grekow, 2018</a>; <a href="#ref-hu2017cr" role="doc-biblioref">X. Hu &amp; Yang, 2017</a>)</span>, random forests (RFs) <span class="citation" data-cites="beveridge2018po xu2021us">(<a href="#ref-beveridge2018po" role="doc-biblioref">Beveridge &amp; Knox, 2018</a>; <a href="#ref-xu2021us" role="doc-biblioref">Xu et al., 2021</a>)</span>, and convolutional neural networks <span class="citation" data-cites="orjesek2022en">(<a href="#ref-orjesek2022en" role="doc-biblioref">Orjesek et al., 2022</a>)</span>.</p>
</section>
<section id="dataset-size-and-scope" class="level2">
<h2 class="anchored" data-anchor-id="dataset-size-and-scope">Dataset size and scope</h2>
<p>MER studies apply regression and classification techniques to predict emotion in diverse datasets, using features derived from both music (e.g., audio, MIDI, metadata) and participants (e.g., demographic information, survey responses, physiological signals, etc.). To facilitate model comparison, several databases have been shared publicly, including <em>MediaEval</em> <span class="citation" data-cites="soleymani2013">(<a href="#ref-soleymani2013" role="doc-biblioref">Soleymani et al., 2013</a>)</span>, <em>DEAM</em> <span class="citation" data-cites="aljanaki2017developing">(<a href="#ref-aljanaki2017developing" role="doc-biblioref">Aljanaki et al., 2017</a>)</span>, and <em>AMG1608</em> <span class="citation" data-cites="chen2015amg1608">(<a href="#ref-chen2015amg1608" role="doc-biblioref">Chen et al., 2015</a>)</span>. These datasets predominately use Western pop music, are moderate in size (containing from 744 to 1802 music excerpts) and have been manually annotated by a variable numbers of participants (either by experts, students, or crowdsourced workers). Several publicly available datasets include features analyzed using audio software suites such as <em>OpenSMILE</em> <span class="citation" data-cites="eyben2010opensmile">(<a href="#ref-eyben2010opensmile" role="doc-biblioref">Eyben et al., 2010</a>)</span> and <em>MIRToolbox</em> <span class="citation" data-cites="lartillot2007matlab">(<a href="#ref-lartillot2007matlab" role="doc-biblioref">Lartillot &amp; Toiviainen, 2007</a>)</span> – enabling predictions from tens, or even hundreds, of audio features.</p>
<p>An important factor often affecting the size of datasets employed in MER concerns whether they use a <em>predictive</em> or <em>explanatory</em> modelling framework. Large datasets are necessary in predictive studies, where the predominant goal is to generalize predictions across diverse samples, especially for deep learning and complex machine-learning models that require extensive pre-training. Conversely, small, carefully-curated, datasets are useful when attempting to <em>explain</em> how musical factors such as amplitude normalization <span class="citation" data-cites="gingras2014be">(<a href="#ref-gingras2014be" role="doc-biblioref">Gingras et al., 2014</a>)</span> or different performers’ interpretations <span class="citation" data-cites="battcock2021in">(<a href="#ref-battcock2021in" role="doc-biblioref">Battcock &amp; Schutz, 2021</a>)</span>, affect variance in emotion ratings. In these studies, statistical models serve a different goal. Instead of predicting emotion labels for new music, psychological studies on music emotion test causal theories about the relationship between musical predictors and emotion labels. Whether models serve predictive or explanatory goals is important—affecting both decisions about data curation and modelling, and the models’ resultant predictive and explanatory power <span class="citation" data-cites="shmueli2010explain">(<a href="#ref-shmueli2010explain" role="doc-biblioref">Shmueli, 2010</a>)</span>.</p>
<p>In predictive MER tasks, dataset sizes tend to be modest in comparison to other fields, as direct annotation of music examples is resource-intensive. For example, in the visual domain datasets are often significantly larger (e.g., <em>EmoSet</em> with 118,102 images and <em>AffectNet</em> with 450,000, see <span class="citation" data-cites="yang2023emoset">Jingyuan Yang et al. (<a href="#ref-yang2023emoset" role="doc-biblioref">2023</a>)</span>;<span class="citation" data-cites="mollahosseini2017affectnet">Mollahosseini et al. (<a href="#ref-mollahosseini2017affectnet" role="doc-biblioref">2017</a>)</span>). Some efforts have been made to scale up MER datasets by inferring emotions from tags (<em>MTG-Jamendo</em> with 18,486 excerpts <span class="citation" data-cites="bogdanov2019mtg">Bogdanov et al. (<a href="#ref-bogdanov2019mtg" role="doc-biblioref">2019</a>)</span> and <em>Music4all</em> with 109,269 excerpts <span class="citation" data-cites="santana2020">Santana et al. (<a href="#ref-santana2020" role="doc-biblioref">2020</a>)</span>), but these have not found their way into standard emotion prediction or classification tasks yet. However, small datasets can also be useful in predictive contexts when greater control over stimuli or features is necessary. These have been useful in applications testing new feature representations <span class="citation" data-cites="saizclar2022pr">(<a href="#ref-saizclar2022pr" role="doc-biblioref">Saiz-Clar et al., 2022</a>)</span> or identifying relevant features for multi-genre predictions <span class="citation" data-cites="griffiths2021am">(<a href="#ref-griffiths2021am" role="doc-biblioref">Griffiths et al., 2021</a>)</span>, or as reference standards for comparison with novel feature sets <span class="citation" data-cites="chowdhury2021perceived">(<a href="#ref-chowdhury2021perceived" role="doc-biblioref">Chowdhury &amp; Widmer, 2021</a>)</span>. Findings from explanatory studies often inform theory-driven applications in predictive tasks, helping improve upon current benchmarks.</p>
</section>
<section id="the-current-benchmarks-of-mer" class="level2">
<h2 class="anchored" data-anchor-id="the-current-benchmarks-of-mer">The current benchmarks of MER</h2>
<!--
- Wide range of MER models attempting to outperform previous benchmarks, comparing performance against baseline models [MENTIONED NOW]
- Introduce ceilings for classification, valence, and arousal, citing Barthet study [MENTIONED]
- Citing what some researchers attribute limitations to (e.g., lower valence than arousal) [MENTIONED]
- MIR discussions of "semantic" challenge [LEFT OUT NOW, GLASS-CEILING]
- inherent error in all stages (annotation, features, models) [MENTIONED]
- Due to wide range of approaches, difficult to evaluate how successful attempts to raise glass ceiling have been
- How do modelling techniques, and the number of features used affect performance? Which combinations perform best?

-->
<p>Predictive accuracy in MER tasks has improved as datasets and models have become more sophisticated; Regression models for arousal/valence have been reported to peak at 58%/28% accuracy in 2008 <span class="citation" data-cites="yang2008">(<a href="#ref-yang2008" role="doc-biblioref">Y. H. Yang et al., 2008</a>)</span>, 70%/26% in 2010 <span class="citation" data-cites="huq2010automated">(<a href="#ref-huq2010automated" role="doc-biblioref">Huq et al., 2010</a>)</span>, and 67%/46% in 2021 <span class="citation" data-cites="yang2021an">(<a href="#ref-yang2021an" role="doc-biblioref">Jing Yang, 2021</a>)</span>. In the same period, classification rates have increased from 53% <span class="citation" data-cites="downie_music_2008">(<a href="#ref-downie_music_2008" role="doc-biblioref">Downie, 2008</a>)</span> to 70% <span class="citation" data-cites="park2017representation">(<a href="#ref-park2017representation" role="doc-biblioref">Park et al., 2017</a>)</span> to 83% <span class="citation" data-cites="sarkar2020recognition">(<a href="#ref-sarkar2020recognition" role="doc-biblioref">Sarkar et al., 2020</a>)</span>. Comparing these past efforts, however, is challenging due to inconsistencies between studies in metrics, modelling architectures, datasets, and evaluation criteria. Although we assume that overall accuracy has improved significantly over the past decade, valence remains more challenging to predict than arousal.</p>
<p>Recent studies have sought to enhance emotion prediction by identifying more relevant feature sets <span class="citation" data-cites="panda2020audio chowdhury2021perceived">(<a href="#ref-chowdhury2021perceived" role="doc-biblioref">Chowdhury &amp; Widmer, 2021</a>; <a href="#ref-panda2020audio" role="doc-biblioref">Panda et al., 2023</a>)</span>, integrating low-, mid-, and high-level features through multimodal data <span class="citation" data-cites="celma_foafing_2006">(<a href="#ref-celma_foafing_2006" role="doc-biblioref">Celma, 2006</a>)</span>, and leveraging neural networks to learn features directly from audio <span class="citation" data-cites="zhang2016br alvarez2023ri agarwal2021an">(<a href="#ref-agarwal2021an" role="doc-biblioref">Agarwal &amp; Om, 2021</a>; <a href="#ref-alvarez2023ri" role="doc-biblioref">Álvarez et al., 2023</a>; <a href="#ref-zhang2016br" role="doc-biblioref">J. Zhang et al., 2016</a>)</span>. These approaches aim to overcome ceiling effects in predictive accuracy <span class="citation" data-cites="downie_music_2008">(<a href="#ref-downie_music_2008" role="doc-biblioref">Downie, 2008</a>)</span>, which some scholars refer to as a <em>semantic gap</em> <span class="citation" data-cites="wiggins_semantic_2009 celma_foafing_2006">(<a href="#ref-celma_foafing_2006" role="doc-biblioref">Celma, 2006</a>; <a href="#ref-wiggins_semantic_2009" role="doc-biblioref">Wiggins, 2009</a>)</span>. However, this prediction ceiling may be better understood as an inherent <em>measurement error</em> arising from annotations, feature representations, and model limitations. Although isolating the sources of these errors remains infeasible at this stage, comparing success rates across modelling techniques, feature set sizes, and other meaningful factors offers a step toward addressing this challenge. To date, however, no study has systematically compared the results of the diverse approaches employed in MER research.</p>
</section>
<section id="aims" class="level2">
<h2 class="anchored" data-anchor-id="aims">Aims</h2>
<!--This paragraph now is somewhat inconsistent with the analyses we actually perform motivating the journal/feature N/model discussion-->
<p>Our aim is to evaluate the predictive accuracy of two types emotional expression in music: (a) models that predict track-specific coordinates in affective circumplex space (valence and arousal), and (b) models that classify discrete emotion categories. We focus on recent studies to identify the overall success rate in MER tasks and the key factors such as modelling techniques, the number of features, or the inferential goal (explanation vs prediction) that might contribute to the prediction accuracy of the models. To achieve this, we conduct a meta-analysis of journal articles published in the past 10 years, focusing on subgroup analyses capturing these differences (model type, feature N, and predictive vs explanatory modelling). Based on existing literature, we hypothesize that arousal will be predicted with higher accuracy than valence, as valence tends to be more context-dependent and challenging to model <span class="citation" data-cites="yang2018review">(<a href="#ref-yang2018review" role="doc-biblioref">X. Yang et al., 2018</a>)</span>.<br>
<!-- yang et al. attribute this to fewer salient features, and individual differences, which I think is consistent with these descriptions --><br>
In terms of the modelling techniques and the number of features, a reasonable hypothesis is that advanced techniques (e.g., neural networks) and larger amount of features will lead to higher prediction rates than conventional techniques (e.g., logistic regression or linear regression) and smaller feature sets. However, this relationship might not be as straightforward as this due to the demands complex models place on dataset size <span class="citation" data-cites="alwosheel2018 sun2017revisiting">(<a href="#ref-alwosheel2018" role="doc-biblioref">Alwosheel et al., 2018</a>; <a href="#ref-sun2017revisiting" role="doc-biblioref">Sun et al., 2017</a>)</span>.</p>
<!-- incorporate splits into above? -->
</section>
</section>
<section id="methods" class="level1">
<h1>Methods</h1>
<p>We preregistered the meta-analysis plan on 21 June 2024 at OSF, <a href="https://osf.io/c5wgd" class="uri">https://osf.io/c5wgd</a>, and the plan is also available at <a href="https://tuomaseerola.github.io/metaMER/preregistration/preregistration.html" class="uri">https://tuomaseerola.github.io/metaMER/preregistration/preregistration.html</a>).</p>
<section id="study-identification" class="level2">
<h2 class="anchored" data-anchor-id="study-identification">Study identification</h2>
<p>In the search stage, we used three databases, <em>Web of Science</em>, <em>Scopus</em>, and <em>Open Alex</em> to identify journal articles published between 2014 and 2024 containing keywords/title <code>valence OR arousal OR classi* OR categor* OR algorithm AND music AND emotion AND recognition</code> (see specific search strings for each database in <a href="https://tuomaseerola.github.io/metaMER/studies/search_syntax.html">Study Data and Code Repository - Search Syntax</a>). All searches were done in May 2024.</p>
<p>The initial search yielded 553 potential studies after excluding duplicate entries. We interactively screened them for relevance in three stages, resulting in 46 studies that passed our inclusion criteria (music emotion studies using classification or regression methods to predict emotion ratings of music using symbolic or audio features, and containing sufficient detail to convert results to <span class="math inline">\(r\)</span> or <span class="math inline">\(MCC\)</span> values see <a href="https://tuomaseerola.github.io/metaMER/studies/extraction_details.html">Study Data and Code Repository - Extraction Details</a> for a breakdown). After the screening stage, we defined a set of entities to extract characterising (i) music (genre, stimulus number [N], duration), (ii) features extracted (number, type, source, defined by <span class="citation" data-cites="panda2020audio">(<a href="#ref-panda2020audio" role="doc-biblioref">Panda et al., 2023</a>)</span>), (iii) model type (regression, neural network, SVM, etc.) and outcome measure (<span class="math inline">\(R^2\)</span>, <em>MSE</em>, <em>MCC</em>), (iv) model complexity (i.e., approximate number of features used to predict ratings), and (v) type of model cross-validation. Summary of the common datasets used in the studies in available at <a href="https://tuomaseerola.github.io/metaMER/manuscript/datasets.html">Study Data and Code Repository - Datasets</a>.</p>
<p>We converted all regression results from <span class="math inline">\(R^2\)</span> values into <span class="math inline">\(r\)</span> values for valence and arousal, and classification results into Matthews correlation coefficient <span class="citation" data-cites="chicco2020advantages">(<em>MCC</em>, <a href="#ref-chicco2020advantages" role="doc-biblioref">Chicco &amp; Jurman, 2020</a>)</span>. To increase consistency in our analyses, we excluded studies using incompatible features (e.g., spectrograms of audio files <span class="citation" data-cites="nag2022">(<a href="#ref-nag2022" role="doc-biblioref">Nag et al., 2022</a>)</span>), or dependent variables (e.g., one regression study analyzed valence and arousal together, but not separately <span class="citation" data-cites="chin2018">(<a href="#ref-chin2018" role="doc-biblioref">Chin et al., 2018</a>)</span>).</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div id="fig1">
<div>
<svg width="672" height="480" viewbox="0.00 0.00 602.00 488.00" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" style="; max-width: none; max-height: none">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 484)">
<title>sub</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-484 598,-484 598,4 -4,4"></polygon>
<!-- A -->
<g id="node1" class="node">
<title>A</title>
<polygon fill="none" stroke="black" points="288,-480.1 0,-480.1 0,-404.7 288,-404.7 288,-480.1"></polygon>
<text text-anchor="middle" x="144" y="-463.4" font-family="Helvetica,sans-Serif" font-size="14.00">Records identified from:</text>
<text text-anchor="middle" x="144" y="-446.6" font-family="Helvetica,sans-Serif" font-size="14.00"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Web of Science (n = 142)</text>
<text text-anchor="middle" x="144" y="-429.8" font-family="Helvetica,sans-Serif" font-size="14.00"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Scopus (n = 227)</text>
<text text-anchor="middle" x="144" y="-413" font-family="Helvetica,sans-Serif" font-size="14.00"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;OpenALEX (n = 278)</text>
</g>
<!-- B -->
<g id="node2" class="node">
<title>B</title>
<polygon fill="none" stroke="black" points="594,-463 306,-463 306,-421.8 594,-421.8 594,-463"></polygon>
<text text-anchor="middle" x="450" y="-446.6" font-family="Helvetica,sans-Serif" font-size="14.00">Records removed before screening: </text>
<text text-anchor="middle" x="450" y="-429.8" font-family="Helvetica,sans-Serif" font-size="14.00"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Duplicates entries/errors (n = 94)</text>
</g>
<!-- A&#45;&gt;B -->
<g id="edge1" class="edge">
<title>A-&gt;B</title>
<path fill="none" stroke="black" d="M288.04,-442.4C290.53,-442.4 293.02,-442.4 295.51,-442.4"></path>
<polygon fill="black" stroke="black" points="295.75,-445.9 305.75,-442.4 295.75,-438.9 295.75,-445.9"></polygon>
</g>
<!-- C -->
<g id="node3" class="node">
<title>C</title>
<polygon fill="none" stroke="black" points="288,-368.8 0,-368.8 0,-332.8 288,-332.8 288,-368.8"></polygon>
<text text-anchor="middle" x="144" y="-346.6" font-family="Helvetica,sans-Serif" font-size="14.00">Records after duplicates removed (n = 553)</text>
</g>
<!-- A&#45;&gt;C -->
<g id="edge2" class="edge">
<title>A-&gt;C</title>
<path fill="none" stroke="black" d="M144,-404.7C144,-396.1 144,-387.08 144,-378.98"></path>
<polygon fill="black" stroke="black" points="147.5,-378.97 144,-368.97 140.5,-378.97 147.5,-378.97"></polygon>
</g>
<!-- D -->
<g id="node4" class="node">
<title>D</title>
<polygon fill="none" stroke="black" points="288,-277.2 0,-277.2 0,-241.2 288,-241.2 288,-277.2"></polygon>
<text text-anchor="middle" x="144" y="-255" font-family="Helvetica,sans-Serif" font-size="14.00">Records screened (n = 553)</text>
</g>
<!-- C&#45;&gt;D -->
<g id="edge3" class="edge">
<title>C-&gt;D</title>
<path fill="none" stroke="black" d="M144,-332.52C144,-319.76 144,-302.12 144,-287.46"></path>
<polygon fill="black" stroke="black" points="147.5,-287.42 144,-277.42 140.5,-287.42 147.5,-287.42"></polygon>
</g>
<!-- E -->
<g id="node5" class="node">
<title>E</title>
<polygon fill="none" stroke="black" points="594,-296.9 306,-296.9 306,-221.5 594,-221.5 594,-296.9"></polygon>
<text text-anchor="middle" x="450" y="-280.2" font-family="Helvetica,sans-Serif" font-size="14.00">Records removed: </text>
<text text-anchor="middle" x="450" y="-263.4" font-family="Helvetica,sans-Serif" font-size="14.00"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Irrelevant titles/themes (n = 338)</text>
<text text-anchor="middle" x="450" y="-246.6" font-family="Helvetica,sans-Serif" font-size="14.00"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Abstract screening (n = 95)</text>
<text text-anchor="middle" x="450" y="-229.8" font-family="Helvetica,sans-Serif" font-size="14.00"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Discussion (n = 24)</text>
</g>
<!-- D&#45;&gt;E -->
<g id="edge4" class="edge">
<title>D-&gt;E</title>
<path fill="none" stroke="black" d="M288.04,-259.2C290.53,-259.2 293.02,-259.2 295.51,-259.2"></path>
<polygon fill="black" stroke="black" points="295.75,-262.7 305.75,-259.2 295.75,-255.7 295.75,-262.7"></polygon>
</g>
<!-- F -->
<g id="node6" class="node">
<title>F</title>
<polygon fill="none" stroke="black" points="288,-182.8 0,-182.8 0,-146.8 288,-146.8 288,-182.8"></polygon>
<text text-anchor="middle" x="144" y="-160.6" font-family="Helvetica,sans-Serif" font-size="14.00">Full text articles assessed (n = 96)</text>
</g>
<!-- D&#45;&gt;F -->
<g id="edge5" class="edge">
<title>D-&gt;F</title>
<path fill="none" stroke="black" d="M144,-240.82C144,-227.4 144,-208.57 144,-193.14"></path>
<polygon fill="black" stroke="black" points="147.5,-193.1 144,-183.1 140.5,-193.1 147.5,-193.1"></polygon>
</g>
<!-- G -->
<g id="node7" class="node">
<title>G</title>
<polygon fill="none" stroke="black" points="288,-108 0,-108 0,-72 288,-72 288,-108"></polygon>
<text text-anchor="middle" x="144" y="-85.8" font-family="Helvetica,sans-Serif" font-size="14.00">Studies included in final review (n = 46)</text>
</g>
<!-- F&#45;&gt;G -->
<g id="edge7" class="edge">
<title>F-&gt;G</title>
<path fill="none" stroke="black" d="M144,-146.55C144,-138.12 144,-127.77 144,-118.3"></path>
<polygon fill="black" stroke="black" points="147.5,-118.13 144,-108.13 140.5,-118.13 147.5,-118.13"></polygon>
</g>
<!-- H -->
<g id="node8" class="node">
<title>H</title>
<polygon fill="none" stroke="black" points="594,-185.4 306,-185.4 306,-144.2 594,-144.2 594,-185.4"></polygon>
<text text-anchor="middle" x="450" y="-169" font-family="Helvetica,sans-Serif" font-size="14.00">Full text articles excluded</text>
<text text-anchor="middle" x="450" y="-152.2" font-family="Helvetica,sans-Serif" font-size="14.00"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(n = 50)</text>
</g>
<!-- F&#45;&gt;H -->
<g id="edge6" class="edge">
<title>F-&gt;H</title>
<path fill="none" stroke="black" d="M288.04,-164.8C290.53,-164.8 293.02,-164.8 295.51,-164.8"></path>
<polygon fill="black" stroke="black" points="295.75,-168.3 305.75,-164.8 295.75,-161.3 295.75,-168.3"></polygon>
</g>
<!-- I -->
<g id="node9" class="node">
<title>I</title>
<polygon fill="none" stroke="black" points="594,-108 306,-108 306,-72 594,-72 594,-108"></polygon>
<text text-anchor="middle" x="450" y="-85.8" font-family="Helvetica,sans-Serif" font-size="14.00">Studies excluded in final review (n = 12)</text>
</g>
<!-- G&#45;&gt;I -->
<g id="edge8" class="edge">
<title>G-&gt;I</title>
<path fill="none" stroke="black" d="M288.04,-90C290.53,-90 293.02,-90 295.51,-90"></path>
<polygon fill="black" stroke="black" points="295.75,-93.5 305.75,-90 295.75,-86.5 295.75,-93.5"></polygon>
</g>
<!-- J -->
<g id="node10" class="node">
<title>J</title>
<polygon fill="none" stroke="black" points="288,-36 0,-36 0,0 288,0 288,-36"></polygon>
<text text-anchor="middle" x="144" y="-13.8" font-family="Helvetica,sans-Serif" font-size="14.00">Final selection (n = 34)</text>
</g>
<!-- G&#45;&gt;J -->
<g id="edge9" class="edge">
<title>G-&gt;J</title>
<path fill="none" stroke="black" d="M144,-71.7C144,-63.98 144,-54.71 144,-46.11"></path>
<polygon fill="black" stroke="black" points="147.5,-46.1 144,-36.1 140.5,-46.1 147.5,-46.1"></polygon>
</g>
</g>
</svg>
</div>
<p>Flowchart of the study inclusions/eliminations.</p>
</div>
</div>
</div>
</section>
<section id="quality-control" class="level2">
<h2 class="anchored" data-anchor-id="quality-control">Quality control</h2>
<p>The search yielded studies of variable (and occasionally questionable) quality. To mitigate potentially spurious effects resulting from the inclusion of low-quality studies, we excluded studies lacking sufficient details about stimuli, analyzed features, or model architecture <a href="https://tuomaseerola.github.io/metaMER/studies/pass3_comparison.html">Study Data and Code Repository - Comparison</a>. Finally, we excluded studies published in journals of questionable relevance/quality, (e.g., <em>Mathematical Problems in Engineering</em> ceased publication following 17 retractions published between July and September 2024). Overall this step eliminated 12 studies, leaving us with 34 studies in total.</p>
</section>
<section id="study-encoding" class="level2">
<h2 class="anchored" data-anchor-id="study-encoding">Study encoding</h2>
<p>To capture key details of each study, we added extra fields to BibTeX entries for each study. Fields included information about the genre/type of stimuli employed, along with their duration and number; the number of analyzed features; and the model type – Neural Nets (NN), Support Vector Machines (SVM),<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Linear Methods (LM), Tree-based Methods (TM), KS, Add. and KNN Methods (KM) – validation procedure and output measures. Additionally, we included study results using executable <em>R</em> code containing custom functions for meta-analysis. For complete details about our encoding procedure, see <a href="https://tuomaseerola.github.io/metaMER/studies/extraction_details.html">Study Data and Code Repository - Extraction Details</a>.</p>
</section>
</section>
<section id="results" class="level1">
<h1>Results</h1>
<p>First we describe the overall pattern of data (regression vs.&nbsp;classification, modelling techniques, feature numbers, stimulus numbers, datasets, and other details). The analysis code is available at <a href="https://tuomaseerola.github.io/metaMER/analysis/analysis.html">Study Data and Code Repository - Analysis</a>.</p>
<!-- table 1 is part of `analysis/preprocessing.qmd` updated 20/2/2025) -->
<p>TABLE 1: Summary of data</p>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 27%">
<col style="width: 25%">
<col style="width: 23%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">Regression</th>
<th style="text-align: left;">Classification</th>
<th style="text-align: left;">Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Study N</td>
<td style="text-align: left;">22</td>
<td style="text-align: left;">12</td>
<td style="text-align: left;">34</td>
</tr>
<tr class="even">
<td style="text-align: left;">Model N</td>
<td style="text-align: left;">204</td>
<td style="text-align: left;">86</td>
<td style="text-align: left;">290</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Techniques</td>
<td style="text-align: left;">Neural Nets (NN): 64</td>
<td style="text-align: left;">21</td>
<td style="text-align: left;">85</td>
</tr>
<tr class="even">
<td style="text-align: left;">Techniques</td>
<td style="text-align: left;">Support Vector Machines (SVM): 62</td>
<td style="text-align: left;">26</td>
<td style="text-align: left;">88</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Techniques</td>
<td style="text-align: left;">Linear Methods (LM): 62</td>
<td style="text-align: left;">19</td>
<td style="text-align: left;">81</td>
</tr>
<tr class="even">
<td style="text-align: left;">Techniques</td>
<td style="text-align: left;">Tree-based Methods (TM): 14</td>
<td style="text-align: left;">16</td>
<td style="text-align: left;">30</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Techniques</td>
<td style="text-align: left;">KS, Add. &amp; KNN: 2</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">6</td>
</tr>
<tr class="even">
<td style="text-align: left;">Feature N</td>
<td style="text-align: left;">Min=3, Md=653, Max=14460</td>
<td style="text-align: left;">Min=6, Md=98, Max=8904</td>
<td style="text-align: left;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Stimulus N</td>
<td style="text-align: left;">Min=20, Md=324, Max=2486</td>
<td style="text-align: left;">Min=124, Md=300, Max=5192</td>
<td style="text-align: left;">NA</td>
</tr>
</tbody>
</table>
<p>Although the total number of studies meeting the criteria described in the previous section is modest (34 in total), they encompass a large array of models (290 in total) with a relatively even distribution among the three most popular techniques: Neural Nets (85 models in total), SVMs (88), and Linear Methods (81). Tree-based models are less frequently used (30 in total), and there is a small number (6) of other model techniques such as kernel smoothing or K-nearest neighbors (KNN) techniques used in the models. However, these techniques will not be visible in the breakdown of the results as these models were not among the strongest models per study (see reporting principles in the results). The number of features and stimuli within these studies varies significantly, ranging from as few as three features <span class="citation" data-cites="battcock2021in">(<a href="#ref-battcock2021in" role="doc-biblioref">Battcock &amp; Schutz, 2021</a>)</span> to a maximum of almost 14,500 features <span class="citation" data-cites="zhang2023mo">(<a href="#ref-zhang2023mo" role="doc-biblioref">M. Zhang et al., 2023</a>)</span>. The median number of features differs between regression (653) and classification (98) studies, primarily reflecting the nature of the datasets used in each approach. The number of stimuli is typically around 300-400 (with a median of 324 for regression and 300 for classification), though there is substantial variation, with the extremes from 20 stimuli in <span class="citation" data-cites="beveridge2018po">Beveridge &amp; Knox (<a href="#ref-beveridge2018po" role="doc-biblioref">2018</a>)</span> to 5192 stimuli in <span class="citation" data-cites="alvarez2023ri">Álvarez et al. (<a href="#ref-alvarez2023ri" role="doc-biblioref">2023</a>)</span>. There are also additional dimensions to consider, such as the type of cross-validation used, the music genres analyzed (whether a single genre, multiple genres, or a mix), the type of modelling (predictive or explanatory) framework, and the extraction tool used to extract features. However, these variables do not lend themselves to a simple summary, so we will revisit them during the interpretation and discussion stages.</p>
<p>We first report regression studies that predict valence and arousal.</p>
<section id="results-for-regression-studies" class="level2">
<h2 class="anchored" data-anchor-id="results-for-regression-studies">Results for regression studies</h2>
<!-- See `analysis/analysis.qmd` (updated 17/1/2025) -->
<p>Since there are many models contained within each of the studies, we will report the results in two parts; We first give an overview of the results for all models, and then we focus on the best performing models of each study. The best performing model is the model within each study with the highest correlation coefficient. This reduction is done to avoid the issue of multiple models from the same study deflating the results as majority of the models included are relative modest baseline or alternative models that do not represent the novelty or content of the article. We also provide a summary of the results with all models included in addition to the chosen strategy, where the best model of each study is considered.</p>
<section id="results-for-valence" class="level3">
<h3 class="anchored" data-anchor-id="results-for-valence">Results for valence</h3>
<p>Table 2 summarises the results for all models (All) as well as best performing models (Max) for each study for valence. The summary includes the number of models and observations, the correlation coefficient and its 95% confidence interval, the t-value and p-value for the correlation, the heterogeneity statistics <span class="math inline">\(\tau^2\)</span> and <span class="math inline">\(I^2\)</span>, calculated through appropriate transformations (Fisher’s Z) for the correlation coefficient as part of a random-effects model using <code>meta</code> library <span class="citation" data-cites="balduzzi2019">(<a href="#ref-balduzzi2019" role="doc-biblioref">Balduzzi et al., 2019</a>)</span>. We used Paule-Mandel estimator for between-study heterogeneity <span class="citation" data-cites="langan2019comparison">(<a href="#ref-langan2019comparison" role="doc-biblioref">Langan et al., 2019</a>)</span> and Knapp-Hartung <span class="citation" data-cites="knapp2003improved">(<a href="#ref-knapp2003improved" role="doc-biblioref">Knapp &amp; Hartung, 2003</a>)</span> adjustments for confidence intervals. In this table we also report two subgroup analyses. One where we have divided the studies according to the number of features they contain (three categories based on quantiles to keep the group size comparable) and into four modelling techniques introduced earlier (Table 1).</p>
<!-- DISCUSS: I have changed the modelling techniques into FOUR categories, TE 19/3/2025 -->
<p>Table 2. Meta-analytic diagnostic for all regression studies predicting valence from audio. See Table 1 for the acronyms of the modelling techniques.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Concept</th>
<th style="text-align: left;">Models, obs</th>
<th style="text-align: left;"><span class="math inline">\(r\)</span> [95%-CI]</th>
<th style="text-align: left;"><span class="math inline">\(t\)</span></th>
<th style="text-align: left;"><span class="math inline">\(p\)</span></th>
<th style="text-align: left;"><span class="math inline">\(\tau^2\)</span></th>
<th style="text-align: left;"><span class="math inline">\(I^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Valence All</td>
<td style="text-align: left;">102,60017</td>
<td style="text-align: left;">0.583 [0.541-0.623]</td>
<td style="text-align: left;">21.41</td>
<td style="text-align: left;">.0001</td>
<td style="text-align: left;">0.094</td>
<td style="text-align: left;">97.7%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Valence Max</td>
<td style="text-align: left;">22,14172</td>
<td style="text-align: left;">0.669 [0.560-0.755]</td>
<td style="text-align: left;">9.58</td>
<td style="text-align: left;">.0001</td>
<td style="text-align: left;">0.148</td>
<td style="text-align: left;">98.4%</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><em>N Features</em></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">&lt;30 F</td>
<td style="text-align: left;">6,3140</td>
<td style="text-align: left;">0.766 [0.488-0.903]</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.198</td>
<td style="text-align: left;">98.6%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">(Range: 3-22; Mdn = 16.5; <em>M</em> = 15.7)</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">30-300 F</td>
<td style="text-align: left;">8,4098</td>
<td style="text-align: left;">0.580 [0.276-0.778]</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.188</td>
<td style="text-align: left;">97.4%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">(Range: 45-260; Mdn = 101; <em>M</em> = 136)</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">300+ F</td>
<td style="text-align: left;">8,6934</td>
<td style="text-align: left;">0.666 [0.531-0.767]</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.062</td>
<td style="text-align: left;">98.0%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">(Range: 388-14460; Mdn = 654; <em>M</em> = 3360)</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><em>Techniques</em></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">LM</td>
<td style="text-align: left;">9,2457</td>
<td style="text-align: left;">0.784 [0.652-0.870]</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.1194</td>
<td style="text-align: left;">96.3%</td>
</tr>
<tr class="even">
<td style="text-align: left;">SVM</td>
<td style="text-align: left;">4,5068</td>
<td style="text-align: left;">0.539 [0.171-0.774]</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.0702</td>
<td style="text-align: left;">97.1%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">NN</td>
<td style="text-align: left;">6,3317</td>
<td style="text-align: left;">0.473 [0.167-0.696]</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.1029</td>
<td style="text-align: left;">98.2%</td>
</tr>
<tr class="even">
<td style="text-align: left;">TM</td>
<td style="text-align: left;">3,3330</td>
<td style="text-align: left;">0.750 [0.292-0.928]</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.0740</td>
<td style="text-align: left;">98.8%</td>
</tr>
</tbody>
</table>
<!-- table updated CA 20/2/2025, revised terms 28/1/2025 -->
<p>The results indicate that valence can generally be predicted with moderately accuracy, with the best model from each of the 22 studies achieving an average correlation of <em>r</em> = 0.669 (95% CI: 0.560-0.755), called “valence Max” in Table 2. However, when considering all models across these studies (n = 102), the overall prediction rate drops significantly to <em>r</em> = 0.583. We argue that this lower correlation is likely due to the inclusion of baseline models reported in these studies, which may not reflect the true success of the task for the purposes of our analysis.</p>
<section id="quantifying-study-heterogeneity" class="level4">
<h4 class="anchored" data-anchor-id="quantifying-study-heterogeneity">Quantifying study heterogeneity</h4>
<p>Further analysis of between-study heterogeneity, as indexed by the <span class="math inline">\(\tau^2\)</span> (0.148) and Higgins &amp; Thompson’s <span class="math inline">\(I^2\)</span> statistic <span class="citation" data-cites="higgins2002quantifying">(<a href="#ref-higgins2002quantifying" role="doc-biblioref">Higgins &amp; Thompson, 2002</a>)</span> at 98.4%, reveals substantial heterogeneity. Since <span class="math inline">\(I^2\)</span> is heavily influenced by study size (with larger N leading to lower sampling error), its value may be less insightful in this context. In contrast, <span class="math inline">\(\tau^2\)</span>, which is less sensitive to the number of studies and directly linked to the outcome metric (<em>r</em>), provides a more reliable measure of heterogeneity in this case. Also, we note that because the overall heterogeneity in the data is high, we are cautious in our interpretation of the publication bias <span class="citation" data-cites="van-aertwicherts_2016">(<a href="#ref-van-aertwicherts_2016" role="doc-biblioref">Van Aert et al., 2016</a>)</span>.</p>
<p>To better understand the effects across studies and the nature of the observed heterogeneity, Figure 2 presents a forest of the random-effects model, based on the best-performing models from all studies. In terms of the forest plot, the range of prediction values (correlations) is broad, spanning from 0.13 to 0.92, with all studies except Koh et al.&nbsp;(2023) demonstrating evidence of positive correlations. A mean estimate of 0.67 is achieved by 15 out of the 22 models. While the confidence intervals are generally narrow due to the large sample sizes in each study, there are exceptions, such as smaller sample sizes in <span class="citation" data-cites="beveridge2018po">Beveridge &amp; Knox (<a href="#ref-beveridge2018po" role="doc-biblioref">2018</a>)</span> (n = 20), and in <span class="citation" data-cites="griffiths2021am">Griffiths et al. (<a href="#ref-griffiths2021am" role="doc-biblioref">2021</a>)</span> (n = 40). If we explore the asymmetry of the model prediction rate across standard error, we do not observe particular asymmetries that would indicate particular bias in the reported studies. This is verified by non-significant Egger’s test (<span class="math inline">\(\beta\)</span> = 5.05, CI95% -0.99-11.09, <em>t</em> = 1.64, <em>p</em> = 0.112, <span class="citation" data-cites="eggersmith_1997">Egger et al. (<a href="#ref-eggersmith_1997" role="doc-biblioref">1997</a>)</span>).</p>
<p>Coming back to the mean of valence correlation of 0.669 by all studies and the possible impact of study heterogeneity on this estimation, we also calculated the correlation without the studies that lie outside the 95% CI for pooled effect. This left 12 studies in the data and resulted in the meta-analytical pooled correlation of 0.686 (CI95% 0.635-0.731). In other words, despite the large variation in the correlations and standard errors across the studies, this variation in itself does not seem to be a significant driver behind the overall effect.</p>
<!-- numbers in the text updated up to here, CA 20/2/2025 -->
<!-- I've removed the reference to the funnel plot, TE 19/3/2025 -->
</section>
<section id="reporting-splits" class="level4">
<h4 class="anchored" data-anchor-id="reporting-splits">Reporting splits</h4>
<p>To gain insights into the factors contributing to the wide range of model success, we explored several ways of splitting the data. Table 2 presents two key splits: one based on the number of features used, which we hypothesized might influence model performance, and another based on the modelling techniques employed. In terms of feature sets, we categorized them into three groups: few features (&lt;30), a large number of features (30–300), and massive feature sets (300+). These splits produced reasonably comparable representations of regression and classification studies, though the actual ranges differed. For transparency, we report a summary of the actual feature counts in Table 2. Models (6 in total) using a relatively small number of features (&lt;30) performed best (<em>r</em> = 0.766, 95% CI: 0.488–0.903) compared to those utilizing larger feature sets. However, it is worth noting that the models using massive feature sets (300+, 8 studies in total) also performed reasonably well (<em>r</em> = 0.666), achieving more consistent results than the overall prediction rate (<em>r</em> = 0.669). This observation is supported by the lowest heterogeneity index for the massive feature set group (<span class="math inline">\(\tau^2\)</span> = 0.062), indicating more consistent results across studies. Studies with large number of features (30-300 features, 8 studies in total) delivered the worst results, <em>r</em> = 0.580 (95% CI: 0.276–0.778). Despite the fluctuation in the overall model accuracy between the number of features, the differences are not substantially large to pass the test of statistical significance (<em>Q</em>(2) = 2.03, <em>p</em>=.363).</p>
<!-- numbers in the text updated up to here, CA 20/2/2025 -->
<p>When analyzing the studies across the four modelling techniques used, the predictions differ significantly (<em>Q</em>(3) = 12.43, <em>p</em> = .0061). Notably, linear models (LM) and neural networks (NN) were the most common, with 9 and 6 studies, respectively, allowing for more confident interpretations. Linear models achieved the highest prediction rate (<em>r</em> = 0.784, 95% CI: 0.652–0.870), though this may be influenced by the smaller datasets typically used in these studies. These studies also exhibited higher heterogeneity (<span class="math inline">\(\tau^2\)</span> = 0.119) compared to other techniques. While there were only 3 studies involving tree-based model (TM), these performed well, achieving <em>r</em> = 0.750, 95% CI: 0.292–0.928), and the relatively poor performance of the neural network (NN) models represented in six studies (<em>r</em> = 0.473, 95% CI: 0.167–0.696) is difficult to explain without a deeper examination of the specific model architectures and the stimuli used in these studies.</p>
<p>We also ran the sub-grouping analyses across stimulus genres (single vs mixed), finding no significant difference (<em>Q</em>(1) = 0.01, <em>p</em> = .9158). Both single-genre (<em>r</em> = 0.675 95% CI: 0.465, 0.813, <em>n</em> = 8) and multi-genre (<em>r</em> = 0.665, 95% CI: 0.508-0.779, <em>n</em> = 14) achieved similar results, although multi-genre studies exhibited slightly higher heterogeneity (<span class="math inline">\(\tau^2\)</span> = 0.167) than single-genre studies (<span class="math inline">\(\tau^2\)</span> = 0.137).</p>
<!-- However, these analyses did not reveal any clear patterns, likely due to the small number of studies in each subgroup.  -->
<!-- see this to ignore I\^2 and rely on prediction interval: https://onlinelibrary.wiley.com/doi/full/10.1002/jrsm.1678 -->
<p>These comparisons of sub-groupings may also be influenced by other factors, such as whether the study utilised predictive or explanatory modelling framework. To address this, we used the type of journal in which each study was published as a proxy indicator for predictive or explanatory modelling, where we classified studies published in psychology journals as explanatory and those published in engineering journals as predictive. Of the 22 regression studies, 13 were classified into predictive frameworks, yielding an average correlation of <em>r</em> = 0.656 (95% CI: 0.505–0.769). Studies with explanatory frameworks (9 in total) showed a similar overall correlation of <em>r</em> = 0.688 (95% CI: 0.468–0.827). No significant difference in model accuracy was observed between the two types of frameworks, <em>Q</em>(1) = 0.10, <em>p</em> = 0.748. More broadly, while the sub-groupings based on modelling techniques result in an uneven distribution of studies and observations, the two main sub-groupings presented in Table 2 highlight valuable differences in model performance across the studies. <!-- numbers in the text updated up to here, CA 20/2/2025 --></p>
<!-- DISCUSS: we don't report journal type of explain/predict differences, although we do some of these in arousal, TE 19/3/2025: Now added! -->
</section>
</section>
<section id="results-for-arousal" class="level3">
<h3 class="anchored" data-anchor-id="results-for-arousal">Results for arousal</h3>
<p>Moving on the arousal, we carry out the same meta-analytical analysis applying the random-effects model to arousal. Table 3 describes the broad pattern of results in tabular format, and Figure 3 illustrates the spread and heterogeneity of all studies for arousal. The overall correlation across the studies using the best performing model out of each study (Max) is 0.809 (95% CI: 0.740-0.860). If we examine all the models reported in each study, the correlation drops marginally, to 0.791 (95% CI: 0.770-0.810), despite this analysis including about four times as many models as taking the best model out of each study. For arousal, even the baseline models seem to perform at a relatively high level.</p>
<section id="quantifying-study-heterogeneity-1" class="level4">
<h4 class="anchored" data-anchor-id="quantifying-study-heterogeneity-1">Quantifying study heterogeneity</h4>
<p>For arousal, the indicators of heterogeneity are again high (<span class="math inline">\(\tau^2\)</span> = 0.141 and <span class="math inline">\(I^2\)</span>=97.9%), which suggests that summary may be misleading. However, the analysis of asymmetry does not reveal significant issues (Eggers test, <span class="math inline">\(\beta\)</span> = 0.789 95% CI: -4.87-6.45, <em>t</em> = 0.273, <em>p</em> = 0.788). If we remove the studies that are outside the 95% CI in heterogeneity, this leaves 13 studies in the summary where <em>r</em> = 0.826 (95% CI: 0.806-0.845), <span class="math inline">\(\tau^2\)</span> = 0.0042 and <span class="math inline">\(I^2\)</span> = 76.8%. In other words, we observed no material difference to the results obtained with all 22 studies.</p>
<p>Table 3. Meta-analytic diagnostic for all regression studies predicting arousal from audio.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Concept</th>
<th style="text-align: left;">Models, obs</th>
<th style="text-align: left;"><span class="math inline">\(r\)</span> [95%-CI]</th>
<th style="text-align: left;"><span class="math inline">\(t\)</span></th>
<th style="text-align: left;"><span class="math inline">\(p\)</span></th>
<th style="text-align: left;"><span class="math inline">\(\tau^2\)</span></th>
<th style="text-align: left;"><span class="math inline">\(I^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Arousal All</td>
<td style="text-align: left;">102, 60017</td>
<td style="text-align: left;">0.791 [0.770-0.810]</td>
<td style="text-align: left;">39.9</td>
<td style="text-align: left;">0.0001</td>
<td style="text-align: left;">0.069</td>
<td style="text-align: left;">96.2%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Arousal Max</td>
<td style="text-align: left;">22, 14172</td>
<td style="text-align: left;">0.809 [0.740-0.860]</td>
<td style="text-align: left;">13.6</td>
<td style="text-align: left;">0.0001</td>
<td style="text-align: left;">0.141</td>
<td style="text-align: left;">97.9%</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><em>N Features</em></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">&lt;30 F</td>
<td style="text-align: left;">6, 3140</td>
<td style="text-align: left;">0.885 [0.782-0.940]</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">0.0948</td>
<td style="text-align: left;">93.5%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">(Range: 3-22; Mdn = 16.5; <em>M</em> = 15.7)</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">30-300 F</td>
<td style="text-align: left;">8, 4098</td>
<td style="text-align: left;">0.735 [0.501-0.868]</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">0.1971</td>
<td style="text-align: left;">98.2%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">(Range: 45-260; Mdn = 101; <em>M</em> = 136)</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">300+ F</td>
<td style="text-align: left;">8, 6934</td>
<td style="text-align: left;">0.804 [0.716-0.867]</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">0.0612</td>
<td style="text-align: left;">97.4%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">(Range: 388-14460; Mdn = 654; <em>M</em> = 3360)</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><em>Techniques</em></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">LM</td>
<td style="text-align: left;">8, 1713</td>
<td style="text-align: left;">0.882 [0.809-0.928]</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">0.0846</td>
<td style="text-align: left;">93.3%</td>
</tr>
<tr class="even">
<td style="text-align: left;">SVM</td>
<td style="text-align: left;">5, 5812</td>
<td style="text-align: left;">0.796 [0.559-0.913]</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">0.1325</td>
<td style="text-align: left;">98.3%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">NN</td>
<td style="text-align: left;">6, 3317</td>
<td style="text-align: left;">0.660 [0.395-0.823]</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">0.1209</td>
<td style="text-align: left;">98.1%</td>
</tr>
<tr class="even">
<td style="text-align: left;">TM</td>
<td style="text-align: left;">3, 3330</td>
<td style="text-align: left;">0.809 [0.733-0.864]</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">0.0025</td>
<td style="text-align: left;">65.4%</td>
</tr>
</tbody>
</table>
<!-- numbers in the text updated up to here, CA 20/2/2025 -->
<!-- Schwarzer and colleagues (Schwarzer, Carpenter, and Rücker 2015, chap. 4.3) mention, as a general rule of thumb, that subgroup analyses only make sense when your meta-analysis contains at least 
K= 10 studies.-->
<p>Figure 3 presents a forest plot of the random-effects model of the best-performing models from all studies. Similarly to valence, the range of correlations is also wide for arousal, ranging from 0.35 to 0.95, with all studies demonstrating evidence of positive correlations. A mean estimate of 0.81 or higher is achieved by the majority (14 out of the 22 models). Due to large sample in most studies, the confidence intervals are narrow, although the exceptions (<span class="math inline">\(N &lt; 55\)</span>) are clearly visible <span class="citation" data-cites="beveridge2018po griffiths2021am koh2023me saizclar2022pr wang2021ac">(<a href="#ref-beveridge2018po" role="doc-biblioref">Beveridge &amp; Knox, 2018</a>; <a href="#ref-griffiths2021am" role="doc-biblioref">Griffiths et al., 2021</a>; <a href="#ref-koh2023me" role="doc-biblioref">Koh et al., 2023</a>; <a href="#ref-saizclar2022pr" role="doc-biblioref">Saiz-Clar et al., 2022</a>; <a href="#ref-wang2021ac" role="doc-biblioref">Wang et al., 2021</a>)</span>.</p>
<!-- Removed reference to the funnel plot, TE 19/3/2025 -->
</section>
<section id="reporting-splits-1" class="level4">
<h4 class="anchored" data-anchor-id="reporting-splits-1">Reporting splits</h4>
<p>The analysis of the subdivision of studies shows that there is no significant differences between the studies using different number of features (<em>Q</em>(2) = 5.20, <em>p</em> = .074) despite the differing means (<em>r</em> = 0.885 for studies with less than 30 features, <em>r</em> = 0.735 for 30 to 300 features, and <em>r</em> = 0.804 for studies utilising over 300 features). The differences in the techniques, however, show statistically significant variance between subgroups (<em>Q</em>(3) = 10.83, <em>p</em> = .0127). The Neural Nets (NN) achieve poor prediction of arousal (<em>r</em> = 0.660) in comparison to other techniques. The caveat of this subgroup analysis is the small number of observations for four techniques. We also found no difference between studies utilising predictive (<em>r</em> = 0.656, 95% CI: 0.505-0.769) or explanatory (<em>r</em> = 0.688, 95% CI: 0.468-0.828) frameworks (<em>Q</em>(1) = 0.94, <em>p</em> = .333).</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="manuscript_files/figure-html/fig3-1.png" class="img-fluid figure-img" width="1152"></p>
<figcaption>Forest plot of the best arousal models from all MER studies.</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="results-for-classification-studies" class="level2">
<h2 class="anchored" data-anchor-id="results-for-classification-studies">Results for classification studies</h2>
<p>We next evaluated classification studies. The number of emotion classes predicted varied between studies, ranging from two to eight. Across studies, most operationalizations corresponded to either affective dimensions of the circumplex model or discrete emotion labels mapping to those dimensions <span class="citation" data-cites="agarwal2021an alvarez2023ri yeh2014po">(e.g., happy, sad, nervous, and calm and similar variants used in <a href="#ref-agarwal2021an" role="doc-biblioref">Agarwal &amp; Om, 2021</a>; <a href="#ref-alvarez2023ri" role="doc-biblioref">Álvarez et al., 2023</a>; <a href="#ref-yeh2014po" role="doc-biblioref">Yeh et al., 2014</a>)</span>. Despite common adoption of the circumplex quadrants, studies differed in modelling decisions: one study predicted quadrants in a multi-class problem <span class="citation" data-cites="panda2020no">(<a href="#ref-panda2020no" role="doc-biblioref">Panda et al., 2020</a>)</span>, whereas another predicted them in a series of binary classification tasks <span class="citation" data-cites="bhuvanakumar2023em">(<a href="#ref-bhuvanakumar2023em" role="doc-biblioref">Bhuvana Kumar &amp; Kathiravan, 2023</a>)</span>. Others predicted valence and arousal in separate tasks <span class="citation" data-cites="hu2022de zhang2016br">(<a href="#ref-hu2022de" role="doc-biblioref">Xiao Hu et al., 2022</a>; <a href="#ref-zhang2016br" role="doc-biblioref">J. Zhang et al., 2016</a>)</span>, and some of the models analyzed only predicted arousal <span class="citation" data-cites="zhang2017fe zhang2016br">(e.g., all models in <a href="#ref-zhang2017fe" role="doc-biblioref">J. L. Zhang et al., 2017</a>; the CART model in <a href="#ref-zhang2016br" role="doc-biblioref">J. Zhang et al., 2016</a>)</span>. Finally, some studies excluded specific quadrants <span class="citation" data-cites="hizlisoy2021mu">(<a href="#ref-hizlisoy2021mu" role="doc-biblioref">Hizlisoy et al., 2021</a>)</span> due to a lack of relevant stimuli, whereas others divided quadrants into multiple levels of valence and arousal <span class="citation" data-cites="nguyen2017an sorussa2020em">(<a href="#ref-nguyen2017an" role="doc-biblioref">Nguyen et al., 2017</a>; <a href="#ref-sorussa2020em" role="doc-biblioref">Sorussa et al., 2020</a>)</span>. Because only a few studies separately reported valence and arousal or quadrant classification, we averaged prediction success across them for each model to increase comparability with other studies.</p>
<p>Figure 4 shows forest plot visualization from the random-effects model of the best-performing models in classification studies. <span class="math inline">\(MCC\)</span>s vary across a wide range, ranging from 0.55 to 0.98. Table 4 indicates that using the best model from each study increases performance relative to all models (<span class="math inline">\(MCC\)</span> = 0.868 95%CI: 0.748-0.934), yet slightly increases heterogeneity (<span class="math inline">\(\tau^2\)</span> = 0.318, <span class="math inline">\(I^2\)</span> = 99.8%).</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="manuscript_files/figure-html/fig4-1.png" class="img-fluid figure-img" width="1152"></p>
<figcaption>Forest plot of the best classification models from all MER studies.</figcaption>
</figure>
</div>
</div>
</div>
<p>Table 4. Meta-analytic diagnostic for all classification studies predicting emotion categories from audio.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Concept</th>
<th style="text-align: left;">Models, obs</th>
<th style="text-align: left;"><span class="math inline">\(r\)</span> [95%-CI]</th>
<th style="text-align: left;"><span class="math inline">\(t\)</span></th>
<th style="text-align: left;"><span class="math inline">\(p\)</span></th>
<th style="text-align: left;"><span class="math inline">\(\tau^2\)</span></th>
<th style="text-align: left;"><span class="math inline">\(I^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">All Models</td>
<td style="text-align: left;">86, 80544</td>
<td style="text-align: left;">0.8245 [0.7904-0.8535]</td>
<td style="text-align: left;">23.7</td>
<td style="text-align: left;">0.0001</td>
<td style="text-align: left;">0.2082</td>
<td style="text-align: left;">99.7%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Best Models</td>
<td style="text-align: left;">12, 15696</td>
<td style="text-align: left;">0.8684 [0.7475-0.9336]</td>
<td style="text-align: left;">8.13</td>
<td style="text-align: left;">0.0001</td>
<td style="text-align: left;">0.3180</td>
<td style="text-align: left;">99.8%</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><em>N Features</em></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">&lt;30 F</td>
<td style="text-align: left;">4, 6179</td>
<td style="text-align: left;">0.9293 [0.8160-0.9738]</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">0.0978</td>
<td style="text-align: left;">96.2%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">(Range: 6-11; Mdn = 9; <em>M</em> = 8.75)</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">30-300 F</td>
<td style="text-align: left;">5, 8193</td>
<td style="text-align: left;">0.8458 [0.3613-0.9707]</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">0.4822</td>
<td style="text-align: left;">99.9%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">(Range: 98-231; Mdn = 122; <em>M</em> = 139)</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">300+ F</td>
<td style="text-align: left;">3, 1324</td>
<td style="text-align: left;">0.7754 [-0.2709-0.9818]</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">0.2724</td>
<td style="text-align: left;">97.7%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">(Range: 397-8904; Mdn = 1702; <em>M</em> = 3668)</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><em>Techniques</em></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">LM</td>
<td style="text-align: left;">2, 735</td>
<td style="text-align: left;">0.7280 [-0.9957-0.9999]</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">0.1935</td>
<td style="text-align: left;">98.0%</td>
</tr>
<tr class="even">
<td style="text-align: left;">SVM</td>
<td style="text-align: left;">3, 6556</td>
<td style="text-align: left;">0.8699 [-0.7268-0.9985]</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">0.8233</td>
<td style="text-align: left;">99.9%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">NN</td>
<td style="text-align: left;">3, 2313</td>
<td style="text-align: left;">0.9307 [0.6523-0.9878]</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">0.1250</td>
<td style="text-align: left;">98.8%</td>
</tr>
<tr class="even">
<td style="text-align: left;">TM</td>
<td style="text-align: left;">4, 6092</td>
<td style="text-align: left;">0.8534 [0.4464-0.9679]</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">0.2430</td>
<td style="text-align: left;">99.1%</td>
</tr>
</tbody>
</table>
<!-- DISCUSS: 4 digits is too much, let's make this consistent, 2 in all places?, TE 19/3/2025 -->
<section id="quantifying-study-heterogeneity-2" class="level3">
<h3 class="anchored" data-anchor-id="quantifying-study-heterogeneity-2">Quantifying study heterogeneity</h3>
<p>Accounting for outliers among the classifications results removes 6 studies, affecting both performance (<span class="math inline">\(MCC\)</span> = 0.894, 95% CI: 0.828-0.936) and heterogeneity scores (<span class="math inline">\(\tau^2\)</span> = 0.057, <span class="math inline">\(I^2\)</span> = 97.7%). However, we observed no significant issues in analysis for asymmetry (<span class="math inline">\(\beta\)</span> = -19.769, 95% CI: -39.29-0.24). After aggregating across dimensions, we found that some studies with the best results only involved arousal classification <span class="citation" data-cites="zhang2016br zhang2017fe">(<a href="#ref-zhang2017fe" role="doc-biblioref">J. L. Zhang et al., 2017</a>; <a href="#ref-zhang2016br" role="doc-biblioref">J. Zhang et al., 2016</a>)</span>. To assess their impact on interpretations, we evaluated how their exclusion affected average classification accuracy. These analyses revealed that accuracy (<span class="math inline">\(MCC\)</span> = 0.8758, 95%CI: 0.7235-0.9468) did not change significantly, nor did the rank order of model classes in terms of performance. Consequently, we report on all 12 in subsequent analyses.</p>
</section>
<section id="reporting-splits-2" class="level3">
<h3 class="anchored" data-anchor-id="reporting-splits-2">Reporting splits</h3>
<p>Analyzing subgroups revealed that the number of features (as classified into under 30, between 30 and 300, and above 300 features; see Table 3 for actual ranges) does not significantly impact results (<em>Q</em>(2) = 3.91, <em>p</em> = 0.1419), despite <span class="math inline">\(MCC\)</span>s differing on average (&lt; 30 F: <span class="math inline">\(MCC\)</span> = 0.929; 30-300 F: <span class="math inline">\(MCC\)</span> = 0.846; &gt;300 F: <span class="math inline">\(MCC\)</span> = 0.775). Similarly, model classes did not differ significantly (<em>Q</em>(3) = 4.22, <em>p</em> = 0.239) although neural networks attained higher <span class="math inline">\(MCCs\)</span> (0.9307), followed by SVMs (0.870), tree-based methods (0.854), then linear methods (0.728). Finally, neither single vs.&nbsp;multigenre (<em>Q</em>(1) = 0.12, <em>p</em> = 0.732), nor binary vs.&nbsp;multi-class models (<em>Q</em>(1) = 0.03, <em>p</em> = 0.869) differed significantly. All classification studies were published in engineering journals and thus represent predictive framework in our earlier typology.</p>
</section>
<section id="model-success-across-concepts-model-types-and-feature-counts" class="level3">
<h3 class="anchored" data-anchor-id="model-success-across-concepts-model-types-and-feature-counts">Model success across concepts, model types and feature counts</h3>
<!-- Changed the header slightly, TE 19/3/2025-->
<p>To assess how the use of different model types affected performance, we prepared heatmap visualizations depicting differences in success across feature <em>n</em> categories and algorithms. We collapsed SVM and Tree-Based categories due to their low representation in the model summary. Figure 5 summarizes differences in success (a) across categories, as well as (b) the algorithms in each model class. Overall, studies using smaller feature sets tend to perform best, whereas the best model type largely depends on the nature of the prediction task. For valence and arousal, linear models perform better than other model types, whereas for emotion classification, neural networks show the best overall performance. The overall pattern aligns with the analyses of the splits reported earlier concerning model types and feature counts, but the visualization also highlights concurrent information about the feature n, model types, study counts, and the average number of stimuli within each combination. For instance, studies with the lowest number of features (&lt;30) also tend to have the highest mean number of stimuli (M = 749.5), while the poorest-performing feature count range (30–300) corresponds to the lowest mean number of stimuli (M = 522.5). It is also reassuring to observe that studies utilizing neural networks and other model types tend to use a higher number of features and stimuli than those employing linear models, as this reflects the capabilities and internal training requirements built into these models.</p>
<!-- DISCUSS the interpretation above -->
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../manuscript/model-summary-fig.png" title="Heatmap depicting success across tasks according to Feature N category and model class. Labels indicate the number of studies and the median number of stimuli is shown in parentheses." class="img-fluid figure-img" width="668"></p>
<figcaption>Heatmaps summarizing models. Panel a depicts overall success across tasks according to model type and feature <em>n</em> summaries. Text labels summarize the number of models belonging to each subgroup, whereas the numbers in parentheses summarize the median number of stimuli used. Panel b summarizes the algorithms pertaining to each class, colour-coded by frequency. Grey rectangles are visual aids aligning with <em>x</em>-axis groupings in panel a (white: LM; light grey: NN; dark grey: other).</figcaption>
</figure>
</div>
<!-- Notes to results: In the pre-registration, we promised the following:
- groupings based on genre, model complexity (high, medium, low), model validation (exists or not)
- we promised to carry out sensitivity analyses based on type of journal the studies were published in.

CA: It looks like we've accounted for genre (single- vs. multi-genre), complexity (if feature n is a suitable substitute), and subgroup analyses for journal type (though this is not possible for classification studies). We have info on validation, though I'm not sure how this will play out depending on model type and whether cross-validation is necessary based on the algorithm. We might still be able to do a simple T/F encoding for cross-validation, and report the outcomes regardless of what the actual algorithm is.
-->
</section>
</section>
</section>
<section id="discussion-and-conclusions" class="level1">
<h1>Discussion and conclusions</h1>
<p>Research on Music Emotion Recognition has steadily increased in popularity since the early 2000s, with technological advancements facilitating sharing of data sets, analysis tools, and music stimuli. Public forums like the Music Information Retrieval Exchange (MIREX) have facilitated collaborations between computer scientists, musicologists, and psychologists alike – spurring improvements in performance. Despite the increasing complexity of models and datasets, no existing study has rigorously compared the overall success of Music Emotion Recognition research using standardized metrics, nor has there been an analysis of the relative merits of the model techniques and feature sets employed. This study presents the first meta-analysis of music emotion recognition, breaking down accuracy in terms of the model types, number of features, and empirical frameworks employed.</p>
<!-- Concise summary of what we did and found -->
<p>We initially identified 96 studies involving MER models, but narrowed our selection to 34 after filtering studies to ensure consistent quality, reporting standards, and a specific focus on predicting emotions using music features. From these studies, we encoded accuracy scores for 290 models, with 204 related to regression and 86 to classification. Comparing the most accurate model in each study revealed reasonably accurate prediction of valence (<em>r</em> = 0.669 [0.560, 0.755]), and arousal (<em>r</em> = 0.809 [0.740, 0.860]) in regression studies. For both affective dimensions of the circumplex model, linear methods (valence <em>r</em> = 0.784, arousal <em>r</em> = 0.882) and tree-based models (valence <em>r</em> = 0.750, arousal <em>r</em> = 0.809) outperformed support vector machines (SVMs) (valence <em>r</em> = 0.539, arousal <em>r</em> = 0.796) and neural networks (valence = 0.473, arousal = 0.660). In contrast, neural networks performed most accurately in classification studies (<span class="math inline">\(MCC\)</span> = 0.931), followed by SVMs (<span class="math inline">\(MCC\)</span> = 0.870) and tree-based models (<span class="math inline">\(MCC\)</span> = 0.853). Despite the high overall success of the research in this topic, the models exhibited several differences relating to (i) the scale and quantity of data sets employed, (ii) feature extraction methods, (iii) the number and types of features and reduction methods used, (iv) the actual modelling architecture, and (v) how model outcomes are cross-validated.</p>
<section id="improvements-to-predictive-accuracy" class="level2">
<h2 class="anchored" data-anchor-id="improvements-to-predictive-accuracy">Improvements to predictive accuracy</h2>
<p>The results of the meta-analysis offer several important insights. First, compared to a 2013 report by <span class="citation" data-cites="barthet2013">Barthet et al. (<a href="#ref-barthet2013" role="doc-biblioref">2013</a>)</span>, the glass ceiling for valence has risen from <em>r</em> = 0.51 to 0.67.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> Conversely, predictive accuracy for arousal has shown no improvement (reported already in 2013 at <em>r</em> = 0.83). For classification, studies reached variable classification rates from 0.497 using <em>F<sub>1</sub></em> score <span class="citation" data-cites="sanden2011empirical">(<a href="#ref-sanden2011empirical" role="doc-biblioref">Sanden &amp; Zhang, 2011</a>)</span> to 79% average precision <span class="citation" data-cites="trohidis2008multi">(<a href="#ref-trohidis2008multi" role="doc-biblioref">Trohidis et al., 2008</a>)</span> in the past, whereas prediction rates have improved considerably, here <span class="math inline">\(MCC\)</span> = 0.87, albeit the differences in metrics.</p>
<!-- Note. there is no clear conversion from Average precision to MCC, although we could estimate this from our data using regression? or to calculate both metrics from the confusion matrix. CA: this sounds like an interesting and achievable idea. The confusion matrices are encoded and then summarized with the caret and yardstick packages, and we can potentially extract many other useful measures from them with yardstick::summary(), demonstrated here: https://yardstick.tidymodels.org/reference/summary.conf_mat.html. 

TE: Interesting, it is a potential weakness and it would make our promotion of MCC more valuable when we could demonstrate how these two are linked, but perhaps to limit the workload, we could try identify a small subset to which to apply to this idea. -->
<p>Second, there is no single modelling technique that seems to arise on top, although linear models and random forests perform best in regression studies, and neural networks and support vector machines excel in classification tasks. We note that the model accuracy is surprisingly little affected by the number of features or the dataset size—likely the result of cross-validation techniques used to avoid overfitting—but the heterogeneity of the materials used in different studies may also mask substantial differences. For instance, the smaller datasets tend to use linear models and deliver higher model accuracy than larger datasets and those with a large number of features. We surmise that this might relate to disciplinary differences. For example, the smaller datasets often come from music psychology studies, which put a premium on data quality (quality control of the ground-truth data and features) rather than on dataset size and model techniques. This argument is largely consistent with the analysis of the studies divided across predictive and explanatory modelling frameworks (which we determined based on journals representing psychology or engineering discipline), even though the pattern is not well-defined.</p>
<!-- minor improvements to the ending of the paragraph above -->
<p>Third, there is little work on the relevance of the features and how these impact the accuracy of the models. Unfortunately, in many cases, it is impossible to attribute the sources of error to features (as opposed to target emotions obtained from participants or from the modelling architecture, cross-validation, or the size of data), as the studies so far have not compared the feature sets systematically, nor has a comparison of the datasets with identical features has been carried out <span class="citation" data-cites="panda2020audio">(<a href="#ref-panda2020audio" role="doc-biblioref">Panda et al., 2023</a>)</span>. In the future, it would be advantageous to systematically assess these sources of error in MER studies to allow us to focus on where and how significant improvements can be made.</p>
<!-- Added at least a reference to Pandas work on survey of features to para above -->
<!-- Question to @Cameron: We are a bit coy here as we don't really mention the tools by name (e.g. MIR toolbox, psysound, essentia, librosa, etc.). We have the information and should we write about them? Another gem to the supporting information as the space does not allow expansion. 

CA: This is a great idea, and reminds me we also have the Panda categories, which we have encoded, but haven't touched. We could explore whether we can find additional patterns about feature categories, and possibly libraries by extending out our new heat map visualization approach. 

TE: Or if we submit the meta-analysis soon and create a more pragmatic breakdown of what is the state of the art technologies in MER, this could be sufficient for a brief article separately. The selling point would not be the prediction rate, but quality control issues and the techniques/tools/crossvals/datasets chosen so far.  -->
</section>
<section id="recommendations-for-future-mer-studies" class="level2">
<h2 class="anchored" data-anchor-id="recommendations-for-future-mer-studies">Recommendations for future MER studies</h2>
<p>Broadly speaking, the present study revealed uncomfortably large variability in overall quality control and reporting in MER studies. In many cases, the reporting was insufficient to determine the features used in the models or their sources. Additionally, a significant number of studies had to be discarded due to a lack of information about the data, model architectures, or outcome measures. We summarize these issues below as recommendations for improving (1) reporting and transparency, (2) feature definition, (3) dataset scale and content, and (4) the selection of emotion frameworks.</p>
<!-- ### Reporting and transparency -->
<p>Future reports should contain viable information about the data, models, and success metrics. The modelling process should include a description of cross-validation, feature extraction technique, feature reduction (if used), and actual accuracy measures. We would recommend future studies to use <em>Matthews correlation coefficient</em> (<span class="math inline">\(MCC\)</span>) for classification studies, as currently there are numerous metrics (<em>F<sub>1</sub></em>, precision, recall, balanced accuracy, etc.), which do not operate in the same range. The situation for regression studies tends to be less volatile, but model accuracy may be reported with error measures (Mean Squared Error, Root Mean Squared Error, or Mean Absolute Error, or <span class="math inline">\(MSE\)</span>, <span class="math inline">\(RMSE\)</span>, <span class="math inline">\(MAE\)</span>), which cannot be converted to relative measure such as <span class="math inline">\(R^2\)</span> without knowing the variance of the data. Again, we would recommend utilising <span class="math inline">\(R^2\)</span> due it being a relative measure and scale invarant to allow comparison across models, datasets, and studies. There is potential confusion in using the <span class="math inline">\(R^2\)</span> measure since some studies report <span class="math inline">\(R^2_{\text{adj}}\)</span>, which is a less biased estimate of the model’s explanatory power as it incorporates the number of predictors as a penalty. However, since there are other measures that calculate penalties for complex models (like AIC or BIC), and the difference between the two measures is small in large datasets, we nevertheless recommend <span class="math inline">\(R^2\)</span> as the most transparent accuracy measure for regression studies.</p>
<!-- added a recommendation for regression study outputs as well to para above. We could add a mention about R2 adjusted to penalise complex models, but there are other ways to punish for model complexity as well. This has now been added to the para above -->
<p>It would be also highly beneficial to share models and features transparently. If a study relies on published datasets, as most do, sharing the actual model (as code/notebooks) and features would allow for a more direct comparison of model techniques for the same dataset. Also, not all datasets are shared <span class="citation" data-cites="akiki2021 santana2020 hizlisoy2021mu zhang2017fe">(<a href="#ref-akiki2021" role="doc-biblioref">Akiki &amp; Burghardt, 2021</a>; <a href="#ref-hizlisoy2021mu" role="doc-biblioref">Hizlisoy et al., 2021</a>; <a href="#ref-santana2020" role="doc-biblioref">Santana et al., 2020</a>; <a href="#ref-zhang2017fe" role="doc-biblioref">J. L. Zhang et al., 2017</a>)</span>. Standardizing reporting of datasets to include subsections explaining detailed information about stimuli (including genre, duration, sampling rate, and encoding format), features (types, extraction/analysis software, quantity of features, transformations, reduction methods), and models (types, tuning parameters, cross-validation) will enable more accurate comparisons between studies. The reporting in <span class="citation" data-cites="grekow2018au">Grekow (<a href="#ref-grekow2018au" role="doc-biblioref">2018</a>)</span> serves as a useful example by providing enough detail in these areas to facilitate reproducibility. Although copyright restrictions may limit the sharing of some datasets, features and examples should be made available through reliable external repositories (e.g., <a href="https://zenodo.org/">Zenodo</a>, <a href="https://osf.io/">Open Science Framework</a>, or <a href="https://osf.io/">Kaggle datasets</a>).</p>
<!-- Added some examples of where the features/models can be shared -->
<!-- ### Feature definition/validations -->
<p>One of the crucial aspects of modelling is the choice of features, their quantity, and their validation. In the present materials, we observe a continuum defined by two extremes: one approach relies on domain knowledge, starting with a limited number of features that are assumed to be perceptually relevant, while the other employs a large number of features, allowing the model to determine which are most relevant for prediction. The former is typically favored in music psychology studies, whereas the latter is more common in engineering and machine learning fields. Our results suggest that the domain knowledge-driven approach leads to the highest model accuracy across all studies and techniques. However, as our sample includes studies from the past 10 years, it is important to note that deep learning models have only gained prominence in this field since 2021. Consequently, it is too early to generalize that models based on domain knowledge, with a limited number of features and classical techniques, will continue to outperform machine learning approaches that utilize a large (300+) feature sets. We believe that the relatively modest size of datasets available so far has prevented deep learning approaches from fully leveraging their potential to learn generalizable patterns in the data. For reference, the size of the datasets in MER are currently comparable to dataset sizes for modelling facial expressions (median N=502, <span class="citation" data-cites="krumhuber2017">Krumhuber et al. (<a href="#ref-krumhuber2017" role="doc-biblioref">2017</a>)</span>) and speech expressions (median of 1287, <span class="citation" data-cites="hashem2023">Hashem et al. (<a href="#ref-hashem2023" role="doc-biblioref">2023</a>)</span>). However, annotated datasets for visual scenes and objects tend to be much larger, often exceeding 100,000 annotated examples <span class="citation" data-cites="deng2009imagenet krishna2017visual">(<a href="#ref-deng2009imagenet" role="doc-biblioref">Deng et al., 2009</a>; <a href="#ref-krishna2017visual" role="doc-biblioref">Krishna et al., 2017</a>)</span>. Datasets of these magnitudes seem to be required for appropriate utilisation of deep learning algorithms <span class="citation" data-cites="alwosheel2018 sun2017revisiting">(<a href="#ref-alwosheel2018" role="doc-biblioref">Alwosheel et al., 2018</a>; <a href="#ref-sun2017revisiting" role="doc-biblioref">Sun et al., 2017</a>)</span>, which may explain the modest results observed in the music emotion recognition studies.</p>
<!-- ### Validation -->
<p>An encouraging finding is that ~94% of the studies analyzed here reported some form of model validation. The majority of studies validated models by splitting the dataset into separate sets for training and testing <span class="citation" data-cites="yang2021an">(e.g., <a href="#ref-yang2021an" role="doc-biblioref">Jing Yang, 2021</a>)</span>, sometimes including an additional set for validation <span class="citation" data-cites="sorussa2020em">(<a href="#ref-sorussa2020em" role="doc-biblioref">Sorussa et al., 2020</a>)</span>. Most used some form of cross validation (CV), with the most common type being 10-fold CV. Other varieties included 3-fold, 5-fold, or 6-fold CV, as well as more complex variants like nested leave-one-out CV <span class="citation" data-cites="coutinho2017sh">(<a href="#ref-coutinho2017sh" role="doc-biblioref">Coutinho &amp; Schuller, 2017</a>)</span> and 20 x 10-fold CV <span class="citation" data-cites="panda2020no">(<a href="#ref-panda2020no" role="doc-biblioref">Panda et al., 2020</a>)</span>. Whereas many engineering studies performed model validation using one or more large databases, some psychological studies evaluating smaller datasets validated models by designing new experiments. Examples include comparing a model’s performance on ground-truth data with annotations from a second experiment <span class="citation" data-cites="griffiths2021am beveridge2018po">(<a href="#ref-beveridge2018po" role="doc-biblioref">Beveridge &amp; Knox, 2018</a>; <a href="#ref-griffiths2021am" role="doc-biblioref">Griffiths et al., 2021</a>)</span>, or comparing a model’s performance across different versions of the same music pieces <span class="citation" data-cites="battcock2021in">(<a href="#ref-battcock2021in" role="doc-biblioref">Battcock &amp; Schutz, 2021</a>)</span>.</p>
<!-- Western bias -->
<p>At present, the majority of the datasets are Western pop, which represent only a fraction of the musical styles consumed globally. Also, the annotators representing the Global North dominate the studies at the moment, with some exceptions <span class="citation" data-cites="gomez-canon2023 zhangPMEmo2018">(<a href="#ref-gomez-canon2023" role="doc-biblioref">Gómez-Cañón et al., 2023</a>; <a href="#ref-zhangPMEmo2018" role="doc-biblioref">K. Zhang et al., 2018</a>)</span>. This lack of diversity may contribute the success of the task but presents a significant limitation in our understanding of MER more broadly <span class="citation" data-cites="bornDiversifyingMIRKnowledge2020">(<a href="#ref-bornDiversifyingMIRKnowledge2020" role="doc-biblioref">Born, 2020</a>)</span>. Greater exploration of multi-genre MER <span class="citation" data-cites="griffiths2021am">(e.g., <a href="#ref-griffiths2021am" role="doc-biblioref">Griffiths et al., 2021</a>)</span>, and cross-cultural applications [<span class="citation" data-cites="wang2022cr">Wang et al. (<a href="#ref-wang2022cr" role="doc-biblioref">2022</a>)</span>; wang2021ac; <span class="citation" data-cites="hu2017cr">X. Hu &amp; Yang (<a href="#ref-hu2017cr" role="doc-biblioref">2017</a>)</span>; <span class="citation" data-cites="agarwal2021an">Agarwal &amp; Om (<a href="#ref-agarwal2021an" role="doc-biblioref">2021</a>)</span>] will provide an important step toward establishing more generalizable models.</p>
<!-- ### Emotion frameworks chosen -->
<p>Finally, we note that the relevance of the emotion frameworks used in MER is not always explicitly discussed. The majority of studies rely either on valence and arousal or some combination of basic emotion categories. However, these frameworks may have limited usefulness in practical applications that aim to capture the diversity of emotion experiences with music. For example, some experiences align with models of music-induced emotions, such as GEMS by <span class="citation" data-cites="zentner2008emotions">Zentner et al. (<a href="#ref-zentner2008emotions" role="doc-biblioref">2008</a>)</span> or AESTHEMOS <span class="citation" data-cites="schindler2017me">(<a href="#ref-schindler2017me" role="doc-biblioref">Schindler et al., 2017</a>)</span>, whereas other explore what emotions can be expressed through music <span class="citation" data-cites="eerola2025what">(<a href="#ref-eerola2025what" role="doc-biblioref">Eerola &amp; Saari, 2025</a>)</span>, or what are assumed to be worthwhile clusters of concepts (moods, emotions, tags) from crowdsourced, non-theory driven data <span class="citation" data-cites="saari_et_al_2015">(<a href="#ref-saari_et_al_2015" role="doc-biblioref">Saari et al., 2015</a>)</span>. Understanding the limitations of different emotion taxonomies can also help improve modelling practices. For example, some scholars have explored treating MER as a circular regression problem, which can help overcome practical challenges such as the difficulty of relating to the abstract dimensions of valence and arousal, as well as the modelling assumptions required when translating a circular affective space to regression problems <span class="citation" data-cites="dufour2021us">(<a href="#ref-dufour2021us" role="doc-biblioref">Dufour &amp; Tzanetakis, 2021</a>)</span>.</p>
<p>The present meta-analysis demonstrates that significant progress has been made toward developing accurate and scalable MER models over the past decade. Future efforts should prioritize feature validation, standardized reporting, the construction of larger and more diverse datasets, and the transparent sharing of research materials to ensure further consistent improvements in MER.</p>
<section id="funding-statement" class="level3">
<h3 class="anchored" data-anchor-id="funding-statement">Funding statement</h3>
<p>CA was funded by Mitacs Globalink Research Award (Mitacs &amp; British High Commission - Ottawa, Canada).</p>
</section>
<section id="competing-interests-statement" class="level3">
<h3 class="anchored" data-anchor-id="competing-interests-statement">Competing interests statement</h3>
<p>There were no competing interests.</p>
</section>
<section id="open-practices-statement" class="level3">
<h3 class="anchored" data-anchor-id="open-practices-statement">Open practices statement</h3>
<p>Study preregistration, data, analysis scripts and supporting information is available at GitHub, <a href="https://tuomaseerola.github.io/metaMER" class="uri">https://tuomaseerola.github.io/metaMER</a>.</p>
</section>
</section>
</section>
<section id="references" class="level1">




</section>


<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a><div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-agarwal2021an" class="csl-entry" role="listitem">
Agarwal, G., &amp; Om, H. (2021). An efficient supervised framework for music mood recognition using autoencoder-based optimised support vector regression model. <em>IET Signal Processing</em>, <em>15</em>(2), 98–121. <a href="https://doi.org/10.1049/sil2.12015">https://doi.org/10.1049/sil2.12015</a>
</div>
<div id="ref-agres2021music" class="csl-entry" role="listitem">
Agres, K. R., Schaefer, R. S., Volk, A., Van Hooren, S., Holzapfel, A., Dalla Bella, S., Müller, M., De Witte, M., Herremans, D., Ramirez Melendez, R., et al. (2021). Music, computing, and health: A roadmap for the current and future roles of music technology for health care and well-being. <em>Music &amp; Science</em>, <em>4</em>, 2059204321997709.
</div>
<div id="ref-akiki2021" class="csl-entry" role="listitem">
Akiki, C., &amp; Burghardt, M. (2021). <span>MuSe</span>: <span>The Musical Sentiment Dataset</span>. <em>Journal of Open Humanities Data</em>, <em>7</em>, 10. <a href="https://doi.org/10.5334/johd.33">https://doi.org/10.5334/johd.33</a>
</div>
<div id="ref-aljanaki2017developing" class="csl-entry" role="listitem">
Aljanaki, A., Yang, Y.-H., &amp; Soleymani, M. (2017). Developing a benchmark for emotional analysis of music. <em>PloS One</em>, <em>12</em>(3), e0173392.
</div>
<div id="ref-alvarez2023ri" class="csl-entry" role="listitem">
Álvarez, P., Quirós, J. G. de, &amp; Baldassarri, S. (2023). RIADA: A machine-learning based infrastructure for recognising the emotions of spotify songs. <em>International Journal of Interactive Multimedia and Artificial Intelligence</em>, <em>8</em>(2), 168–181. <a href="https://doi.org/10.9781/ijimai.2022.04.002">https://doi.org/10.9781/ijimai.2022.04.002</a>
</div>
<div id="ref-alwosheel2018" class="csl-entry" role="listitem">
Alwosheel, A., van Cranenburgh, S., &amp; Chorus, C. G. (2018). Is your dataset big enough? Sample size requirements when using artificial neural networks for discrete choice analysis. <em>Journal of Choice Modelling</em>, <em>28</em>, 167–182. https://doi.org/<a href="https://doi.org/10.1016/j.jocm.2018.07.002">https://doi.org/10.1016/j.jocm.2018.07.002</a>
</div>
<div id="ref-anderson2022ex" class="csl-entry" role="listitem">
Anderson, C. J., &amp; Schutz, M. (2022). Exploring historic changes in musical communication: Deconstructing emotional cues in preludes by bach and chopin. <em>Psychology of Music</em>, <em>50</em>(5), 1424–1442.
</div>
<div id="ref-bai2016dimensional" class="csl-entry" role="listitem">
Bai, J., Peng, J., Shi, J., Tang, D., Wu, Y., Li, J., &amp; Luo, K. (2016). Dimensional music emotion recognition by valence-arousal regression. <em>2016 IEEE 15th International Conference on Cognitive Informatics &amp; Cognitive Computing (ICCI* CC)</em>, 42–49.
</div>
<div id="ref-balduzzi2019" class="csl-entry" role="listitem">
Balduzzi, S., Rücker, G., &amp; Schwarzer, G. (2019). How to perform a meta-analysis with <span>R</span>: A practical tutorial. <em>Evidence-Based Mental Health</em>, <em>22</em>, 153–160.
</div>
<div id="ref-barthet2013" class="csl-entry" role="listitem">
Barthet, M., Fazekas, G., &amp; Sandler, M. (2013). Music emotion recognition: From content- to context-based models. In M. Aramaki, M. Barthet, R. Kronland-Martinet, &amp; S. Ystad (Eds.), <em>From sounds to music and emotions</em> (pp. 228–252). Springer Berlin Heidelberg.
</div>
<div id="ref-battcock2021in" class="csl-entry" role="listitem">
Battcock, A., &amp; Schutz, M. (2021). Individualized interpretation: Exploring structural and interpretive effects on evaluations of emotional content in bach’s well tempered clavier. <em>JOURNAL OF NEW MUSIC RESEARCH</em>, <em>50</em>(5), 447–468. <a href="https://doi.org/10.1080/09298215.2021.1979050">https://doi.org/10.1080/09298215.2021.1979050</a>
</div>
<div id="ref-beveridge2018po" class="csl-entry" role="listitem">
Beveridge, S., &amp; Knox, D. (2018). Popular music and the role of vocal melody in perceived emotion. <em>Psychology of Music</em>, <em>46</em>(3), 411–423. <a href="https://doi.org/10.1177/0305735617713834">https://doi.org/10.1177/0305735617713834</a>
</div>
<div id="ref-bhuvanakumar2023em" class="csl-entry" role="listitem">
Bhuvana Kumar, V., &amp; Kathiravan, M. (2023). Emotion recognition from MIDI musical file using enhanced residual gated recurrent unit architecture. <em>Frontiers in Computer Science</em>, <em>5</em>. <a href="https://doi.org/10.3389/fcomp.2023.1305413">https://doi.org/10.3389/fcomp.2023.1305413</a>
</div>
<div id="ref-bogdanov2019mtg" class="csl-entry" role="listitem">
Bogdanov, D., Won, M., Tovstogan, P., Porter, A., &amp; Serra, X. (2019). The <span class="nocase">MTG-jamendo</span> dataset for automatic music tagging. <em>Machine Learning for Music Discovery Workshop, International Conference on Machine Learning (<span>ICML</span> 2019)</em>.
</div>
<div id="ref-bornDiversifyingMIRKnowledge2020" class="csl-entry" role="listitem">
Born, G. (2020). Diversifying <span>MIR</span>: Knowledge and real-world challenges, and new interdisciplinary futures. <em>Transactions of the International Society for Music Information Retrieval</em>, <em>3</em>(1), 193–204. <a href="https://doi.org/10.5334/tismir.58">https://doi.org/10.5334/tismir.58</a>
</div>
<div id="ref-celma_foafing_2006" class="csl-entry" role="listitem">
Celma, O. (2006). Foafing the music: <span>Bridging</span> the semantic gap in music recommendation. <em>International Semantic Web Conference</em>, 927–934.
</div>
<div id="ref-chen2017co" class="csl-entry" role="listitem">
Chen, Y.-A., Wang, J.-C., Yang, Y.-H., &amp; Chen, H. H. (2017). Component tying for mixture model adaptation in personalization of music emotion recognition. <em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, <em>25</em>(7), 1409–1420. <a href="https://doi.org/10.1109/TASLP.2017.2693565">https://doi.org/10.1109/TASLP.2017.2693565</a>
</div>
<div id="ref-chen2015amg1608" class="csl-entry" role="listitem">
Chen, Y.-A., Yang, Y.-H., Wang, J.-C., &amp; Chen, H. (2015). The AMG1608 dataset for music emotion recognition. <em>2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 693–697.
</div>
<div id="ref-chicco2020advantages" class="csl-entry" role="listitem">
Chicco, D., &amp; Jurman, G. (2020). The advantages of the matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation. <em>BMC Genomics</em>, <em>21</em>, 1–13.
</div>
<div id="ref-chin2018" class="csl-entry" role="listitem">
Chin, Y.-H., Wang, J.-C., Wang, J.-C., &amp; Yang, Y.-H. (2018). Predicting the probability density function of music emotion using emotion space mapping. <em>IEEE Transactions on Affective Computing</em>, <em>9</em>(4), 541 549. <a href="https://doi.org/10.1109/TAFFC.2016.2628794">https://doi.org/10.1109/TAFFC.2016.2628794</a>
</div>
<div id="ref-chowdhury2021perceived" class="csl-entry" role="listitem">
Chowdhury, S., &amp; Widmer, G. (2021). On perceived emotion in expressive piano performance: Further experimental evidence for the relevance of mid-level perceptual features. <em>International Society for Music Information Retrieval Conference (ISMIR 2023)</em>.
</div>
<div id="ref-coutinho2013psychoacoustic" class="csl-entry" role="listitem">
Coutinho, E., &amp; Dibben, N. (2013). Psychoacoustic cues to emotion in speech prosody and music. <em>Cognition &amp; Emotion</em>, <em>27</em>(4), 658–684.
</div>
<div id="ref-coutinho2017sh" class="csl-entry" role="listitem">
Coutinho, E., &amp; Schuller, B. (2017). Shared acoustic codes underlie emotional communication in music and speech-evidence from deep transfer learning. <em>PLOS ONE</em>, <em>12</em>(6). <a href="https://doi.org/10.1371/journal.pone.0179289">https://doi.org/10.1371/journal.pone.0179289</a>
</div>
<div id="ref-deng2009imagenet" class="csl-entry" role="listitem">
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., &amp; Fei-Fei, L. (2009). Imagenet: <span>A</span> large-scale hierarchical image database. <em>2009 <span>IEEE</span> Conference on Computer Vision and Pattern Recognition</em>, 248–255.
</div>
<div id="ref-downie_music_2008" class="csl-entry" role="listitem">
Downie, J. S. (2008). The music information retrieval evaluation exchange (2005–2007): <span>A</span> window into music information retrieval research. <em>Acoustical Science and Technology</em>, <em>29</em>(4), 247–255.
</div>
<div id="ref-dufour2021us" class="csl-entry" role="listitem">
Dufour, I., &amp; Tzanetakis, G. (2021). Using circular models to improve music emotion recognition. <em>IEEE Transactions on Affective Computing</em>, <em>12</em>(3), 666–681. <a href="https://doi.org/10.1109/TAFFC.2018.2885744">https://doi.org/10.1109/TAFFC.2018.2885744</a>
</div>
<div id="ref-eerola2011c" class="csl-entry" role="listitem">
Eerola, T. (2011). Are the emotions expressed in music genre-specific? An audio-based evaluation of datasets spanning classical, film, pop and mixed genres. <em>Journal of New Music Research</em>, <em>40</em>(4), 349–366. <a href="https://doi.org/10.1080/09298215.2011.602195">https://doi.org/10.1080/09298215.2011.602195</a>
</div>
<div id="ref-eerola_friberg_bresin_2013" class="csl-entry" role="listitem">
Eerola, T., Friberg, A., &amp; Bresin, R. (2013). Emotional expression in music: Contribution, linearity, and additivity of primary musical cues. <em>Frontiers in Psychology</em>, <em>4</em>(487). <a href="https://doi.org/10.3389/fpsyg.2013.00487">https://doi.org/10.3389/fpsyg.2013.00487</a>
</div>
<div id="ref-eerola2025what" class="csl-entry" role="listitem">
Eerola, T., &amp; Saari, P. (2025). What emotions does music express? Structure of affect terms in music using iterative crowdsourcing paradigm. <em>Plos ONE</em>, <em>20</em>(1), e0313502. https://doi.org/<a href="https://doi.org/10.1371/journal.pone.0313502">https://doi.org/10.1371/journal.pone.0313502</a>
</div>
<div id="ref-eggersmith_1997" class="csl-entry" role="listitem">
Egger, M., Smith, G. D., Schneider, M., &amp; Minder, C. (1997). Bias in meta-analysis detected by a simple, graphical test. <em>Bmj</em>, <em>315</em>(7109), 629–634.
</div>
<div id="ref-er2019music" class="csl-entry" role="listitem">
Er, M. B., &amp; Aydilek, I. B. (2019). Music emotion recognition by using chroma spectrogram and deep visual features. <em>International Journal of Computational Intelligence Systems</em>, <em>12</em>(2), 1622–1634.
</div>
<div id="ref-eyben2010opensmile" class="csl-entry" role="listitem">
Eyben, F., Wöllmer, M., &amp; Schuller, B. (2010). Opensmile: The munich versatile and fast open-source audio feature extractor. <em>Proceedings of the 18th ACM International Conference on Multimedia</em>, 1459–1462.
</div>
<div id="ref-fairthorne1968" class="csl-entry" role="listitem">
Fairthorne, R. A. (1968). <em>Towards information retrieval</em>.
</div>
<div id="ref-feng_popular_2003" class="csl-entry" role="listitem">
Feng, Y., Zhuang, Y., &amp; Pan, Y. (2003). Popular music retrieval by detecting mood. <em>Proceedings of the 26th Annual International <span>ACM</span> <span>SIGIR</span> Conference on <span>Research</span> and Development in Informaion Retrieval</em>, 375–376.
</div>
<div id="ref-friberg_automatic_2002" class="csl-entry" role="listitem">
Friberg, A., Schoonderwaldt, E., Juslin, P. N., &amp; Bresin, R. (2002). Automatic real-time extraction of musical expression. <em>International <span>Computer</span> <span>Music</span> <span>Conference</span>, <span>ICMC</span> 2002, <span>Gothenburg</span>, <span>Sweden</span></em>, 365–367.
</div>
<div id="ref-fu2010survey" class="csl-entry" role="listitem">
Fu, Z., Lu, G., Ting, K. M., &amp; Zhang, D. (2010). A survey of audio-based music classification and annotation. <em>IEEE Transactions on Multimedia</em>, <em>13</em>(2), 303–319.
</div>
<div id="ref-gingras2014be" class="csl-entry" role="listitem">
Gingras, B., Marin, M. M., &amp; Fitch, W. T. (2014). Beyond intensity: Spectral features effectively predict music-induced subjective arousal. <em>Quarterly Journal of Experimental Psychology</em>, <em>67</em>(7), 1428–1446. <a href="https://doi.org/10.1080/17470218.2013.863954">https://doi.org/10.1080/17470218.2013.863954</a>
</div>
<div id="ref-gomez2021" class="csl-entry" role="listitem">
Gómez-Cañón, J. S., Cano, E., Eerola, T., Herrera, P., Hu, X., Yang, Y.-H., &amp; Gómez, E. (2021). Music emotion recognition: Toward new, robust standards in personalized and context-sensitive applications. <em>IEEE Signal Processing Magazine</em>, <em>38</em>(6), 106–114. <a href="https://doi.org/10.1109/MSP.2021.3106232">https://doi.org/10.1109/MSP.2021.3106232</a>
</div>
<div id="ref-gomez-canon2023" class="csl-entry" role="listitem">
Gómez-Cañón, J. S., Gutiérrez-Páez, N., Porcaro, L., Porter, A., Cano, E., Herrera-Boyer, P., Gkiokas, A., Santos, P., Hernández-Leo, D., Karreman, C., &amp; Gómez, E. (2023). <span>TROMPA-MER</span>: An open dataset for personalized music emotion recognition. <em>Journal of Intelligent Information Systems</em>, <em>60</em>(2), 549–570. <a href="https://doi.org/10.1007/s10844-022-00746-0">https://doi.org/10.1007/s10844-022-00746-0</a>
</div>
<div id="ref-grekow2018au" class="csl-entry" role="listitem">
Grekow, J. (2018). Audio features dedicated to the detection and tracking of arousal and valence in musical compositions. <em>Journal of Information and Telecommunication</em>, <em>2</em>(3), 322–333.
</div>
<div id="ref-grekow2021music" class="csl-entry" role="listitem">
Grekow, J. (2021). Music emotion recognition using recurrent neural networks and pretrained models. <em>Journal of Intelligent Information Systems</em>, <em>57</em>(3), 531–546.
</div>
<div id="ref-griffiths2021am" class="csl-entry" role="listitem">
Griffiths, D., Cunningham, S., Weinel, J., &amp; Picking, R. (2021). A multi-genre model for music emotion recognition using linear regressors. <em>Journal of New Music Research</em>, <em>50</em>(4), 355–372. <a href="https://doi.org/10.1080/09298215.2021.1977336">https://doi.org/10.1080/09298215.2021.1977336</a>
</div>
<div id="ref-grimaud_eerola_2022" class="csl-entry" role="listitem">
Grimaud, A. M., &amp; Eerola, T. (2022). An interactive approach to emotional expression through musical cues. <em>Music &amp; Science</em>, <em>5</em>, 1–23. https://doi.org/<a href="https://doi.org/10.1177/20592043211061745">https://doi.org/10.1177/20592043211061745</a>
</div>
<div id="ref-hashem2023" class="csl-entry" role="listitem">
Hashem, A., Arif, M., &amp; Alghamdi, M. (2023). Speech emotion recognition approaches: <span>A</span> systematic review. <em>Speech Communication</em>, <em>154</em>, 102974. <a href="https://doi.org/10.1016/j.specom.2023.102974">https://doi.org/10.1016/j.specom.2023.102974</a>
</div>
<div id="ref-higgins2002quantifying" class="csl-entry" role="listitem">
Higgins, J. P., &amp; Thompson, S. G. (2002). Quantifying heterogeneity in a meta-analysis. <em>Statistics in Medicine</em>, <em>21</em>(11), 1539–1558.
</div>
<div id="ref-hizlisoy2021mu" class="csl-entry" role="listitem">
Hizlisoy, S., Yildirim, S., &amp; Tufekci, Z. (2021). Music emotion recognition using convolutional long short term memory deep neural networks. <em>Engineering Science and Technology, an International Journal</em>, <em>24</em>(3), 760–767. <a href="https://doi.org/10.1016/j.jestch.2020.10.009">https://doi.org/10.1016/j.jestch.2020.10.009</a>
</div>
<div id="ref-hu2022de" class="csl-entry" role="listitem">
Hu, Xiao, Li, F., &amp; Liu, R. (2022). Detecting music-induced emotion based on acoustic analysis and physiological sensing: A multimodal approach. <em>Applied Sciences</em>, <em>12</em>(18). <a href="https://doi.org/10.3390/app12189354">https://doi.org/10.3390/app12189354</a>
</div>
<div id="ref-hu2017cr" class="csl-entry" role="listitem">
Hu, X., &amp; Yang, Y.-H. (2017). Cross-dataset and cross-cultural music mood prediction: A case on western and chinese pop songs. <em>IEEE Transactions on Affective Computing</em>, <em>8</em>(2), 228–240. <a href="https://doi.org/10.1109/TAFFC.2016.2523503">https://doi.org/10.1109/TAFFC.2016.2523503</a>
</div>
<div id="ref-huq2010automated" class="csl-entry" role="listitem">
Huq, A., Bello, J. P., &amp; Rowe, R. (2010). Automated music emotion recognition: A systematic evaluation. <em>Journal of New Music Research</em>, <em>39</em>(3), 227–244.
</div>
<div id="ref-juslin2022emotions" class="csl-entry" role="listitem">
Juslin, P. N., Sakka, L. S., Barradas, G. T., &amp; Lartillot, O. (2022). Emotions, mechanisms, and individual differences in music listening: A stratified random sampling approach. <em>Music Perception: An Interdisciplinary Journal</em>, <em>40</em>(1), 55–86.
</div>
<div id="ref-kassler1966toward" class="csl-entry" role="listitem">
Kassler, M. (1966). Toward musical information retrieval. <em>Perspectives of New Music</em>, 59–67.
</div>
<div id="ref-katayose_sentiment_1988" class="csl-entry" role="listitem">
Katayose, H., Imai, M., &amp; Inokuchi, S. (1988). Sentiment extraction in music. <em>9th <span>International</span> <span>Conference</span> on <span>Pattern</span> <span>Recognition</span></em>, 1083–1084.
</div>
<div id="ref-knapp2003improved" class="csl-entry" role="listitem">
Knapp, G., &amp; Hartung, J. (2003). Improved tests for a random effects meta-regression with a single covariate. <em>Statistics in Medicine</em>, <em>22</em>(17), 2693–2710.
</div>
<div id="ref-koh2023me" class="csl-entry" role="listitem">
Koh, E. Y., Cheuk, K. W., Heung, K. Y., Agres, K. R., &amp; Herremans, D. (2023). MERP: A music dataset with emotion ratings and raters’ profile information. <em>SENSORS</em>, <em>23</em>(1). <a href="https://doi.org/10.3390/s23010382">https://doi.org/10.3390/s23010382</a>
</div>
<div id="ref-krishna2017visual" class="csl-entry" role="listitem">
Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.-J., Shamma, D. A., et al. (2017). Visual genome: <span>Connecting</span> language and vision using crowdsourced dense image annotations. <em>International Journal of Computer Vision</em>, <em>123</em>, 32–73.
</div>
<div id="ref-krumhuber2017" class="csl-entry" role="listitem">
Krumhuber, E. G., Skora, L., Küster, D., &amp; Fou, L. (2017). A review of dynamic datasets for facial expression research. <em>Emotion Review</em>, <em>9</em>(3), 280–292. <a href="https://doi.org/10.1177/1754073916670022">https://doi.org/10.1177/1754073916670022</a>
</div>
<div id="ref-langan2019comparison" class="csl-entry" role="listitem">
Langan, D., Higgins, J. P., Jackson, D., Bowden, J., Veroniki, A. A., Kontopantelis, E., Viechtbauer, W., &amp; Simmonds, M. (2019). A comparison of heterogeneity variance estimators in simulated random-effects meta-analyses. <em>Research Synthesis Methods</em>, <em>10</em>(1), 83–98.
</div>
<div id="ref-lartillot2007matlab" class="csl-entry" role="listitem">
Lartillot, O., &amp; Toiviainen, P. (2007). A matlab toolbox for musical feature extraction from audio. <em>International Conference on Digital Audio Effects</em>, <em>237</em>, 244.
</div>
<div id="ref-lin2009eeg" class="csl-entry" role="listitem">
Lin, Y.-P., Wang, C.-H., Wu, T.-L., Jeng, S.-K., &amp; Chen, J.-H. (2009). EEG-based emotion recognition in music listening: A comparison of schemes for multiclass support vector machine. <em>2009 IEEE International Conference on Acoustics, Speech and Signal Processing</em>, 489–492.
</div>
<div id="ref-lindstrom2003expressivity" class="csl-entry" role="listitem">
Lindström, E., Juslin, P. N., Bresin, R., &amp; Williamon, A. (2003). "Expressivity comes from within your soul": A questionnaire study of music students’ perspectives on expressivity. <em>Research Studies in Music Education</em>, <em>20</em>(1), 23–47.
</div>
<div id="ref-liu_automatic_2003" class="csl-entry" role="listitem">
Liu, D., Lu, L., &amp; Zhang, H.-J. (2003, January). Automatic <span>Mood</span> <span>Detection</span> from <span>Acoustic</span> <span>Music</span> <span>Data</span>. <em>Proc. <span>ISMIR</span> 2003; 4th <span>Int</span>. <span>Symp</span>. <span>Music</span> <span>Information</span> <span>Retrieval</span></em>.
</div>
<div id="ref-lu_automatic_2005" class="csl-entry" role="listitem">
Lu, L., Liu, D., &amp; Zhang, H.-J. (2005). Automatic mood detection and tracking of music audio signals. <em>IEEE Transactions on Audio, Speech, and Language Processing</em>, <em>14</em>(1), 5–18.
</div>
<div id="ref-mandel_support_2006" class="csl-entry" role="listitem">
Mandel, M. I., Poliner, G. E., &amp; Ellis, D. P. (2006). Support vector machine active learning for music retrieval. <em>Multimedia Systems</em>, <em>12</em>(1), 3–13.
</div>
<div id="ref-mandel2007labrosa" class="csl-entry" role="listitem">
Mandel, M., &amp; Ellis, D. (2007). LABROSA’s audio music similarity and classification submissions. In <em>MIREX 2007-Music Information Retrieval Evaluation eXchange</em>.
</div>
<div id="ref-mashao2003co" class="csl-entry" role="listitem">
Mashao, D. J. (2003). Comparing SVM and GMM on parametric feature-sets. <em>Proceedings of the 14th Annual Symposium of the Pattern Recognition Association of South Africa</em>, 27–28.
</div>
<div id="ref-mendel1969some" class="csl-entry" role="listitem">
Mendel, A. (1969). Some preliminary attempts at computer-assisted style analysis in music. <em>Computers and the Humanities</em>, 41–52.
</div>
<div id="ref-mollahosseini2017affectnet" class="csl-entry" role="listitem">
Mollahosseini, A., Hasani, B., &amp; Mahoor, M. H. (2017). Affectnet: A database for facial expression, valence, and arousal computing in the wild. <em>IEEE Transactions on Affective Computing</em>, <em>10</em>(1), 18–31.
</div>
<div id="ref-nag2022" class="csl-entry" role="listitem">
Nag, S., Basu, M., Sanyal, S., Banerjee, A., &amp; Ghosh, D. (2022). On the application of deep learning and multifractal techniques to classify emotions and instruments using indian classical music. <em>Physica A: Statistical Mechanics and Its Applications</em>, <em>597</em>. <a href="https://doi.org/10.1016/j.physa.2022.127261">https://doi.org/10.1016/j.physa.2022.127261</a>
</div>
<div id="ref-nguyen2017an" class="csl-entry" role="listitem">
Nguyen, V. L., Kim, D., Ho, V. P., &amp; Lim, Y. (2017). A new recognition method for visualizing music emotion. <em>International Journal of Electrical and Computer Engineering</em>, <em>7</em>(3), 1246–1254. <a href="https://doi.org/10.11591/ijece.v7i3.pp1246-1254">https://doi.org/10.11591/ijece.v7i3.pp1246-1254</a>
</div>
<div id="ref-orjesek2022en" class="csl-entry" role="listitem">
Orjesek, R., Jarina, R., &amp; Chmulik, M. (2022). End-to-end music emotion variation detection using iteratively reconstructed deep features. <em>Multimedia Tools and Applications</em>, <em>81</em>(4), 5017–5031. <a href="https://doi.org/10.1007/s11042-021-11584-7">https://doi.org/10.1007/s11042-021-11584-7</a>
</div>
<div id="ref-panda2020no" class="csl-entry" role="listitem">
Panda, R., Malheiro, R., &amp; Paiva, R. P. (2020). Novel audio features for music emotion recognition. <em>IEEE Transactions on Affective Computing</em>, <em>11</em>(4), 614–626. <a href="https://doi.org/10.1109/TAFFC.2018.2820691">https://doi.org/10.1109/TAFFC.2018.2820691</a>
</div>
<div id="ref-panda2020audio" class="csl-entry" role="listitem">
Panda, R., Malheiro, R., &amp; Paiva, R. P. (2023). Audio features for music emotion recognition: A survey. <em>IEEE Transactions on Affective Computing</em>, <em>14</em>(1), 68–88. <a href="https://doi.org/10.1109/TAFFC.2020.3032373">https://doi.org/10.1109/TAFFC.2020.3032373</a>
</div>
<div id="ref-panda2013multi" class="csl-entry" role="listitem">
Panda, R., Malheiro, R., Rocha, B., Oliveira, A. P., &amp; Paiva, R. P. (2013). Multi-modal music emotion recognition: A new dataset, methodology and comparative analysis. <em>10th International Symposium on Computer Music Multidisciplinary Research (CMMR 2013)</em>, 570–582.
</div>
<div id="ref-park2017representation" class="csl-entry" role="listitem">
Park, J., Lee, J., Nam, J., Park, J., &amp; Ha, J.-W. (2017). Representation learning using artist labels for audio classification tasks. <em>The 13th Music Information Retrieval Evaluation eXchange, MIREX</em>.
</div>
<div id="ref-picard_affective_1997" class="csl-entry" role="listitem">
Picard, R. (1997). <em>Affective <span>Computing</span></em>. MIT Press.
</div>
<div id="ref-saari_et_al_2015" class="csl-entry" role="listitem">
Saari, P., Eerola, T., Barthet, M., Fazekas, G., &amp; Lartillot, O. (2015). Genre-adaptive semantic computing and audio-based modelling for music mood annotation. <em>IEEE Transactions on Affective Computing</em>, <em>7</em>(2), 122–135.
</div>
<div id="ref-saizclar2022pr" class="csl-entry" role="listitem">
Saiz-Clar, E., Angel Serrano, M., &amp; Manuel Reales, J. (2022). Predicting emotions in music using the onset curve. <em>Psychology of Music</em>, <em>50</em>(4), 1107–1120. <a href="https://doi.org/10.1177/03057356211031658">https://doi.org/10.1177/03057356211031658</a>
</div>
<div id="ref-sanden2011empirical" class="csl-entry" role="listitem">
Sanden, C., &amp; Zhang, J. Z. (2011). An empirical study of multi-label classifiers for music tag annotation. <em>ISMIR</em>, 717–722.
</div>
<div id="ref-santana2020" class="csl-entry" role="listitem">
Santana, I. A. P., Pinhelli, F., Donini, J., Catharin, L., Mangolin, R. B., Feltrim, V. D., Domingues, M. A., et al. (2020). Music4all: <span>A</span> new music database and its applications. <em>2020 International Conference on Systems, Signals and Image Processing (<span>IWSSIP</span>)</em>, 399–404.
</div>
<div id="ref-sarkar2020recognition" class="csl-entry" role="listitem">
Sarkar, R., Choudhury, S., Dutta, S., Roy, A., &amp; Saha, S. K. (2020). Recognition of emotion in music based on deep convolutional neural network. <em>Multimedia Tools and Applications</em>, <em>79</em>(1), 765–783.
</div>
<div id="ref-schindler2017me" class="csl-entry" role="listitem">
Schindler, I., Hosoya, G., Menninghaus, W., Beermann, U., Wagner, V., Eid, M., &amp; Scherer, K. R. (2017). Measuring aesthetic emotions: A review of the literature and a new assessment tool. <em>PloS One</em>, <em>12</em>(6), e0178899.
</div>
<div id="ref-shmueli2010explain" class="csl-entry" role="listitem">
Shmueli, G. (2010). To explain or to predict? <em>Statistical Science</em>, <em>25</em>(3), 289–310.
</div>
<div id="ref-soleymani2013" class="csl-entry" role="listitem">
Soleymani, M., Caro, M. N., Schmidt, E. M., Sha, C.-Y., &amp; Yang, Y.-H. (2013). 1000 songs for emotional analysis of music. <em>Proceedings of the 2nd ACM International Workshop on Crowdsourcing for Multimedia</em>, 1–6. <a href="https://doi.org/10.1145/2506364.2506365">https://doi.org/10.1145/2506364.2506365</a>
</div>
<div id="ref-song2018audio" class="csl-entry" role="listitem">
Song, G., Ding, S., &amp; Wang, Z. (2018). Audio classification tasks using recurrent neural network. In <em>MIREX 2018-music information retrieval evaluation eXchange</em>.
</div>
<div id="ref-sorussa2020em" class="csl-entry" role="listitem">
Sorussa, K., Choksuriwong, A., &amp; Karnjanadecha, M. (2020). Emotion classi cation system for digital music with a cascaded technique. <em>ECTI Transactions on Computer and Information Technology</em>, <em>14</em>(1), 53–66. <a href="https://doi.org/10.37936/ecti-cit.2020141.205317">https://doi.org/10.37936/ecti-cit.2020141.205317</a>
</div>
<div id="ref-sun2017revisiting" class="csl-entry" role="listitem">
Sun, C., Shrivastava, A., Singh, S., &amp; Gupta, A. (2017). Revisiting unreasonable effectiveness of data in deep learning era. <em>Proceedings of the IEEE International Conference on Computer Vision</em>, 843–852.
</div>
<div id="ref-trohidis2008multi" class="csl-entry" role="listitem">
Trohidis, K., Tsoumakas, G., Kalliris, G., Vlahavas, I. P., et al. (2008). Multi-label classification of music into emotions. <em>ISMIR</em>, <em>8</em>, 325–330.
</div>
<div id="ref-tzanetakis2007marsyas" class="csl-entry" role="listitem">
Tzanetakis, G. (2007). Marsyas submissions to MIREX 2007. <em>MIREX 2007-Music Information Retrieval Evaluation eXchange</em>.
</div>
<div id="ref-van-aertwicherts_2016" class="csl-entry" role="listitem">
Van Aert, R. C., Wicherts, J. M., &amp; Assen, M. A. van. (2016). Conducting meta-analyses based on p values: Reservations and recommendations for applying p-uniform and p-curve. <em>Perspectives on Psychological Science</em>, <em>11</em>(5), 713–729.
</div>
<div id="ref-wang2021ac" class="csl-entry" role="listitem">
Wang, X., Wei, Y., Heng, L., &amp; McAdams, S. (2021). A cross-cultural analysis of the influence of timbre on affect perception in western classical music and chinese music traditions. <em>Frontiers in Psychology</em>, <em>12</em>. <a href="https://doi.org/10.3389/fpsyg.2021.732865">https://doi.org/10.3389/fpsyg.2021.732865</a>
</div>
<div id="ref-wang2022cr" class="csl-entry" role="listitem">
Wang, X., Wei, Y., &amp; Yang, D. (2022). Cross-cultural analysis of the correlation between musical elements and emotion. <em>COGNITIVE COMPUTATION AND SYSTEMS</em>, <em>4</em>(2, SI), 116–129. <a href="https://doi.org/10.1049/ccs2.12032">https://doi.org/10.1049/ccs2.12032</a>
</div>
<div id="ref-wiggins_semantic_2009" class="csl-entry" role="listitem">
Wiggins, G. A. (2009). Semantic gap?? <span>Schemantic</span> schmap‼ <span>Methodological</span> considerations in the scientific study of music. <em>2009 11th <span>IEEE</span> <span>International</span> <span>Symposium</span> on <span>Multimedia</span></em>, 477–482.
</div>
<div id="ref-xu2021us" class="csl-entry" role="listitem">
Xu, L., Sun, Z., Wen, X., Huang, Z., Chao, C., &amp; Xu, L. (2021). Using machine learning analysis to interpret the relationship between music emotion and lyric features. <em>PEERJ COMPUTER SCIENCE</em>, <em>7</em>. <a href="https://doi.org/10.7717/peerj-cs.785">https://doi.org/10.7717/peerj-cs.785</a>
</div>
<div id="ref-yang2021an" class="csl-entry" role="listitem">
Yang, Jing. (2021). A novel music emotion recognition model using neural network technology. <em>Frontiers in Psychology</em>, <em>12</em>. <a href="https://doi.org/10.3389/fpsyg.2021.760060">https://doi.org/10.3389/fpsyg.2021.760060</a>
</div>
<div id="ref-yang2023emoset" class="csl-entry" role="listitem">
Yang, Jingyuan, Huang, Q., Ding, T., Lischinski, D., Cohen-Or, D., &amp; Huang, H. (2023). Emoset: A large-scale visual emotion dataset with rich attributes. <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 20383–20394.
</div>
<div id="ref-yang2018review" class="csl-entry" role="listitem">
Yang, X., Dong, Y., &amp; Li, J. (2018). Review of data features-based music emotion recognition methods. <em>Multimedia Systems</em>, <em>24</em>, 365–389.
</div>
<div id="ref-yang2008" class="csl-entry" role="listitem">
Yang, Y. H., Lin, Y. C., Su, Y. F., &amp; Chen, H. H. (2008). A regression approach to music emotion recognition. <em>IEEE Transactions on Audio Speech and Language Processing</em>, <em>16</em>(2), 448–457.
</div>
<div id="ref-yang2011" class="csl-entry" role="listitem">
Yang, Y.-H., &amp; Chen, H. H. (2011). <em>Music emotion recognition</em>. CRC Press. https://doi.org/<a href="https://doi.org/10.1201/b10731">https://doi.org/10.1201/b10731</a>
</div>
<div id="ref-yang2007music" class="csl-entry" role="listitem">
Yang, Y.-H., Su, Y.-F., Lin, Y.-C., &amp; Chen, H. H. (2007). Music emotion recognition: The role of individuality. <em>Proceedings of the International Workshop on Human-Centered Multimedia</em>, 13–22.
</div>
<div id="ref-yeh2014po" class="csl-entry" role="listitem">
Yeh, C.-H., Tseng, W.-Y., Chen, C.-Y., Lin, Y.-D., Tsai, Y.-R., Bi, H.-I., Lin, Y.-C., &amp; Lin, H.-Y. (2014). Popular music representation: Chorus detection &amp; emotion recognition. <em>Multimedia Tools and Applications</em>, <em>73</em>(3), 2103–2128. <a href="https://doi.org/10.1007/s11042-013-1687-2">https://doi.org/10.1007/s11042-013-1687-2</a>
</div>
<div id="ref-zaripov1969" class="csl-entry" role="listitem">
Zaripov, R. K., &amp; Russell, J. (1969). Cybernetics and music. <em>Perspectives of New Music</em>, 115154.
</div>
<div id="ref-zentner2008emotions" class="csl-entry" role="listitem">
Zentner, M., Grandjean, D., &amp; Scherer, K. R. (2008). Emotions evoked by the sound of music: Characterization, classification, and measurement. <em>Emotion</em>, <em>8</em>(4), 494–521.
</div>
<div id="ref-zhang2017fe" class="csl-entry" role="listitem">
Zhang, J. L., Huang, X. L., Yang, L. F., Xu, Y., &amp; Sun, S. T. (2017). Feature selection and feature learning in arousal dimension of music emotion by using shrinkage methods. <em>Multimedia Systems</em>, <em>23</em>(2), 251–264. <a href="https://doi.org/10.1007/s00530-015-0489-y">https://doi.org/10.1007/s00530-015-0489-y</a>
</div>
<div id="ref-zhang2016br" class="csl-entry" role="listitem">
Zhang, J., Huang, X., Yang, L., &amp; Nie, L. (2016). Bridge the semantic gap between pop music acoustic feature and emotion: Build an interpretable model. <em>Neurocomputing</em>, <em>208</em>(SI), 333–341. <a href="https://doi.org/10.1016/j.neucom.2016.01.099">https://doi.org/10.1016/j.neucom.2016.01.099</a>
</div>
<div id="ref-zhangPMEmo2018" class="csl-entry" role="listitem">
Zhang, K., Zhang, H., Li, S., Yang, C., &amp; Sun, L. (2018). The <span>PMEmo</span> dataset for music emotion recognition. <em>Proceedings of the 2018 <span>ACM</span> on <span>International Conference</span> on <span>Multimedia Retrieval</span></em>, 135–142. <a href="https://doi.org/10.1145/3206025.3206037">https://doi.org/10.1145/3206025.3206037</a>
</div>
<div id="ref-zhang2023mo" class="csl-entry" role="listitem">
Zhang, M., Zhu, Y., Zhang, W., Zhu, Y., &amp; Feng, T. (2023). Modularized composite attention network for continuous music emotion recognition. <em>Multimedia Tools and Applications</em>, <em>82</em>(5), 7319–7341. <a href="https://doi.org/10.1007/s11042-022-13577-6">https://doi.org/10.1007/s11042-022-13577-6</a>
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p><span class="citation" data-cites="chen2017co">Chen et al. (<a href="#ref-chen2017co" role="doc-biblioref">2017</a>)</span> was the only study of those included to use Gaussian Mixture Models. We decided to group this with Support Vector Machines as they have been reported to perform similarly on mid-sized data sets <span class="citation" data-cites="mashao2003co">(<a href="#ref-mashao2003co" role="doc-biblioref">Mashao, 2003</a>)</span>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Values reported as <span class="math inline">\(R^2\)</span> in original study.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/tuomaseerola\.github\.io\/metaMER\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../analysis/analysis.html" class="pagination-link" aria-label="Analysis">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Analysis</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../manuscript/datasets.html" class="pagination-link" aria-label="Common datasets">
        <span class="nav-page-text">Common datasets</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/tuomaseerola/metaMER/blob/main/manuscript/manuscript.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/tuomaseerola/metaMER/edit/main/manuscript/manuscript.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/tuomaseerola/metaMER/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/tuomaseerola/">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>