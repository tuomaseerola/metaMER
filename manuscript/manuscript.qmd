---
title             : "A Meta-Analysis of Music Emotion Recognition Studies"
shorttitle        : "Meta-analysis of music emotion recognition"

author: 
  - name          : "Tuomas Eerola"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Palace Green, Department of Music, Durham University, DH1 3RL Durham, United Kingdom"
    email         : "tuomas.eerola@durham.ac.uk"
  - name          : "Cameron J. Anderson"
    affiliation   : "2"
    address       : "Department of Psychology, Neuroscience & Behaviour, McMaster University, Hamilton, ON, Canada"
    email         : "andersoc@mcmaster.ca"

affiliation:
  - id            : "1"
    institution   : "Department of Music, Durham University"
  - id            : "2"
    institution   : "Department of Psychology, Neuroscience & Behaviour, McMaster University"

author_note: |
  Tuomas Eerola is at the Department of Music, Durham University, UK.
  Cameron J. Anderson is at the Department of Psychology, Neuroscience & Behaviour, McMaster University, Canada.

abstract: |
 This meta-analysis examines music emotion recognition (MER) models published between 2014 and 2024, focusing on predictions of valence, arousal, and categorical emotions. A total of 553 studies were identified, of which 96 full-text articles were assessed, resulting in a final review of 34 studies. These studies reported 204 models, including 86 for emotion classification and 204 for regression. Using the best-performing model from each study, we found that valence and arousal were predicted with reasonable accuracy (r = 0.67 and r = 0.81, respectively), while classification models achieved an accuracy of 0.xx. Across modeling approaches, linear and tree-based methods generally outperformed neural networks in regression tasks, whereas neural networks and support vector machines (SVMs) showed superior performance in classification tasks. We highlight key recommendations for future MER research, emphasizing the need for greater transparency, feature validation, and standardized reporting to improve comparability across studies.  
keywords          : "music, emotion, recognition, meta-analysis"
wordcount         : "5555"

#bibliography      : ["references.bib"]

figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
mask              : no
csl: apa7.csl

class             : "man"
output            : papaja::apa6_pdf
bibliography: references.bib
---

```{r global options, include = FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```

```{r include = FALSE, echo=FALSE}
#source('load_libraries.R')
## set options ##
```

# Introduction

Emotional engagement is a key reason why people engage with music in their every day activities, and it is also why music is increasingly being used in various health applications [@juslin2022emotions;@agres2021music]. In recent years, significant advances have been made in music information retrieval, particularly in music emotion recognition (MER) tasks [@gomez2021;@panda2020audio]. Improvements in available features, modeling techniques, and datasets have provided the field with opportunities to enhance the accuracy and reliability of predicting annotated emotions from audio. Over the past 25 years, numerous studies have established the types of emotions that listeners perceive and recognize in music. In the last 15 years, research has increasingly focused on tracing these recognized emotions back to specific musical components, such as expressive features [@lindstrom2003expressivity], structural aspects of music [@eerola_friberg_bresin_2013; @anderson2022ex; @grimaud_eerola_2022], acoustic features [@yang2008; @panda2013multi; @saari_et_al_2015; @eerola2011c; @panda2020audio], or emergent properties revealed through deep learning techniques [@er2019music; @sarkar2020recognition].

However, there is no consensus on the extent to which emotions can be accurately recognized by computational models. The current literature presents a diverse and mixed picture regarding the success of models in predicting emotions within the affective circumplex -- valence and arousal-- [@Russell1980] and in classifying distinct emotion categories [@fu2010survey].

# A brief history of MER

Emotion has been widely discussed since the earliest artificial intelligence (AI) applications to music in the 1950s. Whereas early discourse largely focused on generative composition using computers [@zaripov1969], attention later shifted to creating methods to predict emotion using music's structural cues. Novel techniques for information retrieval emerged in the 1950s and 1960s [@fairthorne1968], inspiring analogous developments for automated music analysis (@kassler1966toward; @mendel1969some). These developments would set the stage for early work in music emotion recognition (MER). Katayose et al. (1988) conducted the first study of this nature, creating an algorithm that associated emotions with analyzed chords to generate descriptions like "there is hopeful mood on chord from 69 to 97 \[*sic*\]." [@katayose_sentiment_1988, p. 1087].

In the early 2000s, several research groups conducted studies using regression [@friberg_automatic_2002; @liu_automatic_2003] and classification [@lu_automatic_2005; @feng_popular_2003; @mandel_support_2006] techniques to predict emotion in music audio or MIDI. Citing "MIR researchers' growing interest in classifying music by moods" [@downie_music_2008, p. 1], the Music Information Retrieval EXchange (MIREX) introduced Audio Mood Classification (AMC) to their rotation of tasks in 2007. In the first year, nine systems classified mood labels in a common data set, reaching 52.65% accuracy (SD = 11.19%). These events, along with growing interest in the burgeoning field of affective computing, would lead to an explosion of interest in MER research.[^1]

[^1]: The best-performing model to date reached 69.83 % in the 2017 competition [@park2017representation].

Researchers have assessed regression and classification techniques on diverse corpora with features drawn from music (e.g., audio, MIDI, metadata) and participants (e.g., demographic information, survey responses, physiological signals, etc.). In one widely-cited study, Yang (2008) approached MER as a regression task, predicting the valence (i.e., the negative---positive emotional quality) and arousal (i.e., the calm---exciting quality) of 195 Chinese pop songs, achieving 62% accuracy for arousal but only 28% for valence [@yang2008]. This difference in prediction accuracy between dimensions has reappeared in several subsequent studies [e.g., @bai2016dimensional; @coutinho2013psychoacoustic], with some research suggesting this challenge reflects fewer well-established predictors and more individual differences for valence than arousal [@yang2007music; @eerola2011c].

# The semantic gap in MER

The difficulty in predicting valence reflects a broader challenge in information retrieval. Specifically, relations between low-level predictors from music and text and the perceptual phenomena they model remain poorly understood, reaching a ceiling in prediction accuracy [@celma_foafing_2006]. To address this so-called *semantic gap*, researchers have attempted to identify new feature sets with greater relevance to emotion [@panda2020audio; @chowdhury2021perceived], combine low-, mid-, and high-level features using multimodal data [@celma_foafing_2006], or train neural networks to automatically learn features from audio [@zhang_bridge_2016]. Through these approaches, MER researchers attempt to shatter a so-called *glass ceiling* [@downie_music_2008] by establishing central predictors for emotions. To date, however, no study has systematically compared results of these diverse approaches.

## Aims

Our aim is to evaluate the predictive accuracy of two models of emotional expression in music: (a) models that predict track-specific coordinates in affective circumplex space (valence and arousal), and (b) models that classify discrete emotion categories. We focus on recent to identify key factors such as modeling techniques and features that significantly affect  prediction accuracy. To achieve this, we conduct a meta-analysis of journal articles published in the past 10 years. Based on existing literature, we hypothesize that arousal will be predicted with higher accuracy than valence, as valence tends to be more context-dependent and challenging to model. For emotion classification, we expect simple utilitarian emotions (e.g., fear, anger) will be easier to predict than more complex social emotions (e.g., sadness, nostalgia).

# Methods

We preregistered the meta-analysis plan on 21 June 2024 at OSF, <https://osf.io/c5wgd>, and the plan is also available at <https://tuomaseerola.github.io/metaMER/preregistration/preregistration.html>).

In the search stage, we used three databases, *Web of Science*, *Scopus*, and *Open Alex* to identify journal articles published between 2014 and 2024 containing keywords/title `valence OR arousal OR classi* OR categor* OR algorithm AND music AND emotion AND recognition` (see specific search strings for each database in [SI](http://)). All searches were done in May 2024.

The initial search yielded 553 potential studies after excluding duplicate entries. We interactively screened them for relevance in three stages, resulting in 46 studies that passed our inclusion criteria (music emotion studies using classification or regression methods to predict emotion ratings of music using symbolic or audio features, and containing sufficient detail to convert results to $r$ or $MCC$ values see [SI](http://) for a breakdown). After the screening stage, we defined a set of entities to extract characterising (i) music (genre, stimulus number \[N\], duration), (ii) features extracted (number, type, source, defined by [@panda2020audio]), (iii) model type (regression, neural network, SVM, etc.) and outcome measure ($R^2$, *MSE*, *MCC*), (iv) model complexity (i.e., approximate number of features used to predict ratings), and (v) type of model cross-validation.

We converted all regression results from $R^2$ values into $r$ values for valence and arousal, and classification results into Matthews correlation coefficient [*MCC*, @chicco2020advantages]. To increase consistency in our analyses, we excluded studies using incompatible features (e.g., spectrograms of audio files [@nag2022]), or dependent variables (e.g., one regression study analyzed valence and arousal together, but not separately [@chin2018]).

{{< include _figure1.qmd >}}

## Quality Control

The search yielded studies of variable (and occasionally questionable) quality. To mitigate potentially spurious effects resulting from the inclusion of low-quality studies, [we excluded studies lacking sufficient details about stimuli, analyzed features, or model architecture](include%20approx%20#%20for%20each?%20This%20would%20be%20good%20since%20the%20rest%20of%20this%20para%20already%20mentions%20counts.%20If%20this%20take%20a%20long%20time,%20we%20could%20save%20this%20for%20an%20additional%20analysis). Finally, we excluded studies published in journals of questionable relevance/quality, (e.g., *Mathematical Problems in Engineering* ceased publication following 17 retractions published between July and September 2024). Overall this step eliminated 12 studies, leaving us with 34 studies in total.

## Study Encoding

To capture key details of each study, we added additional fields to BibTeX entries for each study. Fields included information about the genre/type of stimuli employed, along with their duration and number; the number of analyzed features; and the model type -- Neural Nets (NN), Support Vector Machines (SVM), Linear Methods (LM), Tree-based Methods (TM), KS, Add. and KNN Methods (KM) -- validation procedure and output measures. Additionally, we included study results using executable *R* code containing custom functions for meta-analysis. For complete details about our encoding procedure, see `studies/extraction_details.qmd` .

# Results

First we describe the overall pattern of data (regression vs classification, modelling techniques, feature numbers, stimulus numbers, datasets, and other details).

TABLE 1: Summary of data (part of `analysis/preprocessing.qmd` updated 21/1/2025)

|            | Regression                      | Classification            | Total |
|:-----------|:--------------------------------|:--------------------------|:------|
| Study N    | 22                              | 12                        | 34    |
| Model N    | 204                             | 86                        | 290   |
| Techniques | Neural Nets: 66                 | 25                        | 91    |
| Techniques | Support Vector Machines: 50     | 26                        | 76    |
| Techniques | Linear Methods: 62              | 13                        | 75    |
| Techniques | Tree-based Methods: 14          | 10                        | 34    |
| Techniques | KS, Add. & KNN: 12              | 12                        | 24    |
| Feature N  | Min=3, Md=605, Max=6670         | Min=8, Md=126, Max=8904   | NA    |
| Stimulus N | Min=20, Md=324, Max=2486        | Min=124, Md=300, Max=5192 | NA    |

Although the total number of studies meeting the criteria described in the previous section is modest (34 in total), they encompass a large array of models (290 in total) with a relatively even distribution among the three most popular techniques: Neural Nets, SVMs, and Linear Methods. The number of features and stimuli within these studies varies significantly, ranging from as few as three features [@battcock2021in] to a maximum of almost 9000 features [@zhang2023mo]. The median number of features differs between regression (605) and classification (126) studies, primarily reflecting the nature of the datasets used in each approach. The number of stimuli is typically around 300-400 (with a median of 324 for regression and 300 for classification), though there is substantial variation, with the extremes from 20 stimuli in @beveridge2018po to 5192 stimuli in @alvarez2023ri. There are also additional dimensions to consider, such as the type of cross-validation used, the music genres analyzed (whether a single genre, multiple genres, or a mix), the type of journal in which the studies were published, and the extraction tool used to extract features. However, these variables do not lend themselves to a simple summary, so we will revisit them during the interpretation and discussion stages.

We first report regression studies that predict valence and arousal.

## Prediction success for affect dimensions

See `analysis/analysis.qmd` (updated 17/1/2025)

Since there are many models contained within each of the studies, we will report the results in two parts; We first give an overview of the results for all models, and then we focus on the best performing models of each study. The best performing model is the model within each study with the highest correlation coefficient. This reduction is done to avoid the issue of multiple models from the same study deflating the results as majority of the models included are relative modest baseline or alternative models that do not represent the novelty or content of the article. 

### Results for valence

Table 2 summarises the results for all models (All) as well as best performing models (Max) for each study for valence. The summary includes the number of models and observations, the correlation coefficient and its 95% confidence interval, the t-value and p-value for the correlation, the heterogeneity statistics $\tau^2$ and $I^2$, calculated through appropriate transformations (Fisher's Z) for the correlation coefficient as part of a random-effects model using `meta` library [@balduzzi2019]. We used Paule-Mandel estimator for between-study heterogeneity [@langan2019comparison] and Knapp-Hartung [@knapp2003improved] adjustments for confidence intervals. In this table we also report two subgroup analyses. One where we have divided the studies according to the number of features they contain (three categories based on quantiles to keep the group size comparable) and into five modelling techniques introduced earlier (Table 1).

Table 2. Meta-analytic diagnostic for all regression studies predicting valence from audio. See Table 1 for the acronyms of the modelling techniques.

| Concept      | Models, obs | $r$ \[95%-CI\]          | $t$   | $p$   | $\tau^2$ | $I^2$ |
|:-------------|:------------|:------------------------|:------|:------|:---------|:------|
| Valence All  | 102,60017   | 0.583 \[0.541-0.623\]   | 21.40 | .0001 | 0.094    | 97.7% |
| Valence Max  | 22,14172    | 0.669 \[0.560-0.755\]   | 9.58  | .0001 | 0.148    | 98.4% |
| *N Features* |             |                         |       |       |          |       |
| \<18 F       | 5,3036      | 0.811 \[0.542-0.823\]   | \-    | \-    | 0.182    | 98.9% |
| 18-260 F     | 11,7042     | 0.584 \[0.534-0.931\]   | \-    | \-    | 0.133    | 97.6% |
| 260+ F       | 5,3494      | 0.710 \[0.542-0.823\]   | \-    | \-    | 0.044    | 97.2% |
| *Techniques* |             |                         |       |       |          |       |
| KS           | 1,1838      | 0.466 \[-0.634-0.942\]  | \-    | \-    | 0.0185   | 95.1% |
| LM           | 8,1762      | 0.797 \[0.614-0.899\]   | \-    | \-    | 0.1370   | 96.4% |
| SVM          | 6,4993      | 0.656 \[0.484-0.779\]   | \-    | \-     | 0.0574  | 96.9% |
| NN           | 4,2249      | 0.393 \[-0.355-0.835\]  | \-    | \-     | 0.0761  | 97.1% |
| TM           | 3,3330      | 0.750 \[0.292-0.928\]   | \-    | \-     | 0.093   | 98.5% |

<!-- table updated TE 21/1/2025, revised terms 28/1/2025 -->

The results indicate that valence can generally be predicted with moderately accuracy, with the best model from each of the 22 studies achieving an average correlation of _r_ = 0.669 (95% CI: 0.560-0.755), called "valence Max" in Table 2. However, when considering all models across these studies (n = 102), the overall prediction rate drops significantly to _r_ = 0.583. We argue that this lower correlation is likely due to the inclusion of baseline models reported in these studies, which may not reflect the true success of the task for the purposes of our analysis.

Further analysis of between-study heterogeneity, as indexed by the $\tau^2$ (0.148) and Higgins & Thompson's $I^2$ statistic [@higgins2002quantifying] at 98.4%, reveals substantial heterogeneity. Since $I^2$ is heavily influenced by study size (with larger N leading to lower sampling error), its value may be less insightful in this context. In contrast, $\tau^2$, which is less sensitive to the number of studies and directly linked to the outcome metric (_r_), provides a more reliable measure of heterogeneity in this case. Also, we note that because the overall heterogeneity in the data is high, we are cautious in our interpretation of the publication bias [@van-aertwicherts_2016].

To better understand the effects across studies and the nature of the observed heterogeneity, Figure 2 presents (A) a forest and (B) funnel plot of the random-effects model, based on the best-performing models from all studies. In terms of the forest plot, the range of prediction values (correlations) is broad, spanning from 0.13 to 0.92, with all studies except Koh et al. (2023) demonstrating evidence of positive correlations. A mean estimate of 0.67 is achieved by 15 out of the 22 models. While the confidence intervals are generally narrow due to the large sample sizes in each study, there are exceptions, such as smaller sample sizes in @beveridge2018po (n = 20), and in @griffiths2021am (n = 40). The funnel plot in panel B of Figure 2 shows clustering at the top of the plot (studies with low standard error) and no assumed larger diversity in the correlations when the error rates increase. However, there is no clear asymmetry in the plot, verified by non-significant Egger's test ($\beta$ = 5.05, CI95% -0.99-11.09, _t_ = 1.64, _p_ = 0.112, @eggersmith_1997). Coming back to the mean of valence correlation of 0.669 by all studies and the possible impact of study heterogeneity on this estimation, we also calculated the correlation without the studies that lie outside the 95% CI for pooled effect. This left 12 studies in the data and resulted in the meta-analytical pooled correlation of 0.686 (CI95% 0.635-0.731). In other words, despite the large variation in the correlations and standard errors across the studies, this variation in itself does not seem to be a significant driver behind the overall effect.

<!-- numbers in the text updated up to here, TE 21/1/2025 -->

{{< include _figure2.qmd >}}

To gain insights into the factors contributing to the wide range of model success, we explored several ways of splitting the data. Table 2 presents two key splits: one based on the number of features used, which we hypothesized might influence model performance, and another based on the modeling techniques employed. In terms of feature sets, we categorized them into three groups: few features (<18), a large number of features (18–260), and massive feature sets (260+). Interestingly, models (4 in total) using a relatively small number of features (<18) performed best (_r_ = 0.817, 95% CI: 0.361–0.958) compared to those utilizing larger feature sets. However, it is worth noting that the models using massive feature sets (260+, 7 studies in total) also performed well (_r_ = 0.702), achieving better and more consistent results than the overall prediction rate (_r_ = 0.669). This observation is supported by the lowest heterogeneity index for the massive feature set group ($\tau^2$ = 0.029), indicating more consistent results across studies. Studies with large number of features (18-260 features, 10 studies in total) delivered the worst results, _r_ = 0.586 (95% CI: 0.360–0.748). Despite the fluctuation in the overall model accuracy between the number of features, the differences are not substantially large to pass the test of statistical significance (_Q_(2) = 3.47, _p_=.176).

<!-- numbers in the text updated up to here, TE 21/1/2025 -->

When analyzing the studies across the five modeling techniques used, the prediction do differ significantly (_Q_(4) = 45.7, _p_ < .0001). Notably, linear models (LM) and Support Vector Machines (SVM) were the most common, with 8 and 6 studies, respectively, allowing for more confident interpretations. Linear models achieved the highest prediction rate (_r_ = 0.784, 95% CI: 0.625–0.881), though this may be influenced by the smaller datasets typically used in these studies. These studies also exhibited substantially higher heterogeneity ($\tau^2$ = 0.137) compared to other techniques, where $\tau^2$ values were less than half of this. While there were only 3 studies involving tree-based model (TM), these performed well, achieving _r_ = 0.750, 95% CI: 0.292–0.928), and the relatively poor performance of the neural network (NN) models represented in four studies (_r_ = 0.340, 95% CI: -0.097–0.668) is difficult to explain without a deeper examination of the specific model architectures and the stimuli used in these studies. Kernel smoothing and KNN models (with only 1 study represented) achieved relatively poor success as well, _r_ = 0.383, 95% CI: 0.348–0.426).

We also ran the sub-grouping analyses across a combination of stimulus genres (single vs mixed) and number of the features to explore where the differences in the model prediction rates might lie. For this purpose, we grouped the studies into small single genre/multigenre studies, medium single genre/multigenre studies, and medium-large single genre/multigenre studies, and huge multigenre studies. The small single genre/multigenre studies generally performed best (_r_ = 0.836), followed by huge multi-genre studies (_r_ = 0.693) while the medium and medium-large sized studies performance was between close the overall average (medium-large, _r_ = 0.768 and medium _r_ = 0.607). The heterogeneity was lowest in the huge multigenre studies ($\tau^2$ = 0.013) and highest in the small single genre/multigenre studies ($\tau^2$ = 0.318). 

<!-- However, these analyses did not reveal any clear patterns, likely due to the small number of studies in each subgroup.  -->

<!-- see this to ignore I\^2 and rely on prediction interval: https://onlinelibrary.wiley.com/doi/full/10.1002/jrsm.1678 -->

These comparisons of sub-groupings are also influenced by other factors, such as the type of journal (psychology vs engineering) or whether the objective is to explain or predict emotions. Although the sub-groupings result in an uneven distribution of studies and observations, they still offer valuable insights. Despite these caveats, the two main sub-groupings portrayed in Table 2 enable us to identify valuable differences related to model success across the studies.

<!-- numbers in the text updated up to here, TE 21/1/2025 -->

### Results for arousal

Moving on the arousal, we carry out the same meta-analytical analysis applying the random-effects model to arousal. Table 3 describes the broad pattern of results in tabular format, and Figure 3 illustrates the spread and heterogeneity of all studies for arousal. The overall correlation across the studies using the best performing model out of each study (Max) is 0.809 (CI95% 0.740-0.860). If we examine all the models reported in each study, the correlation drops marginally, to 0.791 (CI95% 0.770-0.810), despite this analysis includes about four times as many models as taking the best model out of each study. For arousal, even the baseline models seem to be performing on a relative high level. However, the indicators of heterogeneity are again high ($\tau^2$ = 0.141 and $I^2$=97.9%), which suggests that summary may be misleading. However, the analysis of asymmetry does not reveal significant issues (Eggers test, $\beta$ = 0.789 95%CI -4.87-6.45, _t_ = 0.273, _p_ = 0.788). If we remove the studies that are outside the 95%CI in heterogeneity, leaves this 13 studies in the summary that has _r_ = 0.826 (95%CI 0.806-0.845) with $\tau^2$ = 0.0042 and $I^2$ = 76.8%. In other words, no material difference to the results obtained with all 22 studies.

Table 3. Meta-analytic diagnostic for all regression studies predicting arousal from audio.

| Concept      | Models, obs | $r$ \[95%-CI\]            | $t$  | $p$    | $\tau^2$ | $I^2$ |
|:-------------|:------------|:--------------------------|:-----|:-------|:---------|:------|
| Arousal All  | 102, 60017  | 0.791 \[0.770-0.810\]     | 39.9 | 0.0001 | 0.069    | 96.2% |
| Arousal Max  | 22, 14172   | 0.809 \[0.740-0.860\]     | 13.6 | 0.0001 | 0.141    | 97.9% |
| *N Features* |             |                           |      |        |          |       |
| \<18 F       | 4, 3016     | 0.817 \[0.361-0.958\]     |      |        | 0.2295   | 99.2% |
| 18-260 F     | 10, 4222    | 0.586 \[0.356-0.748\]     |      |        | 0.1542   | 96.9% |
| 260+ F       | 7, 6190     | 0.702 \[0.612-0.774\]     |      |        | 0.0285   | 96.6% |
| *Techniques* |             |                           |      |        |          |       |
| KS           | 1, 1838     | 0.819 \[0.803-0.833\]     |      |        |  -       | -     |
| LM           | 8, 1762     | 0.881 \[0.808-0.928\]     |      |        | 0.0846   | 93.3% |
| SVM          | 6, 4993     | 0.808 \[0.644-0.901\]     |      |        | 0.1125   | 98.0% |
| NN           | 4, 2249     | 0.533 \[0.328-0.691\]     |      |        | 0.0190   | 85.8% |
| TM           | 3, 3330     | 0.809 \[0.733-0.864\]     |      |        | 0.0025   | 65.4% |

<!-- numbers in the text updated up to here, TE 22/1/2025 -->

<!-- Schwarzer and colleagues (Schwarzer, Carpenter, and Rücker 2015, chap. 4.3) mention, as a general rule of thumb, that subgroup analyses only make sense when your meta-analysis contains at least 
K= 10 studies.-->

Figure 3 presents (A) a forest and (B) funnel plot of the random-effects model of the best-performing models from all studies. Similarly to valence, the range of correlations is also wide for arousal, ranging from 0.35 to 0.95, with all studies demonstrating evidence of positive correlations. A mean estimate of 0.81 or higher is achieved by the majority (15 out of the 22 models). Due to large sample in most studies, the confidence intervals are narrow, although the exceptions ($N < 55$) are clearly visible [@beveridge2018po; @griffiths2021am;@koh2023me; @saizclar2022pr; @wang2021ac]. The funnel plot in panel B illustrates the limited range of the correlations and show heavy clustering at the top of the plot (studies with low standard error). There is no clear asymmetry in the plot, verified by non-significant Egger's test reported above.

The analysis of the subdivision of studies shows that there is no significant differences between the studies using different number of features (_Q_(2) = 3.47, _p_ = .176) despite the differing means (_r_ = 0.817 for studies with less than 18 features, _r_ = 0.586 for 18 to 260 features, and _r_ = 0.702 for studies utilising over 260 features). The differences in the techniques, however, does show statistically significant variance between subgroups (_Q_(4) = 50.47, _p_ < .0001). The Neural Nets (NN) achieve poor prediction of arousal (_r_ = 0.533) in comparison to other techniques. The caveat of this subgroup analysis is the small number of observations for five techniques. When other subgroupings were explored, such as the type of journal (Engineering with 13 studies vs Psychology with 9 studies), no significant differences were observed (_Q_(1) = 0.94, _p_ = .333). 

{{< include _figure3.qmd >}}


## Classification studies

Summary of details contained in Table 1, but summarise at least the categories predicted before moving onto the main findings.

Table 4. Meta-analytic diagnostic for all classification studies predicting emotion categories from audio.

| Model | Models, obs | $r$ \[95%-CI\]            | $t$  | $p$    | $\tau^2$ | $I^2$ |
|:------|:------------|:--------------------------|:-----|:-------|:---------|:------|
| All   | 89,87347    | 0.8074 \[0.7681; 0.8407\] | 21.4 | 0.0001 | 0.2415   | 99.7% |
| Max   | 14,17184    | 0.8564 \[0.7386; 0.9234\] | 8.32 | 0.0001 | 0.329    | 99.8% |

<!-- | All Trim | 29,6499     | 0.8185 \[0.8046; 0.8314\] | 58.3 | 0.0001 | 0.0066   | 60.4% | -->

<!-- | Max Trim | 6,3653      | 0.8689 \[0.7760; 0.9249\] | 11.6 | 0.0001 | 0.0749   | 97.5% | -->

Heterogeneity issues

Figure 3. Forest plot of arousal prediction (Max?) (Unless we do some custom plotting)

-   Figure Optional: Funnel plot (I haven't seen this yet)


#### Notes to results: In the pre-registration, we promised the following:

- groupings based on genre, model complexity (high, medium, low), model validation (exists or not)
- we promised to carry out sensitivity analyses based on type of journal the studies were published in.



# Conclusion and Discussion

<!-- first para is your text from ICMPC paper -->

Research on Music Emotion Recognition has steadily increased in popularity since the early 2000s, with technological advancements facilitating sharing of data sets, analysis tools, and music stimuli. Public forums like the Music Information Retrieval Exchange (MIREX) have facilitated collaborations between computer scientists, musicologists, and psychologists alike—spurring improvements in performance. Despite increasing complexity in the types of models and data sets used, no extant study has evaluated the relative merits of the tools, stimuli, and feature sets employed in MER studies. To provide a benchmark to guide future research, we present the first meta-analysis of music emotion recognition. 

<!-- Concise summary of what we did and found -->

We initially identified 96 studies involving MER models, but in many cases the reporting standards or other quality issues or the lack of dedicated focus on music led us to include 34 studies in the analysis that overall reported 290 models, 204 related to regression and 86 classification. Comparing the most accurate model in each study, we observed reasonably accurate prediction of valence (_r_ = 0.67 [0.56, 0.76]), and arousal (_r_ = 0.81 [0.74, 0.86]) in regression studies. For both affective dimensions of the circumplex model, linear methods (valence _r_ = 0.78, arousal _r_ = 0.88) and tree-based models (valence _r_ = 0.75, arousal _r_ = 0.81) outperformed neural networks (valence = 0.47, arousal = 0.66) and support vector machines (SVMs) (valence _r_ = 0.58, arousal _r_ = 0.79). In contrast, neural networks performed most accurately in classification studies (_MCC_ = 0.93), followed by SVMs (_MCC_ = 0.87) and tree-based models (_MCC_ = 0.85). The models applied to a variety of datasets differ substantially in terms of size, annotation principles, and the number of features and sources of these features.

These results offer several important insights. First, compared to a 2013 report by @barthet2013, the glass ceiling for valence has risen from _r_ = 0.51 to 0.67. Conversely, predictive accuracy for arousal has shown no improvement (reported already in 2013 at _r_ = 0.83). For classification, studies reached variable classification rates from 0.497 using _F~1~_ score [@sanden2011empirical] to 79% average precision [@trohidis2008multi] in the past, whereas now the accuracy is 0.930 (?).

<!-- These numbers above could be reinterpreted here, they are not comparable with our MCC -->

Second, there is no single modelling technique that seems to arise on top, although linear models and random forests perform best in regression studies, and neural networks and support vector machines excel in classification tasks. We note that the model accuracy is surprisingly little affected by the number of features or the dataset size, likely the result of cross-validation techniques used to avoid overfitting, but the heterogeneity of the materials used in different studies may also mask substantial differences. For instance, the smaller datasets tend to use linear models and deliver higher model accuracy than larger datasets and those with a large number of features. We surmise that this might relate to disciplinary difference, where the smaller datasets come from music psychology, which puts a premium on data quality (quality control of the ground-truth data and features) rather than on data size and modelling techniques. If quality control is succesful, it decreases the random error inherent in human judgements. Furthermore, the relevance and the precision of the features impacts the accuracy of the models. Unfortunately, in many cases, it is impossible to attribute the sources of error, as the studies so far have not compared the feature sets systematically, nor a comparison of the datasets with identical features has been carried out. In the future, it would be advantageous to systematically assess these sources of error in MER studies to allow us to focus on where significant improvements can be made.

More broadly, the present study revealed an uncomfortable large variation in overall quality control and reporting in MER studies. In many cases, the reporting was insufficient to determine the features or their sources used in the models, and a large number of studies were discarded due to insufficient information about the data, model architectures, or outcome measures.

<!-- recommendations 1: reporting --> 

Future reports should contain viable information about the data, models, and success metrics. The modelling process should include a description of cross-validation, feature extraction technique, feature reduction (if used), and actual accuracy measures. We would recommend future studies to use Matthews correlation coefficient for classification studies as currently there are numerous metrics (_F~1~_, precision, recall, balanced accuracy, etc.), which do not operate in the same range.

<!-- feature validation? -->

Feature validation...? (Two extreme positions: a limited number of features that are perceptual validated (psychology) -- domain knowledge –– vs a large number of features where the models identify what is needed (deep learning/engineering).

<!-- recommendations 2: sharing --> 

It would be beneficial to share models and features transparently. If a study relies on published datasets, as most do, sharing the actual model (as code/notebooks) and features would allow for more direct comparison of model techniques for the same dataset. Also, not all datasets are shared, or at least the sharing is only done partially. The copyright restrictions may limit the sharing of some datasets, but at least features and examples should be available.

<!-- size, type of data --> 
We also note that the majority of the datasets used in MER studies are relatively small, but comparable to datasets for modeling facial expressions (median N=502, @krumhuber2017) and speech expressions (median of 1287, @hashem2023). However, annotated datasets for visual scenes and objects tend to be much larger, often exceeding 100,000 annotated examples [@deng2009imagenet; @krishna2017visual]. Datasets of these magnitudes seem to be required for appropriate utilisation of deep learning algorithms, which may explain the modest results observed in the music emotion recognition studies.

The majority of the datasets are Western pop ... Music and annotators that represent the Global North do dominate the studies at the moment, and ... [@bornDiversifyingMIRKnowledge2020].

Are the emotion recognition targets valid from the psychological and application point of view? Prediction of valence and arousal is popular, but has limited usefulness in practical applications that aim to capture experiences with music that are more aligned with models of music-induced emotions (such as GEMS by @zentner2008emotions). 

Future efforts to prioritize feature validation, standardized reporting, and transparent sharing of research materials are needed for consistent improvements in MER. 

### Funding statement

CA was funded by Mitacs Globalink Research Award (Mitacs & British High Commission - Ottawa, Canada).

### Competing interests statement

There were no competing interests.

### Open practices statement

Study preregistration, data, analysis scripts and supporting information is available at Github, <https://tuomaseerola.github.io/metaMER>.

# References
