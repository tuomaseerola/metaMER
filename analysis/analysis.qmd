# Analysis

This assumes that the data has been parsed (`parse-model-output.R`, `format-study-results.R`) and preprocessed (`processing.qmd`).

## Regression studies

### Valence

For creating Table 2

```{r}
#| warning: false
#| eval: true
library(dmetar,quietly = TRUE)
library(tidyverse,quietly = TRUE)
library(meta)
library(DescTools)
library(ggrepel)
```

#### Using all models

```{r}
R_studies <- read.csv("R_studies.csv")
#R_summary <- read.csv("R_summary.csv")

# select regression studies with r2
#tmp <- dplyr::filter(R_summary,dimension=="valence")
tmp <- dplyr::filter(R_studies,dimension=="valence")
#dim(tmp)

#if all studies, remove two with NA values
tmp <- tmp[!is.na(tmp$values),]
#dim(tmp)

# Take all models
m.cor <- metacor(cor = values,     # values 
                 n = stimulus_n,
                 studlab = unique_id,
                 data = tmp,
                 common = FALSE,
                 random = TRUE,
                 prediction = TRUE,
#                 backtransf = TRUE,
#                 sm = "ZCOR",
                 method.tau = "PM",# was REML, but we switch to Paule-Mandel because Langan et al., 2019
                 method.random.ci = "HK", 
                 title = "MER: Regression: Valence: Summary")

print(m.cor)

print(FisherZInv(m.cor$TE.random))
print(FisherZInv(m.cor$upper.random))
print(FisherZInv(m.cor$lower.random))
```

#### Using the best model out of each study

```{r}
R_summary <- read.csv("R_summary.csv")

# select regression studies with r2
tmp <- dplyr::filter(R_summary,dimension=="valence")
#tmp <- dplyr::filter(R_studies,dimension=="valence")
#dim(tmp)

#if all studies, remove two
#tmp <- tmp[!is.na(tmp$values),]
#dim(tmp)

## Disambiguate the studies
tmp$studyREF[tmp$studyREF=="Wang et al 2022"] <- c("Wang et al. 2022a","Wang et al. 2022b")

# Max values
m.cor <- metacor(cor = valuesMax,     # values 
                 n = stimulus_n,
                 studlab = studyREF,#citekey, # unique_id
                 data = tmp,
                 common = FALSE,
                 random = TRUE,
                 prediction = TRUE,
#                 backtransf = TRUE,
#                 sm = "ZCOR",
                 method.tau = "REML",# could be PM (Paule-Mandel) as well
                 method.random.ci = "HK", 
                 title = "MER: Regression: Valence: Summary")

print(m.cor)

print(FisherZInv(m.cor$TE.random))
print(FisherZInv(m.cor$upper.random))
print(FisherZInv(m.cor$lower.random))
```

### Explore heterogeneity

```{r}
#| eval: true
#| echo: true
# Method 1: Give us the Egger's test about beta coefficient from funnel
print(eggers.test(m.cor))

# Method 2: Find the impact to the results when removing those outside the 95CI
O <- find.outliers(m.cor) # 13 remaining out of 24
print(O)
# Method 3: Leave-out-out analysis etc for individual influence (not useful here)
#infan <- InfluenceAnalysis(m.cor)

# Method 4: Focus on 10% of most precise studies (Stanley, Jarrel, Doucouliagos 2010)
thres<-quantile(m.cor$seTE,0.1)
ind<-m.cor$seTE<=as.numeric(thres)
m.cor10pct <- update(m.cor, subset = which(ind))

# Method 5: P curve analysis (Simonsohn, Simmons & Nelson, 2015)
pcurve(m.cor)
```

### Visualise (forest and funnel plots)

```{r}
#| fig-width: 12
#| fig-height: 6
fig2a <- meta::forest(m.cor,
       sortvar = TE,
       prediction = FALSE, 
             print.tau2 = FALSE,
             leftlabs = c("Author", "N"),studlab = studyREF)

fig2b <- funnel(m.cor, common = FALSE, studlab=TRUE,backtransf=TRUE)

library(forestplot)
data<-tibble::tibble(mean=m.cor$cor,lower=FisherZInv(m.cor$lower),upper=FisherZInv(m.cor$upper),study=m.cor$studlab,n=m.cor$n,cor=round(m.cor$cor,2))
data<-dplyr::arrange(data,mean)

fp1 <- grid.grabExpr(print(data |>
  forestplot(labeltext = c(study, n, cor),
             xlab = "Correlation",
             xticks = c(0, .25,.5,.75, 1),
             clip = c(0, 1))|>
    fp_add_header(study = "Study",n = "N",cor = expression(italic(r))) |>
    fp_append_row(mean  = m.cor$TE.common,
                lower = m.cor$lower.common,
                upper = m.cor$upper.common,
                study = "Summary",
                n = sum(m.cor$n),
                cor = round(m.cor$TE.common,2),
                is.summary = TRUE) |>
  fp_set_style(box = "grey50",
               line = "grey20",
               summary = "black",
                txt_gp = fpTxtGp(label = list(gpar(cex = 0.80)),
                                ticks = gpar(cex = 0.80),
                                xlab  = gpar(cex = 0.80)))|>
    fp_decorate_graph(grid = structure( m.cor$TE.common,gp = gpar(lty = 2, col = "grey30")))
)
)

source('../etc/custom_funnel_plot.R')
fp2 <- custom_funnel_plot(m.cor)

gridExtra::grid.arrange(fp1, fp2, ncol=2, widths=c(2,1),heights=c(2,1))
```


### Sub-divisions

#### Sub-division based on techniques

```{r,results='asis'}
S <- summarise(group_by(tmp,model_class_id),n=n(),obs=sum(stimulus_n))
print(knitr::kable(S))
```

```{r}
m.cor1 <- metacor(cor = valuesMax,     # values 
                 n = stimulus_n,
                 studlab = citekey, # unique_id
                 data = tmp,
                 common = FALSE,
                 random = TRUE,
                 prediction = TRUE,
                 subgroup =  model_class_id,
#                 backtransf = TRUE,
#                 sm = "ZCOR",
                 method.tau = "REML",# could be PM (Paule-Mandel) as well
                 method.random.ci = "HK", 
                 title = "MER: Regression: Valence: Summary")
print(m.cor1)
```

#### Sub-division based on journals (engineering vs psych)

```{r,results='asis'}
S <- summarise(group_by(tmp,journal_type),n=n(),obs=sum(stimulus_n))
print(knitr::kable(S))
```

```{r}
m.cor2 <- metacor(cor = valuesMax,     # values 
                 n = stimulus_n,
                 studlab = citekey, # unique_id
                 data = tmp,
                 common = FALSE,
                 random = TRUE,
                 prediction = TRUE,
                 subgroup =  journal_type,
#                 backtransf = TRUE,
#                 sm = "ZCOR",
                 method.tau = "REML",# could be PM (Paule-Mandel) as well
                 method.random.ci = "HK", 
                 title = "MER: Regression: Valence: Summary")
print(m.cor2)
```

#### Sub-division based on N features

```{r,results='asis'}
tmp <- dplyr::filter(R_summary,dimension=="valence")
tmp<-tmp[!is.na(tmp$feature_n_categories),] # remove 
S <- summarise(group_by(tmp,feature_n_categories),n=n(),obs=sum(stimulus_n))
print(knitr::kable(S))
```


```{r}
tmp <- dplyr::filter(R_summary,dimension=="valence")
tmp<-tmp[!is.na(tmp$feature_n_categories),] # remove missing values
m.cor3 <- metacor(cor = valuesMax,     # values 
                 n = stimulus_n,
                 studlab = citekey, # unique_id
                 data = tmp,
                 common = FALSE,
                 random = TRUE,
                 prediction = TRUE,
                 subgroup =  feature_n_categories,
#                 backtransf = TRUE,
#                 sm = "ZCOR",
                 method.tau = "REML",# could be PM (Paule-Mandel) as well
                 method.random.ci = "HK", 
                 title = "MER: Regression: Valence: Summary")
print(m.cor3)
S<-summarise(group_by(tmp,feature_n_categories),n=n(),obs=sum(stimulus_n))
print(S)

```

#### Sub-division based on N features and genres

```{r, results='asis'}
tmp <- dplyr::filter(R_summary,dimension=="valence")
tmp<-tmp[!is.na(tmp$feature_n_categories),] # removemissing values

S<-summarise(group_by(tmp,feature_n_complexity_genre),n=n(),obs=sum(stimulus_n))
print(knitr::kable(S))

m.cor4 <- metacor(cor = valuesMax,     # values 
                 n = stimulus_n,
                 studlab = citekey, # unique_id
                 data = tmp,
                 common = FALSE,
                 random = TRUE,
                 prediction = TRUE,
                 subgroup =  feature_n_complexity_genre,
#                 backtransf = TRUE,
#                 sm = "ZCOR",
                 method.tau = "REML",# could be PM (Paule-Mandel) as well
                 method.random.ci = "HK", 
                 title = "MER: Regression: Valence: Summary")
print(m.cor4)
```

### Arousal

#### Using all models

```{r}
R_studies <- read.csv("R_studies.csv")
tmp <- dplyr::filter(R_studies,dimension=="arousal")
#if all studies, remove two with NA values
tmp <- tmp[!is.na(tmp$values),]
#dim(tmp)

# Take all models
m.cor <- metacor(cor = values,     # values 
                 n = stimulus_n,
                 studlab = unique_id,
                 data = tmp,
                 common = FALSE,
                 random = TRUE,
                 prediction = TRUE,
#                 backtransf = TRUE,
#                 sm = "ZCOR",
                 method.tau = "PM",# was REML, but we switch to Paule-Mandel because Langan et al., 2019
                 method.random.ci = "HK", 
                 title = "MER: Regression: Arousal: Summary")

print(m.cor)

print(FisherZInv(m.cor$TE.random))
print(FisherZInv(m.cor$upper.random))
print(FisherZInv(m.cor$lower.random))
```

#### Using the best model out of each study

```{r}
R_summary <- read.csv("R_summary.csv")

# select regression studies with r2
tmp <- dplyr::filter(R_summary,dimension=="arousal")
#tmp <- dplyr::filter(R_studies,dimension=="valence")
#dim(tmp)

#if all studies, remove two
#tmp <- tmp[!is.na(tmp$values),]
#dim(tmp)

## Disambiguate the studies
tmp$studyREF[tmp$studyREF=="Wang et al 2022"] <- c("Wang et al. 2022a","Wang et al. 2022b")

# Max values
m.cor <- metacor(cor = valuesMax,     # values 
                 n = stimulus_n,
                 studlab = studyREF,#citekey, # unique_id
                 data = tmp,
                 common = FALSE,
                 random = TRUE,
                 prediction = TRUE,
#                 backtransf = TRUE,
#                 sm = "ZCOR",
                 method.tau = "REML",# could be PM (Paule-Mandel) as well
                 method.random.ci = "HK", 
                 title = "MER: Regression: Arousal: Summary")

print(m.cor)

print(FisherZInv(m.cor$TE.random))
print(FisherZInv(m.cor$upper.random))
print(FisherZInv(m.cor$lower.random))
```

### Explore heterogeneity

```{r}
#| eval: true
#| echo: true
# Method 1: Give us the Egger's test about beta coefficient from funnel
print(eggers.test(m.cor))

# Method 2: Find the impact to the results when removing those outside the 95CI
O <- find.outliers(m.cor) # 13 remaining out of 24
print(O)
# Method 3: Leave-out-out analysis etc for individual influence (not useful here)
#infan <- InfluenceAnalysis(m.cor)

# Method 4: Focus on 10% of most precise studies (Stanley, Jarrel, Doucouliagos 2010)
thres<-quantile(m.cor$seTE,0.1)
ind<-m.cor$seTE<=as.numeric(thres)
m.cor10pct <- update(m.cor, subset = which(ind))

# Method 5: P curve analysis (Simonsohn, Simmons & Nelson, 2015)
pcurve(m.cor)
```

### Visualise (forest and funnel plots)

```{r}
#| fig-width: 12
#| fig-height: 6
fig2a <- forest(m.cor,
       sortvar = TE,
       prediction = FALSE, 
             print.tau2 = FALSE,
             leftlabs = c("Author", "N"),studlab = studyREF)

fig2b<-funnel(m.cor, common = FALSE, studlab=TRUE,backtransf=TRUE)

library(forestplot)
data<-tibble::tibble(mean=m.cor$cor,lower=FisherZInv(m.cor$lower),upper=FisherZInv(m.cor$upper),study=m.cor$studlab,n=m.cor$n,cor=round(m.cor$cor,2))
data<-dplyr::arrange(data,mean)

fp1 <- grid.grabExpr(print(data |>
  forestplot(labeltext = c(study, n, cor),
             xlab = "Correlation",
             xticks = c(0, .25,.5,.75, 1),
             clip = c(0, 1))|>
    fp_add_header(study = "Study",n = "N",cor = expression(italic(r))) |>
    fp_append_row(mean  = m.cor$TE.common,
                lower = m.cor$lower.common,
                upper = m.cor$upper.common,
                study = "Summary",
                n = sum(m.cor$n),
                cor = round(m.cor$TE.common,2),
                is.summary = TRUE) |>
  fp_set_style(box = "grey50",
               line = "grey20",
               summary = "black",
                txt_gp = fpTxtGp(label = list(gpar(cex = 0.80)),
                                ticks = gpar(cex = 0.80),
                                xlab  = gpar(cex = 0.80)))|>
    fp_decorate_graph(grid = structure( m.cor$TE.common,gp = gpar(lty = 2, col = "grey30")))
)
)

source('../etc/custom_funnel_plot.R')
fp2 <- custom_funnel_plot(m.cor)

gridExtra::grid.arrange(fp1, fp2, ncol=2, widths=c(2,1),heights=c(2,1))
```


### Sub-divisions

#### Sub-division based on techniques

```{r,results='asis'}
S <- summarise(group_by(tmp,model_class_id),n=n(),obs=sum(stimulus_n))
print(knitr::kable(S))
```

```{r}
m.cor1 <- metacor(cor = valuesMax,     # values 
                 n = stimulus_n,
                 studlab = citekey, # unique_id
                 data = tmp,
                 common = FALSE,
                 random = TRUE,
                 prediction = TRUE,
                 subgroup =  model_class_id,
#                 backtransf = TRUE,
#                 sm = "ZCOR",
                 method.tau = "REML",# could be PM (Paule-Mandel) as well
                 method.random.ci = "HK", 
                 title = "MER: Regression: Valence: Summary")
print(m.cor1)
```

#### Sub-division based on journals (engineering vs psych)

```{r,results='asis'}
S <- summarise(group_by(tmp,journal_type),n=n(),obs=sum(stimulus_n))
print(knitr::kable(S))
```

```{r}
m.cor2 <- metacor(cor = valuesMax,     # values 
                 n = stimulus_n,
                 studlab = citekey, # unique_id
                 data = tmp,
                 common = FALSE,
                 random = TRUE,
                 prediction = TRUE,
                 subgroup =  journal_type,
#                 backtransf = TRUE,
#                 sm = "ZCOR",
                 method.tau = "REML",# could be PM (Paule-Mandel) as well
                 method.random.ci = "HK", 
                 title = "MER: Regression: Valence: Summary")
print(m.cor2)
```

#### Sub-division based on N features

```{r,results='asis'}
tmp <- dplyr::filter(R_summary,dimension=="arousal")
tmp<-tmp[!is.na(tmp$feature_n_categories),] # remove 
S <- summarise(group_by(tmp,feature_n_categories),n=n(),obs=sum(stimulus_n))
print(knitr::kable(S))
```


```{r}
tmp <- dplyr::filter(R_summary,dimension=="arousal")
tmp<-tmp[!is.na(tmp$feature_n_categories),] # remove missing values
m.cor3 <- metacor(cor = valuesMax,     # values 
                 n = stimulus_n,
                 studlab = citekey, # unique_id
                 data = tmp,
                 common = FALSE,
                 random = TRUE,
                 prediction = TRUE,
                 subgroup =  feature_n_categories,
#                 backtransf = TRUE,
#                 sm = "ZCOR",
                 method.tau = "REML",# could be PM (Paule-Mandel) as well
                 method.random.ci = "HK", 
                 title = "MER: Regression: Valence: Summary")
print(m.cor3)
S<-summarise(group_by(tmp,feature_n_categories),n=n(),obs=sum(stimulus_n))
print(S)

```

#### Sub-division based on N features and genres

```{r, results='asis'}
tmp <- dplyr::filter(R_summary,dimension=="arousal")
tmp<-tmp[!is.na(tmp$feature_n_categories),] # remove missing values

S<-summarise(group_by(tmp,feature_n_complexity_genre),n=n(),obs=sum(stimulus_n))
print(knitr::kable(S))

m.cor4 <- metacor(cor = valuesMax,     # values 
                 n = stimulus_n,
                 studlab = citekey, # unique_id
                 data = tmp,
                 common = FALSE,
                 random = TRUE,
                 prediction = TRUE,
                 subgroup =  feature_n_complexity_genre,
#                 backtransf = TRUE,
#                 sm = "ZCOR",
                 method.tau = "REML",# could be PM (Paule-Mandel) as well
                 method.random.ci = "HK", 
                 title = "MER: Regression: Valence: Summary")
print(m.cor4)
```




## Classification studies

```{r}
#| warning: false
#| eval: true

C_studies <- read.csv("C_studies.csv")
C_studies <- C_studies[!is.na(C_studies$values),]

# yang2021an needs to be encoded for classification (currently just regression)
C_summary <- read.csv("C_summary.csv")

# 2025-01-24: averaging valence and arousal for this study, which makes the max equivalent to both the mean and median (two models: one valence, one arousal)
C_summary[C_summary$citekey == "nguyen2017an",]$valuesMax <- C_summary[C_summary$citekey == "nguyen2017an",]$valuesMean 

# 2025-01-24: averaging across quadrants for bhuvanakumar study
# subset study
bhuvanakumar <- subset(
  C_studies, 
  stringr::str_detect(
    C_studies$citekey,
    "bhuvanakumar"
    )
)
# group by algorithm to average across the four quadrants
bhuvanakumar$model_algorithm <- substr(
  bhuvanakumar$model_id, 
  1, 
  4
)
# take the average for each algorithm
bhuvanakumar |> 
  group_by(model_algorithm) |>
  mutate(values = mean(values)) -> bhuvanakumar
# overwrite values in C_summary
C_summary[C_summary$citekey == "bhuvanakumar2023em",]$valuesMean <- mean(unique(bhuvanakumar$values))
C_summary[C_summary$citekey == "bhuvanakumar2023em",]$valuesMedian <- median(unique(bhuvanakumar$values))
C_summary[C_summary$citekey == "bhuvanakumar2023em",]$valuesMax <- max(unique(bhuvanakumar$values))

remove(bhuvanakumar)


C_classes <- read.csv("C_study_class_n.csv")

C_summary <- merge(C_summary, C_classes)

m.cor <- metacor(cor = valuesMax,     # values 
                 n = stimulus_n,
                 studlab = studyREF, # unique_id
                 data = C_summary,
                 common = FALSE,
                 random = TRUE,
                 prediction = TRUE,
                 backtransf = TRUE,
                 sm = "ZCOR",
                 method.tau = "REML",# could be PM (Paule-Mandel) as well
                 method.random.ci = "HK", 
                 title = "MER: Classification: Summary")

print(m.cor)

#m.cor_backtransformed <- m.cor
#m.cor_backtransformed$TE <- FisherZInv(m.cor_backtransformed$TE)
print(funnel(m.cor, common = FALSE, studlab=TRUE,backtransf=TRUE))

```
#### Custom plot

```{r}
library(ggrepel)
tmpdata <- data.frame(SE = FisherZInv(m.cor$seTE), Zr = FisherZInv(m.cor$TE),studies=m.cor$studlab)

tmpdata$citekey<-str_replace_all(tmpdata$studies,'-.*$','')
tmpdata$citekey<-factor(tmpdata$citekey)

g <- ggplot(tmpdata,aes(y = SE, x = Zr,label=studies,color=citekey,fill=citekey)) +
  geom_point(show.legend = FALSE) +
  geom_label_repel(size=1.5,max.overlaps = 39,color="black",show.legend = FALSE)+
  ylab('Standard Error') + 
  xlab('r')+
  scale_y_reverse()+
#  scale_x_continuous(breaks=seq(0.0,1.0,0.25),limits=c(0,1.0))+
#  coord_flip()+
  theme_bw()
print(g)
```




#### Visualise (forest and funnel plots)

```{r}
forest(m.cor,
       sortvar = TE,
       prediction = FALSE, 
             print.tau2 = FALSE,
             leftlabs = c("Author", "N"),studlab = citekey)

funnel(m.cor, common = FALSE, studlab=TRUE,backtransf=TRUE)


```
#### By class n = 2 vs. > 2
```{r}
C_summary$classes <- 0
C_summary[C_summary$n_classes < 3,]$classes <- "2"
C_summary[C_summary$n_classes >= 3,]$classes <- ">2"



m.cor <- metacor(cor = valuesMax,     # values 
                 n = stimulus_n, # could also change to n_classes
                 studlab = studyREF, # unique_id
                 data = C_summary,
                 subgroup = classes,
                 common = FALSE,
                 random = TRUE,
                 prediction = TRUE,
                 backtransf = TRUE,
                 sm = "ZCOR",
                 method.tau = "REML",# could be PM (Paule-Mandel) as well
                 method.random.ci = "HK", 
                 title = "MER: Classification: Summary")


forest(m.cor,
       sortvar = TE,
       prediction = FALSE, 
             print.tau2 = FALSE,
             leftlabs = c("Author", "N"),studlab = citekey)

funnel(m.cor, common = FALSE, studlab=TRUE,backtransf=TRUE)

```


#### Explore qualities (in progress)

```{r}
O <- find.outliers(m.cor)
# 6 datasets identified as outliers, without them the r drops to 0.5781
#infan <- InfluenceAnalysis(m.cor)
#print(eggers.test(m.cor))



```


#### Re-run the analysis without the outliers

```{r}
#| eval: false
#| echo: false
outliers <- c("")

tmp2<-tmp[!tmp$unique_id %in% outliers,]
m.cor <- metacor(cor = values, 
                 n = stimulus_n,
                 studlab = unique_id, # unique_id
                 data = tmp2,
                 common = FALSE,
                 random = TRUE,
                 sm = "ZCOR",
                 prediction = TRUE,
                 backtransf = TRUE,
                 method.tau = "REML",# could be PM (Paule-Mandel) as well
                 method.random.ci = "HK", 
                 title = "MER: Regression: Valence: Summary")

print(m.cor)

fp <- funnel(m.cor, common = TRUE,studlab=FALSE)



```

#### Custom funnel plot

To show the quality differences between core and eliminated studies (in progress).

```{r}
#| eval: true
tmpdata <- data.frame(SE = m.cor$seTE, Zr = m.cor$TE, studies=m.cor$studlab)

tmpdata$studyREF <- substr(tmpdata$studies,1,nchar(tmpdata$studies)-2)
tmpdata$studyREF <- str_replace_all(tmpdata$studyREF,'([0-9]+)',' et al \\1')
tmpdata$studyREF <- str_to_sentence(tmpdata$studyREF)
tmpdata$studyREF

estimate = m.cor$TE.common
se = m.cor$seTE.common
se.seq=seq(0, max(m.cor$cor), 0.001)
ll95 = estimate-(1.96*se.seq)
ul95 = estimate+(1.96*se.seq)
ll99 = estimate-(3.29*se.seq)
ul99 = estimate+(3.29*se.seq)
meanll95 = estimate-(1.96*se)
meanul95 = estimate+(1.96*se)
dfCI = data.frame(ll95, ul95, ll99, ul99, se.seq, estimate, meanll95, meanul95)

fp = ggplot(NULL) +
  geom_point(aes(x = SE, y = Zr), color='grey50',data=tmpdata) +
  geom_text_repel(aes(x = SE, y = Zr, label=studyREF), data=tmpdata,size=2.5,max.overlaps = 40)+
  xlab('Standard Error') + ylab('Fisher\'s z transformed correlation')+
  geom_line(aes(x = se.seq, y = ll95), linetype = 'dotted', data = dfCI) +
  geom_line(aes(x = se.seq, y = ul95), linetype = 'dotted', data = dfCI) +
  geom_segment(aes(x = min(se.seq), y = meanll95, xend = max(se.seq), yend = meanll95), linetype='dotted', data=dfCI) +
  geom_segment(aes(x = min(se.seq), y = meanul95, xend = max(se.seq), yend = meanul95), linetype='dotted', data=dfCI) +
#  scale_x_reverse()+
  scale_x_reverse(breaks=seq(0,0.2,0.05),limits=c(0.15,0),expand=c(0.00,0.00))+
  scale_y_continuous(breaks=seq(0.3,1.25,0.20),limits=c(0.3,1.23))+
  coord_flip()+
  theme_bw()
print(fp)


```


#### Subgroup analysis according to techniques

add `journal_type` and `stimulus_genre_mixed` as a grouping option

```{r}
#| eval: false
m.cor_subgroups <- update(m.cor, 
#       subgroup = model_class_id, 
#       subgroup = journal_type, 
#       subgroup = stimulus_genre_mixed, 
       tau.common = FALSE,
       prediction = TRUE)

print(m.cor_subgroups)
#forest(m.cor_subgroups,subgroup=TRUE)
```

- Idea: visualise the distributions of the model successes within studies (done in preprocessing)

