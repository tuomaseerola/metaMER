---
title: "Library Parser"
---

# Pre-processing

## Preparing data

The follow code extracts information from the .bib library to format it as a `data.frame` for further processing. The code makes use of the stringr package and uses regular expressions to extract relevant parameters. First we read in the .bib file and use some string manipulations to retrieve the citation keys.

```{r results = 'hide'}
library(stringr)
library(knitr, include.only = 'kable')
source(paste0(here::here(), "/R/format-study-results.R"))
source(paste0(here::here(), "/R/parse-model-output.R"))

```

```{r}
bib_file <- read.delim('bib/extractions.bib',
           sep = '@', header = F)

# get citekeys from bibtex file:
citekeys <- unique(bib_file$V2)
# improve formatting
citekeys <- str_remove(citekeys, '\\{')
citekeys <- str_remove(citekeys, ',')
citekeys <- str_remove(citekeys, '%%.*$')
citekeys <- str_remove(citekeys, 'Article')
citekeys[citekeys ==''] <- NA
citekeys <- na.omit(citekeys)

```

*R* reads the .bib file as a two column data.frame, with the citation key appearing in the second column and the remaining metadata appearing in the first column. When the citation key appears in the second column, the corresponding row in the first column is blank. Because of this quirk, we can index metadata matching each citation key by keeping track of blank rows in the first column. We'll append each to a new entry of a list. The name of each list entry is the citation key; the corresponding value is the remaining unprocessed metadata.

```{r}
# find where new entries begin:
new_entries = which(bib_file$V2 != '')

# loop across unique indices for each entry
meta_list = list()
# loop across unique indices for each entry
meta_list = list()
for(this_entry in 1:(length(new_entries)-1))
{
  # get unique citekey
  this_cite_key <- citekeys[this_entry]
  # capture lines following citekey
  corresponding_lines <- bib_file[new_entries[this_entry]:new_entries[this_entry+1]-1,]$V1
  # store matching lines as data frame
  corresponding_lines <- data.frame(corresponding_lines)
  # assign lines distinct name
  names(corresponding_lines) <- this_cite_key
  # add to a list for further processing
  meta_list <- append(meta_list, corresponding_lines)
}


```

## Extracting Relevant .bib Fields

Not every bibtex field is equally useful for analysis. To facilitate data manipulation, we can save the names of the target fields separately in a .txt file, and use a regular expression to create a new column each time *R* finds one of the target fields in a string containing the bibtex metadata.

```{r}
# read in target bibtex fields
search_fields <- field_names <- readLines('bibtex_fields.txt')

# match casing in bibtex file
field_names <- toupper(field_names)
# add a pattern allowing us to find text between two adjacent bibtex fields
rep_pattern <- paste0(field_names[1:length(field_names)-1], '\\s*(.*?)\\s')
# apply this same pattern to all but the last of the field names                  
field_names[1:length(field_names)-1] <- rep_pattern
# collapse all the new field names into a single string for string manipulation with stringr
field_names[length(field_names)] <- paste0(field_names[length(field_names)], '.*')
field_names <- paste0(field_names, collapse = '')
```

## Prepare dataframe

Now we can convert our list into a data frame with the target bibtex fields. For the last field `MODEL_VALIDATION`, we will apply a different regex pattern which matches all characters following the field name `(?<=MODEL_VALIDATION).*` .

```{r}
# create new column containing information between two adjacent target fields for all entries in list
meta_df <- lapply(meta_list, function(x) str_match(paste0(x, collapse = ' '), field_names))
meta_df <- lapply(meta_list, function(x) str_match(paste0(x, collapse = ' '), field_names))

# collapse list entries into rows
meta_df <- do.call('rbind', meta_df)
# format as a data.frame
meta_df <- data.frame(meta_df)
# match text after final column name
meta_df[,ncol(meta_df)+1] <- sapply(meta_df[,1], function(x) str_match(paste0(x, collapse = ' '), '(?<=FINAL_NOTES).*'))
# replace first column with citationkeys
meta_df[,1] <- names(meta_list)
names(meta_df) <- c('citekey', search_fields)
names(meta_df) <- trimws(names(meta_df))
```

## Formatting

Finally, we'll perform some formatting to remove unwanted characters left over following the conversion (in progress)

```{r}
## remove bibtext field formatting
# remove curly braces
meta_df <- apply(meta_df, 2, function(x) str_remove_all(x, '\\{')) 
meta_df <- apply(meta_df, 2, function(x) str_remove_all(x, '\\},'))
# remove first '=' (from bibtex field )
meta_df <- apply(meta_df, 2, function(x) str_remove(x, '='))
# remove double-commas
#meta_df <- apply(meta_df, 2, function(x) str_remove_all(x, ',,'))
# remove comments
meta_df <- apply(meta_df, 2, function(x) str_remove_all(x, '%%.*'))
#meta_df <- apply(meta_df, 2, function(x) str_remove_all(x, ' , '))
# remove extra characters in final column
meta_df[, ncol(meta_df)] = str_remove_all(meta_df[, ncol(meta_df)], '\\}')
meta_df <- as.data.frame(meta_df)

```

Example: Evaluate `model_rate_emotion_values` as *R* code

```{r}
eval(parse(text = meta_df$model_rate_emotion_values[22]))
```

## Track Excluded Studies (During Extraction)

```{r}
meta_df[which(str_detect(meta_df$final_notes, '!EXCL!')),] |> dplyr::tibble()
```

```{r}
meta_df[-which(str_detect(meta_df$final_notes, '!EXCL!')),] -> included_studies

included_studies |> dplyr::tibble()
```

# Track Incomplete Extractions

Count studies where there are empty fields (exclude `final_notes` in count)

```{r}
check_missing <- function(n_empty_fields = 1) 
{
  included_studies[which(
    rowSums(
      sapply(
        included_studies[,1:ncol(included_studies) - 1],
        function(x) grepl("^\\s*$", x))) > n_empty_fields - 1),]
}
```

```{r}
## AT LEAST 1 MISSING FIELD
check_missing() -> missing_1

missing_1 |> dplyr::tibble()
```

```{r}
## AT LEAST 2 MISSING FIELDS
check_missing(2) |> dplyr::tibble()
## AT LEAST 3 MISSING FIELDS
check_missing(3) |> dplyr::tibble()
```



# Parsing Studies

The following functions provide a working example for converting the *R* code contained in `model_rate_emotion_values` to a data frame for further analysis. `get_study_summaries` is a simple function to demonstrate how the preprocessing routines work in the following examples

```{r}
# high level function to apply parse_model_output to multiple studies
get_study_summaries <- function(df) {
do.call(rbind,
  lapply(df$model_rate_emotion_values, 
         FUN = function(x) {
           study_id <- unique(df$citekey[which(df$model_rate_emotion_values == x)])
           model_results <- parse_model_output(x)
           return(cbind(study_id, model_results))
           }
         )
  )
}

```

## Regression studies

Encoded values should use the following nomenclature. In this synthetic example, dim_measure.summary2 exists for the study in the second row, but not the first, so an NA is encoded for summary2 in row 1.

```{r warning=FALSE}
bind_field(
  'library.model.features.data.exp1' = c(dim_measure1.summary = 0.5, dim_measure2.summary = 0.1), 
  'library.model.features.data.exp2' = c(dim_measure1.summary = 0.2, dim_measure2.summary = 0.5, dim_measure.summary2 = 0.3)
)
```

### Example 2

This example is more extensive, and shows how this procedure can work with multiple studies:

```{r}
regression_example <- data.frame(citekey = c('testStudy2023aa','testStudy2023bb', 'testStudy2020cc'),
  model_rate_emotion_values = c("bind_field('marsyas.random forest.mfcc.msd.1' = c(valence_r2.mean = 0.9, arousal_r2.mean = 0.5,
 valence_r2.sd = 0.4, arousal_r2.sd = 0.3),
  'essentia.random forest.mfcc.deam.1' = c(valence_r2.mean = 0.9, arousal_r2.mean = 0.5))",
  "bind_field('librosa.linear regression.mfcc.ismir.1' = c(valence_r2.mean = 0.1, arousal_r2.mean = 0.3, euclid_euclid.mean = 0.32))",
  "bind_field('mirtoolbox.neural net.pitch.filmMusic.2' = c(valence_r2.mean = 0.1, arousal_r2.mean = 0.3))")
)


dplyr::tibble(get_study_summaries(regression_example)) |>
  knitr::kable()
```

## Classification studies

For classification studies, we can extract results from a matrix-like representation.

`unflatten` is a convenience function for preparing a confusion matrix, and `summarize_matrix` calls on the *caret* package to compute summary statistics for the matrix.

```{r warning = FALSE}
bind_field(
  lapply(
    list(
      'library.model.features.data.exp' = unflatten(
      "Class_A" = 0.8, "Class_B" = 0.2,
      0.2, 0.8
      ),
      'library.model.features.data.exp' = unflatten(
      "Class_A" = 0.4, "Class_B" = 0.6,
      0.6, 0.4
      )
    ),
    summarize_matrix
  )
)
```

```{r warning = FALSE}

classification_example <- data.frame(
  citekey = "classificationStudy2020aa",
  model_rate_emotion_values = 
  "bind_field(
    lapply(
      list(
        'marsyas.smo unambiguous.mixed.new.1' = unflatten(
          'A' = 52.6, 'B' = 17.1, 'C' = 0.0,
          12, 65.2, 7.6, 
          1.1,9.9,73.6),
        'marsyas.smo full.mixed.new.1' = unflatten(
          'C1' = 56.0, 'C2' = 19, 'C3' = 0.0, 'C4' = 2, 
          11.3,58.9,13.7,15,
          0,13.2,68.6, 3,
          15,1,4, 6)
      ),
      summarize_matrix
    )
  )"
)


dplyr::tibble(get_study_summaries(classification_example)) |>
  knitr::kable()
```
