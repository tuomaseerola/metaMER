[
  {
    "objectID": "data/template.html",
    "href": "data/template.html",
    "title": "template",
    "section": "",
    "text": "We code the studies in ascii format into files titled authoryearxx.R stored in data/ folder.\n\n\n\nInitial list, to be modified\nIssues that need to be resolved can be marked after hashtag (#).\nIf there are multiple studies/samples in one article, another data can be created and study_id can be adjusted by adding a postfix (_a,_b,_c).\n\n\nstudy_id &lt;- 'authorlastnameyeartwoletterfromthetitle'\nemotion_type &lt;- c('dimensional','discrete','music-specific','other')\nemotion_list &lt;- c('valence','arousal','happy','sad','anger','fear','tender','other')\nemotion_locus &lt;- c('perceived','induced','not specified')\nstimulus_type &lt;- c('classical','pop','genre','mixed')\nstimulus_duration &lt;- 20\nstimulus_N &lt;- 24\nparticipant_N &lt;- 35\nparticipant_expertise &lt;- c('non-musician','musician','not specified')\nparticipant_origin &lt;- c('Country','not-specified')\nparticipant_sampling &lt;- c('convenience','not-specified')\nparticipant_task &lt;- c('rating','forced choice','ranking','other')\nfeature_list &lt;- c('attack rate','pitch height','mode')\nfeature_validation &lt;- c('annotation','none') # this was done in another study\nfeature_source &lt;- c('librosa','essentia','MIR toolbox','marsyas','custom','not specified')\nmodel_type &lt;- c('correlation','regression','commonality analysis','classification')\nmodel_measure &lt;- c('rsquared','cor','rmse','classification','other')\nmodel_complexity_parameters &lt;- c(numeric, 'not defined')\nmodel_rate_emotion_list &lt;- c(0.00,0.00)\nmodel_validation &lt;- c('none','cross-validation')\nmeta_comments &lt;- 'template'\nmeta_encoder &lt;- 'Initials of the name'\nmeta_date &lt;- '3/5/2024'"
  },
  {
    "objectID": "data/template.html#fields",
    "href": "data/template.html#fields",
    "title": "template",
    "section": "",
    "text": "Initial list, to be modified\nIssues that need to be resolved can be marked after hashtag (#).\nIf there are multiple studies/samples in one article, another data can be created and study_id can be adjusted by adding a postfix (_a,_b,_c).\n\n\nstudy_id &lt;- 'authorlastnameyeartwoletterfromthetitle'\nemotion_type &lt;- c('dimensional','discrete','music-specific','other')\nemotion_list &lt;- c('valence','arousal','happy','sad','anger','fear','tender','other')\nemotion_locus &lt;- c('perceived','induced','not specified')\nstimulus_type &lt;- c('classical','pop','genre','mixed')\nstimulus_duration &lt;- 20\nstimulus_N &lt;- 24\nparticipant_N &lt;- 35\nparticipant_expertise &lt;- c('non-musician','musician','not specified')\nparticipant_origin &lt;- c('Country','not-specified')\nparticipant_sampling &lt;- c('convenience','not-specified')\nparticipant_task &lt;- c('rating','forced choice','ranking','other')\nfeature_list &lt;- c('attack rate','pitch height','mode')\nfeature_validation &lt;- c('annotation','none') # this was done in another study\nfeature_source &lt;- c('librosa','essentia','MIR toolbox','marsyas','custom','not specified')\nmodel_type &lt;- c('correlation','regression','commonality analysis','classification')\nmodel_measure &lt;- c('rsquared','cor','rmse','classification','other')\nmodel_complexity_parameters &lt;- c(numeric, 'not defined')\nmodel_rate_emotion_list &lt;- c(0.00,0.00)\nmodel_validation &lt;- c('none','cross-validation')\nmeta_comments &lt;- 'template'\nmeta_encoder &lt;- 'Initials of the name'\nmeta_date &lt;- '3/5/2024'"
  },
  {
    "objectID": "studies/search_syntax.html",
    "href": "studies/search_syntax.html",
    "title": "Search Syntax",
    "section": "",
    "text": "Search Syntax\n\n\n\n\n\n\n\n\n\nDatabase\nDate of search\nResults\nSearch syntax\n\n\n\n\nPsychINFO\nX May 20XX\nXXX\n(emotion OR mood* OR affect) AND (music OR audio OR recognition OR percei* OR recogn*) AND (music)\n\n\nScopus\nX May 20XX\nXXX\n\n\n\nWeb of Science\nX May 20XX\nXXX\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "preregistration/preregistration.html#authors-te-ca-ms-to-be-decided",
    "href": "preregistration/preregistration.html#authors-te-ca-ms-to-be-decided",
    "title": "metaMER",
    "section": "Authors: TE + CA + MS ? (to be decided)",
    "text": "Authors: TE + CA + MS ? (to be decided)"
  },
  {
    "objectID": "preregistration/preregistration.html#brief-rationale",
    "href": "preregistration/preregistration.html#brief-rationale",
    "title": "metaMER",
    "section": "Brief rationale",
    "text": "Brief rationale\nTo assess what degree models are able to predict emotions in music, what models, what emotions, what genres, etc."
  },
  {
    "objectID": "preregistration/preregistration.html#research-objectives",
    "href": "preregistration/preregistration.html#research-objectives",
    "title": "metaMER",
    "section": "Research objectives",
    "text": "Research objectives\n\nHow good are MER models?\nWhat are the systematic biases of the models?\nHow are models dealing with genres and cultures?\nHow generalisible are the models\nWEIRD\nTheory orientations\n\nFINER: feasible, interesting, novel, ethical, relevant [@browner2022de]"
  },
  {
    "objectID": "preregistration/preregistration.html#hypotheses",
    "href": "preregistration/preregistration.html#hypotheses",
    "title": "metaMER",
    "section": "Hypotheses",
    "text": "Hypotheses\nWe expect that the"
  },
  {
    "objectID": "preregistration/preregistration.html#eligibility-criteria",
    "href": "preregistration/preregistration.html#eligibility-criteria",
    "title": "metaMER",
    "section": "Eligibility criteria",
    "text": "Eligibility criteria\nEnglish, peer-reviewed journal articles indexed in X and Y, published between 20xx-20xx"
  },
  {
    "objectID": "preregistration/preregistration.html#database-sources",
    "href": "preregistration/preregistration.html#database-sources",
    "title": "metaMER",
    "section": "Database sources",
    "text": "Database sources\n(PubMed, Web of Science, Scopus)"
  },
  {
    "objectID": "preregistration/preregistration.html#search-strategy",
    "href": "preregistration/preregistration.html#search-strategy",
    "title": "metaMER",
    "section": "Search strategy",
    "text": "Search strategy\nMusic emotion recognition"
  },
  {
    "objectID": "preregistration/preregistration.html#analysis-plan",
    "href": "preregistration/preregistration.html#analysis-plan",
    "title": "metaMER",
    "section": "Analysis plan",
    "text": "Analysis plan\nFixed effects model Tests of small study/publication bias"
  },
  {
    "objectID": "manuscript/manuscript.html",
    "href": "manuscript/manuscript.html",
    "title": "Music Emotion Recognition…",
    "section": "",
    "text": "blah blah (Anderson & Schutz, 2022)\n\n\nblah blah"
  },
  {
    "objectID": "manuscript/manuscript.html#aims",
    "href": "manuscript/manuscript.html#aims",
    "title": "Music Emotion Recognition…",
    "section": "",
    "text": "blah blah"
  },
  {
    "objectID": "analysis/analysis.html",
    "href": "analysis/analysis.html",
    "title": "analysis",
    "section": "",
    "text": "analysis\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "README",
    "section": "",
    "text": "How well we can predict emotions in music? What is the evidence in the published literature for explaining what emotions the listeners can perceive in music when the source consists of audio examples. To what degree the results are dependent on the actual models, emotions, musical/acoustic features, or musical materials or participants?\nTo obtain answers to these questions, we have set out to record and analyse the current state of the art from the literature using a meta-analysis paradigm. We focus on Music Emotion Recognition and hence the acronym metaMER.\n\n\nWe define the aims and methods in preregistration plan.\n\n\n\nSearch databases and criteria are documented in studies/search_syntax.qmd.\n\n\n\nData coding and extraction is described in data template data/template.qmd.\n\n\n\nData analysis is covered in analysis/analysis.qmd document.\n\n\n\nThe study report is available manuscript/manuscript.qmd document."
  },
  {
    "objectID": "index.html#plan",
    "href": "index.html#plan",
    "title": "README",
    "section": "",
    "text": "We define the aims and methods in preregistration plan."
  },
  {
    "objectID": "index.html#study-search-and-selection",
    "href": "index.html#study-search-and-selection",
    "title": "README",
    "section": "",
    "text": "Search databases and criteria are documented in studies/search_syntax.qmd."
  },
  {
    "objectID": "index.html#data-extraction-and-coding",
    "href": "index.html#data-extraction-and-coding",
    "title": "README",
    "section": "",
    "text": "Data coding and extraction is described in data template data/template.qmd."
  },
  {
    "objectID": "index.html#analysis",
    "href": "index.html#analysis",
    "title": "README",
    "section": "",
    "text": "Data analysis is covered in analysis/analysis.qmd document."
  },
  {
    "objectID": "index.html#manuscript",
    "href": "index.html#manuscript",
    "title": "README",
    "section": "",
    "text": "The study report is available manuscript/manuscript.qmd document."
  },
  {
    "objectID": "studies/library_parser.html",
    "href": "studies/library_parser.html",
    "title": "Library Parser",
    "section": "",
    "text": "Status: in progress.\nTODO:\n- Separate scripts and functions into separate .qmd files/directories.\n- Update model_rate_emotion_values nomenclature process for classification papers.\n- Replace rbind with custom bind function throughout"
  },
  {
    "objectID": "studies/library_parser.html#preparing-data",
    "href": "studies/library_parser.html#preparing-data",
    "title": "Library Parser",
    "section": "Preparing data",
    "text": "Preparing data\nThe follow code extracts information from the .bib library to format it as a data.frame for further processing. The code makes use of the stringr package and uses regular expressions to extract relevant parameters. First we read in the .bib file and use some string manipulations to retrieve the citation keys.\n\nlibrary(stringr)\nlibrary(knitr, include.only = 'kable')\n\n\nbib_file &lt;- read.delim('bib/extractions.bib',\n           sep = '@', header = F)\n\nWarning in scan(file = file, what = what, sep = sep, quote = quote, dec = dec,\n: EOF within quoted string\n\n# get citekeys from bibtex file:\ncitekeys &lt;- unique(bib_file$V2)\n# improve formatting\ncitekeys &lt;- str_remove(citekeys, '\\\\{')\ncitekeys &lt;- str_remove(citekeys, ',')\ncitekeys &lt;- str_remove(citekeys, '%%.*$')\ncitekeys &lt;- str_remove(citekeys, 'Article')\ncitekeys[citekeys ==''] &lt;- NA\ncitekeys &lt;- na.omit(citekeys)\n\nR reads the .bib file as a two column data.frame, with the citation key appearing in the second column and the remaining metadata appearing in the first column. When the citation key appears in the second column, the corresponding row in the first column is blank. Because of this quirk, we can index metadata matching each citation key by keeping track of blank rows in the first column. We’ll append each to a new entry of a list. The name of each list entry is the citation key; the corresponding value is the remaining unprocessed metadata.\n\n# find where new entries begin:\nnew_entries = which(bib_file$V2 != '')\n\n# loop across unique indices for each entry\nmeta_list = list()\n# loop across unique indices for each entry\nmeta_list = list()\nfor(this_entry in 1:(length(new_entries)-1))\n{\n  # get unique citekey\n  this_cite_key &lt;- citekeys[this_entry]\n  # capture lines following citekey\n  corresponding_lines &lt;- bib_file[new_entries[this_entry]:new_entries[this_entry+1]-1,]$V1\n  # store matching lines as data frame\n  corresponding_lines &lt;- data.frame(corresponding_lines)\n  # assign lines distinct name\n  names(corresponding_lines) &lt;- this_cite_key\n  # add to a list for further processing\n  meta_list &lt;- append(meta_list, corresponding_lines)\n}"
  },
  {
    "objectID": "studies/library_parser.html#extracting-relevant-.bib-fields",
    "href": "studies/library_parser.html#extracting-relevant-.bib-fields",
    "title": "Library Parser",
    "section": "Extracting Relevant .bib Fields",
    "text": "Extracting Relevant .bib Fields\nNot every bibtex field is equally useful for analysis. To facilitate data manipulation, we can save the names of the target fields separately in a .txt file, and use a regular expression to create a new column each time R finds one of the target fields in a string containing the bibtex metadata.\n\n# read in target bibtex fields\nsearch_fields &lt;- field_names &lt;- readLines('bibtex_fields.txt')\n\nWarning in readLines(\"bibtex_fields.txt\"): incomplete final line found on\n'bibtex_fields.txt'\n\n# match casing in bibtex file\nfield_names &lt;- toupper(field_names)\n# add a pattern allowing us to find text between two adjacent bibtex fields\nrep_pattern &lt;- paste0(field_names[1:length(field_names)-1], '\\\\s*(.*?)\\\\s')\n# apply this same pattern to all but the last of the field names                  \nfield_names[1:length(field_names)-1] &lt;- rep_pattern\n# collapse all the new field names into a single string for string manipulation with stringr\nfield_names[length(field_names)] &lt;- paste0(field_names[length(field_names)], '.*')\nfield_names &lt;- paste0(field_names, collapse = '')"
  },
  {
    "objectID": "studies/library_parser.html#prepare-dataframe",
    "href": "studies/library_parser.html#prepare-dataframe",
    "title": "Library Parser",
    "section": "Prepare dataframe",
    "text": "Prepare dataframe\nNow we can convert our list into a data frame with the target bibtex fields. For the last field MODEL_VALIDATION, we will apply a different regex pattern which matches all characters following the field name (?&lt;=MODEL_VALIDATION).* .\n\n# create new column containing information between two adjacent target fields for all entries in list\nmeta_df &lt;- lapply(meta_list, function(x) str_match(paste0(x, collapse = ' '), field_names))\nmeta_df &lt;- lapply(meta_list, function(x) str_match(paste0(x, collapse = ' '), field_names))\n\n# collapse list entries into rows\nmeta_df &lt;- do.call('rbind', meta_df)\n# format as a data.frame\nmeta_df &lt;- data.frame(meta_df)\n# match text after final column name\nmeta_df[,ncol(meta_df)+1] &lt;- sapply(meta_df[,1], function(x) str_match(paste0(x, collapse = ' '), '(?&lt;=FINAL_NOTES).*'))\n# replace first column with citationkeys\nmeta_df[,1] &lt;- names(meta_list)\nnames(meta_df) &lt;- c('citekey', search_fields)\nnames(meta_df) &lt;- trimws(names(meta_df))"
  },
  {
    "objectID": "studies/library_parser.html#formatting",
    "href": "studies/library_parser.html#formatting",
    "title": "Library Parser",
    "section": "Formatting",
    "text": "Formatting\nFinally, we’ll perform some formatting to remove unwanted characters left over following the conversion (in progress)\n\n## remove bibtext field formatting\n# remove curly braces\nmeta_df &lt;- apply(meta_df, 2, function(x) str_remove_all(x, '\\\\{')) \nmeta_df &lt;- apply(meta_df, 2, function(x) str_remove_all(x, '\\\\},'))\n# remove first '=' (from bibtex field )\nmeta_df &lt;- apply(meta_df, 2, function(x) str_remove(x, '='))\n# remove double-commas\n#meta_df &lt;- apply(meta_df, 2, function(x) str_remove_all(x, ',,'))\n# remove comments\nmeta_df &lt;- apply(meta_df, 2, function(x) str_remove_all(x, '%%.*'))\n#meta_df &lt;- apply(meta_df, 2, function(x) str_remove_all(x, ' , '))\n# remove extra characters in final column\nmeta_df[, ncol(meta_df)] = str_remove_all(meta_df[, ncol(meta_df)], '\\\\}')\nmeta_df &lt;- as.data.frame(meta_df)\n\nExample: Evaluate model_rate_emotion_values as R code\n\neval(parse(text = meta_df$model_rate_emotion_values[22]))\n\n                             r2_arousal explained_variance_arousal r2_valence\nsvr                              0.7249                     0.7556     0.6119\nsbr                              0.7381                     0.7395     0.6296\nvbr                              0.7108                     0.7415     0.7108\nr2_valence                       0.6328                     0.6328     0.6328\nexplained_variance_valence       0.6340                     0.6340     0.6340\nr2_resonance                     0.5554                     0.5554     0.5554\nexplained_variance_resonance     0.5630                     0.5630     0.5630\n                             explained_variance_valence r2_resonance\nsvr                                              0.6142       0.5374\nsbr                                              0.6376       0.5456\nvbr                                              0.7415       0.7108\nr2_valence                                       0.6328       0.6328\nexplained_variance_valence                       0.6340       0.6340\nr2_resonance                                     0.5554       0.5554\nexplained_variance_resonance                     0.5630       0.5630\n                             explained_variance_resonance\nsvr                                                0.5496\nsbr                                                0.5558\nvbr                                                0.7415\nr2_valence                                         0.6328\nexplained_variance_valence                         0.6340\nr2_resonance                                       0.5554\nexplained_variance_resonance                       0.5630"
  },
  {
    "objectID": "studies/library_parser.html#track-excluded-studies-during-extraction",
    "href": "studies/library_parser.html#track-excluded-studies-during-extraction",
    "title": "Library Parser",
    "section": "Track Excluded Studies (During Extraction)",
    "text": "Track Excluded Studies (During Extraction)\n\nmeta_df[which(str_detect(meta_df$final_notes, '!EXCL!')),] |&gt; dplyr::tibble()\n\n# A tibble: 7 × 27\n  citekey        paradigm notes_ca notes_te emotions emotion_locus stimulus_type\n  &lt;chr&gt;          &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;         &lt;chr&gt;        \n1 \"feng2024ex\"   \" class… \" inclu… \" inclu… \"    \"   \"    \"        \"    \"       \n2 \"nag2022on\"    \" class… \" inclu… \" inclu… \" happy… \"    \"        \"    \"       \n3 \"malheiro2018… \" regre… \" inclu… \" inclu… \"    \"   \"    \"        \"    \"       \n4 \"medina2020em\" \" class… \" inclu… \" inclu… \" valen… \" perceived … \" MediaEval …\n5 \"panwar2019ar\" \" regre… \" inclu… \" inclu… \"    \"   \"    \"        \"    \"       \n6 \"vempala2024p… \" regre… \" inclu… \" inclu… \" valen… \" perceived … \" classical …\n7 \"xia2022st\"    \" regre… \" inclu… \" inclu… \"   \"    \"    \"        \"   \"        \n# ℹ 20 more variables: stimulus_duration &lt;chr&gt;, stimulus_duration_unit &lt;chr&gt;,\n#   stimulus_n &lt;chr&gt;, feature_n &lt;chr&gt;, participant_n &lt;chr&gt;,\n#   participant_expertise &lt;chr&gt;, participant_origin &lt;chr&gt;,\n#   participant_sampling &lt;chr&gt;, participant_task &lt;chr&gt;,\n#   feature_categories &lt;chr&gt;, feature_source &lt;chr&gt;,\n#   feature_reduction_method &lt;chr&gt;, model_category &lt;chr&gt;, model_detail &lt;chr&gt;,\n#   model_measure &lt;chr&gt;, model_complexity_parameters &lt;chr&gt;, …\n\n\n\nmeta_df[-which(str_detect(meta_df$final_notes, '!EXCL!')),] -&gt; included_studies\n\nincluded_studies |&gt; dplyr::tibble()\n\n# A tibble: 35 × 27\n   citekey       paradigm notes_ca notes_te emotions emotion_locus stimulus_type\n   &lt;chr&gt;         &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;         &lt;chr&gt;        \n 1 agarwal2021an \" class… \" inclu… \" inclu… \" discr… \" not specif… \" Hindi musi…\n 2 alvarez2023ri \" class… \" inclu… \" inclu… \" discr… \" not specif… \" Spotify  \" \n 3 bai2017mu     \" class… \" inclu… \" inclu… \" valen… \" perceived … \" MediaEval …\n 4 bhuvanakumar… \" class… \" inclu… \" inclu… \" quadr… \" not specif… \" pop piano …\n 5 dufour2021us  \" class… \" inclu… \" inclu… \"  c1 (… \" not specif… \" pop, disco…\n 6 hizlisoy2021… \" class… \" inclu… \" inclu… \" valen… \" perceived … \" Turkish tr…\n 7 nguyen2017an  \" class… \" inclu… \" inclu… \" 288 e… \" perceived … \" AMG  \"     \n 8 panda2020no   \" class… \" inclu… \" inclu… \"    \"   \"    \"        \" AllMusic  \"\n 9 sorussa2020em \" class… \" inclu… \" inclu… \" valen… \" perceived … \" DEAM  \"    \n10 yang2021an    \" class… \" inclu… \" inclu… \" happy… \" perceived … \" MediaEval …\n# ℹ 25 more rows\n# ℹ 20 more variables: stimulus_duration &lt;chr&gt;, stimulus_duration_unit &lt;chr&gt;,\n#   stimulus_n &lt;chr&gt;, feature_n &lt;chr&gt;, participant_n &lt;chr&gt;,\n#   participant_expertise &lt;chr&gt;, participant_origin &lt;chr&gt;,\n#   participant_sampling &lt;chr&gt;, participant_task &lt;chr&gt;,\n#   feature_categories &lt;chr&gt;, feature_source &lt;chr&gt;,\n#   feature_reduction_method &lt;chr&gt;, model_category &lt;chr&gt;, model_detail &lt;chr&gt;, …"
  },
  {
    "objectID": "studies/library_parser.html#data-frame-expansion",
    "href": "studies/library_parser.html#data-frame-expansion",
    "title": "Library Parser",
    "section": "Data Frame Expansion",
    "text": "Data Frame Expansion\nNext we want to copy the number of rows for each of the bibtex cells requiring special nesting"
  },
  {
    "objectID": "studies/library_parser.html#sanity-check",
    "href": "studies/library_parser.html#sanity-check",
    "title": "Library Parser",
    "section": "Sanity check",
    "text": "Sanity check"
  },
  {
    "objectID": "studies/library_parser.html#print-output-of-all-studies",
    "href": "studies/library_parser.html#print-output-of-all-studies",
    "title": "Library Parser",
    "section": "Print output of all studies:",
    "text": "Print output of all studies:\n\n# low-level function to extract relevant values from named array\nmodel_result_2_df &lt;- function(x) {\n  # evaluate the expression\n  x &lt;- rlang::eval_tidy(rlang::parse_expr(x))\n  # get name of fitted models from column names\n  model_names &lt;- colnames(x)\n  # print(model_names)\n  # split across model name for summary statistic (e.g., mean, sd, etc.)\n  model_statistic &lt;- str_split(model_names, '\\\\.')\n  model_statistic &lt;- unlist(lapply(model_statistic, function(x) x[2]))\n  # get values\n  model_values &lt;- as.numeric(x)\n  # names(model_values) &lt;- 'score'\n  print(model_values)\n  # get additional attributes (feature.data.exp)\n  model_attributes &lt;- rownames(x)\n  data.frame(model_attributes)\n  # get additional model details\n  model_attributes &lt;- data.frame(do.call('rbind', str_split(model_attributes, '\\\\.')))\n  names(model_attributes) &lt;- c('library_id', 'model_id', 'feature_id', 'data_id', 'experiment_id')\n  print(names(model_attributes))\n  # split across '_' to \n  model_measures &lt;- data.frame(do.call('rbind', stringr::str_split(colnames(x), '_')))\n  print(model_measures)\n  names(model_measures) &lt;- c('dimension', 'measure')\n  model_measures_split &lt;- stringr::str_split(model_measures[,'measure'], '\\\\.')\n  model_measures[,'measure'] &lt;- unlist(lapply(model_measures_split, \n                                              function(x) x[1]))\n  return(data.frame(\n             model_attributes, \n             model_measures, \n             values = model_values,\n             statistic = model_statistic))\n}\n\n# high level function to apply model_result_2_df to multiple studies\nget_study_summaries &lt;- function(df) {\n  do.call(rbind,\n          lapply(df$model_rate_emotion_values, \n                 FUN = function(x) {\n                   study_id &lt;- unique(df$citekey[which(df$model_rate_emotion_values == x)])\n                   model_results &lt;- model_result_2_df(x)\n                   return(cbind(study_id, model_results))\n                   }\n                 )\n          )\n}"
  },
  {
    "objectID": "preregistration/preregistration.html",
    "href": "preregistration/preregistration.html",
    "title": "metaMER",
    "section": "",
    "text": "This preregistration is made with preregr package from https://preregr.opens.science/ that implements the BMJ published guidance for meta-analysis protocols (Shamseer et al., 2015).\nMeta-analysis Pre-registration: Music Emotion Recognition\n\nSection: Metadata\n\n\n\nTitle\n\n\ntitle\n\n\n\nMusic emotion recognition: Meta-analysis of regression and classification success of emotion ratings from audio\n\n\n\n\n\nContributors\n\n\nauthors\n\n\n\nEerola, T., Anderson, C. J.\n\n\n\n\n\nSubjects\n\n\ntarget_discipline\n\n\n\nmusic cognition, music information retrieval, music psychology\n\n\n\n\n\nTasks and roles\n\n\ntasks_and_roles\n\n\n\nequal contribution\n\n\n\n\nSection: Review methods\n\n\n\nType of review\n\n\ntype_of_review\n\n\n\nMeta-analysis\n\n\n\n\n\nReview stages\n\n\nreview_stages\n\n\n\nSearch, Screening, Extraction, Synthesis\n\n\n\n\n\nCurrent review stage\n\n\ncurrent_stage\n\n\n\nScreening\n\n\n\n\n\nStart date\n\n\nstart_date\n\n\n\n2024-05-15 2024-05-15\n\n\n\n\n\nEnd date\n\n\nend_date\n\n\n\n2024-06-30\n\n\n\n\n\nBackground\n\n\nbackground\n\n\n\nThe aim is to establish the current state of the model success in predicting emotions expressed by music from audio. We will focus on the last 10 years of research and especially the research that has predicted valence and arousal ratings from music audio. No such analysis exists and there are interesting challenges in predicting emotional content of music that relates to specificity of the music and the type of emotions and features used that would benefit from a systematic analysis.\n\n\n\n\n\nPrimary research question(s)\n\n\nprimary_research_question\n\n\n\nTo what degree can arousal and valence ratings of emotions expressed by music be predicted from audio? How are the prediction rates related to genres of music, the type of models used, the type of features, modelling design and cross-validation utilised, and the model complexity and parsimony?\n\n\n\n\n\nSecondary research question(s)\n\n\nsecondary_research_question\n\n\n\nWhat is the prediction rate related to classification of quadrants in the affective circumplex?\n\n\n\n\n\nExpectations / hypotheses\n\n\nexpectations_hypotheses\n\n\n\nPrediction of arousal ratings is generally high and robust, and in terms of the model outcome metrics (correlation), achieves at least r = 0.77 (R square of 0.60). Prediction of valence ratings from audio is more challenging and more context dependent and will achieve generally a lower prediction rate, r = 0.63 (R square 0.40)\n\n\n\n\n\nDependent variable(s) / outcome(s) / main variables\n\n\ndvs_outcomes_main_vars\n\n\n\nRegression model performance will be converted to Pearson correlation coefficients and classification model performance will be converted to Matthews correlation coefficient (MCC) when possible.\n\n\n\n\n\nIndependent variable(s) / intervention(s) / treatment(s)\n\n\nivs_intervention_treatment\n\n\n\nMusic genre, prediction type (linear or classification), feature type (based on prior work by Panda et al., 2020), model complexity (high, medium, low), model validation (exists or not)\n\n\n\n\n\nAdditional variable(s) / covariate(s)\n\n\nadditional_variables\n\n\n\nUnspecified\n\n\n\n\n\nSoftware\n\n\nsoftware\n\n\n\nR and Github repository\n\n\n\n\n\nFunding\n\n\nfunding\n\n\n\nMitacs Globalink Research Award (Mitacs & British High Commission - Ottawa, Canada)\n\n\n\n\n\nConflicts of interest\n\n\ncois\n\n\n\nThere are no identified conflicts of interests.\n\n\n\n\n\nOverlapping authorships\n\n\noverlapping_authorships\n\n\n\nNot applicable\n\n\n\n\nSection: Search strategy\n\n\n\nDatabases\n\n\ndatabases\n\n\n\nWeb of Science, Scopus, and Open Alex\n\n\n\n\n\nInterfaces\n\n\ninterfaces\n\n\n\nWeb of Science, Scopus, and Open Alex\n\n\n\n\n\nGrey literature\n\n\ngrey_literature\n\n\n\nNot included\n\n\n\n\n\nInclusion and exclusion criteria\n\n\ninclusions_exclusion_criteria\n\n\n\nSample, Phenomenon of Interest, Design, Evaluation, Research type\n\n\n\n\n\nQuery strings\n\n\nquery_strings\n\n\n\nScopus: TITLE-ABS-KEY ( valence OR arousal OR classi OR categor OR algorithm AND music  AND emotion AND recognition ) AND PUBYEAR &gt; 2013 AND PUBYEAR &lt; 2025 AND  ( LIMIT-TO ( DOCTYPE , “ar” ) )  Web of science:  (DT=(Article) AND PY=(2014-2025)) AND ALL=(music emotion recognition valence arousal)  Open Alex:  https://openalex.org/works?page=1&filter=default.search%3Amusic%20emotion%20recognition%20valence%20arousal, type%3Atypes%2Farticle,publication_year%3A2014-2024, keywords.id%3Akeywords%2Femotion-recognition, keywords.id%3Akeywords%2Faffective-computing, language%3Alanguages%2Fen, open_access.any_repository_has_fulltext%3Atrue \n\n\n\n\n\nSearch validation procedure\n\n\nsearch_validation_procedure\n\n\n\nManual checking, separate keywords searches\n\n\n\n\n\nOther search strategies\n\n\nother_search_strategies\n\n\n\nNot applied\n\n\n\n\n\nProcedures to contact authors\n\n\nprocedure_for_contacting_authors\n\n\n\nUnspecified\n\n\n\n\n\nResults of contacting authors\n\n\nresults_of_contacting_authors\n\n\n\nNot carried out\n\n\n\n\n\nSearch expiration and repetition\n\n\nsearch_expiration_and_repetition\n\n\n\nSearches were done during the active search period in late May early June 2024 and no repetition is planned.\n\n\n\n\n\nSearch strategy justification\n\n\nsearch_strategy_justification\n\n\n\nThe three major databases should be able yield a robust picture of the topic\n\n\n\n\n\nMiscellaneous search strategy details\n\n\nmisc_search_strategy_details\n\n\n\nNo alternative searches were articulated or envisaged.\n\n\n\n\nSection: Screening\n\n\n\nScreening stages\n\n\nscreening_stages\n\n\n\nWe completed screening using custom fields inserted to the bibtex file and managed with citation managers (jabref and bibdesk). To filter relevant studies, we followed a three-stage screening procedure.  In stage 1, we screened the 553 studies’ titles for relevance, removing irrelevant studies and recording exclusion criteria (see Used exclusion criteria). CA assigned 63 studies to the High Priority based on titles’ relevance, assigned 338 studies to Low Priority based on irrelevant titles, and 152 studies to Medium Priority for additional screening. In stage 2, CA assessed the 152 Medium Priority studies for relevance by screening abstracts. 95 studies’ status changed to Low Priority, whereas 30 studies’ status changed to High Priority. 27 studies remained in the Medium priority category. TE and CA evaluated the remaining 27 studies’, moving 15 to the High Priority Category and 12 to the Medium Priority Category. For studies moved to Low Priority, brief BiBTex comments summarized the rationale for exclusion. In stage 3, TE and CA independently screened Priority 1 studies for relevance, including an include, exclude, or unsure decision in a user-comment BiBTeX field.\n\n\n\n\n\nScreened fields / masking\n\n\nscreened_fields_masking\n\n\n\nWe left authors, titles, publication years, and journal names unmasked.\n\n\n\n\n\nUsed exclusion criteria\n\n\nused_exclusion_criteria\n\n\n\nWe excluded studies according to the following exclusion criteria: soundscapes/vocalisations, non-music audio, video clips, physiological markers, dance, video/movie, physiological/EEG/ECG/MEG/GSR/brain imaging/heart rate/neuroscience/brain studies, sensor data, multimodal, autism, ageing, review/systematic review/overview/survey, face emotion recognition, mental health, music therapy, schizophrenia, memory/emotion factors as IVs, recommender systems, or systems that identify the location of emotional excerpts. We included results from some studies meeting exclusion criteria (e.g., multimodal studies involving physiological measurements) if they reported separately on acoustic-only models.\n\n\n\n\n\nScreener instructions\n\n\nscreener_instructions\n\n\n\nAs described above.\n\n\n\n\n\nScreening reliability\n\n\nscreening_reliability\n\n\n\nIn the pass 1 and 2, we included a quality control check after the pass to discuss the identified categories. In the third pass, we double-coded decisions, resolving discrepancies through discussion.\n\n\n\n\n\nScreening reconciliation procedure\n\n\nscreening_reconciliation_procedure\n\n\n\nWe reconcile discrepancies through discussion, resolving “unsure” votes first, followed by discrepancies in include/exclude decisions between authors Results of this updating procedure are available in the Pass 3 comparison document.\n\n\n\n\n\nSampling and sample size\n\n\nsampling_and_sample_size\n\n\n\nWe identified and retained 553 articles from Scopus, Web of Science, and Open Alex based on the search strategy outlined above. See table at the end that details the cumulative exclusions.\n\n\n\n\n\nScreening procedure justification\n\n\nscreening_procedure_justification\n\n\n\nTo offer a broad summary of music emotion recognition tasks, we attempted to include all studies involving prediction with acoustic features. We performed screening unblinded and determined inclusion/exclusion criteria based on studies’ relevance to the task explored.\n\n\n\n\n\nData management and sharing\n\n\nscreening_data_management_and_sharing\n\n\n\nSources will be shared as (a) BibTeX library(ies) including reviewer notes.\n\n\n\n\n\nMiscellaneous screening details\n\n\nmisc_screening_details\n\n\n\nUnspecified\n\n\n\n\nSection: Extraction\n\n\n\nEntities to extract\n\n\nentities_to_extract\n\n\n\nThese are listed and defined in extraction details.\n\n\n\n\n\nExtraction stages\n\n\nextraction_stages\n\n\n\nThe data extraction will be completed in stages. In the first stage, CA will complete a pass of the collection using our initial entities to extract document. The challenges are discussed and the entities are revised.\n\n\n\n\n\nExtractor instructions\n\n\nextractor_instructions\n\n\n\nSee extraction details.\n\n\n\n\n\nExtractor blinding\n\n\nextractor_blinding\n\n\n\nBlinding was not used.\n\n\n\n\n\nExtraction reliability\n\n\nextraction_reliability\n\n\n\nCA will perform extractions; TE will verify extractions for quality assurance.\n\n\n\n\n\nExtraction reconciliation procedure\n\n\nextraction_reconciliation_procedure\n\n\n\nDiscussion and joint decision for studies where extraction proves to be challenging and issues of interpretation arise.\n\n\n\n\n\nExtraction procedure justification\n\n\nextraction_procedure_justification\n\n\n\nThese are documented in the extraction details.\n\n\n\n\n\nData management and sharing\n\n\nextraction_data_management_and_sharing\n\n\n\nWe retain the information of the studies in shared bibtex files, extraction data will be stored in ascii data files (.bibtex), and the parser for reading the data from .bibtex files to R for the analysis will be available (as quarto/markdown/R files), and all these are managed, structured, shared and documented in Github repository according to FAIR principles.\n\n\n\n\n\nMiscellaneous extraction details\n\n\nmisc_extraction_details\n\n\n\nNA\n\n\n\n\nSection: Synthesis and Quality Assessment\n\n\n\nPlanned data transformations\n\n\nplanned_data_transformations\n\n\n\nFor regression studies, we convert all metrics to Pearson correlation coefficients. For classification studies, we convert the outcomes of classification to Matthews Correlation Coefficient (MCC) from the precision, accuracy, specificity, F1 scores. Alternatively, we use Cohen’s kappa for multiple classes.\n\n\n\n\n\nMissing data\n\n\nmissing_data\n\n\n\nIf no main outcome variables are available, we exclude the study.\n\n\n\n\n\nData validation\n\n\ndata_validation\n\n\n\nNone planned beyond the staged approached already documented in extraction process.\n\n\n\n\n\nQuality assessment\n\n\nquality_assessment\n\n\n\nNot all the bias assessment tools for clinical studies are relevant for our purposes, we adapt the overall approached advocated in [Higgins et al. (2011)] (https://doi.org/10.1136/bmj.d5928).\n\n\n\n\n\nSynthesis plan\n\n\nsynthesis_plan\n\n\n\nWe analyse regression and classification studies separately, and depending on the quantity of the studies forming suitable sub-groupings based on techniques, materials or music collections/genres, we may further synthesise the results across groupings that are formed along these subsets.\n\n\n\n\n\nCriteria for conclusions / inference criteria\n\n\ncriteria_for_conclusions\n\n\n\nNA\n\n\n\n\n\nSynthesist masking\n\n\nsynthesis_masking\n\n\n\nNA\n\n\n\n\n\nSynthesis reliability\n\n\nsynthesis_reliability\n\n\n\nNA\n\n\n\n\n\nSynthesis reconciliation procedure\n\n\nsynthesis_reconciliation_procedure\n\n\n\nNA\n\n\n\n\n\nPublication bias analyses\n\n\npublication_bias\n\n\n\nWe utilise Egger’s test to assess the publication bias and potentially correct the effect size bias by selecting 10% most precise effect sizes as recommended by Van Aert, Wicherts, & Van Assen (2019).\n\n\n\n\n\nSensitivity analyses / robustness checks\n\n\nsensitivity_analysis\n\n\n\nWithin regression and classificiation tasks, we will carry out sensitivity analysis using sub-groups of studied based on type of models, and the type of journal the studies were published in.\n\n\n\n\n\nSynthesis procedure justification\n\n\nsynthesis_procedure_justification\n\n\n\nWe share our justification of the synthesis and the subsetting carried out in the manuscript but we have not formulated these in advance except for synthesizing classiciation and regression approaches separately and creating subsets within these approaches according to techniques and datasets utilised.\n\n\n\n\n\nSynthesis data management and sharing\n\n\nsynthesis_data_management_and_sharing\n\n\n\nWe share the data, procedures, definitions, the analysis scripts with the outcomes as R code in Quarto notes at Github.\n\n\n\n\n\nMiscellaneous synthesis details\n\n\nmisc_synthesis_details\n\n\n\nUnspecified"
  },
  {
    "objectID": "preregistration/preregistration.html#preregr-prereg-spec-MS1IVl0rPd",
    "href": "preregistration/preregistration.html#preregr-prereg-spec-MS1IVl0rPd",
    "title": "metaMER",
    "section": "Meta-analysis Pre-registration: Music Emotion Recognition",
    "text": "Meta-analysis Pre-registration: Music Emotion Recognition\n\nSection: Metadata\n\n\n\nTitle\n\n\ntitle\n\n\n\nMusic emotion recognition: Meta-analysis of regression and classification success of emotion ratings from audio\n\n\n\n\n\nContributors\n\n\nauthors\n\n\n\nEerola, T., Anderson, C. J.\n\n\n\n\n\nSubjects\n\n\ntarget_discipline\n\n\n\nmusic cognition, music information retrieval, music psychology\n\n\n\n\n\nTasks and roles\n\n\ntasks_and_roles\n\n\n\nequal contribution\n\n\n\n\nSection: Review methods\n\n\n\nType of review\n\n\ntype_of_review\n\n\n\nMeta-analysis\n\n\n\n\n\nReview stages\n\n\nreview_stages\n\n\n\nSearch, Screening, Extraction, Synthesis\n\n\n\n\n\nCurrent review stage\n\n\ncurrent_stage\n\n\n\nScreening\n\n\n\n\n\nStart date\n\n\nstart_date\n\n\n\n2024-05-15 2024-05-15\n\n\n\n\n\nEnd date\n\n\nend_date\n\n\n\n2024-06-30\n\n\n\n\n\nBackground\n\n\nbackground\n\n\n\nThe aim is to establish the current state of the model success in predicting emotions expressed by music from audio. We will focus on the last 10 years of research and especially the research that has predicted valence and arousal ratings from music audio. No such analysis exists and there are interesting challenges in predicting emotional content of music that relates to specificity of the music and the type of emotions and features used that would benefit from a systematic analysis.\n\n\n\n\n\nPrimary research question(s)\n\n\nprimary_research_question\n\n\n\nTo what degree can arousal and valence ratings of emotions expressed by music be predicted from audio? How are the prediction rates related to genres of music, the type of models used, the type of features, modelling design and cross-validation utilised, and the model complexity and parsimony?\n\n\n\n\n\nSecondary research question(s)\n\n\nsecondary_research_question\n\n\n\nWhat is the prediction rate related to classification of quadrants in the affective circumplex?\n\n\n\n\n\nExpectations / hypotheses\n\n\nexpectations_hypotheses\n\n\n\nPrediction of arousal ratings is generally high and robust, and in terms of the model outcome metrics (correlation), achieves at least r = 0.77 (R square of 0.60). Prediction of valence ratings from audio is more challenging and more context dependent and will achieve generally a lower prediction rate, r = 0.63 (R square 0.40)\n\n\n\n\n\nDependent variable(s) / outcome(s) / main variables\n\n\ndvs_outcomes_main_vars\n\n\n\nRegression model performance will be converted to Pearson correlation coefficients and classification model performance will be converted to Matthews correlation coefficient (MCC) when possible.\n\n\n\n\n\nIndependent variable(s) / intervention(s) / treatment(s)\n\n\nivs_intervention_treatment\n\n\n\nMusic genre, prediction type (linear or classification), feature type (based on prior work by Panda et al., 2020), model complexity (high, medium, low), model validation (exists or not)\n\n\n\n\n\nAdditional variable(s) / covariate(s)\n\n\nadditional_variables\n\n\n\nUnspecified\n\n\n\n\n\nSoftware\n\n\nsoftware\n\n\n\nR and Github repository\n\n\n\n\n\nFunding\n\n\nfunding\n\n\n\nMitacs Globalink Research Award (Mitacs & British High Commission - Ottawa, Canada)\n\n\n\n\n\nConflicts of interest\n\n\ncois\n\n\n\nThere are no identified conflicts of interests.\n\n\n\n\n\nOverlapping authorships\n\n\noverlapping_authorships\n\n\n\nNot applicable\n\n\n\n\nSection: Search strategy\n\n\n\nDatabases\n\n\ndatabases\n\n\n\nWeb of Science, Scopus, and Open Alex\n\n\n\n\n\nInterfaces\n\n\ninterfaces\n\n\n\nWeb of Science, Scopus, and Open Alex\n\n\n\n\n\nGrey literature\n\n\ngrey_literature\n\n\n\nNot included\n\n\n\n\n\nInclusion and exclusion criteria\n\n\ninclusions_exclusion_criteria\n\n\n\nSample, Phenomenon of Interest, Design, Evaluation, Research type\n\n\n\n\n\nQuery strings\n\n\nquery_strings\n\n\n\nScopus: TITLE-ABS-KEY ( valence OR arousal OR classi OR categor OR algorithm AND music  AND emotion AND recognition ) AND PUBYEAR &gt; 2013 AND PUBYEAR &lt; 2025 AND  ( LIMIT-TO ( DOCTYPE , “ar” ) )  Web of science:  (DT=(Article) AND PY=(2014-2025)) AND ALL=(music emotion recognition valence arousal)  Open Alex:  https://openalex.org/works?page=1&filter=default.search%3Amusic%20emotion%20recognition%20valence%20arousal, type%3Atypes%2Farticle,publication_year%3A2014-2024, keywords.id%3Akeywords%2Femotion-recognition, keywords.id%3Akeywords%2Faffective-computing, language%3Alanguages%2Fen, open_access.any_repository_has_fulltext%3Atrue \n\n\n\n\n\nSearch validation procedure\n\n\nsearch_validation_procedure\n\n\n\nManual checking, separate keywords searches\n\n\n\n\n\nOther search strategies\n\n\nother_search_strategies\n\n\n\nNot applied\n\n\n\n\n\nProcedures to contact authors\n\n\nprocedure_for_contacting_authors\n\n\n\nUnspecified\n\n\n\n\n\nResults of contacting authors\n\n\nresults_of_contacting_authors\n\n\n\nNot carried out\n\n\n\n\n\nSearch expiration and repetition\n\n\nsearch_expiration_and_repetition\n\n\n\nSearches were done during the active search period in late May early June 2024 and no repetition is planned.\n\n\n\n\n\nSearch strategy justification\n\n\nsearch_strategy_justification\n\n\n\nThe three major databases should be able yield a robust picture of the topic\n\n\n\n\n\nMiscellaneous search strategy details\n\n\nmisc_search_strategy_details\n\n\n\nNo alternative searches were articulated or envisaged.\n\n\n\n\nSection: Screening\n\n\n\nScreening stages\n\n\nscreening_stages\n\n\n\nWe completed screening using custom fields inserted to the bibtex file and managed with citation managers (jabref and bibdesk). To filter relevant studies, we followed a three-stage screening procedure.  In stage 1, we screened the 553 studies’ titles for relevance, removing irrelevant studies and recording exclusion criteria (see Used exclusion criteria). CA assigned 63 studies to the High Priority based on titles’ relevance, assigned 338 studies to Low Priority based on irrelevant titles, and 152 studies to Medium Priority for additional screening. In stage 2, CA assessed the 152 Medium Priority studies for relevance by screening abstracts. 95 studies’ status changed to Low Priority, whereas 30 studies’ status changed to High Priority. 27 studies remained in the Medium priority category. TE and CA evaluated the remaining 27 studies’, moving 15 to the High Priority Category and 12 to the Medium Priority Category. For studies moved to Low Priority, brief BiBTex comments summarized the rationale for exclusion. In stage 3, TE and CA independently screened Priority 1 studies for relevance, including an include, exclude, or unsure decision in a user-comment BiBTeX field.\n\n\n\n\n\nScreened fields / masking\n\n\nscreened_fields_masking\n\n\n\nWe left authors, titles, publication years, and journal names unmasked.\n\n\n\n\n\nUsed exclusion criteria\n\n\nused_exclusion_criteria\n\n\n\nWe excluded studies according to the following exclusion criteria: soundscapes/vocalisations, non-music audio, video clips, physiological markers, dance, video/movie, physiological/EEG/ECG/MEG/GSR/brain imaging/heart rate/neuroscience/brain studies, sensor data, multimodal, autism, ageing, review/systematic review/overview/survey, face emotion recognition, mental health, music therapy, schizophrenia, memory/emotion factors as IVs, recommender systems, or systems that identify the location of emotional excerpts. We included results from some studies meeting exclusion criteria (e.g., multimodal studies involving physiological measurements) if they reported separately on acoustic-only models.\n\n\n\n\n\nScreener instructions\n\n\nscreener_instructions\n\n\n\nAs described above.\n\n\n\n\n\nScreening reliability\n\n\nscreening_reliability\n\n\n\nIn the pass 1 and 2, we included a quality control check after the pass to discuss the identified categories. In the third pass, we double-coded decisions, resolving discrepancies through discussion.\n\n\n\n\n\nScreening reconciliation procedure\n\n\nscreening_reconciliation_procedure\n\n\n\nWe reconcile discrepancies through discussion, resolving “unsure” votes first, followed by discrepancies in include/exclude decisions between authors Results of this updating procedure are available in the Pass 3 comparison document.\n\n\n\n\n\nSampling and sample size\n\n\nsampling_and_sample_size\n\n\n\nWe identified and retained 553 articles from Scopus, Web of Science, and Open Alex based on the search strategy outlined above. See table at the end that details the cumulative exclusions.\n\n\n\n\n\nScreening procedure justification\n\n\nscreening_procedure_justification\n\n\n\nTo offer a broad summary of music emotion recognition tasks, we attempted to include all studies involving prediction with acoustic features. We performed screening unblinded and determined inclusion/exclusion criteria based on studies’ relevance to the task explored.\n\n\n\n\n\nData management and sharing\n\n\nscreening_data_management_and_sharing\n\n\n\nSources will be shared as (a) BibTeX library(ies) including reviewer notes.\n\n\n\n\n\nMiscellaneous screening details\n\n\nmisc_screening_details\n\n\n\nUnspecified\n\n\n\n\nSection: Extraction\n\n\n\nEntities to extract\n\n\nentities_to_extract\n\n\n\nThese are listed and defined in extraction details.\n\n\n\n\n\nExtraction stages\n\n\nextraction_stages\n\n\n\nThe data extraction will be completed in stages. In the first stage, CA will complete a pass of the collection using our initial entities to extract document. The challenges are discussed and the entities are revised.\n\n\n\n\n\nExtractor instructions\n\n\nextractor_instructions\n\n\n\nSee extraction details.\n\n\n\n\n\nExtractor blinding\n\n\nextractor_blinding\n\n\n\nBlinding was not used.\n\n\n\n\n\nExtraction reliability\n\n\nextraction_reliability\n\n\n\nCA will perform extractions; TE will verify extractions for quality assurance.\n\n\n\n\n\nExtraction reconciliation procedure\n\n\nextraction_reconciliation_procedure\n\n\n\nDiscussion and joint decision for studies where extraction proves to be challenging and issues of interpretation arise.\n\n\n\n\n\nExtraction procedure justification\n\n\nextraction_procedure_justification\n\n\n\nThese are documented in the extraction details.\n\n\n\n\n\nData management and sharing\n\n\nextraction_data_management_and_sharing\n\n\n\nWe retain the information of the studies in shared bibtex files, extraction data will be stored in ascii data files (.bibtex), and the parser for reading the data from .bibtex files to R for the analysis will be available (as quarto/markdown/R files), and all these are managed, structured, shared and documented in Github repository according to FAIR principles.\n\n\n\n\n\nMiscellaneous extraction details\n\n\nmisc_extraction_details\n\n\n\nNA\n\n\n\n\nSection: Synthesis and Quality Assessment\n\n\n\nPlanned data transformations\n\n\nplanned_data_transformations\n\n\n\nFor regression studies, we convert all metrics to Pearson correlation coefficients. For classification studies, we convert the outcomes of classification to Matthews Correlation Coefficient (MCC) from the precision, accuracy, specificity, F1 scores. Alternatively, we use Cohen’s kappa for multiple classes.\n\n\n\n\n\nMissing data\n\n\nmissing_data\n\n\n\nIf no main outcome variables are available, we exclude the study.\n\n\n\n\n\nData validation\n\n\ndata_validation\n\n\n\nNone planned beyond the staged approached already documented in extraction process.\n\n\n\n\n\nQuality assessment\n\n\nquality_assessment\n\n\n\nNot all the bias assessment tools for clinical studies are relevant for our purposes, we adapt the overall approached advocated in [Higgins et al. (2011)] (https://doi.org/10.1136/bmj.d5928).\n\n\n\n\n\nSynthesis plan\n\n\nsynthesis_plan\n\n\n\nWe analyse regression and classification studies separately, and depending on the quantity of the studies forming suitable sub-groupings based on techniques, materials or music collections/genres, we may further synthesise the results across groupings that are formed along these subsets.\n\n\n\n\n\nCriteria for conclusions / inference criteria\n\n\ncriteria_for_conclusions\n\n\n\nNA\n\n\n\n\n\nSynthesist masking\n\n\nsynthesis_masking\n\n\n\nNA\n\n\n\n\n\nSynthesis reliability\n\n\nsynthesis_reliability\n\n\n\nNA\n\n\n\n\n\nSynthesis reconciliation procedure\n\n\nsynthesis_reconciliation_procedure\n\n\n\nNA\n\n\n\n\n\nPublication bias analyses\n\n\npublication_bias\n\n\n\nWe utilise Egger’s test to assess the publication bias and potentially correct the effect size bias by selecting 10% most precise effect sizes as recommended by Van Aert, Wicherts, & Van Assen (2019).\n\n\n\n\n\nSensitivity analyses / robustness checks\n\n\nsensitivity_analysis\n\n\n\nWithin regression and classificiation tasks, we will carry out sensitivity analysis using sub-groups of studied based on type of models, and the type of journal the studies were published in.\n\n\n\n\n\nSynthesis procedure justification\n\n\nsynthesis_procedure_justification\n\n\n\nWe share our justification of the synthesis and the subsetting carried out in the manuscript but we have not formulated these in advance except for synthesizing classiciation and regression approaches separately and creating subsets within these approaches according to techniques and datasets utilised.\n\n\n\n\n\nSynthesis data management and sharing\n\n\nsynthesis_data_management_and_sharing\n\n\n\nWe share the data, procedures, definitions, the analysis scripts with the outcomes as R code in Quarto notes at Github.\n\n\n\n\n\nMiscellaneous synthesis details\n\n\nmisc_synthesis_details\n\n\n\nUnspecified"
  },
  {
    "objectID": "preregistration/preregistration.html#references",
    "href": "preregistration/preregistration.html#references",
    "title": "metaMER",
    "section": "References",
    "text": "References\n\nHiggins, J. P. T., Altman, D. G., Gøtzsche, P. C., Jüni, P., Moher, D., Oxman, A. D., Savović, J., Schulz, K. F., Weeks, L., & Sterne, J. A. C. (2011). The Cochrane Collaboration tool for assessing risk of bias in randomised trials. BMJ, 343. https://www.bmj.com/content/343/bmj.d5928\nPanda, R., Malheiro, R., & Paiva, R. P. (2020). Audio features for music emotion recognition: a survey. IEEE Transactions on Affective Computing, 14(1), 68-88. https://doi.org/10.1109/TAFFC.2020.3032373\nShamseer, L., Moher, D., Clarke, M., Ghersi, D., Liberati, A., Petticrew, M., Shekelle, P., & Stewart, L. A. (2015). Preferred reporting items for systematic review and meta-analysis protocols (PRISMA-P) 2015: elaboration and explanation. BMJ, 349. https://www.bmj.com/content/349/bmj.g7647"
  }
]