%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Tuomas Eerola at 2025-02-22 18:15:15 +0000 


%% Saved with string encoding Unicode (UTF-8) 



@article{hizlisoy2021mu,
	author = {Hizlisoy, Serhat and Yildirim, Serdar and Tufekci, Zekeriya},
	date-added = {2025-02-22 17:52:10 +0000},
	date-modified = {2025-02-22 17:52:10 +0000},
	doi = {10.1016/j.jestch.2020.10.009},
	journal = {Engineering Science and Technology, an International Journal},
	number = {3},
	pages = {760 -- 767},
	title = {Music emotion recognition using convolutional long short term memory deep neural networks},
	type = {Article},
	volume = {24},
	year = {2021}}

@article{nguyen2017an,
	author = {Nguyen, Van Loi and Kim, Donglim and Ho, Van Phi and Lim, Younghwan},
	date-added = {2025-02-22 17:52:10 +0000},
	date-modified = {2025-02-22 17:52:10 +0000},
	doi = {10.11591/ijece.v7i3.pp1246-1254},
	journal = {International Journal of Electrical and Computer Engineering},
	number = {3},
	pages = {1246 -- 1254},
	title = {A new recognition method for visualizing music emotion},
	type = {Article},
	volume = {7},
	year = {2017}}

@article{panda2020no,
	author = {Panda, Renato and Malheiro, Ricardo and Paiva, Rui Pedro},
	date-added = {2025-02-22 17:52:10 +0000},
	date-modified = {2025-02-22 17:52:10 +0000},
	doi = {10.1109/TAFFC.2018.2820691},
	journal = {IEEE Transactions on Affective Computing},
	number = {4},
	pages = {614 -- 626},
	title = {Novel Audio Features for Music Emotion Recognition},
	type = {Article},
	volume = {11},
	year = {2020}}

@article{zhang2017fe,
	author = {Zhang, Jiang Long and Huang, Xiang Lin and Yang, Li Fang and Xu, Ye and Sun, Shu Tao},
	date-added = {2025-02-22 17:52:10 +0000},
	date-modified = {2025-02-22 17:52:10 +0000},
	doi = {10.1007/s00530-015-0489-y},
	journal = {Multimedia Systems},
	number = {2},
	pages = {251 -- 264},
	source = {scopus},
	title = {Feature selection and feature learning in arousal dimension of music emotion by using shrinkage methods},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-84944937222&doi = 10.1007%2fs00530-015-0489-y&partnerID = 40&md5 = f12764992ee29a4976644aacf6bbb43d},
	volume = {23},
	year = {2017}}

@article{zhang2016br,
	author = {Zhang, JiangLong and Huang, XiangLin and Yang, Lifang and Nie, Liqiang},
	date-added = {2025-02-22 17:52:10 +0000},
	date-modified = {2025-02-22 17:53:01 +0000},
	doi = {10.1016/j.neucom.2016.01.099},
	journal = {Neurocomputing},
	number = {SI},
	pages = {333-341},
	title = {Bridge the semantic gap between pop music acoustic feature and emotion: Build an interpretable model},
	volume = {208},
	year = {2016}}

@article{zentner2008emotions,
	author = {Zentner, Marcel and Grandjean, Didier and Scherer, Klaus R},
	date-added = {2025-02-09 15:37:34 +0000},
	date-modified = {2025-02-09 15:37:34 +0000},
	journal = {Emotion},
	keywords = {GEMS},
	number = {4},
	pages = {494-521},
	publisher = {American Psychological Association},
	title = {Emotions evoked by the sound of music: characterization, classification, and measurement.},
	volume = {8},
	year = {2008}}

@inproceedings{deng2009imagenet,
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and {Fei-Fei}, Li},
	booktitle = {2009 {{IEEE}} Conference on Computer Vision and Pattern Recognition},
	date-added = {2025-02-09 15:36:37 +0000},
	date-modified = {2025-02-09 15:36:37 +0000},
	pages = {248--255},
	publisher = {Ieee},
	title = {Imagenet: {{A}} Large-Scale Hierarchical Image Database},
	year = {2009}}

@article{krishna2017visual,
	author = {Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and others},
	date-added = {2025-02-09 15:36:14 +0000},
	date-modified = {2025-02-09 15:36:14 +0000},
	journal = {International Journal of Computer Vision},
	pages = {32--73},
	publisher = {Springer},
	title = {Visual Genome: {{Connecting}} Language and Vision Using Crowdsourced Dense Image Annotations},
	volume = {123},
	year = {2017}}

@article{hashem2023,
	abstract = {The speech emotion recognition (SER) field has been active since it became a crucial feature in advanced Human--Computer Interaction (HCI), and wide real-life applications use it. In recent years, numerous SER systems have been covered by researchers, including the availability of appropriate emotional databases, selecting robustness features, and applying suitable classifiers using Machine Learning (ML) and Deep Learning (DL). Deep models proved to perform more accurately for SER than conventional ML techniques. Nevertheless, SER is yet challenging for classification where to separate similar emotional patterns; it needs a highly discriminative feature representation. For this purpose, this survey aims to critically analyze what is being done in this field of research in light of previous studies that aim to recognize emotions using speech audio in different aspects and review the current state of SER using DL. Through a systematic literature review whereby searching selected keywords from 2012--2022, 96 papers were extracted and covered the most current findings and directions. Specifically, we covered the database (acted, evoked, and natural) and features (prosodic, spectral, voice quality, and teager energy operator), the necessary preprocessing steps. Furthermore, different DL models and their performance are examined in depth. Based on our review, we also suggested SER aspects that could be considered in the future.},
	author = {Hashem, Ahlam and Arif, Muhammad and Alghamdi, Manal},
	date-added = {2025-02-09 15:33:02 +0000},
	date-modified = {2025-02-09 15:33:32 +0000},
	doi = {10.1016/j.specom.2023.102974},
	file = {/Users/lqbn73/Zotero/storage/IG6XPA9V/S0167639323001085.html},
	issn = {0167-6393},
	journal = {Speech Communication},
	keywords = {Classification of emotion,Emotional speech database,Speech emotion recognition,Speech features,Systematic review},
	month = oct,
	pages = {102974},
	shorttitle = {Speech Emotion Recognition Approaches},
	title = {Speech Emotion Recognition Approaches: {{A}} Systematic Review},
	urldate = {2024-09-13},
	volume = {154},
	year = {2023}}

@article{krumhuber2017,
	abstract = {Temporal dynamics have been increasingly recognized as an important component of facial expressions. With the need for appropriate stimuli in research and application, a range of databases of dynamic facial stimuli has been developed. The present article reviews the existing corpora and describes the key dimensions and properties of the available sets. This includes a discussion of conceptual features in terms of thematic issues in dataset construction as well as practical features which are of applied interest to stimulus usage. To identify the most influential sets, we further examine their citation rates and usage frequencies in existing studies. General limitations and implications for emotion research are noted and future directions for stimulus generation are outlined.},
	author = {Krumhuber, Eva G. and Skora, Lina and K{\"u}ster, Dennis and Fou, Linyun},
	date-added = {2025-02-09 15:32:28 +0000},
	date-modified = {2025-02-09 15:33:46 +0000},
	doi = {10.1177/1754073916670022},
	file = {/Users/lqbn73/Zotero/storage/PSMP9SIM/Krumhuber et al. - 2017 - A Review of Dynamic Datasets for Facial Expression Research.pdf},
	issn = {1754-0739},
	journal = {Emotion Review},
	langid = {english},
	month = jul,
	number = {3},
	pages = {280--292},
	publisher = {SAGE Publications},
	title = {A Review of Dynamic Datasets for Facial Expression Research},
	urldate = {2024-09-13},
	volume = {9},
	year = {2017}}

@article{bornDiversifyingMIRKnowledge2020,
	author = {Born, Georgina},
	copyright = {http://creativecommons.org/licenses/by/4.0},
	date-added = {2025-02-09 15:24:20 +0000},
	date-modified = {2025-02-09 15:24:20 +0000},
	doi = {10.5334/tismir.58},
	file = {/Users/lqbn73/Zotero/storage/GHPHG6Z2/Born - 2020 - Diversifying MIR Knowledge and Real-World Challenges, and New Interdisciplinary Futures.pdf},
	issn = {2514-3298},
	journal = {Transactions of the International Society for Music Information Retrieval},
	langid = {english},
	month = oct,
	number = {1},
	pages = {193--204},
	shorttitle = {Diversifying {{MIR}}},
	title = {Diversifying {{MIR}}: Knowledge and Real-World Challenges, and New Interdisciplinary Futures},
	urldate = {2024-09-11},
	volume = {3},
	year = {2020}}

@inproceedings{lartillot2007matlab,
	author = {Lartillot, Olivier and Toiviainen, Petri},
	booktitle = {International conference on digital audio effects},
	date-added = {2025-02-09 14:50:58 +0000},
	date-modified = {2025-02-09 14:50:58 +0000},
	organization = {Bordeaux},
	pages = {244},
	title = {A Matlab toolbox for musical feature extraction from audio},
	volume = {237},
	year = {2007}}

@inproceedings{trohidis2008multi,
	author = {Trohidis, Konstantinos and Tsoumakas, Grigorios and Kalliris, George and Vlahavas, Ioannis P and others},
	booktitle = {ISMIR},
	date-added = {2025-02-09 10:52:50 +0000},
	date-modified = {2025-02-09 10:52:50 +0000},
	pages = {325--330},
	title = {Multi-label classification of music into emotions.},
	volume = {8},
	year = {2008}}

@inproceedings{sanden2011empirical,
	author = {Sanden, Chris and Zhang, John Z},
	booktitle = {ISMIR},
	date-added = {2025-02-09 10:52:03 +0000},
	date-modified = {2025-02-09 10:52:03 +0000},
	pages = {717--722},
	title = {An Empirical Study of Multi-Label Classifiers for Music Tag Annotation.},
	year = {2011}}

@inproceedings{chen2015amg1608,
	author = {Chen, Yu-An and Yang, Yi-Hsuan and Wang, Ju-Chiang and Chen, Homer},
	booktitle = {2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
	date-added = {2025-02-09 10:16:12 +0000},
	date-modified = {2025-02-09 10:16:12 +0000},
	organization = {IEEE},
	pages = {693--697},
	title = {The AMG1608 dataset for music emotion recognition},
	year = {2015}}

@inproceedings{soleymani20131000,
	author = {Soleymani, Mohammad and Caro, Micheal N and Schmidt, Erik M and Sha, Cheng-Ya and Yang, Yi-Hsuan},
	booktitle = {Proceedings of the 2nd ACM international workshop on Crowdsourcing for multimedia},
	date-added = {2025-02-09 10:15:07 +0000},
	date-modified = {2025-02-09 10:15:07 +0000},
	pages = {1--6},
	title = {1000 songs for emotional analysis of music},
	year = {2013}}

@inproceedings{eyben2010opensmile,
	author = {Eyben, Florian and W{\"o}llmer, Martin and Schuller, Bj{\"o}rn},
	booktitle = {Proceedings of the 18th ACM international conference on Multimedia},
	date-added = {2025-02-09 10:11:57 +0000},
	date-modified = {2025-02-09 10:11:57 +0000},
	pages = {1459--1462},
	title = {Opensmile: the munich versatile and fast open-source audio feature extractor},
	year = {2010}}

@article{yeh2014po,
	author = {Yeh, Chia-Hung and Tseng, Wen-Yu and Chen, Chia-Yen and Lin, Yu-Dun and Tsai, Yi-Ren and Bi, Hsuan-I and Lin, Yu-Ching and Lin, Ho-Yi},
	doi = {10.1007/s11042-013-1687-2},
	journal = {Multimedia Tools and Applications},
	keywords = {Multimedia systems; Chorus; Emotion; MFCCs; Popular music; Rhythm; Verse; Adaptive boosting},
	note = {Cited by: 11},
	number = {3},
	pages = {2103 -- 2128},
	source = {scopus},
	title = {Popular music representation: chorus detection & emotion recognition},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-84912027522&doi = 10.1007%2fs11042-013-1687-2&partnerID = 40&md5 = 6ce0fe7a0b38a6e2037403e140bd0a60},
	volume = {73},
	year = {2014}}

@article{yang2021an,
	author = {Yang, Jing},
	doi = {10.3389/fpsyg.2021.760060},
	journal = {Frontiers in Psychology},
	note = {Cited by: 14; All Open Access, Gold Open Access, Green Open Access},
	source = {scopus},
	title = {A Novel Music Emotion Recognition Model Using Neural Network Technology},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85116855247&doi = 10.3389%2ffpsyg.2021.760060&partnerID = 40&md5 = d8babd4e6050f14375d2b4632dfdb587},
	volume = {12},
	year = {2021}}

@article{xu2021us,
	author = {Xu, Liang and Sun, Zaoyi and Wen, Xin and Huang, Zhengxi and Chao, Chi-ju and Xu, Liuchang},
	doi = {10.7717/peerj-cs.785},
	eissn = {2376-5992},
	journal = {PEERJ COMPUTER SCIENCE},
	month = {NOV 15},
	source = {web_of_science},
	title = {Using machine learning analysis to interpret the relationship between music emotion and lyric features},
	volume = {7},
	year = {2021}}

@article{xie2020mu,
	author = {Xie, Baijun and Kim, Jonathan C. and Park, Chung Hyuk},
	doi = {10.3390/app10030902},
	eissn = {2076-3417},
	journal = {APPLIED SCIENCES-BASEL},
	month = {FEB},
	number = {3},
	source = {web_of_science},
	title = {Musical Emotion Recognition with Spectral Feature Extraction Based on a Sinusoidal Model with Model-Based and Deep-Learning Approaches},
	volume = {10},
	year = {2020}}

@article{wang2022cr,
	author = {Wang, Xin and Wei, Yujia and Yang, Dasheng},
	doi = {10.1049/ccs2.12032},
	eissn = {2517-7567},
	journal = {COGNITIVE COMPUTATION AND SYSTEMS},
	month = {JUN},
	number = {2, SI},
	pages = {116-129},
	source = {web_of_science},
	title = {Cross-cultural analysis of the correlation between musical elements and emotion},
	volume = {4},
	year = {2022}}

@article{wang2022co,
	author = {Wang, Xin and Wang, Li and Xie, Lingyun},
	date-modified = {2025-02-09 14:31:22 +0000},
	doi = {10.3390/app12125787},
	journal = {Applied Sciences},
	note = {Cited by: 7; All Open Access, Gold Open Access},
	number = {12},
	source = {scopus},
	title = {Comparison and Analysis of Acoustic Features of Western and Chinese Classical Music Emotion Recognition Based on V‚ÄêA Model},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85132193259&doi = 10.3390%2fapp12125787&partnerID = 40&md5 = 96f19b725a52bb52a769612e02f0a74c},
	volume = {12},
	year = {2022}}

@article{sorussa2020em,
	author = {Sorussa, Kanawat and Choksuriwong, Anant and Karnjanadecha, Montri},
	doi = {10.37936/ecti-cit.2020141.205317},
	journal = {ECTI Transactions on Computer and Information Technology},
	note = {Cited by: 4; All Open Access, Gold Open Access},
	number = {1},
	pages = {53 -- 66},
	source = {scopus},
	title = {Emotion classi cation system for digital music with a cascaded technique},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85084006024&doi = 10.37936%2fecti-cit.2020141.205317&partnerID = 40&md5 = 508dd81e9a9120b0c6e87d44e90a2d5a},
	volume = {14},
	year = {2020}}

@article{panwar2019ar,
	author = {Panwar, Sharaj and Rad, Paul and Choo, Kim-Kwang Raymond and Roopaei, Mehdi},
	doi = {10.1007/s11227-018-2499-y},
	eissn = {1573-0484},
	issn = {0920-8542},
	journal = {JOURNAL OF SUPERCOMPUTING},
	month = {JUN},
	number = {6, SI},
	pages = {2986-3009},
	source = {web_of_science},
	title = {Are you emotional or depressed? Learning about your emotional state from your music using machine learning},
	volume = {75},
	year = {2019}}

@article{orjesek2022en,
	author = {Orjesek, Richard and Jarina, Roman and Chmulik, Michal},
	doi = {10.1007/s11042-021-11584-7},
	earlyaccessdate = {JAN 2022},
	eissn = {1573-7721},
	issn = {1380-7501},
	journal = {MULTIMEDIA TOOLS AND APPLICATIONS},
	month = {FEB},
	number = {4},
	pages = {5017-5031},
	source = {web_of_science},
	title = {End-to-end music emotion variation detection using iteratively reconstructed deep features},
	volume = {81},
	year = {2022}}

@article{medina2020em,
	author = {Medina, Yesid Ospitia and Beltran, Jose Ramon and Baldassarri, Sandra},
	doi = {10.1007/s00779-020-01393-4},
	earlyaccessdate = {APR 2020},
	eissn = {1617-4917},
	issn = {1617-4909},
	journal = {PERSONAL AND UBIQUITOUS COMPUTING},
	month = {2020 APR 15},
	source = {web_of_science},
	title = {Emotional classification of music using neural networks with the MediaEval dataset},
	year = {2020}}

@article{markov2014mu,
	author = {Markov, Konstantin and Matsui, Tomoko},
	doi = {10.1109/ACCESS.2014.2333095},
	journal = {IEEE Access},
	keywords = {Classification (of information); Feature extraction; Gaussian distribution; Gaussian noise (electronic); Image retrieval; Support vector machines; Time series analysis; Coefficient of determination; Dimensionality reduction; Feature extraction methods; Gaussian Processes; Music emotions; Music genre classification; Music information retrieval; Prediction uncertainty; Audio acoustics},
	note = {Cited by: 73; All Open Access, Gold Open Access},
	pages = {688 -- 697},
	source = {scopus},
	title = {Music genre and emotion recognition using Gaussian processes},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-84923317754&doi = 10.1109%2fACCESS.2014.2333095&partnerID = 40&md5 = 91033029d14599965aafb73fa7175273},
	volume = {2},
	year = {2014}}

@article{hu2022de,
	author = {Hu, Xiao and Li, Fanjie and Liu, Ruilun},
	doi = {10.3390/app12189354},
	eissn = {2076-3417},
	journal = {APPLIED SCIENCES-BASEL},
	month = {SEP},
	number = {18},
	source = {web_of_science},
	title = {Detecting Music-Induced Emotion Based on Acoustic Analysis and Physiological Sensing: A Multimodal Approach},
	volume = {12},
	year = {2022}}

@article{hu2017cr,
	author = {Hu, Xiao and Yang, Yi-Hsuan},
	doi = {10.1109/TAFFC.2016.2523503},
	issn = {1949-3045},
	journal = {IEEE TRANSACTIONS ON AFFECTIVE COMPUTING},
	month = {APR-JUN},
	number = {2},
	pages = {228-240},
	source = {web_of_science},
	title = {Cross-Dataset and Cross-Cultural Music Mood Prediction: A Case on Western and Chinese Pop Songs},
	volume = {8},
	year = {2017}}

@article{coutinho2017sh,
	author = {Coutinho, Eduardo and Schuller, Bjorn},
	doi = {10.1371/journal.pone.0179289},
	issn = {1932-6203},
	journal = {PLOS ONE},
	month = {JUN 28},
	number = {6},
	source = {web_of_science},
	title = {Shared acoustic codes underlie emotional communication in music and speech-Evidence from deep transfer learning},
	volume = {12},
	year = {2017}}

@article{chin2018pr,
	author = {Chin, Yu-Hao and Wang, Jia-Ching and Wang, Ju-Chiang and Yang, Yi-Hsuan},
	doi = {10.1109/TAFFC.2016.2628794},
	issn = {1949-3045},
	journal = {IEEE TRANSACTIONS ON AFFECTIVE COMPUTING},
	month = {OCT-DEC},
	number = {4},
	pages = {541-549},
	source = {web_of_science},
	title = {Predicting the Probability Density Function of Music Emotion Using Emotion Space Mapping},
	volume = {9},
	year = {2018}}

@article{chen2017co,
	author = {Chen, Yu-An and Wang, Ju-Chiang and Yang, Yi-Hsuan and Chen, Homer H.},
	doi = {10.1109/TASLP.2017.2693565},
	eissn = {2329-9304},
	issn = {2329-9290},
	journal = {IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING},
	month = {JUL},
	number = {7},
	pages = {1409-1420},
	source = {web_of_science},
	title = {Component Tying for Mixture Model Adaptation in Personalization of Music Emotion Recognition},
	volume = {25},
	year = {2017}}

@article{cao2023th,
	author = {Cao, Yujing and Park, Jinwan},
	doi = {10.1109/ACCESS.2023.3341926},
	issn = {2169-3536},
	journal = {IEEE ACCESS},
	pages = {141192-141204},
	source = {web_of_science},
	title = {The Analysis of Music Emotion and Visualization Fusing Long Short-Term Memory Networks Under the Internet of Things},
	volume = {11},
	year = {2023}}

@article{bhuvanakumar2023em,
	author = {Bhuvana Kumar, V. and Kathiravan, M.},
	doi = {10.3389/fcomp.2023.1305413},
	journal = {Frontiers in Computer Science},
	note = {Cited by: 0; All Open Access, Gold Open Access},
	source = {scopus},
	title = {Emotion recognition from MIDI musical file using Enhanced Residual Gated Recurrent Unit architecture},
	type = {Article},
	volume = {5},
	year = {2023}}

@article{bai2017mu,
	author = {Bai, Junjie and Luo, Kan and Peng, Jun and Shi, Jinliang and Wu, Ying and Feng, Lixiao and Li, Jianqing and Wang, Yingxu},
	doi = {10.4018/IJCINI.2017100105},
	journal = {International Journal of Cognitive Informatics and Natural Intelligence},
	keywords = {Artificial intelligence; Arts computing; Barium compounds; Discriminant analysis; Feature extraction; Fuzzy neural networks; Image retrieval; Learning systems; Nearest neighbor search; Pattern recognition; Psychophysiology; Support vector machines; Affective Computing; Classification methodologies; Emotion classification; K nearest neighbor (KNN); Linear discriminant analysis; Multiple disciplines; Music emotions; Neuro-fuzzy network; Learning algorithms},
	number = {4},
	pages = {80 -- 92},
	source = {scopus},
	title = {Music emotions recognition by machine learning with cognitive classification methodologies},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85038434665&doi = 10.4018%2fIJCINI.2017100105&partnerID = 40&md5 = a2e1f1d69ab6ceb6f0cee7f2eb26985a},
	volume = {11},
	year = {2017}}

@article{bai2016di,
	author = {Bai, Junjie and Feng, Lixiao and Peng, Jun and Shi, Jinliang and Luo, Kan and Li, Zuojin and Liao, Lu and Wang, Yingxu},
	doi = {10.4018/IJCINI.2016100104},
	eissn = {1557-3966},
	issn = {1557-3958},
	journal = {INTERNATIONAL JOURNAL OF COGNITIVE INFORMATICS AND NATURAL INTELLIGENCE},
	month = {OCT-DEC},
	number = {4},
	pages = {74-89},
	source = {web_of_science},
	title = {Dimensional Music Emotion Recognition by Machine Learning},
	volume = {10},
	year = {2016}}

@article{alvarez2023ri,
	author = {{\'A}lvarez, P. and de Quir{\'o}s, J. Garc{\'\i}a and Baldassarri, S.},
	doi = {10.9781/ijimai.2022.04.002},
	journal = {International Journal of Interactive Multimedia and Artificial Intelligence},
	note = {Cited by: 4; All Open Access, Gold Open Access, Green Open Access},
	number = {2},
	pages = {168 -- 181},
	source = {scopus},
	title = {RIADA: A Machine-Learning Based Infrastructure for Recognising the Emotions of Spotify Songs},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85147947463&doi = 10.9781%2fijimai.2022.04.002&partnerID = 40&md5 = 8f61f829c998052047f5da67adcf48c9},
	volume = {8},
	year = {2023}}

@article{agarwal2021an,
	author = {Agarwal, Gaurav and Om, Hari},
	doi = {10.1049/sil2.12015},
	journal = {IET Signal Processing},
	keywords = {Character recognition; Emotion Recognition; Learning systems; Recurrent neural networks; Regression analysis; Storms; Auto encoders; Emotion classification; Emotion recognition; Feature representation; Hidden layers; High-level features; K-medoid; Music emotions; Support vector regression models; Support vector regressions; Music},
	note = {Cited by: 23},
	number = {2},
	pages = {98 -- 121},
	source = {scopus},
	title = {An efficient supervised framework for music mood recognition using autoencoder-based optimised support vector regression model},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85116929889&doi = 10.1049%2fsil2.12015&partnerID = 40&md5 = 4e078ee8b1c740d21d4b7d8dbf7f6cd8},
	volume = {15},
	year = {2021}}

@article{wang2021ac,
	author = {Wang, Xin and Wei, Yujia and Heng, Lena and McAdams, Stephen},
	doi = {10.3389/fpsyg.2021.732865},
	issn = {1664-1078},
	journal = {FRONTIERS IN PSYCHOLOGY},
	month = {SEP 29},
	title = {A Cross-Cultural Analysis of the Influence of Timbre on Affect Perception in Western Classical Music and Chinese Music Traditions},
	volume = {12},
	year = {2021}}

@article{koh2023me,
	author = {Koh, En Yan and Cheuk, Kin Wai and Heung, Kwan Yee and Agres, Kat R. and Herremans, Dorien},
	doi = {10.3390/s23010382},
	eissn = {1424-8220},
	journal = {SENSORS},
	month = {JAN},
	number = {1},
	title = {MERP: A Music Dataset with Emotion Ratings and Raters' Profile Information},
	volume = {23},
	year = {2023}}

@article{eggersmith_1997,
	author = {Egger, Matthias and Smith, George Davey and Schneider, Martin and Minder, Christoph},
	date-added = {2024-10-20 18:39:33 +0100},
	date-modified = {2024-10-20 18:39:33 +0100},
	journal = {bmj},
	number = {7109},
	pages = {629--634},
	publisher = {British Medical Journal Publishing Group},
	title = {Bias in meta-analysis detected by a simple, graphical test},
	volume = {315},
	year = {1997}}

@article{van-aertwicherts_2016,
	author = {Van Aert, Robbie CM and Wicherts, Jelte M and van Assen, Marcel ALM},
	date-added = {2024-10-20 10:40:20 +0100},
	date-modified = {2024-10-20 10:40:20 +0100},
	journal = {Perspectives on Psychological Science},
	number = {5},
	pages = {713--729},
	publisher = {Sage Publications Sage CA: Los Angeles, CA},
	title = {Conducting meta-analyses based on p values: Reservations and recommendations for applying p-uniform and p-curve},
	volume = {11},
	year = {2016}}

@article{beveridge2018po,
	author = {Beveridge, Scott and Knox, Don},
	date-added = {2024-10-11 11:31:06 +0100},
	date-modified = {2024-10-11 11:31:06 +0100},
	doi = {10.1177/0305735617713834},
	eissn = {1741-3087},
	issn = {0305-7356},
	journal = {PSYCHOLOGY OF MUSIC},
	month = {MAY},
	number = {3},
	pages = {411-423},
	source = {web_of_science},
	title = {Popular music and the role of vocal melody in perceived emotion},
	volume = {46},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1177/0305735617713834}}

@article{griffiths2021am,
	author = {Griffiths, Darryl and Cunningham, Stuart and Weinel, Jonathan and Picking, Richard},
	date-added = {2024-10-11 11:30:37 +0100},
	date-modified = {2024-10-11 11:30:37 +0100},
	doi = {10.1080/09298215.2021.1977336},
	earlyaccessdate = {SEP 2021},
	eissn = {1744-5027},
	issn = {0929-8215},
	journal = {JOURNAL OF NEW MUSIC RESEARCH},
	month = {AUG 8},
	number = {4},
	pages = {355-372},
	source = {web_of_science},
	title = {A multi-genre model for music emotion recognition using linear regressors},
	volume = {50},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1080/09298215.2021.1977336}}

@article{higgins2002quantifying,
	author = {Higgins, Julian PT and Thompson, Simon G},
	date-added = {2024-10-10 11:48:25 +0100},
	date-modified = {2024-10-10 11:48:30 +0100},
	journal = {Statistics in Medicine},
	number = {11},
	pages = {1539--1558},
	publisher = {Wiley Online Library},
	title = {Quantifying heterogeneity in a meta-analysis},
	volume = {21},
	year = {2002}}

@article{knapp2003improved,
	author = {Knapp, Guido and Hartung, Joachim},
	date-added = {2024-10-10 11:42:03 +0100},
	date-modified = {2024-10-10 11:42:08 +0100},
	journal = {Statistics in Medicine},
	number = {17},
	pages = {2693--2710},
	publisher = {Wiley Online Library},
	title = {Improved tests for a random effects meta-regression with a single covariate},
	volume = {22},
	year = {2003}}

@article{langan2019comparison,
	author = {Langan, Dean and Higgins, Julian PT and Jackson, Dan and Bowden, Jack and Veroniki, Areti Angeliki and Kontopantelis, Evangelos and Viechtbauer, Wolfgang and Simmonds, Mark},
	date-added = {2024-10-10 11:40:09 +0100},
	date-modified = {2024-10-10 11:40:16 +0100},
	journal = {Research Synthesis Methods},
	number = {1},
	pages = {83--98},
	publisher = {Wiley Online Library},
	title = {A comparison of heterogeneity variance estimators in simulated random-effects meta-analyses},
	volume = {10},
	year = {2019}}

@article{agres2021music,
	author = {Agres, Kat R and Schaefer, Rebecca S and Volk, Anja and Van Hooren, Susan and Holzapfel, Andre and Dalla Bella, Simone and M{\"u}ller, Meinard and De Witte, Martina and Herremans, Dorien and Ramirez Melendez, Rafael and others},
	date-added = {2024-10-10 11:03:55 +0100},
	date-modified = {2024-10-10 11:03:55 +0100},
	journal = {Music \& Science},
	pages = {2059204321997709},
	publisher = {SAGE Publications Sage UK: London, England},
	title = {Music, computing, and health: a roadmap for the current and future roles of music technology for health care and well-being},
	volume = {4},
	year = {2021}}

@article{balduzzi2019,
	author = {Sara Balduzzi and Gerta R{\"u}cker and Guido Schwarzer},
	date-added = {2024-09-25 13:43:41 +0100},
	date-modified = {2024-09-25 13:43:41 +0100},
	journal = {Evidence-Based Mental Health},
	number = {22},
	pages = {153--160},
	title = {How to perform a meta-analysis with {R}: a practical tutorial},
	year = {2019}}

@article{zhang2023mo,
	author = {Zhang, Meixian and Zhu, Yonghua and Zhang, Wenjun and Zhu, Yunwen and Feng, Tianyu},
	bdsk = {https://doi.org/10.1007/s11042-022-13577-6},
	date-added = {2024-09-25 11:16:42 +0100},
	date-modified = {2024-09-25 11:16:42 +0100},
	doi = {10.1007/s11042-022-13577-6},
	earlyaccessdate = {AUG 2022},
	eissn = {1573-7721},
	emotion_locus = {perceived},
	emotions = {valence, arousal},
	feature_categories = {filter banks, handcrafted},
	feature_n = {60 handcrafted and filter bank features, extracted filter bank output size: 120 * 120},
	feature_reduction_method = {weighted attention module, feature augmentation},
	feature_source = {daubechie, MIRToolbox, Sound Description Toolbox, filter bank output (size = 120*120), 60 handcrafted features (size = 60*60)},
	issn = {1380-7501},
	journal = {MULTIMEDIA TOOLS AND APPLICATIONS},
	model_category = {regression},
	model_complexity_parameters = {Batch size: 15, mixup technique applied to spectrograms, first convolutional layer alpha: 0.2, learning rate: 0.001, decreasing by 1/10 every 10 epochs. Adam optimizer. Dropout ratio 0.3 and 0.2, alpha = 0.18, beta = 0.49},
	model_detail = {Modularized Composite Attention Network. Case 1: no sample reconstruction. Case 2: no handcrafted features (or feature augmentation). Case 3: No self-attention mechanism or BLSTM. Case 4: no style embedding module},
	model_measure = {RMSE, MAE, CCC},
	model_rate_emotion_names = {valence, arousal},
	model_rate_emotion_values = {bind_field( 'ns.MCAN.handcrafted and filter bank.deam.1' = c(valence_ccc.mean = 0.309, valence_rmse.mean = 0.112, valence_mae.mean = 0.811, valence_ccc.sd = 0.358, valence_rmse.sd = 0.010, valence_mae.sd = 0.009, arousal_ccc.mean = 0.502, arousal_rmse.mean = 0.109, arousal_mae.mean = 0.764, arousal_ccc.sd = 0.287, arousal_rmse.sd = 0.011, arousal_mae.sd = 0.023), 'ns.MCAN. handcrafted and filter bank.PMEmo.1' = c(.215,.144,.892,0.265,.102,.030,.401,.135,.901,.315,0.057,0.103) )},
	model_validation = {ablation, train/test},
	month = {FEB},
	notes_ca = {include},
	notes_te = {include, R2},
	number = {5},
	orcid = {Zhang, Meixian/0000-0002-6696-2814},
	pages = {7319-7341},
	paradigm = {regression},
	participant_expertise = {primarily nonexperts},
	participant_n = {DEAM: at least 10 per piece, PMEmo: 457},
	participant_origin = {DEAM: MTurk; PMEmo: lab},
	participant_sampling = {DEAM: crowdsource; PMEmo: convenience},
	participant_task = {rate},
	researcherid = {Zhu, Yonghua/HSI-1360-2023 Feng, Tian-Yu/AAX-3892-2020},
	source = {web_of_science},
	stimulus_duration = {DEAM: 45, PMEmo: variable},
	stimulus_duration_unit = {s},
	stimulus_genre = {pop, multi (pop of various genres)},
	stimulus_n = {DEAM: 744, PMEmo: 206},
	stimulus_type = {PMEmo, DEAM},
	title = {Modularized composite attention network for continuous music emotion recognition},
	volume = {82},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1007/s11042-022-13577-6}}

@article{battcock2021in,
	author = {Battcock, Aimee and Schutz, Michael},
	bdsk = {https://doi.org/10.1080/09298215.2021.1979050},
	date-added = {2024-09-25 11:16:03 +0100},
	date-modified = {2024-09-25 11:16:03 +0100},
	doi = {10.1080/09298215.2021.1979050},
	earlyaccessdate = {JAN 2022},
	eissn = {1744-5027},
	emotion_locus = {perceived},
	emotions = {valence, arousal},
	feature_categories = {melody, harmony, rhythm},
	feature_n = {3},
	feature_reduction_method = {none},
	feature_source = {manual},
	issn = {0929-8215},
	journal = {JOURNAL OF NEW MUSIC RESEARCH},
	model_category = {regression},
	model_complexity_parameters = {4},
	model_detail = {commonality analysis, multiple regression},
	model_measure = {R2},
	model_rate_emotion_names = {valence, arousal},
	model_rate_emotion_values = {bind_field( manual.lm.score.bachwtc.1 = c('valence_r2' = 0.808, 'arousal_r2' = 0.788) )},
	model_validation = {none},
	month = {OCT 20},
	notes_ca = {include},
	notes_te = {include, commonality analysis},
	number = {5},
	pages = {447-468},
	paradigm = {regression},
	participant_expertise = {nonmusicians},
	participant_n = {180},
	participant_origin = {canada},
	participant_sampling = {convenience},
	participant_task = {rate},
	source = {web_of_science},
	stimulus_duration = {8},
	stimulus_duration_unit = {measure},
	stimulus_genre = {classical},
	stimulus_n = {336},
	stimulus_type = {prelude},
	title = {Individualized interpretation: Exploring structural and interpretive effects on evaluations of emotional content in Bach's Well Tempered Clavier},
	volume = {50},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1080/09298215.2021.1979050}}

@article{saizclar2022pr,
	author = {Saiz-Clar, Elena and Angel Serrano, Miguel and Manuel Reales, Jose},
	bdsk = {https://doi.org/10.1177/03057356211031658},
	date-added = {2024-09-25 11:02:28 +0100},
	date-modified = {2024-09-25 11:02:28 +0100},
	doi = {10.1177/03057356211031658},
	earlyaccessdate = {AUG 2021},
	eissn = {1741-3087},
	emotion_locus = {not specified},
	emotions = {activation, valence},
	feature_categories = {rhythm},
	feature_n = {22},
	feature_reduction_method = {correlation significance, PCA},
	feature_source = {MIRToolbox},
	issn = {0305-7356},
	journal = {PSYCHOLOGY OF MUSIC},
	model_category = {regression},
	model_complexity_parameters = {23},
	model_detail = {MLR, five predictors following PCA},
	model_measure = {R2, eta2},
	model_rate_emotion_names = {valence, activation},
	model_rate_emotion_values = {bind_field('MIRToolbox.MLR.onset curves.1.1' = c(activation_r2.r2 = 0.537, activation_eta2.eta2 = 0.537, valence_r2.r2 = 0.218, valence_eta2.eta2 = 0.218))},
	model_validation = {cross-validation},
	month = {JUL},
	notes_ca = {exclude, no modeling task},
	notes_te = {include, R2},
	number = {4},
	orcid = {Serrano, Miguel-Angel/0000-0002-6574-4532 Serrano, Miguel-Angel/0000-0002-6574-4532 Saiz-Clar, Elena/0000-0002-8714-2305},
	pages = {1107-1120},
	paradigm = {regression},
	participant_expertise = {no formal training},
	participant_n = {16},
	participant_origin = {not specified},
	participant_sampling = {not specified},
	participant_task = {rate, categorize},
	researcherid = {Serrano, Miguel-Angel/ABE-7279-2021 Serrano, Miguel-Angel/AAZ-4393-2020},
	source = {web_of_science},
	stimulus_duration = {8-12},
	stimulus_duration_unit = {s},
	stimulus_genre = {classical},
	stimulus_n = {40},
	stimulus_type = {piano pieces, classical music},
	title = {Predicting emotions in music using the onset curve},
	volume = {50},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1177/03057356211031658}}

@article{fu2010survey,
	author = {Fu, Zhouyu and Lu, Guojun and Ting, Kai Ming and Zhang, Dengsheng},
	date-added = {2024-06-18 08:51:34 +0100},
	date-modified = {2024-06-18 08:53:52 +0100},
	journal = {IEEE Transactions on Multimedia},
	number = {2},
	pages = {303--319},
	title = {A survey of audio-based music classification and annotation},
	volume = {13},
	year = {2010}}

@article{chicco2020advantages,
	author = {Chicco, Davide and Jurman, Giuseppe},
	date-added = {2024-06-18 08:37:28 +0100},
	date-modified = {2024-06-18 08:37:28 +0100},
	journal = {BMC genomics},
	pages = {1--13},
	publisher = {Springer},
	title = {The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation},
	volume = {21},
	year = {2020}}

@article{sarkar2020recognition,
	author = {Sarkar, Rajib and Choudhury, Sombuddha and Dutta, Saikat and Roy, Aneek and Saha, Sanjoy Kumar},
	date-added = {2024-06-18 08:28:19 +0100},
	date-modified = {2024-06-18 08:28:19 +0100},
	journal = {Multimedia Tools and Applications},
	number = {1},
	pages = {765--783},
	publisher = {Springer},
	title = {Recognition of emotion in music based on deep convolutional neural network},
	volume = {79},
	year = {2020}}

@article{er2019music,
	author = {Er, Mehmet Bilal and Aydilek, Ibrahim Berkan},
	date-added = {2024-06-18 08:27:33 +0100},
	date-modified = {2024-06-18 08:27:33 +0100},
	journal = {International Journal of Computational Intelligence Systems},
	number = {2},
	pages = {1622--1634},
	publisher = {Springer},
	title = {Music emotion recognition by using chroma spectrogram and deep visual features},
	volume = {12},
	year = {2019}}

@article{eerola2011c,
	author = {Eerola, T.},
	date-added = {2024-06-18 08:26:30 +0100},
	date-modified = {2024-06-18 08:26:30 +0100},
	journal = {Journal of New Music Research},
	keywords = {Genre, Emotion, Modelling, Acoustic feature},
	number = {4},
	pages = {349-366},
	title = {Are the emotions expressed in music genre-specific? An audio-based evaluation of datasets spanning classical, film, pop and mixed genres},
	url = {https://doi.org/10.1080/09298215.2011.602195},
	volume = {40},
	year = {2011},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfED8uLi8uLi8uLi8uLi9tZXJpdHMvUHVibGljYXRpb25zL2RhdGFiYXNlL1BhcGVycy9FZXJvbGEyMDExYy5wZGZPEQRYYm9va1gEAAAAAAQQMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAVAMAAAUAAAABAQAAVXNlcnMAAAAGAAAAAQEAAGxxYm43MwAABwAAAAEBAABEcm9wYm94AAYAAAABAQAAbWVyaXRzAAAMAAAAAQEAAFB1YmxpY2F0aW9ucwgAAAABAQAAZGF0YWJhc2UGAAAAAQEAAFBhcGVycwAADwAAAAEBAABFZXJvbGEyMDExYy5wZGYAIAAAAAEGAAAEAAAAFAAAACQAAAA0AAAARAAAAFgAAABoAAAAeAAAAAgAAAAEAwAAFV0AAAAAAAAIAAAABAMAAEAFBAAAAAAACAAAAAQDAAAlNssAAAAAAAgAAAAEAwAAkerMAAAAAAAIAAAABAMAACnrzAAAAAAACAAAAAQDAACw98wAAAAAAAgAAAAEAwAAwvfMAAAAAAAIAAAABAMAAOT6zAAAAAAAIAAAAAEGAAC4AAAAyAAAANgAAADoAAAA+AAAAAgBAAAYAQAAKAEAAAgAAAAABAAAQbSzdboAAAAYAAAAAQIAAAEAAAAAAAAADwAAAAAAAAAAAAAAAAAAAAgAAAAEAwAABgAAAAAAAAAEAAAAAwMAAPcBAAAIAAAAAQkAAGZpbGU6Ly8vDAAAAAEBAABNYWNpbnRvc2ggSEQIAAAABAMAAABQoRtzAAAACAAAAAAEAABBxfUErAAAACQAAAABAQAAMjhDOEI0RjktNzc1OC00NUFELUE1NTYtREU0MDJFNEU2MzcwGAAAAAECAACBAAAAAQAAAO8TAAABAAAAAAAAAAAAAAABAAAAAQEAAC8AAAAAAAAAAQUAAPsAAAABAgAAMDg2MjMwN2NjNzFjNTQ4MTgzZDVjNTEwNjk5ZTEyNmFjMDJiODgwYTkwYWZiM2E5OTM4MDYwMTFkYzM0NWE3NTswMDswMDAwMDAwMDswMDAwMDAwMDswMDAwMDAwMDswMDAwMDAwMDAwMDAwMDIwO2NvbS5hcHBsZS5hcHAtc2FuZGJveC5yZWFkLXdyaXRlOzAxOzAxMDAwMDBmOzAwMDAwMDAwMDBjY2ZhZTQ7MDE7L3VzZXJzL2xxYm43My9kcm9wYm94L21lcml0cy9wdWJsaWNhdGlvbnMvZGF0YWJhc2UvcGFwZXJzL2Vlcm9sYTIwMTFjLnBkZgAAzAAAAP7///8BAAAAAAAAABAAAAAEEAAAkAAAAAAAAAAFEAAAOAEAAAAAAAAQEAAAcAEAAAAAAABAEAAAYAEAAAAAAAACIAAAPAIAAAAAAAAFIAAArAEAAAAAAAAQIAAAvAEAAAAAAAARIAAA8AEAAAAAAAASIAAA0AEAAAAAAAATIAAA4AEAAAAAAAAgIAAAHAIAAAAAAAAwIAAASAIAAAAAAAABwAAAkAEAAAAAAAARwAAAFAAAAAAAAAASwAAAoAEAAAAAAACA8AAAUAIAAAAAAAAACAANABoAIwBlAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABME=},
	bdsk-url-1 = {https://doi.org/10.1080/09298215.2011.602195}}

@article{lindstrom2003expressivity,
	author = {Lindstr{\"o}m, Erik and Juslin, Patrik N and Bresin, Roberto and Williamon, Aaron},
	date-added = {2024-06-18 08:26:04 +0100},
	date-modified = {2024-06-18 08:26:04 +0100},
	journal = {Research Studies in Music Education},
	number = {1},
	pages = {23--47},
	publisher = {Sage Publications Sage UK: London, England},
	title = {"Expressivity comes from within your soul'': A questionnaire study of music students' perspectives on expressivity},
	volume = {20},
	year = {2003}}

@article{Russell1980,
	author = {J. A. Russell},
	date-added = {2024-06-18 08:20:59 +0100},
	date-modified = {2024-06-18 08:20:59 +0100},
	journal = {Journal of Personality and Social Psychology},
	number = {6},
	pages = {1161-1178},
	title = {A circumplex model of affect},
	volume = {39},
	year = {1980}}

@article{yang2008,
	author = {Yang, Y.H. and Lin, Y.C. and Su, Y.F. and Chen, H.H.},
	date-added = {2024-06-18 08:20:17 +0100},
	date-modified = {2024-06-18 08:20:17 +0100},
	emotion = {valence and arousal, dimensional, perceived},
	journal = {IEEE Transactions on Audio Speech and Language Processing},
	method = {self-report, computational, music analytic},
	number = {2},
	pages = {448-457},
	publisher = {IEEE},
	rating = {5},
	read = {Yes},
	sample = {N=253 students},
	stimuli = {N=195 popular songs, length 25 s, researcher chosen, real},
	title = {{A regression approach to music emotion recognition}},
	volume = {16},
	year = {2008}}

@inproceedings{barthet2013,
	abstract = {The striking ability of music to elicit emotions assures its prominent status in human culture and every day life. Music is often enjoyed and sought for its ability to induce or convey emotions, which may manifest in anything from a slight variation in mood, to changes in our physical condition and actions. Consequently, research on how we might associate musical pieces with emotions and, more generally, how music brings about an emotional response is attracting ever increasing attention. First, this paper provides a thorough review of studies on the relation of music and emotions from different disciplines. We then propose new insights to enhance automated music emotion recognition models using recent results from psychology, musicology, affective computing, semantic technologies and music information retrieval.},
	address = {Berlin, Heidelberg},
	author = {Barthet, Mathieu and Fazekas, Gy{\"o}rgy and Sandler, Mark},
	booktitle = {From Sounds to Music and Emotions},
	date-added = {2024-06-18 08:19:33 +0100},
	date-modified = {2024-06-18 08:19:33 +0100},
	editor = {Aramaki, Mitsuko and Barthet, Mathieu and Kronland-Martinet, Richard and Ystad, S{\o}lvi},
	isbn = {978-3-642-41248-6},
	pages = {228--252},
	publisher = {Springer Berlin Heidelberg},
	title = {Music Emotion Recognition: From Content- to Context-Based Models},
	year = {2013}}

@inproceedings{panda2013multi,
	author = {Panda, R. and Malheiro, Ricardo and Rocha, Bruno and Oliveira, Ant{\'o}nio Pedro and Paiva, Rui Pedro},
	booktitle = {10th International symposium on computer music multidisciplinary research (CMMR 2013)},
	date-added = {2024-06-18 08:19:27 +0100},
	date-modified = {2024-06-18 08:49:15 +0100},
	pages = {570--582},
	title = {Multi-modal music emotion recognition: A new dataset, methodology and comparative analysis},
	year = {2013}}

@article{eerola_friberg_bresin_2013,
	author = {Eerola, T. and Friberg, A. and Bresin, R.},
	date-added = {2024-06-18 08:19:19 +0100},
	date-modified = {2024-06-18 08:48:23 +0100},
	doi = {10.3389/fpsyg.2013.00487},
	issn = {1664-1078},
	journal = {Frontiers in Psychology},
	keywords = {Modelling, Emotion},
	number = {487},
	title = {Emotional Expression in Music: Contribution, Linearity, and Additivity of Primary Musical Cues},
	url = {http://www.frontiersin.org/emotion_science/10.3389/fpsyg.2013.00487/abstract},
	volume = {4},
	year = {2013},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEEAuLi8uLi8uLi8uLi9yZXNlYXJjaC9iaWJsaW9ncmFwaHkvYmliZGVzay9QYXBlcnMvZWVyb2xhMjAxM2EucGRmTxEEYGJvb2tgBAAAAAAEEDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFwDAAAFAAAAAQEAAFVzZXJzAAAABgAAAAEBAABscWJuNzMAAAkAAAABAQAARG9jdW1lbnRzAAAACAAAAAEBAAByZXNlYXJjaAwAAAABAQAAYmlibGlvZ3JhcGh5BwAAAAEBAABiaWJkZXNrAAYAAAABAQAAUGFwZXJzAAAPAAAAAQEAAGVlcm9sYTIwMTNhLnBkZgAgAAAAAQYAAAQAAAAUAAAAJAAAADgAAABIAAAAXAAAAGwAAAB8AAAACAAAAAQDAAAVXQAAAAAAAAgAAAAEAwAAQAUEAAAAAAAIAAAABAMAAKpe9AAAAAAACAAAAAQDAABpLNEAAAAAAAgAAAAEAwAAlSzRAAAAAAAIAAAABAMAAGMu0QAAAAAACAAAAAQDAACGLtEAAAAAAAgAAAAEAwAA9kDRAAAAAAAgAAAAAQYAALwAAADMAAAA3AAAAOwAAAD8AAAADAEAABwBAAAsAQAACAAAAAAEAABBt6hPpQAAABgAAAABAgAAAQAAAAAAAAAPAAAAAAAAAAAAAAAAAAAACAAAAAQDAAAGAAAAAAAAAAQAAAADAwAA9wEAAAgAAAABCQAAZmlsZTovLy8MAAAAAQEAAE1hY2ludG9zaCBIRAgAAAAEAwAAAFChG3MAAAAIAAAAAAQAAEHGeDvVAAAAJAAAAAEBAAAyOEM4QjRGOS03NzU4LTQ1QUQtQTU1Ni1ERTQwMkU0RTYzNzAYAAAAAQIAAIEAAAABAAAA7xMAAAEAAAAAAAAAAAAAAAEAAAABAQAALwAAAAAAAAABBQAA/gAAAAECAAAwZjViNGE5YTc1MzAyNGNlZDgyNWI3MjczYWQwNTNlMzUxNzRmNzVkYmJkMzI4ZTFmMTA4ZWU4Njc1MDkxZWUwOzAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwMDAwMDAwMjA7Y29tLmFwcGxlLmFwcC1zYW5kYm94LnJlYWQtd3JpdGU7MDE7MDEwMDAwMTE7MDAwMDAwMDAwMGQxNDBmNjs1ODsvdXNlcnMvbHFibjczL2RvY3VtZW50cy9yZXNlYXJjaC9iaWJsaW9ncmFwaHkvYmliZGVzay9wYXBlcnMvZWVyb2xhMjAxM2EucGRmAAAAzAAAAP7///8BAAAAAAAAABAAAAAEEAAAlAAAAAAAAAAFEAAAPAEAAAAAAAAQEAAAdAEAAAAAAABAEAAAZAEAAAAAAAACIAAAQAIAAAAAAAAFIAAAsAEAAAAAAAAQIAAAwAEAAAAAAAARIAAA9AEAAAAAAAASIAAA1AEAAAAAAAATIAAA5AEAAAAAAAAgIAAAIAIAAAAAAAAwIAAATAIAAAAAAAABwAAAlAEAAAAAAAARwAAAFAAAAAAAAAASwAAApAEAAAAAAACA8AAAVAIAAAAAAAAACAANABoAIwBmAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABMo=},
	bdsk-url-1 = {http://www.frontiersin.org/emotion_science/10.3389/fpsyg.2013.00487/abstract},
	bdsk-url-2 = {https://doi.org/10.3389/fpsyg.2013.00487}}

@inproceedings{soleymani2013,
	abstract = {Music is composed to be emotionally expressive, and emotional associations provide an especially natural domain for indexing and recommendation in today's vast digital music libraries. But such libraries require powerful automated tools, and the development of systems for automatic prediction of musical emotion presents a myriad challenges. The perceptual nature of musical emotion necessitates the collection of data from human subjects. The interpretation of emotion varies between listeners thus each clip needs to be annotated by a distribution of subjects. In addition, the sharing of large music content libraries for the development of such systems, even for academic research, presents complicated legal issues which vary by country. This work presents a new publicly available dataset for music emotion recognition research and a baseline system. In addressing the difficulties of emotion annotation we have turned to crowdsourcing, using Amazon Mechanical Turk, and have developed a two-stage procedure for filtering out poor quality workers. The dataset consists entirely of creative commons music from the Free Music Archive, which as the name suggests, can be shared freely without penalty. The final dataset contains 1000 songs, each annotated by a minimum of 10 subjects, which is larger than many currently available music emotion dataset.},
	address = {New York, NY, USA},
	author = {Soleymani, Mohammad and Caro, Micheal N. and Schmidt, Erik M. and Sha, Cheng-Ya and Yang, Yi-Hsuan},
	booktitle = {Proceedings of the 2nd ACM International Workshop on Crowdsourcing for Multimedia},
	date-added = {2024-06-18 08:19:10 +0100},
	date-modified = {2024-06-18 08:19:10 +0100},
	doi = {10.1145/2506364.2506365},
	isbn = {9781450323963},
	keywords = {emotion, crowdsourcing, music},
	location = {Barcelona, Spain},
	numpages = {6},
	pages = {1--6},
	publisher = {Association for Computing Machinery},
	series = {CrowdMM '13},
	title = {1000 Songs for Emotional Analysis of Music},
	url = {https://doi.org/10.1145/2506364.2506365},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1145/2506364.2506365}}

@article{saari_et_al_2015,
	author = {Saari, P. and Eerola, T. and Barthet, M. and Fazekas, G. and Lartillot, O.},
	date-added = {2024-06-18 08:19:00 +0100},
	date-modified = {2024-06-18 08:19:00 +0100},
	journal = {IEEE Transactions on Affective Computing},
	number = {2},
	pages = {122-135},
	title = {Genre-adaptive Semantic Computing and Audio-based Modelling for Music Mood Annotation},
	volume = {7},
	year = {2015},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEEUuLi8uLi8uLi8uLi9yZXNlYXJjaC9iaWJsaW9ncmFwaHkvYmliZGVzay9QYXBlcnMvU2FhcmlfZXRfYWxfMjAxNS5wZGZPEQRoYm9va2gEAAAAAAQQMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAZAMAAAUAAAABAQAAVXNlcnMAAAAGAAAAAQEAAGxxYm43MwAACQAAAAEBAABEb2N1bWVudHMAAAAIAAAAAQEAAHJlc2VhcmNoDAAAAAEBAABiaWJsaW9ncmFwaHkHAAAAAQEAAGJpYmRlc2sABgAAAAEBAABQYXBlcnMAABQAAAABAQAAU2FhcmlfZXRfYWxfMjAxNS5wZGYgAAAAAQYAAAQAAAAUAAAAJAAAADgAAABIAAAAXAAAAGwAAAB8AAAACAAAAAQDAAAVXQAAAAAAAAgAAAAEAwAAQAUEAAAAAAAIAAAABAMAAKpe9AAAAAAACAAAAAQDAABpLNEAAAAAAAgAAAAEAwAAlSzRAAAAAAAIAAAABAMAAGMu0QAAAAAACAAAAAQDAACGLtEAAAAAAAgAAAAEAwAAWTvRAAAAAAAgAAAAAQYAAMAAAADQAAAA4AAAAPAAAAAAAQAAEAEAACABAAAwAQAACAAAAAAEAABBu4sPewAAABgAAAABAgAAAQAAAAAAAAAPAAAAAAAAAAAAAAAAAAAACAAAAAQDAAAGAAAAAAAAAAQAAAADAwAA9wEAAAgAAAABCQAAZmlsZTovLy8MAAAAAQEAAE1hY2ludG9zaCBIRAgAAAAEAwAAAFChG3MAAAAIAAAAAAQAAEHGeDvVAAAAJAAAAAEBAAAyOEM4QjRGOS03NzU4LTQ1QUQtQTU1Ni1ERTQwMkU0RTYzNzAYAAAAAQIAAIEAAAABAAAA7xMAAAEAAAAAAAAAAAAAAAEAAAABAQAALwAAAAAAAAABBQAAAwEAAAECAABjY2E5MzlhMTZmOTg5NzFhMzM1MDU3OWY4YTkyY2M4MDFmNjRjMjU0Y2RmMWQ3ZWM4N2UxYzI1YTQ0ZDhmMGY1OzAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwMDAwMDAwMjA7Y29tLmFwcGxlLmFwcC1zYW5kYm94LnJlYWQtd3JpdGU7MDE7MDEwMDAwMTE7MDAwMDAwMDAwMGQxM2I1OTs1ODsvdXNlcnMvbHFibjczL2RvY3VtZW50cy9yZXNlYXJjaC9iaWJsaW9ncmFwaHkvYmliZGVzay9wYXBlcnMvc2FhcmlfZXRfYWxfMjAxNS5wZGYAAMwAAAD+////AQAAAAAAAAAQAAAABBAAAJgAAAAAAAAABRAAAEABAAAAAAAAEBAAAHgBAAAAAAAAQBAAAGgBAAAAAAAAAiAAAEQCAAAAAAAABSAAALQBAAAAAAAAECAAAMQBAAAAAAAAESAAAPgBAAAAAAAAEiAAANgBAAAAAAAAEyAAAOgBAAAAAAAAICAAACQCAAAAAAAAMCAAAFACAAAAAAAAAcAAAJgBAAAAAAAAEcAAABQAAAAAAAAAEsAAAKgBAAAAAAAAgPAAAFgCAAAAAAAAAAgADQAaACMAawAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAATX}}

@article{aljanaki2017developing,
	author = {Aljanaki, Anna and Yang, Yi-Hsuan and Soleymani, Mohammad},
	date-added = {2024-06-18 08:18:51 +0100},
	date-modified = {2024-06-18 08:18:51 +0100},
	journal = {PloS one},
	number = {3},
	pages = {e0173392},
	publisher = {Public Library of Science San Francisco, CA USA},
	title = {Developing a benchmark for emotional analysis of music},
	volume = {12},
	year = {2017}}

@article{cowen2020music,
	author = {Cowen, Alan S and Fang, Xia and Sauter, Disa and Keltner, Dacher},
	date-added = {2024-06-18 08:18:37 +0100},
	date-modified = {2024-06-18 08:18:37 +0100},
	journal = {Proceedings of the National Academy of Sciences},
	number = {4},
	pages = {1924--1934},
	publisher = {National Acad Sciences},
	title = {What music makes us feel: At least 13 dimensions organize subjective experiences associated with music across different cultures},
	volume = {117},
	year = {2020}}

@article{grimaud_eerola_2022,
	author = {Grimaud, Annaliese Micallef and Eerola, T.},
	date-added = {2024-06-18 08:18:16 +0100},
	date-modified = {2024-06-18 08:18:16 +0100},
	doi = {https://doi.org/10.1177/20592043211061745},
	journal = {Music \& Science},
	keywords = {RL},
	pages = {1-23},
	status = {published},
	title = {An Interactive Approach to Emotional Expression through Musical Cues},
	volume = {5},
	year = {2022},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEEYuLi8uLi8uLi8uLi9yZXNlYXJjaC9iaWJsaW9ncmFwaHkvYmliZGVzay9QYXBlcnMvMjA1OTIwNDMyMTEwNjE3NDUucGRmTxEEbGJvb2tsBAAAAAAEEDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGgDAAAFAAAAAQEAAFVzZXJzAAAABgAAAAEBAABscWJuNzMAAAkAAAABAQAARG9jdW1lbnRzAAAACAAAAAEBAAByZXNlYXJjaAwAAAABAQAAYmlibGlvZ3JhcGh5BwAAAAEBAABiaWJkZXNrAAYAAAABAQAAUGFwZXJzAAAVAAAAAQEAADIwNTkyMDQzMjExMDYxNzQ1LnBkZgAAACAAAAABBgAABAAAABQAAAAkAAAAOAAAAEgAAABcAAAAbAAAAHwAAAAIAAAABAMAABVdAAAAAAAACAAAAAQDAABABQQAAAAAAAgAAAAEAwAAql70AAAAAAAIAAAABAMAAGks0QAAAAAACAAAAAQDAACVLNEAAAAAAAgAAAAEAwAAYy7RAAAAAAAIAAAABAMAAIYu0QAAAAAACAAAAAQDAAC1L9EAAAAAACAAAAABBgAAxAAAANQAAADkAAAA9AAAAAQBAAAUAQAAJAEAADQBAAAIAAAAAAQAAEHDyLm9AAAAGAAAAAECAAABAAAAAAAAAA8AAAAAAAAAAAAAAAAAAAAIAAAABAMAAAYAAAAAAAAABAAAAAMDAAD3AQAACAAAAAEJAABmaWxlOi8vLwwAAAABAQAATWFjaW50b3NoIEhECAAAAAQDAAAAUKEbcwAAAAgAAAAABAAAQcZ4O9UAAAAkAAAAAQEAADI4QzhCNEY5LTc3NTgtNDVBRC1BNTU2LURFNDAyRTRFNjM3MBgAAAABAgAAgQAAAAEAAADvEwAAAQAAAAAAAAAAAAAAAQAAAAEBAAAvAAAAAAAAAAEFAAAEAQAAAQIAAGNmZGY4YWU2MDM4YjYyMWQ4ODg3MzY1NDczZmY3YzYwZDVjOWQ0ZTg0NWI5YmNhNjA2YjA2ODc4MTMyNjIzMmY7MDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDAwMDAwMDAyMDtjb20uYXBwbGUuYXBwLXNhbmRib3gucmVhZC13cml0ZTswMTswMTAwMDAxMTswMDAwMDAwMDAwZDEyZmI1OzU4Oy91c2Vycy9scWJuNzMvZG9jdW1lbnRzL3Jlc2VhcmNoL2JpYmxpb2dyYXBoeS9iaWJkZXNrL3BhcGVycy8yMDU5MjA0MzIxMTA2MTc0NS5wZGYAzAAAAP7///8BAAAAAAAAABAAAAAEEAAAnAAAAAAAAAAFEAAARAEAAAAAAAAQEAAAfAEAAAAAAABAEAAAbAEAAAAAAAACIAAASAIAAAAAAAAFIAAAuAEAAAAAAAAQIAAAyAEAAAAAAAARIAAA/AEAAAAAAAASIAAA3AEAAAAAAAATIAAA7AEAAAAAAAAgIAAAKAIAAAAAAAAwIAAAVAIAAAAAAAABwAAAnAEAAAAAAAARwAAAFAAAAAAAAAASwAAArAEAAAAAAACA8AAAXAIAAAAAAAAACAANABoAIwBsAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABNw=},
	bdsk-url-1 = {https://doi.org/10.1177/20592043211061745}}

@article{panda2020audio,
	author = {Panda, R. and Malheiro, Ricardo and Paiva, Rui Pedro},
	date-added = {2024-06-18 08:18:07 +0100},
	date-modified = {2024-06-18 08:49:04 +0100},
	doi = {10.1109/TAFFC.2020.3032373},
	journal = {IEEE Transactions on Affective Computing},
	number = {1},
	pages = {68-88},
	title = {Audio Features for Music Emotion Recognition: A Survey},
	volume = {14},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TAFFC.2020.3032373}}

@article{russell2003core,
	author = {Russell, James A},
	date-added = {2024-06-18 08:17:01 +0100},
	date-modified = {2024-06-18 08:17:01 +0100},
	journal = {Psychological Review},
	number = {1},
	pages = {145-172},
	publisher = {American Psychological Association},
	title = {Core affect and the psychological construction of emotion},
	volume = {110},
	year = {2003}}

@article{eerola_2012,
	author = {Eerola, T. and Vuoskoski, J. K.},
	date-added = {2024-06-18 08:15:55 +0100},
	date-modified = {2024-06-18 08:15:55 +0100},
	doi = {10.1525/mp.2012.30.3.307},
	journal = {Music Perception},
	keywords = {Emotion},
	number = {3},
	pages = {307-340},
	title = {A review of music and emotion studies: Approaches, emotion models and stimuli},
	url = {https://doi.org/10.1525/mp.2012.30.3.307},
	volume = {30},
	year = {2012},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEEAuLi8uLi8uLi8uLi9yZXNlYXJjaC9iaWJsaW9ncmFwaHkvYmliZGVzay9QYXBlcnMvRWVyb2xhMjAxMmMucGRmTxEEYGJvb2tgBAAAAAAEEDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFwDAAAFAAAAAQEAAFVzZXJzAAAABgAAAAEBAABscWJuNzMAAAkAAAABAQAARG9jdW1lbnRzAAAACAAAAAEBAAByZXNlYXJjaAwAAAABAQAAYmlibGlvZ3JhcGh5BwAAAAEBAABiaWJkZXNrAAYAAAABAQAAUGFwZXJzAAAPAAAAAQEAAEVlcm9sYTIwMTJjLnBkZgAgAAAAAQYAAAQAAAAUAAAAJAAAADgAAABIAAAAXAAAAGwAAAB8AAAACAAAAAQDAAAVXQAAAAAAAAgAAAAEAwAAQAUEAAAAAAAIAAAABAMAAKpe9AAAAAAACAAAAAQDAABpLNEAAAAAAAgAAAAEAwAAlSzRAAAAAAAIAAAABAMAAGMu0QAAAAAACAAAAAQDAACGLtEAAAAAAAgAAAAEAwAAWzPRAAAAAAAgAAAAAQYAALwAAADMAAAA3AAAAOwAAAD8AAAADAEAABwBAAAsAQAACAAAAAAEAABBtq8tbAAAABgAAAABAgAAAQAAAAAAAAAPAAAAAAAAAAAAAAAAAAAACAAAAAQDAAAGAAAAAAAAAAQAAAADAwAA9wEAAAgAAAABCQAAZmlsZTovLy8MAAAAAQEAAE1hY2ludG9zaCBIRAgAAAAEAwAAAFChG3MAAAAIAAAAAAQAAEHGeDvVAAAAJAAAAAEBAAAyOEM4QjRGOS03NzU4LTQ1QUQtQTU1Ni1ERTQwMkU0RTYzNzAYAAAAAQIAAIEAAAABAAAA7xMAAAEAAAAAAAAAAAAAAAEAAAABAQAALwAAAAAAAAABBQAA/gAAAAECAAA4ZjM5Y2I5ODc1ZTJjMWU3YzM1MTc5NjA4MzczOGJjYjMzYWI4YTMyYmM5MjgwOWI4ZTZhNjY3Njc2ZTdkNDc4OzAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwMDAwMDAwMjA7Y29tLmFwcGxlLmFwcC1zYW5kYm94LnJlYWQtd3JpdGU7MDE7MDEwMDAwMTE7MDAwMDAwMDAwMGQxMzM1Yjs1ODsvdXNlcnMvbHFibjczL2RvY3VtZW50cy9yZXNlYXJjaC9iaWJsaW9ncmFwaHkvYmliZGVzay9wYXBlcnMvZWVyb2xhMjAxMmMucGRmAAAAzAAAAP7///8BAAAAAAAAABAAAAAEEAAAlAAAAAAAAAAFEAAAPAEAAAAAAAAQEAAAdAEAAAAAAABAEAAAZAEAAAAAAAACIAAAQAIAAAAAAAAFIAAAsAEAAAAAAAAQIAAAwAEAAAAAAAARIAAA9AEAAAAAAAASIAAA1AEAAAAAAAATIAAA5AEAAAAAAAAgIAAAIAIAAAAAAAAwIAAATAIAAAAAAAABwAAAlAEAAAAAAAARwAAAFAAAAAAAAAASwAAApAEAAAAAAACA8AAAVAIAAAAAAAAACAANABoAIwBmAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABMo=},
	bdsk-url-1 = {https://doi.org/10.1525/mp.2012.30.3.307}}

@article{juslin2014makes,
	author = {Juslin, P. N. and Harmat, L{\'a}szl{\'o} and Eerola, T.},
	date-added = {2024-06-18 08:15:38 +0100},
	date-modified = {2024-06-18 08:48:09 +0100},
	journal = {Psychology of Music},
	keywords = {affect, expectancy, mechanisms, BRECVEMAC, psychophysiology},
	number = {4},
	pages = {599--623},
	publisher = {Sage Publications Sage UK: London, England},
	title = {What makes music emotionally significant? Exploring the underlying mechanisms},
	volume = {42},
	year = {2014}}

@article{juslin_et_al_2014,
	abstract = {A common approach to studying emotional reactions to music is to attempt to obtain direct links between musical surface features such as tempo and a listener's responses. however, such an analysis ultimately fails to explain why emotions are aroused in the listener. in this article we explore an alternative approach, which aims to account for musical emotions in terms of a set of psychological mechanisms that are activated by different types of information in a musical event. this approach was tested in 4 experiments that manipulated 4 mechanisms (brain stem reflex, contagion, episodic memory, musical expectancy) by selecting existing musical pieces that featured information relevant for each mechanism. the excerpts were played to 60 listeners, who were asked to rate their felt emotions on 15 scales. skin conductance levels and facial expressions were measured, and listeners reported subjective impressions of relevance to specific mechanisms. results indicated that the target mechanism conditions evoked emotions largely as predicted by a multimechanism framework and that mostly similar effects occurred across the experiments that included different pieces of music. we conclude that a satisfactory account of musical emotions requires consideration of how musical features and responses are mediated by a range of underlying mechanisms.},
	author = {Juslin, P. N. and Barradas, G. and Eerola, T.},
	date-added = {2024-06-18 08:14:59 +0100},
	date-modified = {2024-06-18 08:14:59 +0100},
	doi = {10.5406/amerjpsyc.128.3.0281},
	journal = {American Journal of Psychology},
	keywords = {emotion},
	number = {3},
	pages = {281-304},
	title = {From Sound to Significance: Exploring the Mechanisms Underlying Emotional Reactions to Music},
	url = {https://doi.org/10.5406/amerjpsyc.128.3.0281},
	volume = {128},
	year = {2015},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEEYuLi8uLi8uLi8uLi9yZXNlYXJjaC9iaWJsaW9ncmFwaHkvYmliZGVzay9QYXBlcnMvSnVzbGluX2V0X2FsXzIwMTUucGRmTxEEbGJvb2tsBAAAAAAEEDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGgDAAAFAAAAAQEAAFVzZXJzAAAABgAAAAEBAABscWJuNzMAAAkAAAABAQAARG9jdW1lbnRzAAAACAAAAAEBAAByZXNlYXJjaAwAAAABAQAAYmlibGlvZ3JhcGh5BwAAAAEBAABiaWJkZXNrAAYAAAABAQAAUGFwZXJzAAAVAAAAAQEAAEp1c2xpbl9ldF9hbF8yMDE1LnBkZgAAACAAAAABBgAABAAAABQAAAAkAAAAOAAAAEgAAABcAAAAbAAAAHwAAAAIAAAABAMAABVdAAAAAAAACAAAAAQDAABABQQAAAAAAAgAAAAEAwAAql70AAAAAAAIAAAABAMAAGks0QAAAAAACAAAAAQDAACVLNEAAAAAAAgAAAAEAwAAYy7RAAAAAAAIAAAABAMAAIYu0QAAAAAACAAAAAQDAAC8ONEAAAAAACAAAAABBgAAxAAAANQAAADkAAAA9AAAAAQBAAAUAQAAJAEAADQBAAAIAAAAAAQAAEG7ixPNAAAAGAAAAAECAAABAAAAAAAAAA8AAAAAAAAAAAAAAAAAAAAIAAAABAMAAAYAAAAAAAAABAAAAAMDAAD3AQAACAAAAAEJAABmaWxlOi8vLwwAAAABAQAATWFjaW50b3NoIEhECAAAAAQDAAAAUKEbcwAAAAgAAAAABAAAQcZ4O9UAAAAkAAAAAQEAADI4QzhCNEY5LTc3NTgtNDVBRC1BNTU2LURFNDAyRTRFNjM3MBgAAAABAgAAgQAAAAEAAADvEwAAAQAAAAAAAAAAAAAAAQAAAAEBAAAvAAAAAAAAAAEFAAAEAQAAAQIAAGRlODQzNzI5NGVlNjU2YmViMDZiOTk1YzJiOTJlM2ZiNWI1YTg0NDc2YjZmNWY5NmE0NzM5ZDI4NDFhOGI4ZjM7MDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDAwMDAwMDAyMDtjb20uYXBwbGUuYXBwLXNhbmRib3gucmVhZC13cml0ZTswMTswMTAwMDAxMTswMDAwMDAwMDAwZDEzOGJjOzU4Oy91c2Vycy9scWJuNzMvZG9jdW1lbnRzL3Jlc2VhcmNoL2JpYmxpb2dyYXBoeS9iaWJkZXNrL3BhcGVycy9qdXNsaW5fZXRfYWxfMjAxNS5wZGYAzAAAAP7///8BAAAAAAAAABAAAAAEEAAAnAAAAAAAAAAFEAAARAEAAAAAAAAQEAAAfAEAAAAAAABAEAAAbAEAAAAAAAACIAAASAIAAAAAAAAFIAAAuAEAAAAAAAAQIAAAyAEAAAAAAAARIAAA/AEAAAAAAAASIAAA3AEAAAAAAAATIAAA7AEAAAAAAAAgIAAAKAIAAAAAAAAwIAAAVAIAAAAAAAABwAAAnAEAAAAAAAARwAAAFAAAAAAAAAASwAAArAEAAAAAAACA8AAAXAIAAAAAAAAACAANABoAIwBsAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABNw=},
	bdsk-url-1 = {https://doi.org/10.5406/amerjpsyc.128.3.0281}}

@article{Mehr2019universality,
	abstract = {It is unclear whether there are universal patterns to music across cultures. Mehr et al. examined ethnographic data and observed music in every society sampled (see the Perspective by Fitch and Popescu). For songs specifically, three dimensions characterize more than 25\% of the performances studied: formality of the performance, arousal level, and religiosity. There is more variation in musical behavior within societies than between societies, and societies show similar levels of within-society variation in musical behavior. At the same time, one-third of societies significantly differ from average for any given dimension, and half of all societies differ from average on at least one dimension, indicating variability across cultures.Science, this issue p. eaax0868; see also p. 944INTRODUCTIONMusic is often assumed to be a human universal, emerging from an evolutionary adaptation specific to music and/or a by-product of adaptations for affect, language, motor control, and auditory perception. But universality has never actually been systematically demonstrated, and it is challenged by the vast diversity of music across cultures. Hypotheses of the evolutionary function of music are also untestable without comprehensive and representative data on its forms and behavioral contexts across societies.RATIONALEWe conducted a natural history of song: a systematic analysis of the features of vocal music found worldwide. It consists of a corpus of ethnographic text on musical behavior from a representative sample of mostly small-scale societies, and a discography of audio recordings of the music itself. We then applied tools of computational social science, which minimize the influence of sampling error and other biases, to answer six questions. Does music appear universally? What kinds of behavior are associated with song, and how do they vary among societies? Are the musical features of a song indicative of its behavioral context (e.g., infant care)? Do the melodic and rhythmic patterns of songs vary systematically, like those patterns found in language? And how prevalent is tonality across musical idioms?RESULTSAnalysis of the ethnography corpus shows that music appears in every society observed; that variation in song events is well characterized by three dimensions (formality, arousal, religiosity); that musical behavior varies more within societies than across them on these dimensions; and that music is regularly associated with behavioral contexts such as infant care, healing, dance, and love. Analysis of the discography corpus shows that identifiable acoustic features of songs (accent, tempo, pitch range, etc.) predict their primary behavioral context (love, healing, etc.); that musical forms vary along two dimensions (melodic and rhythmic complexity); that melodic and rhythmic bigrams fall into power-law distributions; and that tonality is widespread, perhaps universal.CONCLUSIONMusic is in fact universal: It exists in every society (both with and without words), varies more within than between societies, regularly supports certain types of behavior, and has acoustic features that are systematically related to the goals and responses of singers and listeners. But music is not a fixed biological response with a single prototypical adaptive function: It is produced worldwide in diverse behavioral contexts that vary in formality, arousal, and religiosity. Music does appear to be tied to specific perceptual, cognitive, and affective faculties, including language (all societies put words to their songs), motor control (people in all societies dance), auditory analysis (all musical systems have signatures of tonality), and aesthetics (their melodies and rhythms are balanced between monotony and chaos). These analyses show how applying the tools of computational social science to rich bodies of humanistic data can reveal both universal features and patterns of variability in culture, addressing long-standing debates about each.Studying world music systematically.We used primary ethnographic text and field recordings of song performances to build two richly annotated cross-cultural datasets: NHS Ethnography and NHS Discography. The original material in each dataset was annotated by humans (both amateur and expert) and by automated algorithms.What is universal about music, and what varies? We built a corpus of ethnographic text on musical behavior from a representative sample of the world{\textquoteright}s societies, as well as a discography of audio recordings. The ethnographic corpus reveals that music (including songs with words) appears in every society observed; that music varies along three dimensions (formality, arousal, religiosity), more within societies than across them; and that music is associated with certain behavioral contexts such as infant care, healing, dance, and love. The discography{\textemdash}analyzed through machine summaries, amateur and expert listener ratings, and manual transcriptions{\textemdash}reveals that acoustic features of songs predict their primary behavioral context; that tonality is widespread, perhaps universal; that music varies in rhythmic and melodic complexity; and that elements of melodies and rhythms found worldwide follow power laws.},
	author = {Mehr, Samuel A. and Singh, Manvir and Knox, Dean and Ketter, Daniel M. and Pickens-Jones, Daniel and Atwood, S. and Lucas, Christopher and Jacoby, Nori and Egner, Alena A. and Hopkins, Erin J. and Howard, Rhea M. and Hartshorne, Joshua K. and Jennings, Mariela V. and Simson, Jan and Bainbridge, Constance M. and Pinker, Steven and O{\textquoteright}Donnell, Timothy J. and Krasnow, Max M. and Glowacki, Luke},
	date-added = {2024-06-18 08:13:40 +0100},
	date-modified = {2024-06-18 08:13:40 +0100},
	doi = {10.1126/science.aax0868},
	elocation-id = {eaax0868},
	eprint = {https://science.sciencemag.org/content/366/6468/eaax0868.full.pdf},
	issn = {0036-8075},
	journal = {Science},
	number = {6468},
	publisher = {American Association for the Advancement of Science},
	title = {Universality and diversity in human song},
	url = {https://science.sciencemag.org/content/366/6468/eaax0868},
	volume = {366},
	year = {2019},
	bdsk-url-1 = {https://science.sciencemag.org/content/366/6468/eaax0868},
	bdsk-url-2 = {https://doi.org/10.1126/science.aax0868}}

@article{gomez2021,
	author = {G{\'o}mez-Ca{\~n}{\'o}n, Juan Sebasti{\'a}n and Cano, Estefan{\'\i}a and Eerola, T. and Herrera, Perfecto and Hu, Xiao and Yang, Yi-Hsuan and G{\'o}mez, Emilia},
	date-added = {2024-06-18 08:12:32 +0100},
	date-modified = {2024-06-18 08:12:32 +0100},
	doi = {10.1109/MSP.2021.3106232},
	journal = {IEEE Signal Processing Magazine},
	keywords = {RL},
	number = {6},
	pages = {106-114},
	status = {accepted},
	title = {Music Emotion Recognition: Toward new, robust standards in personalized and context-sensitive applications},
	volume = {38},
	year = {2021},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEE4uLi8uLi8uLi8uLi9yZXNlYXJjaC9iaWJsaW9ncmFwaHkvYmliZGVzay9QYXBlcnMvU1BNLU9jdC0yMDIwLTE4Ni5SMV9Qcm9vZi5wZGZPEQR8Ym9va3wEAAAAAAQQMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAeAMAAAUAAAABAQAAVXNlcnMAAAAGAAAAAQEAAGxxYm43MwAACQAAAAEBAABEb2N1bWVudHMAAAAIAAAAAQEAAHJlc2VhcmNoDAAAAAEBAABiaWJsaW9ncmFwaHkHAAAAAQEAAGJpYmRlc2sABgAAAAEBAABQYXBlcnMAAB0AAAABAQAAU1BNLU9jdC0yMDIwLTE4Ni5SMV9Qcm9vZi5wZGYAAAAgAAAAAQYAAAQAAAAUAAAAJAAAADgAAABIAAAAXAAAAGwAAAB8AAAACAAAAAQDAAAVXQAAAAAAAAgAAAAEAwAAQAUEAAAAAAAIAAAABAMAAKpe9AAAAAAACAAAAAQDAABpLNEAAAAAAAgAAAAEAwAAlSzRAAAAAAAIAAAABAMAAGMu0QAAAAAACAAAAAQDAACGLtEAAAAAAAgAAAAEAwAAAzvRAAAAAAAgAAAAAQYAAMwAAADcAAAA7AAAAPwAAAAMAQAAHAEAACwBAAA8AQAACAAAAAAEAABBwtwuzIAAABgAAAABAgAAAQAAAAAAAAAPAAAAAAAAAAAAAAAAAAAACAAAAAQDAAAGAAAAAAAAAAQAAAADAwAA9wEAAAgAAAABCQAAZmlsZTovLy8MAAAAAQEAAE1hY2ludG9zaCBIRAgAAAAEAwAAAFChG3MAAAAIAAAAAAQAAEHGeDvVAAAAJAAAAAEBAAAyOEM4QjRGOS03NzU4LTQ1QUQtQTU1Ni1ERTQwMkU0RTYzNzAYAAAAAQIAAIEAAAABAAAA7xMAAAEAAAAAAAAAAAAAAAEAAAABAQAALwAAAAAAAAABBQAADAEAAAECAAAwZjQzNTNiZGI3MTI0ZTA5NjdkNDM4MDNjMTRjNTdhNjllY2Y2ZTI4NDlhZTkzZWQ0ZmJmZWJhYjRiMGI1OGMyOzAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwMDAwMDAwMjA7Y29tLmFwcGxlLmFwcC1zYW5kYm94LnJlYWQtd3JpdGU7MDE7MDEwMDAwMTE7MDAwMDAwMDAwMGQxM2IwMzs1ODsvdXNlcnMvbHFibjczL2RvY3VtZW50cy9yZXNlYXJjaC9iaWJsaW9ncmFwaHkvYmliZGVzay9wYXBlcnMvc3BtLW9jdC0yMDIwLTE4Ni5yMV9wcm9vZi5wZGYAzAAAAP7///8BAAAAAAAAABAAAAAEEAAApAAAAAAAAAAFEAAATAEAAAAAAAAQEAAAhAEAAAAAAABAEAAAdAEAAAAAAAACIAAAUAIAAAAAAAAFIAAAwAEAAAAAAAAQIAAA0AEAAAAAAAARIAAABAIAAAAAAAASIAAA5AEAAAAAAAATIAAA9AEAAAAAAAAgIAAAMAIAAAAAAAAwIAAAXAIAAAAAAAABwAAApAEAAAAAAAARwAAAFAAAAAAAAAASwAAAtAEAAAAAAACA8AAAZAIAAAAAAAAACAANABoAIwB0AAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABPQ=},
	bdsk-file-2 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEEUuLi8uLi8uLi8uLi9yZXNlYXJjaC9iaWJsaW9ncmFwaHkvYmliZGVzay9QYXBlcnMvR29tZXpfZXRfYWxfMjAyMS5wZGZPEQRoYm9va2gEAAAAAAQQMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAZAMAAAUAAAABAQAAVXNlcnMAAAAGAAAAAQEAAGxxYm43MwAACQAAAAEBAABEb2N1bWVudHMAAAAIAAAAAQEAAHJlc2VhcmNoDAAAAAEBAABiaWJsaW9ncmFwaHkHAAAAAQEAAGJpYmRlc2sABgAAAAEBAABQYXBlcnMAABQAAAABAQAAR29tZXpfZXRfYWxfMjAyMS5wZGYgAAAAAQYAAAQAAAAUAAAAJAAAADgAAABIAAAAXAAAAGwAAAB8AAAACAAAAAQDAAAVXQAAAAAAAAgAAAAEAwAAQAUEAAAAAAAIAAAABAMAAKpe9AAAAAAACAAAAAQDAABpLNEAAAAAAAgAAAAEAwAAlSzRAAAAAAAIAAAABAMAAGMu0QAAAAAACAAAAAQDAACGLtEAAAAAAAgAAAAEAwAA2TfRAAAAAAAgAAAAAQYAAMAAAADQAAAA4AAAAPAAAAAAAQAAEAEAACABAAAwAQAACAAAAAAEAABBw5YDRIAAABgAAAABAgAAAQAAAAAAAAAPAAAAAAAAAAAAAAAAAAAACAAAAAQDAAAGAAAAAAAAAAQAAAADAwAA9wEAAAgAAAABCQAAZmlsZTovLy8MAAAAAQEAAE1hY2ludG9zaCBIRAgAAAAEAwAAAFChG3MAAAAIAAAAAAQAAEHGeDvVAAAAJAAAAAEBAAAyOEM4QjRGOS03NzU4LTQ1QUQtQTU1Ni1ERTQwMkU0RTYzNzAYAAAAAQIAAIEAAAABAAAA7xMAAAEAAAAAAAAAAAAAAAEAAAABAQAALwAAAAAAAAABBQAAAwEAAAECAABiMmQyY2U1NDEzYzQzNjhmNmE4NWZiZGEzMzEwOTM1N2QxZThiOTI2ZmY1YmYzYmRjYTE3ZGQxZDgzOTZhNTQ3OzAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwMDAwMDAwMjA7Y29tLmFwcGxlLmFwcC1zYW5kYm94LnJlYWQtd3JpdGU7MDE7MDEwMDAwMTE7MDAwMDAwMDAwMGQxMzdkOTs1ODsvdXNlcnMvbHFibjczL2RvY3VtZW50cy9yZXNlYXJjaC9iaWJsaW9ncmFwaHkvYmliZGVzay9wYXBlcnMvZ29tZXpfZXRfYWxfMjAyMS5wZGYAAMwAAAD+////AQAAAAAAAAAQAAAABBAAAJgAAAAAAAAABRAAAEABAAAAAAAAEBAAAHgBAAAAAAAAQBAAAGgBAAAAAAAAAiAAAEQCAAAAAAAABSAAALQBAAAAAAAAECAAAMQBAAAAAAAAESAAAPgBAAAAAAAAEiAAANgBAAAAAAAAEyAAAOgBAAAAAAAAICAAACQCAAAAAAAAMCAAAFACAAAAAAAAAcAAAJgBAAAAAAAAEcAAABQAAAAAAAAAEsAAAKgBAAAAAAAAgPAAAFgCAAAAAAAAAAgADQAaACMAawAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAATX},
	bdsk-url-1 = {https://doi.org/10.1109/MSP.2021.3106232}}

@article{juslin2022emotions,
	author = {Juslin, P. N. and Sakka, Laura S and Barradas, Gon{\c{c}}alo T and Lartillot, Olivier},
	date-added = {2024-06-18 08:12:06 +0100},
	date-modified = {2024-06-18 08:12:06 +0100},
	journal = {Music Perception: An Interdisciplinary Journal},
	number = {1},
	pages = {55--86},
	publisher = {University of California Press},
	title = {Emotions, mechanisms, and individual differences in music listening: A stratified random sampling approach},
	volume = {40},
	year = {2022}}

@manual{R-base,
	address = {Vienna, Austria},
	author = {{R Core Team}},
	date-added = {2024-06-18 08:11:51 +0100},
	date-modified = {2024-06-18 08:11:51 +0100},
	organization = {R Foundation for Statistical Computing},
	title = {R: A Language and Environment for Statistical Computing},
	url = {https://www.R-project.org/},
	year = {2023},
	bdsk-url-1 = {https://www.R-project.org/}}

@article{Higginsd5928,
	author = {Higgins, Julian P T and Altman, Douglas G and G{\o}tzsche, Peter C and J{\"u}ni, Peter and Moher, David and Oxman, Andrew D and Savovi{\'c}, Jelena and Schulz, Kenneth F and Weeks, Laura and Sterne, Jonathan A C},
	date-added = {2024-06-17 21:15:10 +0100},
	date-modified = {2024-06-17 21:15:10 +0100},
	doi = {10.1136/bmj.d5928},
	elocation-id = {d5928},
	eprint = {https://www.bmj.com/content/343/bmj.d5928.full.pdf},
	issn = {0959-8138},
	journal = {BMJ},
	publisher = {BMJ Publishing Group Ltd},
	title = {The Cochrane Collaboration{\textquoteright}s tool for assessing risk of bias in randomised trials},
	url = {https://www.bmj.com/content/343/bmj.d5928},
	volume = {343},
	year = {2011},
	bdsk-url-1 = {https://www.bmj.com/content/343/bmj.d5928},
	bdsk-url-2 = {https://doi.org/10.1136/bmj.d5928}}

@article{Shamseerg7647,
	abstract = {Protocols of systematic reviews and meta-analyses allow for planning and documentation of review methods, act as a guard against arbitrary decision making during review conduct, enable readers to assess for the presence of selective reporting against completed reviews, and, when made publicly available, reduce duplication of efforts and potentially prompt collaboration. Evidence documenting the existence of selective reporting and excessive duplication of reviews on the same or similar topics is accumulating and many calls have been made in support of the documentation and public availability of review protocols. Several efforts have emerged in recent years to rectify these problems, including development of an international register for prospective reviews (PROSPERO) and launch of the first open access journal dedicated to the exclusive publication of systematic review products, including protocols (BioMed Central{\textquoteright}s Systematic Reviews). Furthering these efforts and building on the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-analyses) guidelines, an international group of experts has created a guideline to improve the transparency, accuracy, completeness, and frequency of documented systematic review and meta-analysis protocols{\textemdash}PRISMA-P (for protocols) 2015. The PRISMA-P checklist contains 17 items considered to be essential and minimum components of a systematic review or meta-analysis protocol.This PRISMA-P 2015 Explanation and Elaboration paper provides readers with a full understanding of and evidence about the necessity of each item as well as a model example from an existing published protocol. This paper should be read together with the PRISMA-P 2015 statement. Systematic review authors and assessors are strongly encouraged to make use of PRISMA-P when drafting and appraising review protocols.},
	author = {Shamseer, Larissa and Moher, David and Clarke, Mike and Ghersi, Davina and Liberati, Alessandro and Petticrew, Mark and Shekelle, Paul and Stewart, Lesley A},
	date-added = {2024-06-17 21:14:56 +0100},
	date-modified = {2024-06-17 21:14:56 +0100},
	doi = {10.1136/bmj.g7647},
	elocation-id = {g7647},
	eprint = {https://www.bmj.com/content/349/bmj.g7647.full.pdf},
	journal = {BMJ},
	publisher = {BMJ Publishing Group Ltd},
	title = {Preferred reporting items for systematic review and meta-analysis protocols (PRISMA-P) 2015: elaboration and explanation},
	url = {https://www.bmj.com/content/349/bmj.g7647},
	volume = {349},
	year = {2015},
	bdsk-url-1 = {https://www.bmj.com/content/349/bmj.g7647},
	bdsk-url-2 = {https://doi.org/10.1136/bmj.g7647}}

@article{anderson2022ex,
	author = {Anderson, Cameron J and Schutz, Michael},
	date-added = {2024-05-05 20:24:25 +0100},
	date-modified = {2024-05-05 20:24:30 +0100},
	journal = {Psychology of Music},
	number = {5},
	pages = {1424--1442},
	publisher = {SAGE Publications Sage UK: London, England},
	title = {Exploring historic changes in musical communication: Deconstructing emotional cues in preludes by Bach and Chopin},
	volume = {50},
	year = {2022}}

@article{nag2022,
	author = {Nag, Sayan and Basu, Medha and Sanyal, Shankha and Banerjee, Archi and Ghosh, Dipak},
	date = {2022},
	doi = {10.1016/j.physa.2022.127261},
	journal = {Physica A: Statistical Mechanics and its Applications},
	note = {Type: Article},
	title = {On the application of deep learning and multifractal techniques to classify emotions and instruments using Indian Classical Music},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127732290&doi=10.1016%2fj.physa.2022.127261&partnerID=40&md5=634cc77ee93e9ac834ca0dad8b815119},
	volume = {597},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127732290&doi=10.1016%2fj.physa.2022.127261&partnerID=40&md5=634cc77ee93e9ac834ca0dad8b815119},
	bdsk-url-2 = {https://doi.org/10.1016/j.physa.2022.127261}}

@article{chin2018,
	author = {Chin, Yu-Hao and Wang, Jia-Ching and Wang, Ju-Chiang and Yang, Yi-Hsuan},
	date = {2018},
	doi = {10.1109/TAFFC.2016.2628794},
	journal = {IEEE Transactions on Affective Computing},
	note = {Type: Article},
	number = {4},
	pages = {541 {\textendash} 549},
	title = {Predicting the Probability Density Function of Music Emotion Using Emotion Space Mapping},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058048732&doi=10.1109%2fTAFFC.2016.2628794&partnerID=40&md5=a50a624a1dd2a6e53d5920e2499b1aca},
	volume = {9},
	year = {2018},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058048732&doi=10.1109%2fTAFFC.2016.2628794&partnerID=40&md5=a50a624a1dd2a6e53d5920e2499b1aca},
	bdsk-url-2 = {https://doi.org/10.1109/TAFFC.2016.2628794}}

@book{hiller_experimental_1979,
	author = {Hiller, Lejaren Arthur and Isaacson, Leonard M},
	publisher = {Greenwood Publishing Group Inc.},
	title = {Experimental {Music}; {Composition} with an electronic computer},
	year = {1979}}

@article{zaripov_cybernetics_1969,
	author = {Zaripov, R Kh and Russell, JGK},
	journal = {Perspectives of New Music},
	note = {Publisher: JSTOR},
	pages = {115--154},
	title = {Cybernetics and music},
	year = {1969}}

@book{mozart_musikalisches_1957,
	author = {Mozart, Wolfgang Amadeus},
	publisher = {B. Schott's Soehne},
	title = {Musikalisches {W{\"u}rfelspiel}},
	year = {1957}}

@article{deutsch_ink-pot_1952,
	author = {Deutsch, Otto Erich},
	journal = {The Musical Times},
	note = {Publisher: JSTOR},
	number = {1315},
	pages = {401--403},
	title = {Ink-{Pot} and {Squirt}-{Gun} {Or} '{The} {Art} of {Composing} {Music} in the {New}-{Style}'},
	volume = {93},
	year = {1952}}

@inproceedings{katayose_sentiment_1988,
	author = {Katayose, Haruhiro and Imai, Masakazu and Inokuchi, Seiji},
	booktitle = {9th {International} {Conference} on {Pattern} {Recognition}},
	pages = {1083--1084},
	publisher = {IEEE Computer Society},
	title = {Sentiment extraction in music},
	year = {1988}}

@inproceedings{fairthorne1968,
	author = {Fairthorne, Robert A.},
	date-modified = {2024-10-21 19:36:12 +0100},
	publisher = {Archon Books},
	title = {Towards information retrieval},
	year = {1968}}

@phdthesis{meyer_emotion_1954,
	author = {Meyer, Leonard B},
	school = {The University of Chicago},
	title = {Emotion and meaning in music},
	type = {{PhD} {Thesis}},
	year = {1954}}

@book{langer_philosophy_1948,
	author = {Langer, Susanne K},
	publisher = {New American Library},
	title = {Philosophy in a new key: {A} study in the symbolism of reason, rite, and art},
	year = {1948}}

@article{tischler_aesthetic_1956,
	author = {Tischler, Hans},
	journal = {The Music Review},
	pages = {189},
	title = {The {Aesthetic} {Experience}},
	volume = {17},
	year = {1956}}

@book{picard_affective_1997,
	author = {Picard, Rosalind},
	publisher = {MIT Press},
	title = {Affective {Computing}},
	year = {1997}}

@inproceedings{marrin_analysis_1998,
	author = {Marrin, Teresa and Picard, Rosalind},
	booktitle = {{XII} {Colloquium} for {Musical} {Informatics}, {Gorizia}, {Italy}},
	pages = {61--64},
	title = {Analysis of {Affective} {Musical} {Expression} with the {Conductor}'s {Jacket}},
	year = {1998}}

@inproceedings{dabek_new_1998,
	author = {Dabek, Frank and Healey, Jennifer and Picard, Rosalind},
	booktitle = {Proc. from the 1998 {Workshop} on {Perceptual} {User} {Interfaces}},
	title = {A new affect-perceiving interface and its application to personalized music selection},
	year = {1998}}

@inproceedings{friberg_automatic_2002,
	author = {Friberg, Anders and Schoonderwaldt, Erwin and Juslin, Patrik N and Bresin, Roberto},
	booktitle = {International {Computer} {Music} {Conference}, {ICMC} 2002, {Gothenburg}, {Sweden}},
	pages = {365--367},
	title = {Automatic real-time extraction of musical expression},
	year = {2002}}

@inproceedings{liu_automatic_2003,
	author = {Liu, Dan and Lu, Lie and Zhang, Hong-Jiang},
	booktitle = {Proc. {ISMIR} 2003; 4th {Int}. {Symp}. {Music} {Information} {Retrieval}},
	month = jan,
	title = {Automatic {Mood} {Detection} from {Acoustic} {Music} {Data}},
	year = {2003}}

@article{lu_automatic_2005,
	author = {Lu, Lie and Liu, Dan and Zhang, Hong-Jiang},
	journal = {IEEE Transactions on audio, speech, and language processing},
	note = {Publisher: IEEE},
	number = {1},
	pages = {5--18},
	title = {Automatic mood detection and tracking of music audio signals},
	volume = {14},
	year = {2005}}

@article{mandel_support_2006,
	author = {Mandel, Michael I and Poliner, Graham E and Ellis, Daniel PW},
	journal = {Multimedia systems},
	note = {Publisher: Springer},
	number = {1},
	pages = {3--13},
	title = {Support vector machine active learning for music retrieval},
	volume = {12},
	year = {2006}}

@inproceedings{feng_popular_2003,
	author = {Feng, Yazhong and Zhuang, Yueting and Pan, Yunhe},
	booktitle = {Proceedings of the 26th annual international {ACM} {SIGIR} conference on {Research} and development in informaion retrieval},
	pages = {375--376},
	title = {Popular music retrieval by detecting mood},
	year = {2003}}

@inproceedings{hu_2007_2008,
	author = {Hu, Xiao and Downie, John Stephen and Laurier, Cyril and Bay, Mert and Ehmann, Andreas F.},
	booktitle = {Proc. 9th {Int}. {Conf}. {Music} {Inf}. {Retrieval}},
	pages = {462--467},
	title = {The 2007 {MIREX} audio mood classification task: {Lessons} learned},
	year = {2008}}

@article{yang_regression_2008,
	author = {Yang, Yi-Hsuan and Lin, Yu-Ching and Su, Ya-Fan and Chen, Homer H},
	journal = {IEEE Transactions on audio, speech, and language processing},
	note = {Publisher: IEEE},
	number = {2},
	pages = {448--457},
	title = {A regression approach to music emotion recognition},
	volume = {16},
	year = {2008}}

@inproceedings{celma_foafing_2006,
	author = {Celma, Oscar},
	booktitle = {International semantic web conference},
	pages = {927--934},
	publisher = {Springer},
	title = {Foafing the music: {Bridging} the semantic gap in music recommendation},
	year = {2006}}

@inproceedings{wiggins_semantic_2009,
	author = {Wiggins, Geraint A},
	booktitle = {2009 11th {IEEE} {International} {Symposium} on {Multimedia}},
	pages = {477--482},
	publisher = {IEEE},
	title = {Semantic gap?? {Schemantic} schmap‚Äº {Methodological} considerations in the scientific study of music},
	year = {2009}}

@article{downie_music_2008,
	author = {Downie, John Stephen},
	journal = {Acoustical Science and Technology},
	note = {Publisher: Acoustical Society of Japan},
	number = {4},
	pages = {247--255},
	title = {The music information retrieval evaluation exchange (2005--2007): {A} window into music information retrieval research},
	volume = {29},
	year = {2008}}

@article{kassler1966toward,
	author = {Kassler, Michael},
	journal = {Perspectives of New Music},
	pages = {59--67},
	publisher = {JSTOR},
	title = {Toward musical information retrieval},
	year = {1966}}

@article{mendel1969some,
	author = {Mendel, Arthur},
	journal = {Computers and the Humanities},
	pages = {41--52},
	publisher = {JSTOR},
	title = {Some preliminary attempts at computer-assisted style analysis in music},
	year = {1969}}

@inproceedings{grekow2016music,
	author = {Grekow, Jacek},
	booktitle = {Computer Information Systems and Industrial Management: 15th IFIP TC8 International Conference, CISIM 2016, Vilnius, Lithuania, September 14-16, 2016, Proceedings 15},
	organization = {Springer},
	pages = {697--706},
	title = {Music emotion maps in arousal-valence space},
	year = {2016}}

@article{yang2018review,
	author = {Yang, Xinyu and Dong, Yizhuo and Li, Juan},
	journal = {Multimedia systems},
	pages = {365--389},
	publisher = {Springer},
	title = {Review of data features-based music emotion recognition methods},
	volume = {24},
	year = {2018}}

@inproceedings{bai2016dimensional,
	author = {Bai, Junjie and Peng, Jun and Shi, Jinliang and Tang, Dedong and Wu, Ying and Li, Jianqing and Luo, Kan},
	booktitle = {2016 IEEE 15th International Conference on Cognitive Informatics \& Cognitive Computing (ICCI* CC)},
	organization = {IEEE},
	pages = {42--49},
	title = {Dimensional music emotion recognition by valence-arousal regression},
	year = {2016}}

@article{coutinho2013psychoacoustic,
	author = {Coutinho, Eduardo and Dibben, Nicola},
	journal = {Cognition \& emotion},
	number = {4},
	pages = {658--684},
	publisher = {Taylor \& Francis},
	title = {Psychoacoustic cues to emotion in speech prosody and music},
	volume = {27},
	year = {2013}}

@inproceedings{eerola2009prediction,
	author = {Eerola, Tuomas and Lartillot, Olivier and Toiviainen, Petri},
	booktitle = {Ismir},
	pages = {621--626},
	title = {Prediction of Multidimensional Emotional Ratings in Music from Audio Using Multivariate Regression Models.},
	year = {2009}}

@article{grekow2018audio,
	author = {Grekow, Jacek},
	journal = {Journal of Information and Telecommunication},
	number = {3},
	pages = {322--333},
	publisher = {Taylor \& Francis},
	title = {Audio features dedicated to the detection and tracking of arousal and valence in musical compositions},
	volume = {2},
	year = {2018}}

@article{park2017representation,
	author = {Park, Jiyoung and Lee, Jongpil and Nam, Juhan and Park, Jangyeon and Ha, Jung-Woo},
	journal = {The 13th Music Information Retrieval Evaluation eXchange, MIREX},
	title = {Representation learning using artist labels for audio classification tasks},
	year = {2017}}

@inproceedings{yang2007music,
	author = {Yang, Yi-Hsuan and Su, Ya-Fan and Lin, Yu-Ching and Chen, Homer H},
	booktitle = {Proceedings of the international workshop on Human-centered multimedia},
	pages = {13--22},
	title = {Music emotion recognition: The role of individuality},
	year = {2007}}

@inproceedings{chowdhury2021perceived,
	author = {Chowdhury, Shreyan and Widmer, Gerhard},
	booktitle = {International Society for Music Information Retrieval Conference (ISMIR 2023)},
	title = {On perceived emotion in expressive piano performance: Further experimental evidence for the relevance of mid-level perceptual features},
	year = {2021}}

@article{zaripov1969,
	author = {Zaripov, R Kh and Russell, JGK},
	date = {1969},
	journal = {Perspectives of New Music},
	note = {Publisher: JSTOR},
	pages = {115{\textendash}154},
	title = {Cybernetics and music},
	year = {1969}}
