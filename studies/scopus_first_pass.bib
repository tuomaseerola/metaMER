%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Tuomas Eerola at 2024-05-13 15:10:10 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@article{Jakubowski20232001,
	abstract = {Features of visual cues, such as their familiarity and emotionality, influence the quantity and qualities of the autobiographical memories they evoke. Despite increasing use in autobiographical memory research, comparatively little is known about how such features of musical cues influence memory properties. In a repeated-measures design, we presented 24 musical cues selected to vary on their familiarity (high/low), emotional valence (positive/negative), and emotional arousal (high/low) to 100 young adults, who recorded details of any autobiographical memories that were evoked. Familiarity of the music primarily impacted memory accessibility, with high-familiarity music evoking more memories that were retrieved more quickly. More familiar music also elicited more positive and arousing memories; however, these differences were found to be attributed to greater liking of the high-familiarity music. The emotional expression of the music impacted the emotionality and evaluation of the memories, with negative valence/low-arousal (e.g., ``sad'') music evoking the most negative memories, high-arousal and positively valenced music evoking more arousing memories, and low-arousal music evoking memories rated as more important. These results provide important insights for developing effective paradigms for triggering (particular types of) autobiographical memories via music and highlight the need to critically consider potential differences in cue familiarity and emotionality in studies comparing musical with non-musical cues. Future research should extend this approach to other cue types (e.g., visual, olfactory, other auditory cues), to probe how familiarity and emotional qualities of cues conjunctively or interactively constrain autobiographical memory recall across different domains. {\copyright} Experimental Psychology Society 2022.},
	author = {Jakubowski, Kelly and Francini, Emma},
	author_keywords = {Autobiographical memory; emotion; familiarity; music-evoked autobiographical memory; retrieval cues},
	doi = {10.1177/17470218221129793},
	journal = {Quarterly Journal of Experimental Psychology},
	keywords = {Cues; Emotions; Humans; Memory, Episodic; Mental Recall; Recognition, Psychology; Young Adult; association; emotion; episodic memory; human; physiology; recall; recognition; young adult},
	note = {Cited by: 4; All Open Access, Green Open Access, Hybrid Gold Open Access},
	number = {9},
	pages = {2001 -- 2016},
	publication_stage = {Final},
	source = {Scopus},
	title = {Differential effects of familiarity and emotional expression of musical cues on autobiographical memory properties},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140711815&doi=10.1177%2f17470218221129793&partnerID=40&md5=1b31103422267888c9a6f5ea24d375c6},
	volume = {76},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140711815&doi=10.1177%2f17470218221129793&partnerID=40&md5=1b31103422267888c9a6f5ea24d375c6},
	bdsk-url-2 = {https://doi.org/10.1177/17470218221129793}}

@article{Ng20231703,
	abstract = {Emotion plays an important role in our daily lives. Emotional individuals can affect the performance of a company, the harmony of a family, the wellness or growth (physical, mental, and spiritual) of a child etc. It renders a wide range of impacts. The existing works on emotion detection from facial expressions differ from the voice. It is deduced that the facial expression is captured on the face externally, whereas the voice is captured from the air passes through the vocal folds internally. Both captured output models may very much deviate from each other. This paper studies and analyses a person's emotion through dual models - facial expression and voice separately. The proposed algorithm uses a Convolutional Neural Network (CNN) with 2-dimensions convolutional layers for facial expression and 1-Dimension convolutional layers for voice. Feature extraction is done via face detection, and Mel-Spectrogram extraction is done via voice. The network layers are fine-tuned to achieve the higher performance of the CNN model. The trained CNN models can recognize emotions from the input videos, which may cover single or multiple emotions from the facial expression and voice perspective. The experimented videos are clean from the background music and environment noise and contain only a person's voice. The proposed algorithm achieved an accuracy of 62.9% through facial expression and 82.3% through voice. {\copyright} IJASEIT is licensed under a Creative Commons Attribution-Share Alike 4.0 International License.},
	author = {Ng, Kok-Why and Lim, Yixen and Haw, Su-Cheng and Yoong, Yih-Jian},
	author_keywords = {convolutional neural network; Emotion recognition; facial expression; Mel-spectrogram; voice},
	doi = {10.18517/ijaseit.13.5.19023},
	journal = {International Journal on Advanced Science, Engineering and Information Technology},
	note = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
	number = {5},
	pages = {1703 -- 1709},
	publication_stage = {Final},
	source = {Scopus},
	title = {Emotion Recognition on Facial Expression and Voice: Analysis and Discussion},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175050710&doi=10.18517%2fijaseit.13.5.19023&partnerID=40&md5=3059978f5adc8f6d4985c501529514cc},
	volume = {13},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175050710&doi=10.18517%2fijaseit.13.5.19023&partnerID=40&md5=3059978f5adc8f6d4985c501529514cc},
	bdsk-url-2 = {https://doi.org/10.18517/ijaseit.13.5.19023}}

@article{Hao202261,
	abstract = {[Purpose/ Significance] Emotion is one of the common ways to organize and retrieve resources on online music platforms. Exploration and research on emotion classification of song lists and songs using feature fusion can optimize management and utilization of music resources, better meeting the demand of internet users for music culture life. [Meth⁃ od/ Process] In this paper, Hevner's music emotion model was introduced to build an emotion lexicon, which was used with names and introduction of song lists to classify emotions of large-grained lists. Multimodal features of lyrics and audios were fused to identify emotions of small-grained songs through pre-trained model's semantic representation and audio signal processing. [Result/ Conclusion] Introduction of emotion lexicon effectively improves the accuracy of song list emotion clas⁃ sification, and manual preprocessing can help algorithms learn features better. Lyrics and audios both contain rich emotion information and the multimodal fusion model performs best in emotion recognition of songs. {\copyright} 2022 Editorial Board of Journal of Modern Information. All rights reserved.},
	author = {Hao, Wang and Yuanchen, Liu and Meng, Zhao and Jingwen, Qiu},
	author_keywords = {emotion classification of song lists; Mel spectro⁃ gram; multimodal fusion; music emotion classification; NetEase cloud music},
	doi = {10.3969/j.issn.1008-0821.2022.11.006},
	journal = {Journal of Modern Information},
	note = {Cited by: 1},
	number = {11},
	pages = {61 -- 75},
	publication_stage = {Final},
	source = {Scopus},
	title = {Research on Multi-task Music Emotion Recognition Based on Multi-modal Features; [基于多模态特征的音乐情感多任务识别研究]},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174213057&doi=10.3969%2fj.issn.1008-0821.2022.11.006&partnerID=40&md5=866512162b3b137dba6e25cf334ecd62},
	volume = {42},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174213057&doi=10.3969%2fj.issn.1008-0821.2022.11.006&partnerID=40&md5=866512162b3b137dba6e25cf334ecd62},
	bdsk-url-2 = {https://doi.org/10.3969/j.issn.1008-0821.2022.11.006}}

@article{Du2023259,
	abstract = {Music is the carrier through which human beings express their emotions. It can clean up their hearts and seek emotional resonance. The combination of music and artificial intelligence, when music meets artificial intelligence, the mathematical logic part of data and algorithm replaces the image thinking, resulting in automatic music production. The basic principle of music creation is to use artificial intelligence technology to conduct in-depth training on a large number of songs, and then build a database. Then, within a certain period of time, extract a segment from several songs, and then decompose it to form a new melody. The automatic music production is the collision of music and technology. With the flying of code and notes, it creates a different kind of beauty. Music culture is a new field of music application. With its refreshing audio-visual effect, it has made people have a new understanding of music and has become a hot topic in current mass cultural activities. The sound quality characteristics of music culture programs determine the characteristics of simple, fast and efficient design. However, the current audio and video design still stays in the traditional audio and video form design, with low design efficiency, long cycle, high cost and high requirements for designers. In view of the fact that the traditional audio and video design mode in China's music and cultural programs cannot meet the needs of performance programs at present, this paper proposes a design scheme of audio and visual effects of music and cultural programs with intelligent devices as the core, and develops a complete set of audio and visual effects aided design system on this basis. {\copyright} (2023), (European Journal for Philosophy of Religion). All Rights Reserved.},
	author = {Du, Hanfeng},
	author_keywords = {Audiovisual Effect; Feature Recognition; Intelligent Device Assistance; Music Programs},
	doi = {10.24204/ejpr.2023.4154},
	journal = {European Journal for Philosophy of Religion},
	note = {Cited by: 0},
	number = {2},
	pages = {259 -- 277},
	publication_stage = {Final},
	source = {Scopus},
	title = {AUDIOVISUAL EFFECT OF MUSIC AND CULTURAL PROGRAMS IN MASS CULTURAL ACTIVITIES ASSISTED BY INTELLIGENT DEVICES},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188216095&doi=10.24204%2fejpr.2023.4154&partnerID=40&md5=1eba6f069c6af73bea58d27c02c55eb2},
	volume = {15},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188216095&doi=10.24204%2fejpr.2023.4154&partnerID=40&md5=1eba6f069c6af73bea58d27c02c55eb2},
	bdsk-url-2 = {https://doi.org/10.24204/ejpr.2023.4154}}

@article{Li2022,
	abstract = {In chorus activities, the conductor leads chorus members to recreate music works. If you want to interpret music works perfectly with sound, emotion and emotional expression are particularly important. In this paper, a cloud HBD (health big data) integration system based on ensemble learning is designed to realize the high-efficiency and high-precision integration of HBD. An emotional speech database containing three emotions such as pleasure, calmness, and boredom is established, and the corpus problems such as emotional feature analysis and extraction needed for chorus emotion recognition research are solved. It also studies the classification and decision-making in emotional changes, and a DBN (deep belief network) chorus emotion recognition algorithm based on multiple emotional features is proposed. Feature DBN (Deep Belief Network) Chorus Emotion Recognition Algorithm This paper extracts various robust low-level features according to different features' ability to describe emotions and then feeds them into the DBN network to extract high-level feature descriptors. Then, the classification results of ELM (extreme learning machine) are voted and fused with the idea of ensemble learning, and the effectiveness of the algorithm is proved on three public datasets.  {\copyright} 2022 Yu Li and Yao Chen.},
	author = {Li, Yu and Chen, Yao},
	doi = {10.1155/2022/1363690},
	journal = {Journal of Healthcare Engineering},
	keywords = {Algorithms; Big Data; Emotions; Humans; Music; Speech; Big data; Classification (of information); Decision making; Machine learning; Medical applications; Data integration system; Deep belief networks; Emotion expression; Emotion recognition; Emotional expressions; Ensemble learning; High-precision; Higher efficiency; Precision integration; Recognition algorithm; algorithm; article; big data; boredom; decision making; deep belief network; emotion; extraction; human; human experiment; learning; machine learning; pleasure; speech; algorithm; emotion; music; Speech recognition},
	note = {Cited by: 1; All Open Access, Gold Open Access, Green Open Access},
	publication_stage = {Final},
	source = {Scopus},
	title = {Research on Chorus Emotion Recognition and Intelligent Medical Application Based on Health Big Data},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126867615&doi=10.1155%2f2022%2f1363690&partnerID=40&md5=b54742ce069c4c966e83f3d14848e5b3},
	volume = {2022},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126867615&doi=10.1155%2f2022%2f1363690&partnerID=40&md5=b54742ce069c4c966e83f3d14848e5b3},
	bdsk-url-2 = {https://doi.org/10.1155/2022/1363690}}

@article{Deng202419,
	abstract = {This article seeks to introduce a fresh perspective and approach to music emotion analysis by proposing a novel algorithm that integrates Deep Reinforcement Learning (DRL). The algorithm leverages the capabilities of deep neural networks to discern emotional characteristics within music autonomously. By utilizing the reinforcement learning framework, the decision-making process of the model is refined, enabling more precise identification of subtle emotional nuances in music. Experimental findings reveal that this algorithm significantly outperforms traditional methods in music emotion analysis, demonstrating a notable enhancement in detection accuracy. The key advantage of this algorithm lies in its ability to circumvent the intricacies and uncertainties associated with manual feature extraction. Furthermore, it exhibits superior adaptability to intricate music emotion analysis tasks, effectively elevating the accuracy and efficacy of classification. Following extensive training iterations, the model demonstrates a remarkable capacity to swiftly accommodate new data distributions and emotional expression patterns. In conclusion, the DRL-based music emotion analysis algorithm presented in this article contributes innovative research concepts and methodologies to the field and holds substantial theoretical and practical significance. {\copyright} 2024 U-turn Press LLC.},
	author = {Deng, Ya and Lin, Na},
	author_keywords = {CAD; Deep Reinforcement Learning; Music Emotion Analysis},
	doi = {10.14733/cadaps.2024.S23.19-34},
	journal = {Computer-Aided Design and Applications},
	keywords = {Computer aided design; Decision making; Deep neural networks; Emotion Recognition; Learning algorithms; Music; Decision-making process; Deep reinforcement learning; Detection accuracy; Emotion analysis; Learning frameworks; Music emotion analyse; Music emotions; Novel algorithm; Reinforcement learning algorithms; Reinforcement learnings; Reinforcement learning},
	note = {Cited by: 0; All Open Access, Bronze Open Access},
	number = {S23},
	pages = {19 -- 34},
	publication_stage = {Final},
	source = {Scopus},
	title = {Analysis and Expression of Music Emotion Based on CAD and Deep Reinforcement Learning Algorithm},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188622045&doi=10.14733%2fcadaps.2024.S23.19-34&partnerID=40&md5=8655f1e08bc6af376ed60d18654e2591},
	volume = {21},
	year = {2024},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188622045&doi=10.14733%2fcadaps.2024.S23.19-34&partnerID=40&md5=8655f1e08bc6af376ed60d18654e2591},
	bdsk-url-2 = {https://doi.org/10.14733/cadaps.2024.S23.19-34}}

@article{Luo2022,
	abstract = {Aiming at the complex and changeable characteristics of intelligent singing skills in the context of Internet of Things, this paper proposes a feature extraction method suitable for intelligent singing skills in this context. Firstly, focusing on vocal features, the time-domain algorithm based on open-loop and closed-loop gene extraction extracts the genetic features of songs with accompaniment; then, the section and its features are extracted by using the windowed moving matching algorithm, and the segments are divided by using the similarity between adjacent segments to obtain the segment features with emotional factors. The segment features are input into the improved BP emotion recognizer for emotion recognition. Finally, the intelligent singing skills of the whole music are determined. The experimental results show that, with the increase in feature extraction time, the accuracy of the extraction results of the existing methods changes little, which is basically maintained at a low level between 15% and 30%. When the proposed method is for feature extraction of intelligent singing skill information, the accuracy shows a continuous growth trend, and with the growth of time, its accuracy is significantly higher than the existing methods, indicating that the proposed method has significant advantages in the accuracy of feature extraction. Because this waveform feature extraction method is applied to the intelligent singing skills under the background of the Internet of Things, it has the advantages of high extraction efficiency, high accuracy, and reliability.  {\copyright} 2022 Chengping Luo.},
	author = {Luo, Chengping},
	doi = {10.1155/2022/4638801},
	journal = {Mobile Information Systems},
	keywords = {Emotion Recognition; Extraction; Internet of things; Music; Time domain analysis; Closed-loop; Emotion recognition; Emotional factors; Extraction time; Feature extraction methods; Features extraction; Matching algorithm; Open-loop; Time-domain algorithm; Waveform features; Feature extraction},
	note = {Cited by: 0; All Open Access, Gold Open Access},
	publication_stage = {Final},
	source = {Scopus},
	title = {Waveform Feature Extraction of Intelligent Singing Skills under the Background of Internet of Things},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133974440&doi=10.1155%2f2022%2f4638801&partnerID=40&md5=abf47112d02a52aba439fd91540b6f1e},
	volume = {2022},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133974440&doi=10.1155%2f2022%2f4638801&partnerID=40&md5=abf47112d02a52aba439fd91540b6f1e},
	bdsk-url-2 = {https://doi.org/10.1155/2022/4638801}}

@article{Shen2024204,
	abstract = {Music, as a unique artistic form of people's emotional expression, focuses on a rendering of the soul of the music audience and a re-experience of the virtual reality world. This experience is all-round, not just limited to hearing. In the process of music visualization, it is difficult to mechanically transform it into vision by a single rule. How to enhance the musical form and emotional characteristics of visual music works through multi-channel mapping mode is the research purpose of this paper. This article proposes a music based emotion recognition model feature, and a musical animation CAD (Computer Aided Design) system is constructed. The system extracts some basic musical features from MIDI (Musical Instrument Digital Interface) files, and then extracts the musical features of the music. Thus, each music segment is sliced into emotional music visualization programs. And by summarizing the obtained segment features as a whole, design an emotional feature program that can reflect the musical form. Through deep learning algorithms, it visualizes and matches the improved segment nodes, improving the expressive form of music emotional animation. {\copyright} 2024, CAD Solutions, LLC. All rights reserved.},
	author = {Shen, Dan and Zhao, Wenjia},
	author_keywords = {Computer Aided Design; Deep Learning; Emotional Recognition; Music Animation; Musical Features; Visualization},
	doi = {10.14733/cadaps.2024.S1.204-217},
	journal = {Computer-Aided Design and Applications},
	keywords = {Animation; Audio acoustics; Audition; Computer aided design; Computer aided instruction; Deep learning; Emotion Recognition; Learning algorithms; Learning systems; Speech recognition; Virtual reality; Visualization; Computer aided design systems; Computer-aided design; Deep learning; Emotion recognition; Emotional expressions; Emotional recognition; Form recognition; Music animation; Music visualization; Musical features; Music},
	note = {Cited by: 0; All Open Access, Bronze Open Access},
	number = {S1},
	pages = {204 -- 217},
	publication_stage = {Final},
	source = {Scopus},
	title = {Research on the Construction of Music Animation CAD System Based on Music Form and Emotion Recognition},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169049920&doi=10.14733%2fcadaps.2024.S1.204-217&partnerID=40&md5=34fec38348b00e63cecb493ef6e85bad},
	volume = {21},
	year = {2024},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169049920&doi=10.14733%2fcadaps.2024.S1.204-217&partnerID=40&md5=34fec38348b00e63cecb493ef6e85bad},
	bdsk-url-2 = {https://doi.org/10.14733/cadaps.2024.S1.204-217}}

@article{Orjesek20225017,
	abstract = {Automatic music emotion recognition (MER) has received increased attention in areas of music information retrieval and user interface development. Music emotion variation detection (or dynamic MER) captures also temporal changes of emotion, and emotional content in music is expressed as a series of valence-arousal predictions. One of the issues in MER is extraction of emotional characteristics from audio signal. We propose a deep neural network based solution for mining music emotion-related salient features directly from raw audio waveform. The proposed architecture is based on stacking one-dimensional convolution layer, autoencoder-based layer with iterative reconstruction, and bidirectional gated recurrent unit. The tests on the DEAM dataset have shown that the proposed solution, in comparison with other state-of-the-art systems, can bring a significant improvement of the regression accuracy, notably for the valence dimension. It is shown that the proposed iterative reconstruction layer is able to enhance the discriminative properties of the features and further increase regression accuracy. {\copyright} 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	author = {Orjesek, Richard and Jarina, Roman and Chmulik, Michal},
	author_keywords = {Arousal; Bi-directional gated recurrent unit; End-to-end deep learning; Iterative reconstruction; Music emotion recognition; Valence},
	doi = {10.1007/s11042-021-11584-7},
	journal = {Multimedia Tools and Applications},
	keywords = {Audio acoustics; Deep neural networks; Music; Statistical tests; User interfaces; Arousal; Bi-directional; Bi-directional gated recurrent unit; Emotion recognition; End to end; End-to-end deep learning; Iterative reconstruction; Music emotion recognition; Music emotions; Valence; Speech recognition},
	note = {Cited by: 17},
	number = {4},
	pages = {5017 -- 5031},
	publication_stage = {Final},
	source = {Scopus},
	title = {End-to-end music emotion variation detection using iteratively reconstructed deep features},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122663948&doi=10.1007%2fs11042-021-11584-7&partnerID=40&md5=443ca62b323cbb879bcce434b1a20016},
	volume = {81},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122663948&doi=10.1007%2fs11042-021-11584-7&partnerID=40&md5=443ca62b323cbb879bcce434b1a20016},
	bdsk-url-2 = {https://doi.org/10.1007/s11042-021-11584-7}}

@article{Song2024,
	abstract = {The continuous progress of multimedia technology in music educational institutions has led to the recognition of its importance in our country and society. The traditional approach to piano teaching has its limitations, which can be overcome by adopting alternative approaches to the instrument, using advances in science and technology. For pianist, expressing emotions and thoughts through music is crucial, and teachers can now use multimedia tools to exemplify their musical skills to students effectively. This manuscript proposes the Remote Piano Teaching Based on Attention-Induced Multi-Head Convolutional Neural Network Optimized with Hunter--Prey Optimization to improve the piano-teaching quality. At first, input data is taken from Piano Triad Wavset dataset. Afterward, the data are fed to preprocessing stage. The preprocessing stage involve data cleaning or scrubbing that is the process of identifying errors, inconsistencies, and incorrectness in a dataset with the help of adaptive distorted Gaussian matched filter. Then, the preprocessed output is fed to Attention-Induced Multi-Head Convolutional Neural Network (AIMCNN) for effectively predict the piano-teaching quality. The hunter--prey optimization (HPO) algorithm is proposed to optimize the parameters of Attention-Induced Multi-Head Convolutional Neural Network. The performance of the proposed technique is evaluated under performance metrics like accuracy, computational time, learning skill analysis, learning activity analysis, learning behavior analysis; student performance ratio and teaching evaluation analysis are evaluated. The proposed RPT-AIMCNN-HPO attains better prediction accuracy 12.566%, 12.075% and 15.993%, higher learning skill 15.86%, 15.26% and 16.25% compared with existing methods. {\copyright} 2023, The Author(s).},
	author = {Song, Li},
	author_keywords = {Adaptive distorted Gaussian matched filter Attention-Induced Multi-Head Convolutional Neural Network; Hunter--prey optimization; Piano Triads Wavset dataset; Remote piano teaching},
	doi = {10.1007/s44196-023-00379-3},
	journal = {International Journal of Computational Intelligence Systems},
	keywords = {Adaptive filtering; Adaptive filters; Convolution; Convolutional neural networks; Multimedia systems; Music; Musical instruments; Students; Adaptive distorted gaussian matched filter attention-induced multi-head convolutional neural network; Convolutional neural network; Design and implementations; Gaussians; Hunter--prey optimization; Learning skills; Optimisations; Piano triad wavset dataset; Remote piano teaching; Teaching quality; Matched filters},
	note = {Cited by: 1; All Open Access, Gold Open Access},
	number = {1},
	publication_stage = {Final},
	source = {Scopus},
	title = {Design and Implementation of Remote Piano Teaching Based on Attention-Induced Multi-Head Convolutional Neural Network Optimized with Hunter--Prey Optimization},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181247907&doi=10.1007%2fs44196-023-00379-3&partnerID=40&md5=129becf4522cda405212ccfa35c7c53a},
	volume = {17},
	year = {2024},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181247907&doi=10.1007%2fs44196-023-00379-3&partnerID=40&md5=129becf4522cda405212ccfa35c7c53a},
	bdsk-url-2 = {https://doi.org/10.1007/s44196-023-00379-3}}

@article{Bai2022,
	abstract = {Emotion is the important information that people transmit in the process of communication, and the change of emotional state affects people's perception and decision-making, which introduces the emotional dimension into human-computer interaction. The modes of emotional expression include facial expressions, speech, posture, physiological signals, text, and so on. Emotion recognition is essentially a multimodal fusion problem. This paper investigates the different teaching modes of the teachers and students of our school, designs the load capacity through the K-means algorithm, builds a multimedia network sharing classroom, and creates a piano music situation to stimulate students' learning interest, using audiovisual and other tools to mobilize students' emotions, using multimedia guidance to extend students' piano music knowledge, and comprehensively improve students' aesthetic ability and autonomous learning ability. Comparing the changes of students after 3 months of teaching, the results of the study found that multimedia sharing classrooms can be up to 50% ahead of traditional teaching methods in enhancing students' interest, and teachers' acceptance of multimedia network sharing classrooms is also high.  {\copyright} 2022 Jie Bai.},
	author = {Bai, Jie},
	doi = {10.1155/2022/1882739},
	journal = {Mobile Information Systems},
	keywords = {Behavioral research; Decision making; Emotion Recognition; Human computer interaction; K-means clustering; Speech recognition; Emotion recognition; Emotional state; Model-based OPC; Multimedia networks; Multimedia video; Multimodal information fusion; Music education; Network sharing; Piano music; Video networks; Students},
	note = {Cited by: 1; All Open Access, Gold Open Access},
	publication_stage = {Final},
	source = {Scopus},
	title = {Optimized Piano Music Education Model Based on Multimodal Information Fusion for Emotion Recognition in Multimedia Video Networks},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137899411&doi=10.1155%2f2022%2f1882739&partnerID=40&md5=39756c290460dcf527de0696d559b51f},
	volume = {2022},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137899411&doi=10.1155%2f2022%2f1882739&partnerID=40&md5=39756c290460dcf527de0696d559b51f},
	bdsk-url-2 = {https://doi.org/10.1155/2022/1882739}}

@article{Bagherzadeh202347,
	abstract = {Purpose: Emotions are integral brain states that can influence our behavior, decision-making, and functions. Electroencephalogram (EEG) is an appropriate modality for emotion recognition since it has high temporal resolution and is a non-invasive and cheap technique. Materials and Methods: A novel approach based on Ensemble pre-trained Convolutional Neural Networks (ECNNs) is proposed to recognize four emotional classes from EEG channels of individuals watching music video clips. First, scalograms are built from one-dimensional EEG signals by applying the Continuous Wavelet Transform (CWT) method. Then, these images are used to re-train five CNNs: AlexNet, VGG-19, Inception-v1, ResNet-18, and Inception-v3. Then, the majority voting method is applied to make the final decision about emotional classes. The 10-fold cross-validation method is used to evaluate the performance of the proposed method on EEG signals of 32 subjects from the DEAP database. Results: The experiments showed that applying the proposed ensemble approach in combinations of scalograms of frontal and parietal regions improved results. The best accuracy, sensitivity, precision, and F-score to recognize four emotional states achieved 96.90% $\pm$ 0.52, 97.30 $\pm$ 0.55, 96.97 $\pm$ 0.62, and 96.74 $\pm$ 0.56, respectively. Conclusion: So, the newly proposed model from EEG signals improves recognition of the four emotional states in the DEAP database. Copyright {\copyright} 2023 Tehran University of Medical Sciences.},
	author = {Bagherzadeh, Sara and Maghooli, Keivan and Shalbaf, Ahmad and Maghsoudi, Arash},
	author_keywords = {Continuous Wavelet Transform; Deep Learning; Electroencephalogram; Emotion Recognition; Ensemble Approach; Transfer Learning},
	doi = {10.18502/fbt.v10i1.11512},
	journal = {Frontiers in Biomedical Technologies},
	keywords = {accuracy; adolescent; adult; anger; arousal; Article; brain nerve cell; clinical article; clinical decision making; convolutional neural network; cumulative scale; electroencephalogram; emotion assessment; excitement; fear; female; Fourier transform; happiness; human; limbic system; male; mental performance; music; neurophysiology; non invasive procedure; occipital cortex; self evaluation; signal processing; support vector machine; transfer of learning; valence (emotion); wavelet transform},
	note = {Cited by: 2; All Open Access, Gold Open Access},
	number = {1},
	pages = {47 -- 56},
	publication_stage = {Final},
	source = {Scopus},
	title = {Emotion Recognition Using Continuous Wavelet Transform and Ensemble of Convolutional Neural Networks through Transfer Learning from Electroencephalogram Signal},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147424502&doi=10.18502%2ffbt.v10i1.11512&partnerID=40&md5=086552d4079133b586f99b9bdcb4a216},
	volume = {10},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147424502&doi=10.18502%2ffbt.v10i1.11512&partnerID=40&md5=086552d4079133b586f99b9bdcb4a216},
	bdsk-url-2 = {https://doi.org/10.18502/fbt.v10i1.11512}}

@article{Hu2022,
	abstract = {The subjectivity of listeners' emotional responses to music is at the crux of optimizing emotion-aware music recommendation. To address this challenge, we constructed a new multimodal dataset (``HKU956'') with aligned peripheral physiological signals (i.e., heart rate, skin conductance, blood volume pulse, skin temperature) and self-reported emotion collected from 30 participants, as well as original audio of 956 music pieces listened to by the participants. A comprehensive set of features was extracted from physiological signals using methods in physiological computing. This study then compared performances of three feature sets (i.e., acoustic, physiological, and combined) on the task of classifying music-induced emotion. Moreover, the classifiers were also trained on subgroups of users with different Big-Five personality traits for further customized modeling. The results reveal that (1) physiological features contribute to improving performance on valence classification with statistical significance; (2) classification models built for users in different personality groups could sometimes further improve arousal prediction; and (3) the multimodal classifier outperformed single-modality ones on valence classification for most user groups. This study contributes to designing music retrieval systems which incorporate user physiological data and model listeners' emotional responses to music in a customized manner. {\copyright} 2022 by the authors.},
	author = {Hu, Xiao and Li, Fanjie and Liu, Ruilun},
	author_keywords = {customization; multimodal recognition; music retrieval and generation; physiological measures; sound and music computing},
	doi = {10.3390/app12189354},
	journal = {Applied Sciences (Switzerland)},
	note = {Cited by: 0; All Open Access, Gold Open Access},
	number = {18},
	publication_stage = {Final},
	source = {Scopus},
	title = {Detecting Music-Induced Emotion Based on Acoustic Analysis and Physiological Sensing: A Multimodal Approach},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138659559&doi=10.3390%2fapp12189354&partnerID=40&md5=bae964594710aa9b4f357256c8a08e13},
	volume = {12},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138659559&doi=10.3390%2fapp12189354&partnerID=40&md5=bae964594710aa9b4f357256c8a08e13},
	bdsk-url-2 = {https://doi.org/10.3390/app12189354}}

@article{Loukas2022647,
	abstract = {Music is known to induce emotions and activate associated memories, including musical memories. In adults, it is well known that music activates both working memory and limbic networks. We have recently discovered that as early as during the newborn period, familiar music is processed differently from unfamiliar music. The present study evaluates music listening effects at the brain level in newborns, by exploring the impact of familiar or first-time music listening on the subsequent resting-state functional connectivity in the brain. Using a connectome-based framework, we describe resting-state functional connectivity (RS-FC) modulation after music listening in three groups of newborn infants, in preterm infants exposed to music during their neonatal-intensive-care-unit (NICU) stay, in control preterm, and full-term infants. We observed modulation of the RS-FC between brain regions known to be implicated in music and emotions processing, immediately following music listening in all newborn infants. In the music exposed group, we found increased RS-FC between brain regions known to be implicated in familiar and emotionally arousing music and multisensory processing, and therefore implying memory retrieval and associative memory. We demonstrate a positive correlation between the occurrence of the prior music exposure and increased RS-FC in brain regions implicated in multisensory and emotional processing, indicating strong engagement of musical memories; and a negative correlation with the Default Mode Network, indicating disengagement due to the aforementioned cognitive processing. Our results describe the modulatory effect of music listening on brain RS-FC that can be linked to brain correlates of musical memory engrams in preterm infants. {\copyright} 2021 The Authors. Human Brain Mapping published by Wiley Periodicals LLC.},
	author = {Loukas, Serafeim and Lordier, Lara and Meskaldji, Djalel-Eddine and Filippa, Manuela and Sa de Almeida, Joana and Van De Ville, Dimitri and H{\"u}ppi, Petra S.},
	doi = {10.1002/hbm.25677},
	journal = {Human Brain Mapping},
	keywords = {Amygdala; Auditory Perception; Cerebral Cortex; Connectome; Default Mode Network; Emotions; Female; Humans; Infant, Newborn; Infant, Premature; Magnetic Resonance Imaging; Male; Music; Recognition, Psychology; Thalamus; algorithm; Article; associative memory; auditory stimulation; BOLD signal; brain region; caudate nucleus; cingulate gyrus; connectome; controlled study; default mode network; emotional network; female; functional connectivity; gestational age; hospitalization; human; human experiment; inferior temporal gyrus; male; middle frontal gyrus; music; neonatal intensive care unit; newborn; newborn period; normal human; nuclear magnetic resonance imaging; parahippocampal gyrus; prematurity; primary motor cortex; resting state network; superior frontal gyrus; supplementary motor area; thalamus; amygdala; brain cortex; connectome; diagnostic imaging; emotion; hearing; physiology},
	note = {Cited by: 8; All Open Access, Green Open Access},
	number = {2},
	pages = {647 -- 664},
	publication_stage = {Final},
	source = {Scopus},
	title = {Musical memories in newborns: A resting-state functional connectivity study},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118480313&doi=10.1002%2fhbm.25677&partnerID=40&md5=89a9b54fc889ade991c5ca8fda6db634},
	volume = {43},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118480313&doi=10.1002%2fhbm.25677&partnerID=40&md5=89a9b54fc889ade991c5ca8fda6db634},
	bdsk-url-2 = {https://doi.org/10.1002/hbm.25677}}

@article{Priya2024177,
	abstract = {Background: Facial expression, tone of voice, body language, and context are unrecognizable to children with autism. Emotional arousal and emotion recognition (required emotion empathy and cognitive processing empathy) induce downstream illnesses in children with ASD. Thus, the proposed study aimed to develop a computer-based Emotional Recognition Memory Training Program (ERMTP) for ASD. Objective: Firstly, to develop and validate the ERMTP for social cognitive abilities in children with ASD and secondly, to conduct pilot-tested it in typically developing children and children with ASD. Materials and methods: This study consisted of 3 phases. The first phase was developing the ERMTP from the literature review. The second phase was analyzed for content validity with five experts about Task 1 (two activities) and Task 2 (nine activities) comprising ERMTP. Computer-based learning of six fundamental facial emotions (happy, sad, angry, fear, disgusted, and surprised) improves social cognition. Finally, the pilot test was analyzed to discover the ERMTP's challenges for five children with typical development and ASD. Results: The ERMTP's activity items have good content validity, especially regarding clarity and relevance. All five raters gave the intervention a 1.0 IOC for its distinct components. In the training program, we followed the expert instructions regarding background music or voice and the generalization task. Descriptive analysis indicated that all five normal-developing children followed emotional expressions and instructions (100%). All five parents reported there were changes in focus and memory skills. Emotion regulation, memory abilities, and the social cognition index demonstrated statistically significant (p<0.05) effects before and after ERMTP treatment in ASD. Conclusion: ERMTP seeks to improve the social cognition of children with ASD by the use of feedback from both specialists and the children themselves. However, further research will be necessary to investigate ASD using a randomized control trial. {\copyright} 2024, Faculty of Associated Medical Sciences, Chiang Mai University. All rights reserved.},
	author = {Priya, Smily Jesu and Paulraj, Victor and Chinchai, Supaporn and Munkhetvit, Peeraya and Sriphetcharawut, Sarinya},
	author_keywords = {autism; Emotion recognition; facial expression; memory; social cognition},
	doi = {10.12982/JAMS.2024.020},
	journal = {Journal of Associated Medical Sciences},
	note = {Cited by: 0; All Open Access, Bronze Open Access},
	number = {1},
	pages = {177 -- 189},
	publication_stage = {Final},
	source = {Scopus},
	title = {The development and content validity of the emotional recognition memory training program (ERMTP) for children with autism spectrum disorder: A trial phase},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180675901&doi=10.12982%2fJAMS.2024.020&partnerID=40&md5=9f3b9e113cb56d847c534a4ad17577f6},
	volume = {57},
	year = {2024},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180675901&doi=10.12982%2fJAMS.2024.020&partnerID=40&md5=9f3b9e113cb56d847c534a4ad17577f6},
	bdsk-url-2 = {https://doi.org/10.12982/JAMS.2024.020}}

@article{Mazhar2022,
	abstract = {The critical component of HCI is face recognition technology. Emotional computing heavily relies on the identification of facial emotions. Applications for emotion-driven face animation and dynamic assessment are numerous (FER). Universities have started to support real-world face expression recognition research as a result. Short video clips are continually uploaded and shared online, building up a library of videos on various topics. The enormous amount of movie data appeals to system engineers and researchers of autonomous emotion mining and sentiment analysis. The main idea is that categorizing things may be done by looking at how individuals feel about specific issues. People might choose to have a basic or complex facial appearance. People worldwide continually express their feelings through their faces, whether they are happy, sad, or uncertain. An online user can visually express themselves through a video's editing, music, and subtitles. Additionally, before the video data can be used, noise in the data must frequently be eliminated. Automatically figuring out how someone feels in a video is a challenging task that will only get harder over time. Therefore, this paper aims to show how facial recognition video analysis can be used to show how sentiment analysis can help with business growth and essential decision-making. To determine how people are affected by reviewers' writing, we use a technique for deciding emotions in this analysis. The feelings in movies are assessed using machine learning algorithms to categorize them. A lightweight machine learning algorithm is proposed to help in Aspect-oriented emotion classification for movie reviews. Moreover, to analyze real and published datasets, experimental results are compared with different Machine Learning algorithms, i.e., Naive Bayes, Support Vector Machine, Random Forest, and CNN. The proposed approach obtained 84.72 accuracy and 79.24 sensitivity. Furthermore, the method has a specificity of 90.64 and a precision of 90.2. Thus, the proposed method significantly increases the accuracy and sensitivity of the emotion detection system from facial feature recognition. Our proposed algorithm has shown contribution to detect datasets of different emotions with symmetric characteristics and symmetrically-designed facial image recognition tasks. {\copyright} 2022 by the authors.},
	author = {Mazhar, Tehseen and Malik, Muhammad Amir and Nadeem, Muhammad Asgher and Mohsan, Syed Agha Hassnain and Haq, Inayatul and Karim, Faten Khalid and Mostafa, Samih M.},
	author_keywords = {emotion detection; facial recognition; machine learning; reviews classification},
	doi = {10.3390/sym14122607},
	journal = {Symmetry},
	note = {Cited by: 7; All Open Access, Gold Open Access},
	number = {12},
	publication_stage = {Final},
	source = {Scopus},
	title = {Movie Reviews Classification through Facial Image Recognition and Emotion Detection Using Machine Learning Methods},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144888881&doi=10.3390%2fsym14122607&partnerID=40&md5=9be6286981d8f11b382e99ea990fe5c5},
	volume = {14},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144888881&doi=10.3390%2fsym14122607&partnerID=40&md5=9be6286981d8f11b382e99ea990fe5c5},
	bdsk-url-2 = {https://doi.org/10.3390/sym14122607}}

@article{Pandeya2024,
	abstract = {Music is a powerful language capable of eliciting a variety of emotions in individuals. Understanding and recognizing these emotions is pivotal for applications ranging from personalized music recommendations and music therapy to automatic music composition and affective computing. Presently, deep learning for music emotion recognition is gaining popularity, primarily relying on timbre features to capture local spatial information. However, there is an untapped potential in incorporating other pertinent audio features and global correlations in the feature space to capture the repetitive temporal information of music for emotion classification. This study introduces GlocalEmoNet as a method to capture both local and global correlations in music, utilizing timbre and Chroma audio features for tasks related to emotion classification and segmentation. The neural network underwent training and testing on approximately six thousand music audio samples, encompassing six music-emotion categories. The utilization of a genetic algorithm is employed for optimizing the hyperparameters of the proposed neural networks, aiming to attain optimal performance, efficiency, and generalization. The best classifier demonstrated superior performance, surpassing previously published results by a significant margin of approximately 14%. The optimal classifier achieved an accuracy score of 81.66%, an f1-score of 0.812, and an area under the curve score of 0.956. The evaluation of classification and segmentation outcomes also involved the use of visual representations. {\copyright} The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.},
	author = {Pandeya, Yagya Raj and Lee, Joonwhoan},
	author_keywords = {Classification; Genetic algorithm; GlocalEmoNet; Music emotion; Segmentation},
	doi = {10.1007/s11042-024-18246-4},
	journal = {Multimedia Tools and Applications},
	keywords = {Audio acoustics; Classification (of information); Computer music; Deep learning; Emotion Recognition; Music; Audio features; Chroma features; Emotion classification; Global correlation; Glocalemonet; Music emotion classifications; Music emotions; Music recommendation; Neural-networks; Segmentation; Genetic algorithms},
	note = {Cited by: 0},
	publication_stage = {Article in press},
	source = {Scopus},
	title = {GlocalEmoNet: An optimized neural network for music emotion classification and segmentation using timbre and chroma features},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185132967&doi=10.1007%2fs11042-024-18246-4&partnerID=40&md5=96770150ba0f11e7611abf80bffde28a},
	year = {2024},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185132967&doi=10.1007%2fs11042-024-18246-4&partnerID=40&md5=96770150ba0f11e7611abf80bffde28a},
	bdsk-url-2 = {https://doi.org/10.1007/s11042-024-18246-4}}

@article{Yang202280,
	abstract = {As an important part of human life, music can convey emotion and regulate the emotions of listeners. Emotion is one of the essential features of music, and the relationship between music and emotion has become the subject of many academic studies. At present, with the rapid development of information technology and artificial intelligence, music emotion recognition has made rapid progress and become one of the important research directions in the field of digital music. Aiming at the problem of poor classification effect of musical emotion caused by the monotony of Support Vector Machine (SVM) projection space, this paper proposes an optimized SVM model for music feature emotion recognition. The new method can not only improve the accuracy of music emotion classification, but also improve the running speed and interpretability of the model. At the end, the practicality and reliability of the new approach are verified by public classification data sets and real music emotion data sets. This paper proposes an optimized SVM model for music feature emotion recognition. The new method can not only improve the accuracy of music emotion classification, but also improve the running speed and interpretability of the model. Finally, the practicality and reliability of the new approach are verified by both the public classification data sets and real music emotion data sets. {\copyright} 2022 CAD Solutions, LLC,.},
	author = {Yang, Chaode and Li, Qingxun},
	author_keywords = {Computer aided technology; Music feature emotion recognition; Optimization algorithm; SVM},
	doi = {10.14733/cadaps.2022.S6.80-90},
	journal = {Computer-Aided Design and Applications},
	keywords = {Classification (of information); Internet of things; Music; Speech recognition; Vector spaces; Computer aided technologies; Computer-aided technologies; Data set; Emotion recognition; Music emotion classifications; Music emotions; Music feature emotion recognition; Optimization algorithms; Support vector machine models; Support vectors machine; Support vector machines},
	note = {Cited by: 5; All Open Access, Bronze Open Access},
	number = {S6},
	pages = {80 -- 90},
	publication_stage = {Final},
	source = {Scopus},
	title = {Music Emotion Feature Recognition based on Internet of Things and Computer-Aided Technology},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122323526&doi=10.14733%2fcadaps.2022.S6.80-90&partnerID=40&md5=d2f6a750386672cd2f4f123548cbd50b},
	volume = {19},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122323526&doi=10.14733%2fcadaps.2022.S6.80-90&partnerID=40&md5=d2f6a750386672cd2f4f123548cbd50b},
	bdsk-url-2 = {https://doi.org/10.14733/cadaps.2022.S6.80-90}}

@article{Bakariya2023,
	abstract = {An AI interactive robot can identify human faces, determine the emotions of the person it is chatting with, and then pick appropriate replies using algorithms that analyze facial expressions and recognize faces. One example of these algorithms is facing recognition and emotion recognition algorithms. Deep learning is currently the most effective method for carrying out tasks. We have developed a real-time system that can recognize human faces, determine human emotions, and even provide users with music recommendations by utilizing deep learning and a few Python modules. The OAHEGA and FER-2013 datasets train the models presented in this article. The accuracy of our suggested model was compared to several baseline approaches, and the results were quite affirmative. Anger, fear, pleasure, neutral, sorrow, and surprise are the six feelings that our CNN model can predict. {\copyright} 2023, The Author(s), under exclusive licence to The National Academy of Sciences, India.},
	author = {Bakariya, Brijesh and Mohbey, Krishna Kumar and Singh, Arshdeep and Singh, Harmanpreet and Raju, Pankaj and Rajpoot, Rohit},
	author_keywords = {Artificial intelligence; CNN; Deep learning; Face expression recognition; Face recognition},
	doi = {10.1007/s40009-023-01346-4},
	journal = {National Academy Science Letters},
	note = {Cited by: 0},
	publication_stage = {Article in press},
	source = {Scopus},
	title = {An Efficient Model for Facial Expression Recognition with Music Recommendation},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169582274&doi=10.1007%2fs40009-023-01346-4&partnerID=40&md5=2c42839e489d24ad53815dd076b36c41},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169582274&doi=10.1007%2fs40009-023-01346-4&partnerID=40&md5=2c42839e489d24ad53815dd076b36c41},
	bdsk-url-2 = {https://doi.org/10.1007/s40009-023-01346-4}}

@article{Xie2022,
	abstract = {The dynamic of music is an important factor to arouse emotional experience, but current research mainly uses short-term artificial stimulus materials, which cannot effectively awaken complex emotions and reflect their dynamic brain response. In this paper, we used three long-term stimulus materials with many dynamic emotions inside: the ``Waltz No. 2'' containing pleasure and excitement, the ``No. 14 Couplets'' containing excitement, briskness, and nervousness, and the first movement of ``Symphony No. 5 in C minor'' containing passion, relaxation, cheerfulness, and nervousness. Approximate entropy (ApEn) and sample entropy (SampEn) were applied to extract the non-linear features of electroencephalogram (EEG) signals under long-term dynamic stimulation, and the K-Nearest Neighbor (KNN) method was used to recognize emotions. Further, a supervised feature vector dimensionality reduction method was proposed. Firstly, the optimal channel set for each subject was obtained by using a particle swarm optimization (PSO) algorithm, and then the number of times to select each channel in the optimal channel set of all subjects was counted. If the number was greater than or equal to the threshold, it was a common channel suitable for all subjects. The recognition results based on the optimal channel set demonstrated that each accuracy of two categories of emotions based on ``Waltz No. 2'' and three categories of emotions based on ``No. 14 Couplets'' was generally above 80%, respectively, and the recognition accuracy of four categories based on the first movement of ``Symphony No. 5 in C minor'' was about 70%. The recognition accuracy based on the common channel set was about 10% lower than that based on the optimal channel set, but not much different from that based on the whole channel set. This result suggested that the common channel could basically reflect the universal features of the whole subjects while realizing feature dimension reduction. The common channels were mainly distributed in the frontal lobe, central region, parietal lobe, occipital lobe, and temporal lobe. The channel number distributed in the frontal lobe was greater than the ones in other regions, indicating that the frontal lobe was the main emotional response region. Brain region topographic map based on the common channel set showed that there were differences in entropy intensity between different brain regions of the same emotion and the same brain region of different emotions. The number of times to select each channel in the optimal channel set of all 30 subjects showed that the principal component channels representing five brain regions were Fp1/F3 in the frontal lobe, CP5 in the central region, Pz in the parietal lobe, O2 in the occipital lobe, and T8 in the temporal lobe, respectively. {\copyright} 2022 by the authors.},
	author = {Xie, Zun and Pan, Jianwei and Li, Songjie and Ren, Jing and Qian, Shao and Ye, Ye and Bao, Wei},
	author_keywords = {channel optimization; EEG signals; emotion recognition; entropy; musical emotions},
	doi = {10.3390/e24121735},
	journal = {Entropy},
	note = {Cited by: 3; All Open Access, Gold Open Access, Green Open Access},
	number = {12},
	publication_stage = {Final},
	source = {Scopus},
	title = {Musical Emotions Recognition Using Entropy Features and Channel Optimization Based on EEG},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144644545&doi=10.3390%2fe24121735&partnerID=40&md5=53c4a545eaab533629baea2dd1d387fb},
	volume = {24},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144644545&doi=10.3390%2fe24121735&partnerID=40&md5=53c4a545eaab533629baea2dd1d387fb},
	bdsk-url-2 = {https://doi.org/10.3390/e24121735}}

@article{Bao20233602,
	abstract = {We focus on the music generation conditional on human emotions, specifically the positive and negative emotions. There is no existing large-scale music datasets with the annotation of human emotion labels. It is thus not intuitive how to generate music conditioned on emotion labels. In this paper, we propose an annotation-free method to build a new dataset where each sample is a triplet of lyric, melody and emotion label (without requiring any labours). Specifically, we first train the automated emotion recognition model using the BERT (pre-trained on GoEmotions dataset) on Edmonds Dance dataset. We use it to automatically 'label' the music with the emotion labels recognized from the lyrics. We then train the encoder-decoder based model to generate emotional music on that dataset, and call our overall method as Emotional Lyric and Melody Generator (ELMG). The framework of ELMG is consisted of three modules: 1) an encoder-decoder model trained end-to-end to generate lyric and melody; 2) a music emotion classifier trained on labeled data (our proposed dataset); and 3) a modified beam search algorithm that guides the music generation process by incorporating the music emotion classifier. We conduct objective and subjective evaluations on the generated music pieces, and our results show that ELMG is capable of generating tuneful lyric and melody with specified human emotions.  {\copyright} 1999-2012 IEEE.},
	author = {Bao, Chunhui and Sun, Qianru},
	author_keywords = {beam search; Conditional music generation; Seq2Seq; transformer},
	doi = {10.1109/TMM.2022.3163543},
	journal = {IEEE Transactions on Multimedia},
	keywords = {Classification (of information); Decoding; Deep learning; Generative adversarial networks; Large dataset; Music; Signal encoding; Speech recognition; Beam search; Conditional music generation; Deep learning; Emotion recognition; Generator; Human emotion; Natural languages; Seq2seq; Social networking (online); Transformer; Emotion Recognition},
	note = {Cited by: 2; All Open Access, Green Open Access},
	pages = {3602 -- 3614},
	publication_stage = {Final},
	source = {Scopus},
	title = {Generating Music With Emotions},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127520415&doi=10.1109%2fTMM.2022.3163543&partnerID=40&md5=61bc5f8c7a6ea21475248daeb75a7415},
	volume = {25},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127520415&doi=10.1109%2fTMM.2022.3163543&partnerID=40&md5=61bc5f8c7a6ea21475248daeb75a7415},
	bdsk-url-2 = {https://doi.org/10.1109/TMM.2022.3163543}}

@article{Rufino2024,
	abstract = {While the musical instrument classification task is well-studied, there remains a gap in identifying non-pitched percussion instruments which have greater overlaps in frequency bands and variation in sound quality and play style than pitched instruments. In this paper, we present a musical instrument classifier for detecting tambourines, maracas and castanets, instruments that are often used in early childhood music education. We generated a dataset with diverse instruments (e.g., brand, materials, construction) played in different locations with varying background noise and play styles. We conducted sensitivity analyses to optimize feature selection, windowing time, and model selection. We deployed and evaluated our best model in a mixed reality music application with 12 families in a home setting. Our dataset was comprised of over 369,000 samples recorded in-lab and 35,361 samples recorded with families in a home setting. We observed the Light Gradient Boosting Machine (LGBM) model to perform best using an approximate 93 ms window with only 12 mel-frequency cepstral coefficients (MFCCs) and signal entropy. Our best LGBM model was observed to perform with over 84% accuracy across all three instrument families in-lab and over 73% accuracy when deployed to the home. To our knowledge, the dataset compiled of 369,000 samples of non-pitched instruments is first of its kind. This work also suggests that a low feature space is sufficient for the recognition of non-pitched instruments. Lastly, real-world deployment and testing of the algorithms created with participants of diverse physical and cognitive abilities was also an important contribution towards more inclusive design practices. This paper lays the technological groundwork for a mixed reality music application that can detect children's use of non-pitched, percussion instruments to support early childhood music education and play. Copyright: {\copyright} 2024 Rufino et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
	author = {Rufino, Brandon and Khan, Ajmal and Dutta, Tilak and Biddiss, Elaine},
	doi = {10.1371/journal.pone.0299888},
	journal = {PLoS ONE},
	keywords = {Algorithms; Child; Child, Preschool; Cognition; Humans; Music; Percussion; Sound; algorithm; Article; background noise; child; childhood; classifier; clinical article; cognition; education; emotion; entropy; ethnicity; family income; hemiparesis; human; hydrocephalus; machine learning; microgyria; musical instrument; percussion; physical capacity; school child; sensitivity analysis; spinal cord injury; timbre; training; music; preschool child; sound},
	note = {Cited by: 0; All Open Access, Gold Open Access},
	number = {4 April},
	publication_stage = {Final},
	source = {Scopus},
	title = {Musical instrument classifier for early childhood percussion instruments},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189517552&doi=10.1371%2fjournal.pone.0299888&partnerID=40&md5=2321373991cbc08a826f982ca8a35c37},
	volume = {19},
	year = {2024},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189517552&doi=10.1371%2fjournal.pone.0299888&partnerID=40&md5=2321373991cbc08a826f982ca8a35c37},
	bdsk-url-2 = {https://doi.org/10.1371/journal.pone.0299888}}

@article{Tiple20228853,
	abstract = {Music enthusiasts are growing exponentially and based on this, many songs are being introduced to the market and stored in signal music libraries. Due to this development emotion recognition model from music contents has received increasing attention in today's world. Of these technologies, a novel Music Emotion Recognition (MER) system is introduced to meet the ever-increasing demand for easy and efficient access to music information. Even though this system was well-developed it lacks in maintaining accuracy of the system and finds difficulty in predicting multi-label emotion type. To address these shortcomings, in this research article, a novel MER system is developed by inter-linking the pre-processing, feature extraction and classification steps. Initially, pre-processing step is employed to convert larger audio files into smaller audio frames. Afterwards, music related temporal, spectral and energy features are extracted for those pre-processed frames which are subjected to the proposed gradient descent based Spiking Neural Network (SNN) classifier. While learning SNN, it is important to determine the optimal weight values for reducing the training error so that gradient descent optimization approach is adopted. To prove the effectiveness of proposed research, proposed model is compared with conventional classification algorithms. The proposed methodology was experimentally tested using various evaluation metrics and it achieves 94.55% accuracy. Hence the proposed methodology attains a good accuracy measure and outperforms well than other algorithms. {\copyright} 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	author = {Tiple, Bhavana and Patwardhan, Manasi},
	author_keywords = {Convolutional neural network; Gradient descent; Short Term Fourier Transform; Spectral; Spiking neural network; Temporal},
	doi = {10.1007/s11042-022-11975-4},
	journal = {Multimedia Tools and Applications},
	keywords = {Audio acoustics; Convolutional neural networks; Music; Optimization; Speech recognition; Convolutional neural network; Emotion recognition; Gradient-descent; Multi-labels; Music emotions; Neural-networks; Short-term Fourier transform; Spectral; Spiking neural network; Temporal; Gradient methods},
	note = {Cited by: 5},
	number = {6},
	pages = {8853 -- 8870},
	publication_stage = {Final},
	source = {Scopus},
	title = {Multi-label emotion recognition from Indian classical music using gradient descent SNN model},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124341623&doi=10.1007%2fs11042-022-11975-4&partnerID=40&md5=94edd585a680d988f8b434e1fe74cf27},
	volume = {81},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124341623&doi=10.1007%2fs11042-022-11975-4&partnerID=40&md5=94edd585a680d988f8b434e1fe74cf27},
	bdsk-url-2 = {https://doi.org/10.1007/s11042-022-11975-4}}

@article{Wan2024179,
	abstract = {Vocal learning has found its proliferation in recent years due to the advancement in wireless networking. Vocal art encompasses profound cultural values and aesthetic preferences, making wireless platforms, e.g. Wi-Fi and ZigBee, essential for the enhancement of online vocal learning. ZigBee, as a wireless platform, has more recently been used quite frequently for online vocal learning and music. This paper aims to design an efficient wireless-enabled platform by using the lightweight features of ZigBee to support online vocal learning and expressions. The platform enables data transmission, storage, analysis, and playback of vocal learning sounds through the ZigBee network. It is built on a cloud platform, utilizing Docker virtualization technology to deploy a Hadoop distributed cluster, effectively simulating a distributed environment. The platform incorporates a model for recognizing musical sound and noise signals, empowering the online learning control platform to detect various characteristics of vocal music tones. Vocal music learning is enhanced by extracting endpoint fusion spectrum features and employing a tone adjustment and dynamic detection model. The proposed method improves anti-interference capability, online learning control, vocal tone recognition, and emotional expression accuracy. Simulation results demonstrate its effectiveness in vocal music, online learning control, and tone recognition, leading to more precise vocal music pronunciation and intonation registration. {\copyright} 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	author = {Wan, Mingxia},
	author_keywords = {Cloud platform; Emotion; Hadoop; Music score; Online learning platform; Spectrum characteristics; Vocal music singing; Wireless network; ZigBee network},
	doi = {10.1007/s11276-023-03443-0},
	journal = {Wireless Networks},
	keywords = {Digital storage; E-learning; Learning algorithms; Learning systems; Music; Spectrum analysis; Wi-Fi; Cloud platforms; Emotion; Hadoop; Learning platform; Music scores; Online learning; Online learning platform; Spectra characteristic; Vocal music; Vocal music singing; ZigBee networks; Zigbee},
	note = {Cited by: 0},
	number = {1},
	pages = {179 -- 192},
	publication_stage = {Final},
	source = {Scopus},
	title = {Designing an online vocal learning based on ZigBee-enabled wireless platform},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168083530&doi=10.1007%2fs11276-023-03443-0&partnerID=40&md5=0b41c512867fe85f2374bb90d0119e10},
	volume = {30},
	year = {2024},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168083530&doi=10.1007%2fs11276-023-03443-0&partnerID=40&md5=0b41c512867fe85f2374bb90d0119e10},
	bdsk-url-2 = {https://doi.org/10.1007/s11276-023-03443-0}}

@article{Cui2023451,
	abstract = {Piano note recognition is the process of automatically converting music audio files into digital music files, which plays an important role in piano assisted teaching and automatic music recording. Evaluation of timbre can provide reference to the singer in vocal music performance, so that their pronunciation can be corrected. In the process, their emotions will affect the timbre to some degree. Based on aspect of emotion, the application of evaluation systematic standards applied in evaluation software algorithm is studied. Classifier of music emotion characteristics has been established. SVM algorithm has been used to process and analyze the music signal. After feature selection and test of trail, the related parameters have been selected and then the algorithm is tested. The test shows that the method built can well identify and evaluate the timbre of vocal music and has practical significance. {\copyright} 2023, Associacao Iberica de Sistemas e Tecnologias de Informacao. All rights reserved.},
	author = {Cui, Yu},
	author_keywords = {Emotional Computing; Evaluation System; Timbre; Vocal Singing},
	journal = {RISTI - Revista Iberica de Sistemas e Tecnologias de Informacao},
	note = {Cited by: 2},
	number = {Special Issue E55},
	pages = {451 -- 464},
	publication_stage = {Final},
	source = {Scopus},
	title = {Vocal Music Performance Evaluation System Based on Neural Network and Its Application in Piano Teaching},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162808819&partnerID=40&md5=ad7c5ceb0797157d6e549e2b8f6fab0e},
	volume = {2023},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162808819&partnerID=40&md5=ad7c5ceb0797157d6e549e2b8f6fab0e}}

@article{Deshmukh2024294,
	abstract = {This study shows a unique approach of Mood elevating music player based on Text emotion recognition. Emotions play a very vital role in everyday life. In this internet era, textual data is mainly designed for communication. Natural language processing is designed for textual data such as messages, emails, articles, reviews, posts, etc. Sentiment analysis is used in various fields. For emotion recognition from text, Deep learning with machine learning approach is used. CNN(Convolutional Neural Network) with a multiclass support vector machine algorithm is used. One vs Rest approach is used for multiclass SVM classifier. Lexicon database and BBC database are operated. Proposed system is compared with K-nearest Neighbour (KNN), Random Forest (RF), Na{\"\i}ve Bayes (NB) algorithms. Results show the accuracy of around 86.88% using BBC database with and approximately 91.2% using Lexicon database, which is higher than other classifiers. Other classifiers such as Random Forest (RF) shows the accuracy of 68.44% for lexicon and 62.44% for BBC, Na{\"\i}ve Bayes (NB) shows the accuracy of 62.56% for lexicon and 59.28% for BBC, K-nearest neighbour (KNN) shows the accuracy of 74.12% for lexicon and 69.28% for BBC. As a result, CNN with multiclass SVM gives 91.2% accuracy using lexicon database. {\copyright} 2024, Ismail Saritas. All rights reserved.},
	author = {Deshmukh, Shrikala and Gupta, Preeti},
	author_keywords = {BBC database; Convolutional neural network; Deep learning; Lexicon database; Machine learning; Multiclass Support Vector Machine},
	journal = {International Journal of Intelligent Systems and Applications in Engineering},
	note = {Cited by: 0},
	number = {2s},
	pages = {294 -- 302},
	publication_stage = {Final},
	source = {Scopus},
	title = {Emotionally Intelligent Music Player for Mood Improvement based on Text Emotion Recognition using Deep Learning Approach},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177882002&partnerID=40&md5=a02d1a89e03cfe3cdb492915c0187abe},
	volume = {12},
	year = {2024},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177882002&partnerID=40&md5=a02d1a89e03cfe3cdb492915c0187abe}}

@article{Tang20231055,
	abstract = {Emotional expression is important in Chinese national vocal music art. The emotional expression in national vocal music is based on the art of national vocal music, with distinct characteristics and requirements. The ultimate goal is to spread the expression of various emotions in the national vocal music art. Promoting the spread of national vocal music singing art using modern media is an urgent requirement for the inheritance and development of national vocal music singing art. With the rapid development of science and technology, integrating deep learning and traditional music has become the general trend. It has been gradually applied to melody recognition, intelligent composition, virtual performance, and other aspects of traditional music and has achieved good results, but also hidden behind a series of ideas and technical and ethical issues. In this paper, the application of deep learning has been discussed and prospected. The recognition rate of emotional expression in national vocal music is 92 %. In terms of communication, combined with the deep learning algorithm, this paper analyzes the characteristics and requirements of emotional expression in the art of national vocal music singing and puts forward a new method of promoting the development of the art of national vocal music singing, hoping to attract more attention and enhance the social awareness of the application field, to promote the steady development of Chinese traditional music in the information age. {\copyright} (2023), (Science and Information Organization). All Rights Reserved.},
	author = {Tang, Zhangcheng},
	author_keywords = {Deep learning; dissemination; emotion; innovation; national vocal music},
	doi = {10.14569/IJACSA.2023.01411107},
	journal = {International Journal of Advanced Computer Science and Applications},
	keywords = {Deep learning; Emotion Recognition; Learning algorithms; Learning systems; Application models; Deep learning; Dissemination; Emotion; Emotional expressions; Innovation; Model construction; National vocal music; Propagation paths; Vocal music; Music},
	note = {Cited by: 0; All Open Access, Gold Open Access},
	number = {11},
	pages = {1055 -- 1062},
	publication_stage = {Final},
	source = {Scopus},
	title = {Application Model Construction of Emotional Expression and Propagation Path of Deep Learning in National Vocal Music},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179180244&doi=10.14569%2fIJACSA.2023.01411107&partnerID=40&md5=ff6c976bdcf3b3aeaaf465465cd1f940},
	volume = {14},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179180244&doi=10.14569%2fIJACSA.2023.01411107&partnerID=40&md5=ff6c976bdcf3b3aeaaf465465cd1f940},
	bdsk-url-2 = {https://doi.org/10.14569/IJACSA.2023.01411107}}

@article{Yue2024,
	abstract = {Early childhood music education has garnered recognition for its unique contribution to cognitive, emotional, and social development in children. Nevertheless, the industry grapples with numerous challenges, including a struggle to adapt traditional educational paradigms to new curriculum reforms, and an excessive emphasis on skill training at the expense of nurturing a love for music and aesthetics in children. To navigate these challenges and explore growth strategies for the early childhood music education industry, we initiated a comprehensive approach that involved distributing surveys to practitioners and parents and engaging experts for insightful discussions. Consequently, we proposed an analytical method based on dynamic social networks in conjunction with Intuitionistic Fuzzy Sets (IFS), Analytic Hierarchy Process (AHP), and Strengths, Weaknesses, Opportunities, and Threats (SWOT) analysis, collectively referred to as IFS-AHP-SWOT. This integrated methodology synergizes the capabilities of dynamic social networks, IFS, AHP, and SWOT analysis to offer a nuanced perspective on industry development strategies. The findings underscore that institutions within the early childhood music education industry need to adopt a development strategy that leverages their strengths and opportunities to foster sustainable growth. Ultimately, this research aims to provide critical decision-making support for industry practitioners, policymakers, and researchers, contributing significantly to the ongoing discourse on strategic development in the early childhood music education industry. Copyright: {\copyright} 2024 Yue, Shen. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
	author = {Yue, Yuanyang and Shen, Xiaoyan},
	doi = {10.1371/journal.pone.0295419},
	journal = {PLoS ONE},
	keywords = {Analytic Hierarchy Process; Child; Child, Preschool; Emotions; Humans; Industry; Music; Social Networking; algorithm; analytic hierarchy process; Article; artificial intelligence; child; childhood; cognition; early childhood music education industry; education program; emotionality; health education; health promotion; human; intuitionistic fuzzy sets; mathematical analysis; music; psychology; social network; strengths, weaknesses, opportunities, and threats; training; emotion; industry; preschool child; social network},
	note = {Cited by: 0; All Open Access, Gold Open Access},
	number = {2 February},
	publication_stage = {Final},
	source = {Scopus},
	title = {Development strategy of early childhood music education industry: An IFS-AHP-SWOT analysis based on dynamic social network},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186306496&doi=10.1371%2fjournal.pone.0295419&partnerID=40&md5=d09ea2ce9877bba6baaee01aff337d87},
	volume = {19},
	year = {2024},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186306496&doi=10.1371%2fjournal.pone.0295419&partnerID=40&md5=d09ea2ce9877bba6baaee01aff337d87},
	bdsk-url-2 = {https://doi.org/10.1371/journal.pone.0295419}}

@article{Fu2023123,
	abstract = {Due to the influence of factors such as strong music specialty, complex music theory knowledge and various changes, it is difficult to identify music features in the process of music teaching. Therefore, a music feature recognition system based on computer aided technology is proposed. The physical sensing layer of the system is equipped with sound sensors in different locations to collect the original music signals, and the digital signal processor is used to analyze and process the music signals. The network transmission layer will process the music signal and transmit it to the music signal database in the system application layer. The music feature analysis module in the application layer adopts dynamic time warping algorithm to obtain the maximum similarity between the test template and the reference template, realize the feature recognition of music signal, and identify music form and music emotion corresponding music feature content according to the recognition results. The experimental results show that the computer-aided music teaching system for music theory knowledge learning, works of music appreciation, music composition activity provides many functions, such as for teachers and students to provide a lot of learning resources, through the network technology to help music learners learn effectively and quickly, rich music knowledge, expand horizons, meet different users' personalized requirements. {\copyright} 2023 CAD Solutions, LLC.},
	author = {Fu, Ning and Peng, Xia},
	author_keywords = {computer aided technology; data collection; feature recognition; music teaching; signal processing},
	doi = {10.14733/cadaps.2023.S4.123-132},
	journal = {Computer-Aided Design and Applications},
	keywords = {Computer aided instruction; Digital signal processors; Emotion Recognition; Network layers; Application layers; Computer aided technologies; Computer-aided technologies; Data collection; Features recognition; Music signals; Music teaching; Music theory; Recognition methods; Signal-processing; Signal processing},
	note = {Cited by: 0; All Open Access, Bronze Open Access},
	number = {s4},
	pages = {123 -- 132},
	publication_stage = {Final},
	source = {Scopus},
	title = {Feature Recognition Method based on Computer-Aided Technology and its Application in Music Teaching},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142630564&doi=10.14733%2fcadaps.2023.S4.123-132&partnerID=40&md5=511ce1d8cda564d537b16032135537a1},
	volume = {20},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142630564&doi=10.14733%2fcadaps.2023.S4.123-132&partnerID=40&md5=511ce1d8cda564d537b16032135537a1},
	bdsk-url-2 = {https://doi.org/10.14733/cadaps.2023.S4.123-132}}

@article{Wang2022,
	abstract = {With the rapid development of information technology, digital music is subsequently increasing in large quantities, and how a good integration of vocal input and recognition technology can be transformed into digital music can greatly improve the efficiency of music production while ensuring the quality and effect of music. This paper focuses on the implementation and application of human voice input and recognition technology in digital music creation, enabling users to generate digital music forms by simply humming a melodic fragment of a piece of music into a microphone. The paper begins with an introduction to digital music and speech recognition technology and goes on to describe the respective characteristics of various audio formats, which are selected as data sources for digital music creation based on the advantages of the files in terms of retrieval. Following that, the method of extracting musical information from music is described, and the main melody is successfully extracted from the multitrack file to extract the corresponding musical performance information. The feature extraction of humming input melody is further described in detail. The traditional speech recognition method of using short-time energy and short-time overzero rate features for speech endpoint detection is analyzed. Combining the characteristics of humming music, the method of cutting notes by two-stage cutting mode, i.e., combining energy saliency index, overzero rate, and pitch change, is adopted to cut notes, which leads to a substantial improvement in performance. The algorithm uses the melody extraction algorithm to obtain the melody line, merges the short-time segments of the melody line to reduce the error rate of emotion recognition, uses the melody line to segment the music signal to generate segmented segments, then abstracts the features of the segmented segments through a CNN-based structural model, and inputs the output of the model to the regressor in cascade with the melody contour features of the corresponding segmented segments to finally obtain the emotion V/A value of the segmented segments.  {\copyright} 2022 Xiaoning Wang et al.},
	author = {Wang, Xiaoning and Guo, Wei and Tong, Weiwei},
	doi = {10.1155/2022/8175834},
	journal = {Journal of Sensors},
	keywords = {Audio acoustics; Data mining; Extraction; Feature extraction; Speech recognition; Data-source; Digital music; Features recognition; Human voice; Music creation; Music production; Musical information; Sensing technology; Speech recognition technology; Wireless sensing; Music},
	note = {Cited by: 0; All Open Access, Gold Open Access},
	publication_stage = {Final},
	source = {Scopus},
	title = {Digital Music Feature Recognition Based on Wireless Sensing Technology},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127506902&doi=10.1155%2f2022%2f8175834&partnerID=40&md5=6c97cd610bea7cbf83b749e3cc1f73ed},
	volume = {2022},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127506902&doi=10.1155%2f2022%2f8175834&partnerID=40&md5=6c97cd610bea7cbf83b749e3cc1f73ed},
	bdsk-url-2 = {https://doi.org/10.1155/2022/8175834}}

@article{Murphy2023589,
	abstract = {It is thought that the presence of music influences episodic memory encoding. However, no studies have isolated the influence of music liking--the hedonic value listeners attribute to a musical stimulus--from that of the core affect induced by the presence of that music. In an online survey, participants rated musical excerpts in terms of how much they liked them, as well as in terms of felt valence, felt arousal and familiarity. These ratings were then used to inform the stimuli presented in an online episodic memory task which, across different scenarios, involved dragging cued objects to cued locations and then recalling details of what was moved, where they were moved to and the order of movements made. Our results showed an influence of liking and music-reward sensitivity on memory for what was moved, as well as a detrimental effect of arousing musical stimuli on memory for un-cued scenario details. Taken together, our study showcases the importance of episodic memory paradigms that involve rich spatiotemporal contexts and provides insights into how different aspects of episodic memory may be influenced by the presence of music. {\copyright} 2022 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.},
	author = {Murphy, Ellen and North, E. and Nawaz, S. and Omigie, D.},
	author_keywords = {emotion; episodic memory; liking; Music; what-where-when},
	doi = {10.1080/09658211.2022.2154367},
	journal = {Memory},
	keywords = {Emotions; Humans; Memory, Episodic; Mental Recall; Music; Recognition, Psychology; emotion; episodic memory; human; music; recall; recognition},
	note = {Cited by: 0; All Open Access, Green Open Access, Hybrid Gold Open Access},
	number = {5},
	pages = {589 -- 604},
	publication_stage = {Final},
	source = {Scopus},
	title = {The influence of music liking on episodic memory for rich spatiotemporal contexts},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153256318&doi=10.1080%2f09658211.2022.2154367&partnerID=40&md5=af0176624bef7889c8895abf1f5d4e3a},
	volume = {31},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153256318&doi=10.1080%2f09658211.2022.2154367&partnerID=40&md5=af0176624bef7889c8895abf1f5d4e3a},
	bdsk-url-2 = {https://doi.org/10.1080/09658211.2022.2154367}}

@article{Tian2023,
	abstract = {Music emotion representation learning forms the foundation of user emotion recognition, addressing the challenges posed by the vast volume of digital music data and the scarcity of emotion annotation data. This article introduces a novel music emotion representation model, leveraging the nonnegative matrix factorization algorithm (NMF) to derive emotional embeddings of music by utilizing user-generated listening lists and emotional labels. This approach facilitates emotion recognition by positioning music within the emotional space. Furthermore, a dedicated music emotion recognition algorithm is formulated, alongside the proposal of a user emotion recognition model, which employs similarity-weighted calculations to obtain user emotion representations. Experimental findings demonstrate the method's convergence after a mere 400 iterations, yielding a remarkable 47.62% increase in F1 value across all emotion classes. In practical testing scenarios, the comprehensive accuracy rate of user emotion recognition attains an impressive 52.7%, effectively discerning emotions within seven emotion categories and accurately identifying users' emotional states. {\copyright} Copyright 2023 Tian},
	author = {Tian, Yuan},
	author_keywords = {Depression; Emotional labels; Music; NMF; Social media},
	doi = {10.7717/peerj-cs.1590},
	journal = {PeerJ Computer Science},
	keywords = {Matrix algebra; Non-negative matrix factorization; Social networking (online); Speech recognition; Depression; Emotion recognition; Emotion representation; Emotional label; Label information; Music emotions; Non-negative matrix factorization algorithms; Social media; User emotions; User labels; Emotion Recognition},
	note = {Cited by: 0; All Open Access, Gold Open Access, Green Open Access},
	publication_stage = {Final},
	source = {Scopus},
	title = {Music emotion representation based on non-negative matrix factorization algorithm and user label information},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173055873&doi=10.7717%2fpeerj-cs.1590&partnerID=40&md5=ab056c686a7baf460d0ef7d1868a6763},
	volume = {9},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173055873&doi=10.7717%2fpeerj-cs.1590&partnerID=40&md5=ab056c686a7baf460d0ef7d1868a6763},
	bdsk-url-2 = {https://doi.org/10.7717/peerj-cs.1590}}

@article{Xin2024,
	abstract = {In order to improve the accuracy and reliability of EEG emotion recognition and avoid the problems of poor decomposition effect and long time consumption caused by manual parameter selection, this paper constructs an EEG emotion recognition model based on optimized variational modal decomposition. Aiming at the modal aliasing problem existing in traditional decomposition methods, the KH algorithm is used to search for the optimal penalty factor and the number of decomposition layers of the VMD, and KH-VMD decomposition is performed on the EEG signals in the DEAP dataset. The time-domain, frequency-domain, and nonlinear features of IMFs under different time windows are extracted, respectively, and the Catboost classifier completes the construction of the EEG emotion recognition model and emotion classification. Considering the two conditions of the complexity of the network structure of the KH-VMD model and the average classification accuracy of different brain regions in different music environments, the WEE features of the target EEG can constitute the optimal classification network by taking the WEE features of the target EEG as the input of the KH-VMD classification model. At this time, the average classification accuracy that can be obtained with differentiated brain regions and differentiated music environments is 0.8314 and 0.8204. After 8 weeks of music therapy, the experimental group's low anxiety scores of pleasure and arousal on the Negative Picture SAM scale were 3.11 and 3.2, which were significantly lower than those of the control group's low-anxiety subjects. The experimental group with high anxiety had anxiety scores and sleep quality scores that were 5.23 and 3.01 points lower than before the intervention. Therefore, music therapy can effectively alleviate psychological anxiety and enhance sleep quality.  {\copyright} 2023 Lei Xin, published by Sciendo.},
	author = {Xin, Lei},
	author_keywords = {DEAP dataset; Electroencephalographic emotion awareness; IMFs; KH-VMD; Music therapy},
	doi = {10.2478/amns.2023.2.01517},
	journal = {Applied Mathematics and Nonlinear Sciences},
	keywords = {Biomedical signal processing; Brain; Classification (of information); Emotion Recognition; Frequency domain analysis; Speech recognition; Time domain analysis; Brain regions; Classification accuracy; DEAP dataset; Electroencephalographic emotion awareness; Emotion recognition; Experimental groups; IMF; KH-VMD; Music therapy; Recognition models; Electroencephalography},
	note = {Cited by: 0; All Open Access, Gold Open Access},
	number = {1},
	publication_stage = {Final},
	source = {Scopus},
	title = {Analysis of the effect of music therapy on psychological anxiety relief based on artificial intelligence recognition},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183700726&doi=10.2478%2famns.2023.2.01517&partnerID=40&md5=e2f31fee9f1c7d562ff19408d0bd84f2},
	volume = {9},
	year = {2024},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183700726&doi=10.2478%2famns.2023.2.01517&partnerID=40&md5=e2f31fee9f1c7d562ff19408d0bd84f2},
	bdsk-url-2 = {https://doi.org/10.2478/amns.2023.2.01517}}

@article{Zhou2023,
	abstract = {Music therapy is increasingly being used to promote physical health. Emotion semantic recognition is more objective and provides direct awareness of the real emotional state based on electroencephalogram (EEG) signals. Therefore, we proposed a music therapy method to carry out emotion semantic matching between the EEG signal and music audio signal, which can improve the reliability of emotional judgments, and, furthermore, deeply mine the potential influence correlations between music and emotions. Our proposed EER model (EEG-based Emotion Recognition Model) could identify 20 types of emotions based on 32 EEG channels, and the average recognition accuracy was above 90% and 80%, respectively. Our proposed music-based emotion classification model (MEC model) could classify eight typical emotion types of music based on nine music feature combinations, and the average classification accuracy was above 90%. In addition, the semantic mapping was analyzed according to the influence of different music types on emotional changes from different perspectives based on the two models, and the results showed that the joy type of music video could improve fear, disgust, mania, and trust emotions into surprise or intimacy emotions, while the sad type of music video could reduce intimacy to the fear emotion. {\copyright} 2022 by the authors.},
	author = {Zhou, Tie Hua and Liang, Wenlong and Liu, Hangyu and Wang, Ling and Ryu, Keun Ho and Nam, Kwang Woo},
	author_keywords = {EEG signals; emotion recognition; music therapy; semantic analysis},
	doi = {10.3390/ijerph20010378},
	journal = {International Journal of Environmental Research and Public Health},
	keywords = {Algorithms; Electroencephalography; Emotions; Humans; Music; Reproducibility of Results; health care; music; psychology; recognition; anger; arousal; Article; classification algorithm; convolutional neural network; disgust; electroencephalogram; emotion; emotion recognition; fear; feature extraction; human; intimacy; mania; mental health; mood change; music therapy; positive valence; recognition; sadness; trust; wavelet transform; algorithm; electroencephalography; emotion; music; procedures; psychology; reproducibility},
	note = {Cited by: 0; All Open Access, Gold Open Access, Green Open Access},
	number = {1},
	publication_stage = {Final},
	source = {Scopus},
	title = {EEG Emotion Recognition Applied to the Effect Analysis of Music on Emotion Changes in Psychological Healthcare},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145707741&doi=10.3390%2fijerph20010378&partnerID=40&md5=eede633cad4f1689b0430bdf89fe1f08},
	volume = {20},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145707741&doi=10.3390%2fijerph20010378&partnerID=40&md5=eede633cad4f1689b0430bdf89fe1f08},
	bdsk-url-2 = {https://doi.org/10.3390/ijerph20010378}}

@article{Na2022,
	abstract = {At present, the existing music classification and recognition algorithms have the problem of low accuracy. Therefore, this paper proposes a music recognition and classification algorithm considering the characteristics of audio emotion. Firstly, the emotional features of music are extracted from the feedforward neural network and parameterized with the mean square deviation. Gradient descent learning algorithm is used to train audio emotion features. The neural network models of input layer, output layer, and hidden layer are established to realize the classification and recognition of music emotion. Experimental results show that the algorithm has good effect on music emotion classification. The data stream driven by the algorithm is higher than 55 MBbs, the anti-attack ability is 91%, the data integrity is 83%, the average accuracy is 85%, and it has good effectiveness and feasibility. {\copyright} 2022 Wang Na and Fang Yong.},
	author = {Na, Wang and Yong, Fang},
	doi = {10.1155/2022/3138851},
	journal = {Scientific Programming},
	keywords = {Audio acoustics; Gradient methods; Multilayer neural networks; Classification algorithm; Classification and recognition; Emotion feature; Gradient descent learning algorithm; Mean-square deviation; Music classification; Music recognition; Neural network model; Parameterized; Recognition algorithm; Music},
	note = {Cited by: 5; All Open Access, Gold Open Access},
	publication_stage = {Final},
	source = {Scopus},
	title = {Music Recognition and Classification Algorithm considering Audio Emotion},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124103186&doi=10.1155%2f2022%2f3138851&partnerID=40&md5=b3cf6939f5b991ba48c037de5319363b},
	volume = {2022},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124103186&doi=10.1155%2f2022%2f3138851&partnerID=40&md5=b3cf6939f5b991ba48c037de5319363b},
	bdsk-url-2 = {https://doi.org/10.1155/2022/3138851}}

@article{Yang20231442,
	abstract = {Current music emotion recognition (MER) systems rely on emotion data averaged across listeners and over time to infer the emotion expressed by a musical piece, often neglecting time- and listener-dependent factors. These limitations can restrict the efficacy of MER systems and cause misjudgements. We present two exploratory studies on music emotion perception. First, in a live music concert setting, fifteen audience members annotated perceived emotion in the valence-arousal space over time using a mobile application. Analyses of inter-rater reliability yielded widely varying levels of agreement in the perceived emotions. A follow-up lab-based study to uncover the reasons for such variability was conducted, where twenty-one participants annotated their perceived emotions whilst viewing and listening to a video recording of the original performance and offered open-ended explanations. Thematic analysis revealed salient features and interpretations that help describe the cognitive processes underlying music emotion perception. Some of the results confirm known findings of music perception and MER studies. Novel findings highlight the importance of less frequently discussed musical attributes, such as musical structure, performer expression, and stage setting, as perceived across audio and visual modalities. Musicians are found to attribute emotion change to musical harmony, structure, and performance technique more than non-musicians. We suggest that accounting for such listener-informed music features can benefit MER in helping to address variability in emotion perception by providing reasons for listener similarities and idiosyncrasies.  {\copyright} 2010-2012 IEEE.},
	author = {Yang, Simin and Reed, Courtney N. and Chew, Elaine and Barthet, Mathieu},
	author_keywords = {individual factors; inter-rater reliability; live performance; Music and emotion; music emotion recognition; music information retrieval; music perception},
	doi = {10.1109/TAFFC.2021.3093787},
	journal = {IEEE Transactions on Affective Computing},
	keywords = {Behavioral research; Emotion Recognition; Music; Reliability; Search engines; Speech recognition; Annotation; Computational modelling; Emotion recognition; Individual factors; Interrater reliability; Live performance; Mood; Music and emotions; Music emotion recognition; Music emotions; Music information retrieval; Music information retrieval F; Music perception; Performance; Semantics},
	note = {Cited by: 8; All Open Access, Green Open Access},
	number = {2},
	pages = {1442 -- 1460},
	publication_stage = {Final},
	source = {Scopus},
	title = {Examining Emotion Perception Agreement in Live Music Performance},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112142520&doi=10.1109%2fTAFFC.2021.3093787&partnerID=40&md5=ba2f8f5538ecefe961085f7295a48f9d},
	volume = {14},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112142520&doi=10.1109%2fTAFFC.2021.3093787&partnerID=40&md5=ba2f8f5538ecefe961085f7295a48f9d},
	bdsk-url-2 = {https://doi.org/10.1109/TAFFC.2021.3093787}}

@article{Akter20242413,
	abstract = {Music has a control over human moods and it can make someone calm or excited. It allows us to feel all emotions we experience. Nowadays, people are often attached with their phones and computers listening to music on Spotify, SoundCloud, or any other internet platform. Music information retrieval plays an important role for music recommendation according to lyrics, pitch, pattern of choices, and genre. In this study, we have tried to recognize the music genre for a better music recommendation system. We have collected an amount of 1820 Bangla songs from six different genres including Adhunik, rock, hip hop, Nazrul, Rabindra, and folk music. We have started with some traditional machine learning algorithms having k-nearest neighbor, logistic regression, random forest, support vector machine, and decision tree but ended up with a deep learning algorithm named artificial neural network with an accuracy of 78% for recognizing music genres from six different genres. All mentioned algorithms are experimented with transformed mel-spectrograms and mean chroma frequency values of that raw amplitude data. But we found that music tempo having beats per minute value with two previous features present better accuracy. {\copyright} 2024, Institute of Advanced Engineering and Science. All rights reserved.},
	author = {Akter, Mariam and Sultana, Nishat and Noori, Sheak Rashed Haider and Hasan, Md Zahid},
	author_keywords = {Artificial neural network; Bangla song genre recognition; Chroma frequency; coefficients; Deep learning; Mel frequency cepstral; Tempo},
	doi = {10.11591/ijai.v13.i2.pp2413-2422},
	journal = {IAES International Journal of Artificial Intelligence},
	note = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
	number = {2},
	pages = {2413 -- 2422},
	publication_stage = {Final},
	source = {Scopus},
	title = {Bangla song genre recognition using artificial neural network},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190888686&doi=10.11591%2fijai.v13.i2.pp2413-2422&partnerID=40&md5=dd6d39e1586265d959d099c54b6cb1fa},
	volume = {13},
	year = {2024},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190888686&doi=10.11591%2fijai.v13.i2.pp2413-2422&partnerID=40&md5=dd6d39e1586265d959d099c54b6cb1fa},
	bdsk-url-2 = {https://doi.org/10.11591/ijai.v13.i2.pp2413-2422}}

@article{Baradaran2023,
	abstract = {Automatic emotion recognition from electroencephalogram (EEG) signals can be considered as the main component of brain--computer interface (BCI) systems. In the previous years, many researchers in this direction have presented various algorithms for the automatic classification of emotions from EEG signals, and they have achieved promising results; however, lack of stability, high error, and low accuracy are still considered as the central gaps in this research. For this purpose, obtaining a model with the precondition of stability, high accuracy, and low error is considered essential for the automatic classification of emotions. In this research, a model based on Deep Convolutional Neural Networks (DCNNs) is presented, which can classify three positive, negative, and neutral emotions from EEG signals based on musical stimuli with high reliability. For this purpose, a comprehensive database of EEG signals has been collected while volunteers were listening to positive and negative music in order to stimulate the emotional state. The architecture of the proposed model consists of a combination of six convolutional layers and two fully connected layers. In this research, different feature learning and hand-crafted feature selection/extraction algorithms were investigated and compared with each other in order to classify emotions. The proposed model for the classification of two classes (positive and negative) and three classes (positive, neutral, and negative) of emotions had 98% and 96% accuracy, respectively, which is very promising compared with the results of previous research. In order to evaluate more fully, the proposed model was also investigated in noisy environments; with a wide range of different SNRs, the classification accuracy was still greater than 90%. Due to the high performance of the proposed model, it can be used in brain--computer user environments. {\copyright} 2023 by the authors.},
	author = {Baradaran, Farzad and Farzan, Ali and Danishvar, Sebelan and Sheykhivand, Sobhan},
	author_keywords = {CNN; deep learning; EEG; emotion recognition; music},
	doi = {10.3390/electronics12102232},
	journal = {Electronics (Switzerland)},
	note = {Cited by: 8; All Open Access, Gold Open Access, Green Open Access},
	number = {10},
	publication_stage = {Final},
	source = {Scopus},
	title = {Customized 2D CNN Model for the Automatic Emotion Recognition Based on EEG Signals},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160305800&doi=10.3390%2felectronics12102232&partnerID=40&md5=72cc2b2fb8dcd99fbdf510ef4de6680c},
	volume = {12},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160305800&doi=10.3390%2felectronics12102232&partnerID=40&md5=72cc2b2fb8dcd99fbdf510ef4de6680c},
	bdsk-url-2 = {https://doi.org/10.3390/electronics12102232}}

@article{Bhangale2023,
	abstract = {Emotions are very crucial for humans for expressing perception and daily activities such as communication, learning, and decision-making. Human emotion recognition using machines is a very complex task. Recently deep learning techniques have been widely used to automate this task by providing machines with a huge learning capability. However, Speech emotion recognition (SER) is challenging due to language, regional, gender, age, and cultural variations. Most of the previous SER techniques have used only one type of feature representation to train deep learning algorithms, which limits the performance of SER. This paper presents a novel Parallel Emotion Network (PEmoNet) that includes Deep Convolution Neural Network (DCNN) with three parallel arms to address effective SER. The three parallel arms of the proposed PEmoNet accept the Multitaper Mel Frequency Spectrogram (MTMFS), Gammatonegram spectrogram (GS), and Constant Q-Transform Spectrogram (CQTS) as input to improve the feature distinctiveness of the emotion signal. The performance of the proposed SER scheme is evaluated on EMODB and RAVDESS datasets based on accuracy, recall, precision, and F1-score. The proposed technique shows 97.14% and 97.41% accuracy for the EMODB and RAVDESS datasets. It shows that the proposed PEmoNet with different spectral representation inputs helps improve the emotions' distinctiveness and outperforms the existing state of the arts. {\copyright} 2023 Elsevier Ltd},
	author = {Bhangale, Kishor B. and Kothandaraman, Mohanaprasad},
	author_keywords = {Deep convolution neural network; Deep learning; Human-computer interaction; Speech emotion recognition; Speech spectrogram},
	doi = {10.1016/j.apacoust.2023.109613},
	journal = {Applied Acoustics},
	keywords = {Behavioral research; Convolution; Deep neural networks; Emotion Recognition; Human computer interaction; Learning algorithms; Learning systems; Music; Spectrographs; Speech recognition; Convolution neural network; Daily activity; Decisions makings; Deep convolution neural network; Deep learning; Perception activity; Performance; Spectrograms; Speech emotion recognition; Speech spectrogram; Decision making},
	note = {Cited by: 1},
	publication_stage = {Final},
	source = {Scopus},
	title = {Speech emotion recognition using the novel PEmoNet (Parallel Emotion Network)},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169028038&doi=10.1016%2fj.apacoust.2023.109613&partnerID=40&md5=0b2273c9072fa9b58e507eeda83a022f},
	volume = {212},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169028038&doi=10.1016%2fj.apacoust.2023.109613&partnerID=40&md5=0b2273c9072fa9b58e507eeda83a022f},
	bdsk-url-2 = {https://doi.org/10.1016/j.apacoust.2023.109613}}

@article{Duan2022,
	abstract = {With the continuous development of communication technology, computer technology, and network technology, a large amount of information such as images, videos, and audios has grown exponentially, and people have started to be exposed to massive multimedia contents, which can easily and quickly access the increasingly rich music resources, so new technologies are urgently needed for their effective management, and automatic classification of audio signals has become the focus of engineering and academic attention. Currently, music retrieval can be achieved by selecting song titles and singer names, but as people's living standards continue to improve, the spiritual realm is also enriched. People want to be able to select music with different types of emotional expressions with their emotions. It mainly includes the basic principles of audio classification, the analysis and extraction of music emotion features, and the selection of the best classifier. Two classification algorithms, hybrid Gaussian model and AdaBoost, are used to classify music emotions, and the two classifiers are combined. In this paper, we propose the Discrete Harmonic Transform (DHT), a sparse transform based on harmonic frequencies. This paper derives and proves the formula of Discrete Harmonic Transform and further analyzes the harmonic structure of musical tone signal and the accuracy of harmonic structure. Since the timbre of musical instruments depends on the harmonic structure, and similar instruments have similar harmonic structures, the discrete harmonic transform coefficients can be defined as objective indicators corresponding to the timbre of musical instruments, and thus the concept of timbre expression spectrum is proposed, and a specific construction algorithm is given in this paper. In the application of musical instrument recognition, the 53-dimensional combined features of LPCC, MFCC, and timbre expression spectrum are selected, and a nonlinear support vector machine is used as the classifier. The classification recognition rate is improved by reducing the number of feature dimensions. {\copyright} 2022 Ying Duan.},
	author = {Duan, Ying},
	doi = {10.1155/2022/6893128},
	journal = {Scientific Programming},
	keywords = {Adaptive boosting; Audio acoustics; Classification (of information); Emotion Recognition; Harmonic analysis; Multimedia systems; Musical instruments; Support vector machines; Classification algorithm; Communicationtechnology; Computer technology; Continuous development; Discrete harmonic transforms; Harmonic structures; Large amounts; Music emotions; Network technologies; Spectra's; Music},
	note = {Cited by: 0; All Open Access, Gold Open Access},
	publication_stage = {Final},
	source = {Scopus},
	title = {Construction of Vocal Timbre Evaluation System Based on Classification Algorithm},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132509543&doi=10.1155%2f2022%2f6893128&partnerID=40&md5=86a4d695f04445072861b069f30d2dab},
	volume = {2022},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132509543&doi=10.1155%2f2022%2f6893128&partnerID=40&md5=86a4d695f04445072861b069f30d2dab},
	bdsk-url-2 = {https://doi.org/10.1155/2022/6893128}}

@article{Kothuri2022354,
	abstract = {Emotion recognition method is required for therapy to recognize the emotions of patient and helps in treatment. Many computer science based emotion recognition works focused on facial expression, speech, body gesture and multi-modal based machine learning techniques. Existing methods have limitations of poor convergence and easily trap into local optima. In this research, the Shuffled Frog Leaping Algorithm (SFLA)-Incremental Wrapper-based Subset Selection (IWSS) hybrid method is proposed to improve the emotion recognition. The proposed method involves in analysis the emotion of user through video, audio, and text features and recommends the music to the users. The analysis shows that hybrid modality shows the higher performance in emotion recognition. AlexNet model is applied for the feature extraction in video data and Latent Dirichlet Allocation (LDA) is applied for text feature extraction. Multi-Class Support Vector Machine (MC-SVM) model is used for the classification. The proposed SFLA-IWSS method has 97.05 % accuracy and existing gSpan method has 90 % accuracy. {\copyright} 2022, Engg Journals Publications. All rights reserved.},
	author = {Kothuri, Sri Raman and Rajalakshmi, N.R.},
	author_keywords = {AlexNet; Incremental Wrapper-based Subset Selection; Latent Dirichlet Allocation; Multi-Class Support Vector Machine; Shuffled Frog Leaping Algorithm},
	doi = {10.21817/indjcse/2022/v13i2/221302040},
	journal = {Indian Journal of Computer Science and Engineering},
	note = {Cited by: 2; All Open Access, Gold Open Access},
	number = {2},
	pages = {354 -- 364},
	publication_stage = {Final},
	source = {Scopus},
	title = {A Hybrid Feature Selection Model for Emotion Recognition using Shuffled Frog Leaping Algorithm (SFLA)-Incremental Wrapper-Based Subset Feature Selection (IWSS)},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129313552&doi=10.21817%2findjcse%2f2022%2fv13i2%2f221302040&partnerID=40&md5=a8848c9abe21c7b9c04fc185389aecce},
	volume = {13},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129313552&doi=10.21817%2findjcse%2f2022%2fv13i2%2f221302040&partnerID=40&md5=a8848c9abe21c7b9c04fc185389aecce},
	bdsk-url-2 = {https://doi.org/10.21817/indjcse/2022/v13i2/221302040}}

@article{Feng2024,
	abstract = {With the development of information technology, music teaching methods are getting more affluent and more prosperous. This paper proposes a model for emotion classification and assessment that integrates traditional music culture elements with information technology in music teaching. The research first combines TextCNN and BiLSTM algorithms to establish the emotion classification model of conventional music. Then it combines PYIN and DTW algorithms to establish the evaluation model of traditional music, which completes the auxiliary efficacy of music informationized teaching. In the emotion classification test of the model, the classification accuracy and F1 value of emotions of different music samples are 82.98% and 75.22%, respectively. The model's recognition accuracy of the four voices is 86.76%, and the overall effective scoring percentage is 81.98% under different playing abnormalities. This study has had an impact on traditional music evaluation. The model in this paper can be used to classify and evaluate emotions in conventional music, providing more intelligent and high-quality technical services for integrating traditional music into music teaching. {\copyright} 2023 Qiyue Feng, published by Sciendo.},
	author = {Feng, Qiyue},
	author_keywords = {BiLSTM; DTW; Music evaluation; PYIN; TextCNN; Traditional music},
	doi = {10.2478/amns-2024-0973},
	journal = {Applied Mathematics and Nonlinear Sciences},
	keywords = {Quality control; BiLSTM; Classification models; DTW; Emotion classification; Informatization; Music evaluation; PYIN; Teaching methods; TextCNN; Traditional music; Classification (of information)},
	note = {Cited by: 0},
	number = {1},
	publication_stage = {Final},
	source = {Scopus},
	title = {Exploration of the Integration of Traditional Music Cultural Elements in Music Informatization Teaching},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192244090&doi=10.2478%2famns-2024-0973&partnerID=40&md5=f61a9cbc265402b97c5600f8b5f1d679},
	volume = {9},
	year = {2024},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192244090&doi=10.2478%2famns-2024-0973&partnerID=40&md5=f61a9cbc265402b97c5600f8b5f1d679},
	bdsk-url-2 = {https://doi.org/10.2478/amns-2024-0973}}

@article{Ansani202359,
	abstract = {Several studies have employed music to affect various tasks through mood induction procedures. In this perspective, music's emotional content coherently affects the listeners' mood, which, in turn, affects performance. On the contrary, in film music cognition, schema theories suggest that music adds semantic information that interacts with the viewers' previous knowledge and influences visual information processing. As in this interpretation the viewers' mood is not deeply considered, it is not clear the extent to which music effects are also due to its power of affecting the viewers' mood or rather a mere cognitive priming-like influence. An experiment (N = 169) on how music biases the recognition memory of a scene was built comparing semantic and emotional effects of soundtracks differing in valence (happy vs scary) during a recognition task. The results show that 1) music affected the viewers' mood coherently with its valence, 2) music led to falsely recognise unseen objects as truly present when coherent with the soundtrack valence; and 3) the effect of music on the biased remembering was not mediated by the viewers' mood, thus suggesting a strong interpretation of the schema theory in film music processing. Finally, a methodological reflection is provided on the issue of the manipulation check in experiments that employ musical stimuli to assess their influence on cognition. {\copyright} 2022 Informa UK Limited, trading as Taylor & Francis Group.},
	author = {Ansani, Alessandro and Marini, Marco and Poggi, Isabella and Mallia, Luca},
	author_keywords = {audiovisual; false memories; memory; mood induction; schema theory; soundtrack},
	doi = {10.1080/20445911.2022.2116448},
	journal = {Journal of Cognitive Psychology},
	keywords = {adult; Article; auditory stimulation; cognition; controlled study; emotion regulation; false memory; fear; female; happiness; human; human experiment; male; memory; memory bias; mood; music; semantic memory; valence (emotion); visual information; visual stimulation; young adult},
	note = {Cited by: 0},
	number = {1},
	pages = {59 -- 75},
	publication_stage = {Final},
	source = {Scopus},
	title = {Recognition memory in movie scenes: the soundtrack induces mood-coherent bias, but not through mood induction},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136566305&doi=10.1080%2f20445911.2022.2116448&partnerID=40&md5=f006dac2b1fc9242ec00d2b9b8d6eb7f},
	volume = {35},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136566305&doi=10.1080%2f20445911.2022.2116448&partnerID=40&md5=f006dac2b1fc9242ec00d2b9b8d6eb7f},
	bdsk-url-2 = {https://doi.org/10.1080/20445911.2022.2116448}}

@article{Khabiri202459,
	abstract = {Purpose: Listening to music has a great impact on people's emotions and would change brain activity. In other words, music-induced emotions are trackable in electrical brain activities. Therefore, Electroencephalography can be a suitable tool to detect these induced emotions. The present study attempted to use electroencephalography in to recognize four types of emotions (happy, relaxing, stressful, and sad) induced in response to listening to music excerpts, using three classifiers. Materials and Methods: In this empirical study, electroencephalography signals were collected from 20 participants, as they were listening to pieces of selected music. The collected data were then pre-processed, and 28 linear and nonlinear features for recognizing the aforementioned emotions were extracted. Feature-space components were then reduced through a principal components analysis. Finally, the first ten components of feature-space were used as input for three classifiers based on Neural Network (NN), K-Nearest Neighbors (KNN), and Support Vector Machine (SVM) algorithms to identify the induced emotions. Results: The outputs showed that the suggested method was well capable of emotion recognition. Evaluating the music excerpts, on the self-assessment manikin scale, demonstrated that the labeling of the music tracks was accurate. The highest accuracy found among NN, KNN, and SVM algorithms were %84, %84, and %89 for happy emotions, respectively. Conclusion: The findings of this study provide useful insights into emotion classification and brain behavior related to induced emotion extraction. Happiness was the most recognizable emotion and the support vector machine had the highest performance among the classifiers. In the end, the outcomes of the proposed method demonstrate that this system is better than the previous research in EEG-based emotion recognition. {\copyright} 2024 Tehran University of Medical Sciences.},
	author = {Khabiri, Hamid and Talebi, Mohammad Naseh and Kamran, Mehdi Fakhimi and Akbari, Shadi and Zarrin, Farzaneh and Mohandesi, Fatemeh},
	author_keywords = {Classification; Electroencephalography; Emotion Recognition; Music; Principal Component Analysis},
	doi = {10.18502/fbt.v11i1.14512},
	journal = {Frontiers in Biomedical Technologies},
	keywords = {adult; Article; brain mapping; calculation; confusion matrix; cross validation; depression; electroencephalogram; electroencephalography; electromyogram; electrooculogram; emotion; empiricism; entropy; false negative result; false positive result; hearing impairment; human; independent component analysis; k nearest neighbor; music; nerve cell network; principal component analysis; recognition; self evaluation; sensitivity and specificity; sleep disorder; support vector machine},
	note = {Cited by: 0; All Open Access, Gold Open Access},
	number = {1},
	pages = {59 -- 68},
	publication_stage = {Final},
	source = {Scopus},
	title = {Music-Induced Emotion Recognition Based on Feature Reduction Using PCA From EEG Signals},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181574150&doi=10.18502%2ffbt.v11i1.14512&partnerID=40&md5=159ea31765f7dbce92a7d3dd9fe41694},
	volume = {11},
	year = {2024},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181574150&doi=10.18502%2ffbt.v11i1.14512&partnerID=40&md5=159ea31765f7dbce92a7d3dd9fe41694},
	bdsk-url-2 = {https://doi.org/10.18502/fbt.v11i1.14512}}

@article{Wang2022,
	abstract = {Music emotion recognition is increasingly becoming important in scientific research and practical applications. Due to the differences in musical characteristics between Western and Chinese classical music, it is necessary to investigate the distinctions in music emotional feature sets to improve the accuracy of cross‐cultural emotion recognition models. Therefore, a comparative study on emotion recognition in Chinese and Western classical music was conducted. Using the V‐A model as an emotional perception model, approximately 1000 pieces of Western and Chinese classical excerpts in total were selected, and approximately 20‐dimension feature sets for different emotional dimensions of different datasets were finally extracted. We considered different kinds of al-gorithms at each step of the training process, from pre‐processing to feature selection and regression model selection. The results reveal that the combination of MaxAbsScaler pre‐processing and the wrapper method using the recursive feature elimination algorithm based on extremely randomized trees is the optimal algorithm. The harmonic change detection function is a culturally universal fea-ture, whereas spectral flux is a culturally specific feature for Chinese classical music. It is also found that pitch features are more significant for Western classical music, whereas loudness and rhythm features are more significant for Chinese classical music. {\copyright} 2022 by the authors. Licensee MDPI, Basel, Switzerland.},
	author = {Wang, Xin and Wang, Li and Xie, Lingyun},
	author_keywords = {classical music; extreme random tree; feature selection; music emotion recognition; V‐A model},
	doi = {10.3390/app12125787},
	journal = {Applied Sciences (Switzerland)},
	note = {Cited by: 7; All Open Access, Gold Open Access},
	number = {12},
	publication_stage = {Final},
	source = {Scopus},
	title = {Comparison and Analysis of Acoustic Features of Western and Chinese Classical Music Emotion Recognition Based on V‐A Model},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132193259&doi=10.3390%2fapp12125787&partnerID=40&md5=96f19b725a52bb52a769612e02f0a74c},
	volume = {12},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132193259&doi=10.3390%2fapp12125787&partnerID=40&md5=96f19b725a52bb52a769612e02f0a74c},
	bdsk-url-2 = {https://doi.org/10.3390/app12125787}}

@article{Jin2024,
	abstract = {People's judgment of music emotion is highly subjective; how to quantify the music emotion characteristics is the key to solving the music emotion recognition problem. This paper utilizes the Fourier transform method to preprocess the input music sample signal. A digital filter accomplishes the pre-emphasis operation, and the number of frames in the music signal is determined by splitting and windowing through a convolution operation. By utilizing the Mel frequency cepstrum coefficient and cochlear frequency, emotional features of music can be extracted. Improve the multimodal model based on the RCNN algorithm, propose the TWC music emotion framework, and construct a music emotion recognition model that incorporates the improved multimodal RCNN. The proposed model's impact on music emotion appreciation is evaluated through experiments to identify music emotions and an analysis of college music teaching practices that emphasize emotion appreciation. The results show that 1376 songs belonging to the category of "relaxation"are assigned to the category of "healing", which is only 4 songs short of the target, and the labeling of the songs is not homogeneous, and the emotional recognition of the model is consistent with the cognition. The mean value of the empathy ability of college students in music emotion appreciation is 69.13, which is in the middle-upper level, indicating that the model proposed in this paper has a good effect on the cultivation of students' music emotion appreciation. {\copyright} 2023 Fenglin Jin, published by Sciendo.},
	author = {Jin, Fenglin},
	author_keywords = {Fourier Transform; Mel Frequency Cepstrum Coefficient; Multimodal RCNN; Music Emotion Characterization; Music Emotion Recognition},
	doi = {10.2478/amns-2024-0129},
	journal = {Applied Mathematics and Nonlinear Sciences},
	keywords = {Digital filters; Speech recognition; Students; Emotion recognition; Fourier transform method; Mel frequency cepstrum coefficients; Multi-modal; Multimodal RCNN; Music emotion characterization; Music emotion recognition; Music emotions; Preprocess; Emotion Recognition},
	note = {Cited by: 0; All Open Access, Gold Open Access},
	number = {1},
	publication_stage = {Final},
	source = {Scopus},
	title = {Emotion Appreciation Strategy in College Music Teaching Based on Improved Multimodal RCNN},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185194732&doi=10.2478%2famns-2024-0129&partnerID=40&md5=489a975bb74741ebd5569ca8d4f0abd0},
	volume = {9},
	year = {2024},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185194732&doi=10.2478%2famns-2024-0129&partnerID=40&md5=489a975bb74741ebd5569ca8d4f0abd0},
	bdsk-url-2 = {https://doi.org/10.2478/amns-2024-0129}}

@article{He2024133,
	abstract = {The perceptual and auditory standard of music is deeply integrated with the emerging multimedia to a higher degree, thus forming the music visualization. It is a process presentation method, which provides a brand-new way of interpretation and deduction for music appreciation. In this article, the application of computer aided design (CAD) in music emotion visualization system is studied, and a mapping model between music characteristics and emotion for digital music emotion recognition is constructed by combining with convolutional neural network (CNN). Combined with CAD technology, the structural music features are extracted and calculated, and the main melody and auxiliary melody of music are obtained. Then, based on the separated main melody and auxiliary melody, comprehensive visualization design is carried out to realize the visualization method of highlighting the main melody. In the experimental part, the performance of music emotion recognition algorithm is tested and the user experience is assessed. The results show that the simulation accuracy and user interaction experience of this system have achieved good results, which can improve the interaction between CAD design and viewing of music emotion visualization. Compared with the recurrent neural network (RNN), support vector machine (SVM) and other emotion recognition models, this model has a higher recognition rate of music emotion, which is of great significance to the research of music emotion visualization system. {\copyright} 2024 U-turn Press LLC.},
	author = {He, Ruidi and Geng, Miaoping and Guo, Jia},
	author_keywords = {CAD; Emotion Recognition; Human-Computer Interaction; Music Visualization},
	doi = {10.14733/cadaps.2024.S7.133-147},
	journal = {Computer-Aided Design and Applications},
	keywords = {Emotion Recognition; Human computer interaction; Music; Recurrent neural networks; Speech recognition; Support vector machines; Visualization; Computer-aided design; Convolutional neural network; Design technologies; Digital music; Emotion recognition; Mapping modeling; Music emotions; Music visualization; User experience assessments; Visualization system; Computer aided design},
	note = {Cited by: 0; All Open Access, Bronze Open Access},
	number = {S7},
	pages = {133 -- 147},
	publication_stage = {Final},
	source = {Scopus},
	title = {Human-computer Interaction Based Music Emotion Visualization System and User Experience Assessment},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171627576&doi=10.14733%2fcadaps.2024.S7.133-147&partnerID=40&md5=1b915e8c337698f1c747b31632a37d0d},
	volume = {21},
	year = {2024},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171627576&doi=10.14733%2fcadaps.2024.S7.133-147&partnerID=40&md5=1b915e8c337698f1c747b31632a37d0d},
	bdsk-url-2 = {https://doi.org/10.14733/cadaps.2024.S7.133-147}}

@article{Justel2023,
	abstract = {Research has shown that memory is influenced by emotion. Several studies demonstrated the effectiveness of pharmacological and non-pharmacological interventions to modulate emotional memory pursuing clinical and educational aims. Music has been identified as a potential memory modulator, with results differing widely depending on whether the participant had musical training or not. The current study examined the effect of listening to music on musicians' and non-musicians' positive (study 1) and negative (study 2) emotional memory, in a group of 163 volunteers, aged 18--40. After the information was encoded, the groups of participants were exposed to arousing music (Symphony No. 70, D major by Joseph Haydn) or a control stimulus (white noise) for three minutes. Then memory was evaluated through free recall and recognition (immediate and deferred measures). Memory performance was compared between musicians (people with five or more years of music education) and non-musicians. Positive and negative images were better recalled than neutral ones, positive images were better recognized than neutral ones however neutral images were better recognized than negative ones. In Study 1, listening to white noise enhanced recall compared to listening to music. In Study 2, listening to arousing music enhanced recall compared to listening to white noise, and this effect was more pronounced in musicians than non-musicians. Our findings suggest that music has a great impact on memory, especially in those with experience in the field, which is reflected in cognitive performance. {\copyright} 2023 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.},
	author = {Justel, Nadia and Diaz Abrahan, Ver{\'o}nika and Moltrasio, Julieta and Rubinstein, Wanda},
	author_keywords = {arousing music; emotional memory; modulation; music training},
	doi = {10.1080/23311908.2023.2234692},
	journal = {Cogent Psychology},
	note = {Cited by: 1; All Open Access, Gold Open Access},
	number = {1},
	publication_stage = {Final},
	source = {Scopus},
	title = {Differential effect of music on memory depends on emotional valence: An experimental study about listening to music and music training},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165277047&doi=10.1080%2f23311908.2023.2234692&partnerID=40&md5=65f5761a226af9f58805e434e5e27294},
	volume = {10},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165277047&doi=10.1080%2f23311908.2023.2234692&partnerID=40&md5=65f5761a226af9f58805e434e5e27294},
	bdsk-url-2 = {https://doi.org/10.1080/23311908.2023.2234692}}

@article{Liu2023167,
	abstract = {The timbre of different melodies may have different personalities, so as to express different feelings and artistic styles. The effective extraction of timbre information is the key to successfully identify musical instruments. In order to solve the problems of low recognition accuracy and high time cost in the current timbre recognition system, an intelligent musical instrument timbre classification system is proposed and designed in combination with the computer-aided technology of the Internet of things. Firstly, a 5-Dimensional emotion space is determined by MDS method. According to the 5-Dimensional emotion space, the emotion evaluation experiment is carried out, and the reliability and validity of the experimental data are tested and the noise is eliminated. Then, the effects of performance content, time domain characteristics and instrument type on the relationship between timbre perception characteristics and emotion are studied. It is found that the time domain characteristics and performance content have little impact on the relationship between timbre perception characteristics and emotion, and the instrument type will have a certain impact on the relationship between timbre perception characteristics and emotion. Finally, five emotion prediction models are established by using multiple linear regression algorithm, and the models have good prediction ability for the five emotions. Simulation and experimental results show that the proposed system can quickly extract the characteristics of harmonic structure of musical signal, and the timbre recognition system based on this can well reflect the timbre characteristics of musical instruments, which provides a new idea for the feature extraction of musical signal. {\copyright} 2023 CAD Solutions, LLC.},
	author = {Liu, Dingding and Bu, Su},
	author_keywords = {computer aided design; feature extraction; Internet of things; musical instrument classification; timbre},
	doi = {10.14733/cadaps.2023.S2.167-179},
	journal = {Computer-Aided Design and Applications},
	keywords = {Behavioral research; Classification (of information); Computer aided design; Emotion Recognition; Extraction; Internet of things; Linear regression; Music; Musical instruments; Time domain analysis; Computer aided technologies; Computer-aided design; Computer-aided technologies; Emotion spaces; Features extraction; Musical instrument classifications; Performance; Recognition systems; Timbre; Timbre classification; Feature extraction},
	note = {Cited by: 0; All Open Access, Bronze Open Access},
	number = {S2},
	pages = {167 -- 179},
	publication_stage = {Final},
	source = {Scopus},
	title = {Timbre Classification Method based on Computer-Aided Technology for Internet of Things},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136208115&doi=10.14733%2fcadaps.2023.S2.167-179&partnerID=40&md5=9ba739616426ea79c4270609e5df0b7c},
	volume = {20},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136208115&doi=10.14733%2fcadaps.2023.S2.167-179&partnerID=40&md5=9ba739616426ea79c4270609e5df0b7c},
	bdsk-url-2 = {https://doi.org/10.14733/cadaps.2023.S2.167-179}}

@article{Martins20221044,
	abstract = {Music training has been linked to facilitated processing of emotional sounds. However, most studies have focused on speech, and less is known about musicians' brain responses to other emotional sounds and in relation to instrument-specific experience. The current study combined behavioral and EEG methods to address two novel questions related to the perception of auditory emotional cues: whether and how long-term music training relates to a distinct emotional processing of nonverbal vocalizations and music; and whether distinct training profiles (vocal vs. instrumental) modulate brain responses to emotional sounds from early to late processing stages. Fifty-eight participants completed an EEG implicit emotional processing task, in which musical and vocal sounds differing in valence were presented as nontarget stimuli. After this task, participants explicitly evaluated the same sounds regarding the emotion being expressed, their valence, and arousal. Compared with nonmusicians, musicians displayed enhanced salience detection (P2), attention orienting (P3), and elaborative processing (Late Positive Potential) of musical (vs. vocal) sounds in event-related potential (ERP) data. The explicit evaluation of musical sounds also was distinct in musicians: accuracy in the emotional recognition of musical sounds was similar across valence types in musicians, who also judged musical sounds to be more pleasant and more arousing than nonmusicians. Specific profiles of music training (singers vs. instrumentalists) did not relate to differences in the processing of vocal vs. musical sounds. Together, these findings reveal that music has a privileged status in the auditory system of long-term musically trained listeners, irrespective of their instrument-specific experience. {\copyright} 2022, The Psychonomic Society, Inc.},
	author = {Martins, In{\^e}s and Lima, C{\'e}sar F. and Pinheiro, Ana P.},
	author_keywords = {Emotion; Event-related potential; Music; Music expertise; Voice},
	doi = {10.3758/s13415-022-01007-x},
	journal = {Cognitive, Affective and Behavioral Neuroscience},
	keywords = {Acoustic Stimulation; Auditory Perception; Electroencephalography; Humans; Music; Singing; Voice; auditory stimulation; electroencephalography; hearing; human; music; physiology; singing; voice},
	note = {Cited by: 5; All Open Access, Bronze Open Access},
	number = {5},
	pages = {1044 -- 1062},
	publication_stage = {Final},
	source = {Scopus},
	title = {Enhanced salience of musical sounds in singers and instrumentalists},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129486533&doi=10.3758%2fs13415-022-01007-x&partnerID=40&md5=b6a8107fcf1f433ffdc0a2aa2d572ce2},
	volume = {22},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129486533&doi=10.3758%2fs13415-022-01007-x&partnerID=40&md5=b6a8107fcf1f433ffdc0a2aa2d572ce2},
	bdsk-url-2 = {https://doi.org/10.3758/s13415-022-01007-x}}

@article{Wang2022,
	abstract = {Music is a kind of art which is to express the thoughts and emotion and reflect the reality life by organized sounds; every piece of music expresses emotions through lyrics and melodies. Human emotions are rich and colorful, and there are also differences in music. It is unreasonable for a song to correspond to only one emotional feature. According to the results of existing algorithms, some music has obviously wrong category labels. To solve the above problems, we established a two-layer feature extraction model based on a spectrogram from shallow to deep (from primary feature extraction to deep feature extraction), which can not only extract the most basic musical features but also dig out the deep emotional features. And further classify the features with the improved CRNN neural network, and get the final music emotion category. Through a large number of comparative experiments, it is proven that our model is suitable for a music classification task.  {\copyright} 2022 Chen Wang and Yu Zhao.},
	author = {Wang, Chen and Zhao, Yu},
	doi = {10.1155/2022/7832548},
	journal = {Wireless Communications and Mobile Computing},
	keywords = {Arts computing; Classification (of information); Emotion Recognition; Extraction; Feature extraction; Bi-layer; Emotion recognition; Express emotions; Extraction modeling; Features extraction; Human emotion; Model-based OPC; Music emotions; Spectrograms; Two-layer; Music},
	note = {Cited by: 1; All Open Access, Gold Open Access},
	publication_stage = {Final},
	source = {Scopus},
	title = {Music Emotion Recognition Based on Bilayer Feature Extraction},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134532351&doi=10.1155%2f2022%2f7832548&partnerID=40&md5=52ee0d7f530c4b616baf2965109291f8},
	volume = {2022},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134532351&doi=10.1155%2f2022%2f7832548&partnerID=40&md5=52ee0d7f530c4b616baf2965109291f8},
	bdsk-url-2 = {https://doi.org/10.1155/2022/7832548}}

@article{Qiao2024,
	abstract = {Music is one of the primary ways to evoke human emotions. However, the feeling of music is subjective, making it difficult to determine which emotions music triggers in a given individual. In order to correctly identify emotional problems caused by different types of music, we first created an electroencephalogram (EEG) data set stimulated by four different types of music (fear, happiness, calm, and sadness). Secondly, the differential entropy features of EEG were extracted, and then the emotion recognition model CNN-SA-BiLSTM was established to extract the temporal features of EEG, and the recognition performance of the model was improved by using the global perception ability of the self-attention mechanism. The effectiveness of the model was further verified by the ablation experiment. The classification accuracy of this method in the valence and arousal dimensions is 93.45% and 96.36%, respectively. By applying our method to a publicly available EEG dataset DEAP, we evaluated the generalization and reliability of our method. In addition, we further investigate the effects of different EEG bands and multi-band combinations on music emotion recognition, and the results confirm relevant neuroscience studies. Compared with other representative music emotion recognition works, this method has better classification performance, and provides a promising framework for the future research of emotion recognition system based on brain computer interface. Copyright {\copyright} 2024 Qiao, Mu, Xie, Hu and Liu.},
	author = {Qiao, Yinghao and Mu, Jiajia and Xie, Jialan and Hu, Binghui and Liu, Guangyuan},
	author_keywords = {BiLSTM; CNN; EEG; music emotion recognition; self-attention},
	doi = {10.3389/fnhum.2024.1324897},
	journal = {Frontiers in Human Neuroscience},
	keywords = {accuracy; adult; area under the curve; arousal; Article; attention network; auditory stimulation; college student; convolutional neural network; cross validation; decision tree; deep learning; electroencephalogram; electromyography; electrooculogram; emotion; emotional disorder; entropy; fear; feature extraction; female; happiness; human; independent component analysis; long short term memory network; machine learning; major clinical study; male; music; nerve cell network; perception; random forest; recognition; reliability; sadness; training; valence (emotion)},
	note = {Cited by: 0; All Open Access, Gold Open Access},
	publication_stage = {Final},
	source = {Scopus},
	title = {Music emotion recognition based on temporal convolutional attention network using EEG},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189978440&doi=10.3389%2ffnhum.2024.1324897&partnerID=40&md5=4d95b5604a4867105b708b1af8cda6b8},
	volume = {18},
	year = {2024},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189978440&doi=10.3389%2ffnhum.2024.1324897&partnerID=40&md5=4d95b5604a4867105b708b1af8cda6b8},
	bdsk-url-2 = {https://doi.org/10.3389/fnhum.2024.1324897}}

@article{Gu2024,
	abstract = {In recent years, the integration of brain--computer interface technology and neural networks in the field of music generation has garnered widespread attention. These studies aimed to extract individual-specific emotional and state information from electroencephalogram (EEG) signals to generate unique musical compositions. While existing research has focused primarily on brain regions associated with emotions, this study extends this research to brain regions related to musical composition. To this end, a novel neural network model incorporating attention mechanisms and steady-state activation mapping (SSAM) was proposed. In this model, the self-attention module enhances task-related information in the current state matrix, while the extended attention module captures the importance of state matrices over different time frames. Additionally, a convolutional neural network layer is used to capture spatial information. Finally, the ECA module integrates the frequency information learned by the model in each of the four frequency bands, mapping these by learning their complementary frequency information into the final attention representation. Evaluations conducted on a dataset specifically constructed for this study revealed that the model surpassed representative models in the emotion recognition field, with recognition rate improvements of 1.47% and 3.83% for two different music states. Analysis of the attention matrix indicates that the left frontal lobe and occipital lobe are the most critical brain regions in distinguishing between `recall and creation' states, while FP1, FPZ, O1, OZ, and O2 are the electrodes most related to this state. In our study of the correlations and significances between these areas and other electrodes, we found that individuals with musical training exhibit more extensive functional connectivity across multiple brain regions. This discovery not only deepens our understanding of how musical training can enhance the brain's ability to work in coordination but also provides crucial guidance for the advancement of brain--computer music generation technologies, particularly in the selection of key brain areas and electrode configurations. We hope our research can guide the work of EEG-based music generation to create better and more personalized music. {\copyright} 2024 by the authors.},
	author = {Gu, Xiaohu and Jiang, Leqi and Chen, Hao and Li, Ming and Liu, Chang},
	author_keywords = {EEG state recognition; music creation; steady-state activation map (SSAM)},
	doi = {10.3390/brainsci14030216},
	journal = {Brain Sciences},
	keywords = {adult; Article; artificial neural network; auditory stimulation; BOLD signal; brain depth stimulation; brain region; convolutional neural network; dynamics; electroencephalogram; electroencephalography; emotion; entropy; female; frontal lobe; functional connectivity; functional magnetic resonance imaging; human; human experiment; image analysis; learning algorithm; machine learning; male; measurement accuracy; music; nerve cell network; neuroimaging; occipital lobe; receiver operating characteristic; skin conductance; steady state; time series analysis; transcranial direct current stimulation; visual field; young adult},
	note = {Cited by: 0; All Open Access, Gold Open Access},
	number = {3},
	publication_stage = {Final},
	source = {Scopus},
	title = {Exploring Brain Dynamics via EEG and Steady-State Activation Map Networks in Music Composition},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188728231&doi=10.3390%2fbrainsci14030216&partnerID=40&md5=cff276b6c0efb64c099a17554e32391c},
	volume = {14},
	year = {2024},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188728231&doi=10.3390%2fbrainsci14030216&partnerID=40&md5=cff276b6c0efb64c099a17554e32391c},
	bdsk-url-2 = {https://doi.org/10.3390/brainsci14030216}}

@article{Ramirez-Melendez2022,
	abstract = {The Autistic Spectrum Disorder (ASD) is characterized by a difficulty in expressing and interpreting others' emotions. In particular, people with ASD have difficulties when interpreting emotions encoded in facial expressions. In the past, music interventions have been shown to improve autistic individuals' emotional and social skills. The present study describes a pilot study to explore the usefulness of music as a tool for improving autistic children's emotion recognition in facial expressions. Twenty-five children (mean age = 8.8 y, SD = 1.24) with high-functioning ASD and normal hearing participated in the study consisting of four weekly sessions of 15 min each. Fifteen participants were randomly divided into an experimental group (N = 14) and a control group (N = 11). During each session, participants in the experimental group were exposed to images of facial expressions for four emotions (happy, sad, angry, and fear). Images were shown in three conditions, with the second condition consisting of music of congruent emotion with the shown images. Participants in the control group were shown only images in all three conditions. For six participants in each group, EEG data were acquired during the sessions, and instantaneous emotional responses (arousal and valence values) were extracted from the EEG data. Inter-and intra-session emotion identification improvement was measured in terms of verbal response accuracy, and EEG response differences were analyzed. A comparison of the verbal responses of the experimental group pre-and post-intervention showed a significant (p = 0.001) average improvement in emotion identification accuracy responses of 26% (SD = 3.4). Furthermore, emotional responses of the experimental group at the end of the study showed a higher correlation with the emotional stimuli being presented, compared with their emotional responses at the beginning of the study. No similar verbal responses improvement or EEG-stimuli correlation was found in the control group. These results seem to indicate that music can be used to improve both emotion identification in facial expressions and emotion induction through facial stimuli in children with high-functioning ASD. {\copyright} 2022 by the authors. Licensee MDPI, Basel, Switzerland.},
	author = {Ramirez-Melendez, Rafael and Matamoros, Elisabet and Hernandez, Davinia and Mirabel, Julia and Sanchez, Elisabet and Escude, Nuria},
	author_keywords = {affective facial expressions; autistic spectrum disorder (ASD); brain activity; EEG; emotions; music},
	doi = {10.3390/brainsci12060704},
	journal = {Brain Sciences},
	keywords = {adult; Article; autism; Autism Diagnostic Observation Schedule; child; clinical article; clinical practice; computer model; controlled study; cost benefit analysis; DSM-5; electroencephalogram; electroencephalography; emotion assessment; emotionality; facial expression; facial recognition; female; habituation; human; human experiment; imagery; impedance; information processing; male; mathematical analysis; music therapy; pilot study; task performance; verbal response; visual stimuli; Wechsler intelligence scale for children; Wechsler Non-Verbal Scale of ability},
	note = {Cited by: 3; All Open Access, Gold Open Access, Green Open Access},
	number = {6},
	publication_stage = {Final},
	source = {Scopus},
	title = {Music-Enhanced Emotion Identification of Facial Emotions in Autistic Spectrum Disorder Children: A Pilot EEG Study},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131697160&doi=10.3390%2fbrainsci12060704&partnerID=40&md5=49fcdc803dfcfb189fa0c66fc12ba0f9},
	volume = {12},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131697160&doi=10.3390%2fbrainsci12060704&partnerID=40&md5=49fcdc803dfcfb189fa0c66fc12ba0f9},
	bdsk-url-2 = {https://doi.org/10.3390/brainsci12060704}}

@article{Latif20232193,
	abstract = {Deep reinforcement learning (DRL) is poised to revolutionise the field of artificial intelligence (AI) by endowing autonomous systems with high levels of understanding of the real world. Currently, deep learning (DL) is enabling DRL to effectively solve various intractable problems in various fields including computer vision, natural language processing, healthcare, robotics, to name a few. Most importantly, DRL algorithms are also being employed in audio signal processing to learn directly from speech, music and other sound signals in order to create audio-based autonomous systems that have many promising applications in the real world. In this article, we conduct a comprehensive survey on the progress of DRL in the audio domain by bringing together research studies across different but related areas in speech and music. We begin with an introduction to the general field of DL and reinforcement learning (RL), then progress to the main DRL methods and their applications in the audio domain. We conclude by presenting important challenges faced by audio-based DRL agents and by highlighting open areas for future research and investigation. The findings of this paper will guide researchers interested in DRL for the audio domain. {\copyright} 2022, The Author(s).},
	author = {Latif, Siddique and Cuay{\'a}huitl, Heriberto and Pervez, Farrukh and Shamshad, Fahad and Ali, Hafiz Shehbaz and Cambria, Erik},
	author_keywords = {(Embodied) dialogue; Deep learning; Emotion recognition; Reinforcement learning; Speech recognition},
	doi = {10.1007/s10462-022-10224-2},
	journal = {Artificial Intelligence Review},
	keywords = {Audio acoustics; Audio systems; Deep learning; Emotion Recognition; Learning systems; Music; Reinforcement learning; Surveys; (embodied) dialog; Audio-based; Deep learning; Emotion recognition; Healthcare robotics; Language processing; Natural languages; Real-world; Reinforcement learning algorithms; Reinforcement learnings; Speech recognition},
	note = {Cited by: 12; All Open Access, Hybrid Gold Open Access},
	number = {3},
	pages = {2193 -- 2240},
	publication_stage = {Final},
	source = {Scopus},
	title = {A survey on deep reinforcement learning for audio-based applications},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133247161&doi=10.1007%2fs10462-022-10224-2&partnerID=40&md5=59663fd9ccec2b8bccbbb3623f03249c},
	volume = {56},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133247161&doi=10.1007%2fs10462-022-10224-2&partnerID=40&md5=59663fd9ccec2b8bccbbb3623f03249c},
	bdsk-url-2 = {https://doi.org/10.1007/s10462-022-10224-2}}

@article{Moltrasio2023,
	abstract = {Emotional stimuli are better remembered than neutral ones. Music generates emotional arousal and can modulate memories in young and older adults. Studies show that in patients with Alzheimer's Disease (AD) music improves word encoding and retrieval of autobiographical memories. Few studies used music as a post-learning treatment, showing a decrease in false positives in recognition. The aim of this work is to study the modulation of memory through music in patients with AD. 75 patients with AD were assessed. They observed emotional and neutral pictures, and then received a musical or neutral treatment: arousing music, relaxing music or white noise. Then, they recalled the pictures they remembered followed by a recognition task. We repeated this task a week later (delayed recall). The results indicated a decrease in false positives in delayed recognition in the group exposed to arousing music. In conclusion, music is capable of modulating memories in patients with AD. This modulation differs from what happens in other populations, which could be due to anatomical differences. The results support the use of music as a possible treatment for memory consolidation. {\copyright} 2023 Universidad Catolica del Uruguay. All rights reserved.},
	author = {Moltrasio, Julieta and Rubinstein, Wanda},
	author_keywords = {Alzheimer's disease; emotion; memory; music},
	doi = {10.22235/cp.v17i2.3270},
	journal = {Ciencias Psicologicas},
	note = {Cited by: 0; All Open Access, Gold Open Access},
	number = {2},
	publication_stage = {Final},
	source = {Scopus},
	title = {Emotional memory modulation through music in patients with Alzheimer's Disease; [Modula{\c c}{\~a}o da mem{\'o}ria emocional por meio da m{\'u}sica em pacientes com dem{\^e}ncia do tipo Alzheimer]; [Modulaci{\'o}n de la memoria emocional a trav{\'e}s de la m{\'u}sica en pacientes con demencia tipo alzh{\'e}imer]},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188462596&doi=10.22235%2fcp.v17i2.3270&partnerID=40&md5=971482db54569d0a6c0d5c3993f864c4},
	volume = {17},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188462596&doi=10.22235%2fcp.v17i2.3270&partnerID=40&md5=971482db54569d0a6c0d5c3993f864c4},
	bdsk-url-2 = {https://doi.org/10.22235/cp.v17i2.3270}}

@article{Wang2022373,
	abstract = {With the development of artificial intelligence and digital audio technology, music information retrieval (MIR) has gradually become a research hotspot. Meanwhile, music emotion recognition (MER) is becoming an important research direction, due to its great research value for video soundtracks. Although some researchers combine Mel Frequency Cepstral coefficient (MFCC) and Residual Phase (RP) to extract music emotional features and improve classification accuracy, the training models in traditional deep learning takes longer time. In order to improve the efficiency of feature mining of music emotional features, MFCC and RP are weighted and combined in this work to extract music emotion features so that the mining efficiency of music emotion features can be effectively improved. At the same time, in order to improve the classification accuracy of music emotion and shorten the training time of the model, by integrating the Long Short-Term Memory (LSTM) and the Broad Learning System (BLS), a new wide and deep learning network (LSTM-BLS) is further built to train music emotion recognition and classification by using LSTM as the feature mapping node of BLS. The network structure of this model makes full use of the ability of BLS to quickly process complex data. Its advantages are simple structure and short model training time, thereby improving recognition efficiency, and LSTM has excellent performance in extracting time series features from time series data. The time sequence relationship of music can be extracted so that the emotional characteristics of the music can be preserved to the greatest extent. Finally, the experimental results on the emotion dataset show that the proposed algorithm can achieve higher recognition accuracy than other complex networks and provide new feasible ideas for the music emotion recognition. {\copyright} 2022 East China University of Science and Technology. All rights reserved.},
	author = {Wang, Jingjing and Huang, Ru},
	author_keywords = {broad learning; deep learning; long short-term memory; music emotion recognition; residual phase},
	doi = {10.14135/j.cnki.1006-3080.20210225007},
	journal = {Huadong Ligong Daxue Xuebao/Journal of East China University of Science and Technology},
	note = {Cited by: 2},
	number = {3},
	pages = {373 -- 380},
	publication_stage = {Final},
	source = {Scopus},
	title = {Music Emotion Recognition Based on the Broad and Deep Learning Network; [基于宽深学习网络的音乐情感识别]},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167579262&doi=10.14135%2fj.cnki.1006-3080.20210225007&partnerID=40&md5=dd8ef11135681629c977177fd3d05597},
	volume = {48},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167579262&doi=10.14135%2fj.cnki.1006-3080.20210225007&partnerID=40&md5=dd8ef11135681629c977177fd3d05597},
	bdsk-url-2 = {https://doi.org/10.14135/j.cnki.1006-3080.20210225007}}

@article{Xia2022,
	abstract = {In recent years, the explosive growth of online music resources makes it difficult to retrieve and manage music information. To efficiently retrieve and classify music information has become a hot research topic. Thayer's two-dimensional emotion plane is selected as the basis for establishing the music emotion database. Music is divided into five categories, the concept of continuous emotion perception is introduced, and music emotion is regarded as a point on a two-dimensional emotional plane, together with the two sentiment variables to determine its location. The artificial labeling method is used to determine the position range of the five types of emotions on the emotional plane, and the regression method is used to obtain the relationship between the VA value and the music features so that the music emotion classification problem is transformed into a regression problem. A regression-based music emotion classification system is designed and implemented, which mainly includes a training part and a testing part. In the training part, three algorithms, namely, polynomial regression, support vector regression, and k-plane piecewise regression, are used to obtain the regression model. In the test part, the input music data is regressed and predicted to obtain its VA value and then classified, and the system performance is considered by classification accuracy. Results show that the combined method of support vector regression and k-plane piecewise regression improves the accuracy by 3 to 4 percentage points compared to using one algorithm alone; compared with the traditional classification method based on a support vector machine, the accuracy improves by 6 percentage points. Music emotion is classified by algorithms such as support vector machine classification, K-neighborhood classification, fuzzy neural network classification, fuzzy K-neighborhood classification, Bayesian classification, and Fisher linear discrimination, among which the support vector machine, fuzzy K-neighborhood, and the accuracy rate of music emotion classification realized by Fisher linear discriminant algorithm are more than 80%; a new algorithm "mixed classifier"is proposed, and the music emotion recognition rate based on this algorithm reaches 84.9%. {\copyright} 2022 Yu Xia and Fumei Xu.},
	author = {Xia, Yu and Xu, Fumei},
	doi = {10.1155/2022/9256586},
	journal = {Mathematical Problems in Engineering},
	keywords = {Behavioral research; Classification (of information); Clustering algorithms; Fuzzy neural networks; Information management; Regression analysis; Speech recognition; Support vector machines; Vectors; Classifieds; Emotion recognition; K neighborhoods; Music emotion classifications; Music emotions; Music information; Percentage points; Piecewise regression; Support vector regressions; Two-dimensional; Emotion Recognition},
	note = {Cited by: 6; All Open Access, Gold Open Access},
	publication_stage = {Final},
	source = {Scopus},
	title = {Study on Music Emotion Recognition Based on the Machine Learning Model Clustering Algorithm},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140847927&doi=10.1155%2f2022%2f9256586&partnerID=40&md5=95604b41d31354a5954af89be5f47193},
	volume = {2022},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140847927&doi=10.1155%2f2022%2f9256586&partnerID=40&md5=95604b41d31354a5954af89be5f47193},
	bdsk-url-2 = {https://doi.org/10.1155/2022/9256586}}

@article{Jandaghian202326037,
	abstract = {Listening to music can evoke different emotions in humans. Music emotion recognition (MER) can predict a person's emotions before listening to a song. However, there are three problems with MER studies. First, the brain is the seat of music perception, but the simulation of MER based on the brain's limbic system has not been examined so far. Secondly, although the effect of individual differences is recognized on the perception and induction of music emotion in the literature, less attention has been paid to the personalization of the model. Finally, most previous studies have emphasized the classification of music pieces into emotional groups, while often a piece of music creates several emotions with different values. The purpose of the present study is to introduce an optimized model of brain emotional learning (BEL) which is combined with Thayer's psychological model to predict the quantitative value of all emotions that hat would reach a specific person by listening to a new piece of music. The proposed model consists of 12 emotional parts that work in parallel where each part is responsible for evaluating one Thayer's specific emotion. Four neural areas of the emotional brain are simulated for each part. The input signal is adjusted using Thayer's dimensions and a fuzzy system. The average of the results obtained with the proposed model were: R2 = 0.69 for arousal, R2 = 0.36 for valence, and MSE = 0.051, which was better and faster than the multilayer network models and even the original BEL model for all emotions. {\copyright} 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	author = {Jandaghian, Maryam and Setayeshi, Saeed and Razzazi, Farbod and Sharifi, Arash},
	author_keywords = {Brain emotional learning; Feature extraction; Fuzzy system; Music emotion recognition; Symbolic analysis; Thayer model},
	doi = {10.1007/s11042-023-14345-w},
	journal = {Multimedia Tools and Applications},
	keywords = {Behavioral research; Fuzzy neural networks; Fuzzy systems; Learning systems; Music; Speech recognition; Brain emotional learning; Emotion recognition; Features extraction; Learning models; Limbic system; Music emotion recognition; Music emotions; Music perception; Symbolic analysis; Thay model; Emotion Recognition},
	note = {Cited by: 1},
	number = {17},
	pages = {26037 -- 26061},
	publication_stage = {Final},
	source = {Scopus},
	title = {Music emotion recognition based on a modified brain emotional learning model},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146176732&doi=10.1007%2fs11042-023-14345-w&partnerID=40&md5=0e4b72fb700ac870a77021b746c99c38},
	volume = {82},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146176732&doi=10.1007%2fs11042-023-14345-w&partnerID=40&md5=0e4b72fb700ac870a77021b746c99c38},
	bdsk-url-2 = {https://doi.org/10.1007/s11042-023-14345-w}}

@article{Asadzadeh2023176,
	abstract = {Automated analysis and recognition of human emotion play an important role in the development of a human--computer interface. High temporal resolution of EEG signals enables us to noninvasively study the emotional brain activities. However, one major obstacle in this procedure is extracting the essential information in presence of the low spatial resolution of EEG recordings. The pattern of each emotion is clearly defined by mapping from scalp sensors to brain sources using the standardized low-resolution electromagnetic tomography (sLORETA) method. A graph neural network (GNN) is then used for EEG-based emotion recognition in which sLORETA sources are considered as the nodes of the underlying graph. In the proposed method, the inter-source relations in EEG source signals are encoded in the adjacency matrix of GNN. Finally, the labels of the unseen emotions are recognized using a GNN classifier. The experiments on the recorded EEG dataset by inducing excitement through music (recorded in brain-computer interface research lab, University of Tabriz) indicate that the brain source activity modeling by ESB-G3N significantly improves the accuracy of emotion recognition. Experimental results show classification accuracy of 98.35% for two-class classification of positive and negative emotions. In this paper, we concentrate on extracting active emotional cortical sources using EEG source imaging (ESI) techniques. Auditory stimuli are used to rapidly and efficiently induce emotions in participants (visual stimuli in terms of video/image are either slow or inefficient in inducing emotions). We propose the use of active EEG sources as graph nodes by EEG source-based GNN node (ESB-G3N) algorithm. The results show an absolute improvement of 1--2% over subject-dependent and subject-independent scenarions compared to the existing approaches. {\copyright} 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	author = {Asadzadeh, Shiva and Rezaii, Tohid Yousefi and Beheshti, Soosan and Meshgini, Saeed},
	author_keywords = {EEG source localization; Emotion recognition; Graph neural network; sLORETA; The inverse problem},
	doi = {10.1007/s12559-022-10077-5},
	journal = {Cognitive Computation},
	keywords = {Brain; Brain mapping; Emotion Recognition; Graph theory; Speech recognition; AS graph; Automated analysis; EEG source localization; Electromagnetic tomography; Emotion recognition; Graph neural networks; Lower resolution; Network node; Standardized low-resolution electromagnetic tomography; The inverse problem; Inverse problems},
	note = {Cited by: 6},
	number = {1},
	pages = {176 -- 189},
	publication_stage = {Final},
	source = {Scopus},
	title = {Accurate Emotion Recognition Utilizing Extracted EEG Sources as Graph Neural Network Nodes},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143664829&doi=10.1007%2fs12559-022-10077-5&partnerID=40&md5=296e6f35e6af88aadc7291a590282f9b},
	volume = {15},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143664829&doi=10.1007%2fs12559-022-10077-5&partnerID=40&md5=296e6f35e6af88aadc7291a590282f9b},
	bdsk-url-2 = {https://doi.org/10.1007/s12559-022-10077-5}}

@article{Li2024,
	abstract = {Evolutionary computation is derived from the simulation of natural selection and genetic processes in biological evolution. This approach provides a method for optimizing the structure and parameters of neural networks. When combined with neural networks, forming what's termed as evolutionary computation based neural networks, it offers a systematic approach to optimize neural network models in diverse applications. In this study, we introduce a method that employs differential evolution algorithms to optimize parameters of convolutional neural network (CNN) for music emotion recognition tasks. This method optimizes the initial weights of the CNN, aiming to achieve near-global optimal solutions and expedite network convergence. Comparative experiments indicate that the proposed approach effectively identifies optimal parameters and structures for CNN, suggesting potential advancements in automated music emotion recognition. {\copyright} 2024 Elsevier B.V.},
	author = {Li, Jiajia and Soradi-Zeid, Samaneh and Yousefpour, Amin and Pan, Daohua},
	author_keywords = {CNN; Differential evolution; Emotional analysis; Evolutionary computation; Music data},
	doi = {10.1016/j.asoc.2024.111262},
	journal = {Applied Soft Computing},
	keywords = {Bioinformatics; Convolution; Convolutional neural networks; Emotion Recognition; Speech recognition; Structural optimization; Convolutional neural network; Differential Evolution; Differential evolution algorithms; Emotion recognition; Emotional analysis; Improved differential evolutions; Music data; Music emotions; Natural selection process; Neural-networks; Evolutionary algorithms},
	note = {Cited by: 1},
	publication_stage = {Final},
	source = {Scopus},
	title = {Improved differential evolution algorithm based convolutional neural network for emotional analysis of music data},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183833934&doi=10.1016%2fj.asoc.2024.111262&partnerID=40&md5=0c127bfaac9efc8e0dd2135c3ef8af5d},
	volume = {153},
	year = {2024},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183833934&doi=10.1016%2fj.asoc.2024.111262&partnerID=40&md5=0c127bfaac9efc8e0dd2135c3ef8af5d},
	bdsk-url-2 = {https://doi.org/10.1016/j.asoc.2024.111262}}

@article{Zhang2024,
	abstract = {In the current research, we integrate distinct learning modalities---Curriculum Learning (CL) and Reinforcement Learning (RL)---in an attempt to develop and optimize Music Emotion Recognition (MER) in piano performance. Classical approaches have never been successful when applied in the field of determining the degree of emotion in the music of the piano, owing to the substantial complexity required. Addressing this particular issue is the primary motivation for the present endeavour. In an approach that's comparable to how human beings acquire information, it trains the RL agent CL in phases; such an approach improves the student's learning model in understanding the diverse emotions expressed by musical compositions. A higher rating of performance can be achieved after learning the model to recognize emotions more effectively and precisely. A set of piano melodies with emotional content notes has been included in the EMOPIA repository for use when conducting the process of evaluation. In order to benchmark the proposed approach with different models, parameters including R2, Mean Absolute Error (MAE), and Root Mean Square Error (RMSE) were deployed. Studies indicate that the recommended approach accurately recognizes the emotions expressed by piano-playing music. In challenging tasks like MER, the significance of implementing the CL paradigm with the RL has been emphasized using the outcomes mentioned earlier. {\copyright} 2024 by author(s).},
	author = {Zhang, Yao and Cai, Delin and Zhang, Dongmei},
	author_keywords = {Curriculum Learning; Machine Learning; MBE; Music Emotion Recognition; piano music; Reinforcement Learning; RMSE},
	doi = {10.54517/esp.v9i4.2344},
	journal = {Environment and Social Psychology},
	note = {Cited by: 0; All Open Access, Gold Open Access},
	number = {4},
	publication_stage = {Final},
	source = {Scopus},
	title = {Application and algorithm optimization of music emotion recognition in piano performance evaluation},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184876855&doi=10.54517%2fesp.v9i4.2344&partnerID=40&md5=f2f5f142bc51490d199e335a63b73647},
	volume = {9},
	year = {2024},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184876855&doi=10.54517%2fesp.v9i4.2344&partnerID=40&md5=f2f5f142bc51490d199e335a63b73647},
	bdsk-url-2 = {https://doi.org/10.54517/esp.v9i4.2344}}

@article{Wang202444,
	abstract = {Aesthetics is an innate ability. It is a meaningful study to make computers perceive "beauty", discover "beauty" and generate "beauty". With the deepening of intelligent optimization algorithm research, artificial intelligence technology "aesthetic" has penetrated into photos, paintings, web pages, ICONS, men and other aspects. However, there are very few studies on the evaluation of piano performance aesthetics. The study of piano performance aesthetics has certain research significance. First of all, limited by personal time and energy, people cannot select high-quality piano repertoire quickly. Secondly, limited by personal aesthetic consciousness and aesthetic ability, people cannot improve the aesthetic quality of piano music just like professional piano players. In the face of such problems, the aesthetic quality evaluation and improvement technology with artificial intelligence as the core provides economically feasible solutions for people to obtain high-quality tracks. Meanwhile, this technology promotes the development of simulated human aesthetic and thinking technology in the field of artificial intelligence. Since the key to aesthetics lies in the perception and classification of piano music score, timbre, audio and emotion, the emotion recognition of piano performance is crucial for the research of artificial intelligence "aesthetics". Piano performance emotion recognition is realized by using the computer to analyze performance characteristics and according to the mapping relationship between performance characteristics and emotion. The study of automatic emotion recognition of piano performance is of great significance to improve the human-computer emotional interaction ability of computer. Based on the above analysis, the main work and innovations of this paper are as follows:This paper first with MIDI music file as a research sample, follow the research method of classical music theory, combined with music psychology, cognitive psychology, music aesthetics and other related research results, the characteristics of the piano performance of a comprehensive and detailed description, and established a set of suitable for computer understanding and expression of the piano performance characteristics system. In the process of feature extraction of piano performance features, high-level features such as rhythm, speed and melody are mathematically defined. In this paper, we realize the computer recognition of the piano playing emotion by using the BP neural network. Finally, the research in this paper can realize the emotional classification of piano performance from the perspective of artificial intelligence, which can use the above research content to quickly and automatically select high-quality piano performance tracks, saving a lot of time for manual screening. {\copyright} 2024 U-turn Press LLC.},
	author = {Wang, Lu},
	author_keywords = {Aesthetic Research; BP neural network; Digital Art; Embedded Systems; Emotional Recognition; Piano Playing},
	doi = {10.14733/cadaps.2024.S8.44-55},
	journal = {Computer-Aided Design and Applications},
	keywords = {Behavioral research; Embedded systems; Emotion Recognition; Human computer interaction; Musical instruments; Neural networks; Quality control; Speech recognition; Websites; Aesthetic qualities; BP neural networks; Digital art; Embedded-system; Emotional recognition; Esthetic research; High quality; Performance; Performance characteristics; Piano playing; Music},
	note = {Cited by: 0; All Open Access, Bronze Open Access},
	number = {s8},
	pages = {44 -- 55},
	publication_stage = {Final},
	source = {Scopus},
	title = {Embedded Systems for Analyzing Digital Art Aesthetics in Piano Performances using Emotional Recognition},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171447451&doi=10.14733%2fcadaps.2024.S8.44-55&partnerID=40&md5=a5abac1875bbab01840e5162b2f2ad88},
	volume = {21},
	year = {2024},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171447451&doi=10.14733%2fcadaps.2024.S8.44-55&partnerID=40&md5=a5abac1875bbab01840e5162b2f2ad88},
	bdsk-url-2 = {https://doi.org/10.14733/cadaps.2024.S8.44-55}}

@article{Deng20222094,
	abstract = {Depression is characterized by poor emotion regulation that makes it difficult to escape the effects of emotional pain, but the neuromodulation behind these symptoms is still unclear. This study investigated the neural mechanism of emotional state-related responses during music stimuli in participants with major depressive disorder (MDD) compared to never-depressed (ND) controls. A novel two-level feature selection method, integrating recursive feature elimination based on support vector machine (SVM-RFE) and random forest algorithm (RF), was proposed to screen emotional recognition brain regions (ERBRs). On this basis, the differences of functional connectivity (FC) were systematically analyzed by two-sample t-test. The results demonstrate that ND participants show eight pairs of FCs with a significant difference between positive emotional music stimuli (pEMS) versus negative emotional music stimuli (nEMS) in 15 ERBRs of MDD, but the participants with MDD show one pair of significant difference in FC. The decreased number reflects the fuzzy response to positive and negative emotions in MDD, which appears to arise from obstacle to emotional cognition and regulation. Furthermore, there was no significant difference in FC between MDDs and NDs under pEMS, but a significant difference was detected between the two groups under nEMS (p < 0.01), revealing a `bias' against the negative state in MDD. The current study may help to better comprehend the abnormal evolution from normal to depression and inform the utilization of pEMS in formal treatment for depression. {\copyright} 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	author = {Deng, Jin and Chen, Yuewei and Zeng, Weiming and Luo, Xiaoqi and Li, Ying},
	author_keywords = {Depression; Emotion; fMRI; Functional connectivity},
	doi = {10.1007/s12031-022-02061-3},
	journal = {Journal of Molecular Neuroscience},
	keywords = {Brain; Depressive Disorder, Major; Emotions; Humans; Magnetic Resonance Imaging; Music; adult; amygdala; Article; back propagation neural network; brain region; classification algorithm; clinical article; cognition; controlled study; diagnostic accuracy; emotion; emotion regulation; feature selection; female; functional connectivity; functional magnetic resonance imaging; human; k nearest neighbor; linear regression analysis; major depression; male; music; random forest; recognition; recursive feature elimination; support vector machine; time series analysis; brain; emotion; major depression; nuclear magnetic resonance imaging; physiology; procedures},
	note = {Cited by: 2},
	number = {10},
	pages = {2094 -- 2105},
	publication_stage = {Final},
	source = {Scopus},
	title = {Brain Response of Major Depressive Disorder Patients to Emotionally Positive and Negative Music},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137063103&doi=10.1007%2fs12031-022-02061-3&partnerID=40&md5=ef7091a7f43d3575fdabc166b0ab74d6},
	volume = {72},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137063103&doi=10.1007%2fs12031-022-02061-3&partnerID=40&md5=ef7091a7f43d3575fdabc166b0ab74d6},
	bdsk-url-2 = {https://doi.org/10.1007/s12031-022-02061-3}}

@article{Medina20221237,
	abstract = {The proven ability of music to transmit emotions provokes the increasing interest in the development of new algorithms for music emotion recognition (MER). In this work, we present an automatic system of emotional classification of music by implementing a neural network. This work is based on a previous implementation of a dimensional emotional prediction system in which a multilayer perceptron (MLP) was trained with the freely available MediaEval database. Although these previous results are good in terms of the metrics of the prediction values, they are not good enough to obtain a classification by quadrant based on the valence and arousal values predicted by the neural network, mainly due to the imbalance between classes in the dataset. To achieve better classification values, a pre-processing phase was implemented to stratify and balance the dataset. Three different classifiers have been compared: linear support vector machine (SVM), random forest, and MLP. The best results are obtained with the MLP. An averaged F-measure of 50% is obtained in a four-quadrant classification schema. Two binary classification approaches are also presented: one vs. rest (OvR) approach in four-quadrants and binary classifier in valence and arousal. The OvR approach has an average F-measure of 69%, and the second one obtained F-measure of 73% and 69% in valence and arousal respectively. Finally, a dynamic classification analysis with different time windows was performed using the temporal annotation data of the MediaEval database. The results obtained show that the classification F-measures in four quadrants are practically constant, regardless of the duration of the time window. Also, this work reflects some limitations related to the characteristics of the dataset, including size, class balance, quality of the annotations, and the sound features available. {\copyright} 2020, Springer-Verlag London Ltd., part of Springer Nature.},
	author = {Medina, Yesid Ospitia and Beltr{\'a}n, Jos{\'e} Ram{\'o}n and Baldassarri, Sandra},
	author_keywords = {Emotion classification; Multilayer perceptron; Music emotion recognition (MER); Music features; Prediction},
	doi = {10.1007/s00779-020-01393-4},
	journal = {Personal and Ubiquitous Computing},
	keywords = {Decision trees; Multilayer neural networks; Support vector machines; Binary Classification Approach; Binary classifiers; Different time windows; Dynamic classification; Emotional classification; Linear Support Vector Machines; Multi layer perceptron; Prediction systems; Classification (of information)},
	note = {Cited by: 12; All Open Access, Green Open Access},
	number = {4},
	pages = {1237 -- 1249},
	publication_stage = {Final},
	source = {Scopus},
	title = {Emotional classification of music using neural networks with the MediaEval dataset},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083799741&doi=10.1007%2fs00779-020-01393-4&partnerID=40&md5=39fe583e58fde13d829fe2231eeae9c4},
	volume = {26},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083799741&doi=10.1007%2fs00779-020-01393-4&partnerID=40&md5=39fe583e58fde13d829fe2231eeae9c4},
	bdsk-url-2 = {https://doi.org/10.1007/s00779-020-01393-4}}

@article{Elrefaiy20242181,
	abstract = {EEG signals for real-time emotion identification are crucial for affective computing and human-computer interaction. The current emotion recognition models, which rely on a small number of emotion classes and stimuli like music and images in controlled lab conditions, have poor ecological validity. Furthermore, identifying relevant EEG signal features is crucial for efficient emotion identification. According to the complexity, non-stationarity, and variation nature of EEG signals, which make it challenging to identify relevant features to categorize and identify emotions, a novel approach for feature extraction and classification concerning EEG signals is suggested based on invariant wavelet scattering transform (WST) and support vector machine algorithm (SVM). The WST is a new time-frequency domain equivalent to a deep convolutional network. It produces scattering feature matrix representations that are stable against time-warping deformations, noise-resistant, and time-shift invariant existing in EEG signals. So, small, difficult-to-measure variations in the amplitude and duration of EEG signals can be captured. As a result, it addresses the limitations of the previous feature extraction approaches, which are unstable and sensitive to time-shift variations. In this paper, the zero, first, and second order features from DEAP datasets are obtained by performing the WST with two deep layers. Then, the PCA method is used for dimensionality reduction. Finally, the extracted features are fed as inputs for different classifiers. In the classification step, the SVM classifier is utilized with different classification algorithms such as k-nearest neighbours (KNN), random forest (RF), and AdaBoost classifier. This research employs a principal component analysis (PCA) approach to reduce the high dimensionality of scattering characteristics and increase the computational efficiency of our classifiers. The proposed method is performed across four different emotional classification models based on valence, arousal, dominance, and liking dimensions on the DEAP dataset. It achieves over 98% for two emotional classes and over 97% for three, four, and eight emotional classes. The results unequivocally demonstrate the efficacy of the proposed WST, PCA, and SVM-based emotion recognition approach for EEG signal emotion recognition. {\copyright} The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.},
	author = {Elrefaiy, Ahmed and Tawfik, Nahed and Zayed, Nourhan and Elhenawy, Ibrahim},
	author_keywords = {EEG; Emotion recognition; k-Nearest neighbor; Principal component analysis; Support Vector Machine; Wavelet scattering transform},
	doi = {10.1007/s12652-023-04746-y},
	journal = {Journal of Ambient Intelligence and Humanized Computing},
	keywords = {Adaptive boosting; Biomedical signal processing; Classification (of information); Computational efficiency; Convolution; Emotion Recognition; Extraction; Fast Fourier transforms; Feature extraction; Frequency domain analysis; Human computer interaction; Nearest neighbor search; Principal component analysis; Speech recognition; Wavelet transforms; EEG signals; Emotion identifications; Emotion recognition; Principal-component analysis; Real- time; Scattering transforms; Support vector machines algorithms; Support vectors machine; Time shifts; Wavelet scattering transform; Support vector machines},
	note = {Cited by: 0},
	number = {4},
	pages = {2181 -- 2199},
	publication_stage = {Final},
	source = {Scopus},
	title = {EEG emotion recognition framework based on invariant wavelet scattering convolution network},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182982292&doi=10.1007%2fs12652-023-04746-y&partnerID=40&md5=80c3e764eb119cbd575b042c12b480cc},
	volume = {15},
	year = {2024},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182982292&doi=10.1007%2fs12652-023-04746-y&partnerID=40&md5=80c3e764eb119cbd575b042c12b480cc},
	bdsk-url-2 = {https://doi.org/10.1007/s12652-023-04746-y}}

@article{Li2024,
	abstract = {In this paper, the forward neural network multi-feature fusion algorithm is used to extract the emotional features of music culture on artificial intelligence technology, considering the diversity and intermittency of the emotional features of the study, which needs to be parameterized. In the forward neural network architecture, the activation value obtained by using the nonlinear activation function is used, and the results obtained are passed to the next layer of data to realize layer-by-layer forward computation, which leads to the back-propagation activation function. The music culture emotion classification model is constructed based on the propagation mode of the forward neural network to determine the emotion recognition process. The research object is selected, the research process is determined, and in order to ensure the true validity of the research, it is necessary to test the reliability and validity of the research design scheme and to develop an empirical analysis of the comparison between popular music and traditional music culture. The results show that on the model, especially in the recognition of sacred, sad, passionate emotion type of music classification accuracy reached more than 88.2%. This paper's model can improve the classification accuracy of music emotion to a certain extent. In the ontological knowledge analysis of popular music and traditional music culture, all three editions of textbooks show that general knowledge of music is predominant and has a large proportion, appreciation knowledge and extended knowledge are also considerable, and music knowledge is the least and has a small proportion. This study demonstrates the synergistic development of traditional culture and modern popular music, which is of great significance to the development of music education in colleges and universities. {\copyright} 2023 Lin Li, published by Sciendo.},
	author = {Li, Lin},
	author_keywords = {Activation function; Forward neural network; Multi-feature fusion algorithm; Music culture; Sentiment classification model},
	doi = {10.2478/amns.2023.2.01359},
	journal = {Applied Mathematics and Nonlinear Sciences},
	keywords = {Backpropagation; Chemical activation; Classification (of information); Emotion Recognition; Multilayer neural networks; Music; Reliability analysis; Activation functions; Classification models; Forward neural network; Fusion algorithms; Multi-feature fusion; Multi-feature fusion algorithm; Music culture; Neural-networks; Sentiment classification; Sentiment classification model; Network architecture},
	note = {Cited by: 0; All Open Access, Gold Open Access},
	number = {1},
	publication_stage = {Final},
	source = {Scopus},
	title = {Research on the Comparative Development of Modern Popular Music and Traditional Music Culture in Colleges and Universities in the Age of Artificial Intelligence},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180349364&doi=10.2478%2famns.2023.2.01359&partnerID=40&md5=04f5a74d5769a6a8d15be347ef78bee2},
	volume = {9},
	year = {2024},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180349364&doi=10.2478%2famns.2023.2.01359&partnerID=40&md5=04f5a74d5769a6a8d15be347ef78bee2},
	bdsk-url-2 = {https://doi.org/10.2478/amns.2023.2.01359}}

@article{Yan2023230,
	abstract = {Recommendation algorithms can greatly improve the efficiency of information retrieval for users. This article briefly introduced recommendation algorithms based on association rules and algorithms based on interest and emotion analysis. After crawling music and comment data from the NetEase Cloud platform, a simulation experiment was conducted. Firstly, the performance of the Back-Propagation Neural Network (BPNN) in the interest and emotion-based algorithm for recommending music was tested, and then the impact of the proportion of emotion weight between comments and music on the emotion analysis-based algorithm was tested. Finally, the three recommendation algorithms based on association rules, user ratings, and interest and emotion analysis were compared. The results showed that when the BPNN used the dominant interest and emotion and secondary interest and emotion as judgment criteria, the accuracy of interest and emotion recognition for music and comments was higher. When the proportion of interest and emotion weight between comments and music was 6:4, the interest and emotion analysis-based recommendation algorithm had the highest accuracy. The interest and emotion-based recommendation algorithm had higher recommendation accuracy than the association rule-based and user rating-based algorithms, and could provide users with more personalized and emotional music recommendations. {\copyright} 2023, International Journal of Advanced Computer Science and Applications. All Rights Reserved.},
	author = {Yan, Xiuli},
	author_keywords = {Interest and emotion; music; personalization; recommendation algorithm},
	doi = {10.14569/IJACSA.2023.0140426},
	journal = {International Journal of Advanced Computer Science and Applications},
	keywords = {Association rules; Emotion Recognition; Neural networks; Back-propagation neural networks; Cloud platforms; Emotion analysis; Interest and emotion; Multiple algorithms; Music recommendation; Performance; Personalizations; Recommendation algorithms; User rating; Simulation platform},
	note = {Cited by: 0; All Open Access, Gold Open Access},
	number = {4},
	pages = {230 -- 235},
	publication_stage = {Final},
	source = {Scopus},
	title = {Personalized Music Recommendation Based on Interest and Emotion: A Comparison of Multiple Algorithms},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158085107&doi=10.14569%2fIJACSA.2023.0140426&partnerID=40&md5=16fad6be333ec3e3dd27bbdcd10db725},
	volume = {14},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158085107&doi=10.14569%2fIJACSA.2023.0140426&partnerID=40&md5=16fad6be333ec3e3dd27bbdcd10db725},
	bdsk-url-2 = {https://doi.org/10.14569/IJACSA.2023.0140426}}

@article{Koh2023,
	abstract = {Music is capable of conveying many emotions. The level and type of emotion of the music perceived by a listener, however, is highly subjective. In this study, we present the Music Emotion Recognition with Profile information dataset (MERP). This database was collected through Amazon Mechanical Turk (MTurk) and features dynamical valence and arousal ratings of 54 selected full-length songs. The dataset contains music features, as well as user profile information of the annotators. The songs were selected from the Free Music Archive using an innovative method (a Triple Neural Network with the OpenSmile toolkit) to identify 50 songs with the most distinctive emotions. Specifically, the songs were chosen to fully cover the four quadrants of the valence-arousal space. Four additional songs were selected from the DEAM dataset to act as a benchmark in this study and filter out low quality ratings. A total of 452 participants participated in annotating the dataset, with 277 participants remaining after thoroughly cleaning the dataset. Their demographic information, listening preferences, and musical background were recorded. We offer an extensive analysis of the resulting dataset, together with a baseline emotion prediction model based on a fully connected model and an LSTM model, for our newly proposed MERP dataset. {\copyright} 2022 by the authors.},
	author = {Koh, En Yan and Cheuk, Kin Wai and Heung, Kwan Yee and Agres, Kat R. and Herremans, Dorien},
	author_keywords = {affective computing; emotion prediction; music; music emotion dataset},
	doi = {10.3390/s23010382},
	journal = {Sensors},
	keywords = {Arousal; Auditory Perception; Emotions; Humans; Music; Neural Networks, Computer; Behavioral research; Benchmarking; Emotion Recognition; Long short-term memory; User profile; Affective Computing; Amazon's mechanical turks; Emotion predictions; Emotion recognition; Feature profiles; Innovative method; Mechanical feature; Music emotion dataset; Music emotions; User's profiles; arousal; emotion; hearing; human; music; psychology; Music},
	note = {Cited by: 7; All Open Access, Gold Open Access, Green Open Access},
	number = {1},
	publication_stage = {Final},
	source = {Scopus},
	title = {MERP: A Music Dataset with Emotion Ratings and Raters' Profile Information},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145971534&doi=10.3390%2fs23010382&partnerID=40&md5=3148b0e0c2cf88f6a9f1aef719718e08},
	volume = {23},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145971534&doi=10.3390%2fs23010382&partnerID=40&md5=3148b0e0c2cf88f6a9f1aef719718e08},
	bdsk-url-2 = {https://doi.org/10.3390/s23010382}}

@article{Patel2023919,
	abstract = {Music recommender system is an area of information retrieval system that suggests customized music recommendations to users based on their previous preferences and experiences with music. While existing systems often overlook the emotional state of the driver, we propose a hybrid music recommendation system - ConCollA to provide a personalized experience based on user emotions. By incorporating facial expression recognition, ConCollA accurately identifies the driver's emotions using convolution neural network(CNN) model and suggests music tailored to their emotional state. ConCollA combines collaborative filtering, a novel content-based recommendation system named Mood Adjusted Average Similarity (MAAS), and apriori algorithm to generate personalized music recommendations. The performance of ConCollA is assessed using various evaluation parameters. The results show that proposed emotion-aware model outperforms a collaborative-based recommender system. {\copyright} 2023 SCPE. All Rights Reserved.},
	author = {Patel, Jigna and Padaria, Ali Asgar and Mehta, Aryan and Chokshi, Aaryan and Patel, Jitali and Kapdi, Rupal},
	author_keywords = {apriori algorithm; associative rule mining; deep learning; Emotion; matrix factorization collaborative filtering; mood; music; personalized contentbased recommendation; recommendation system},
	doi = {10.12694/scpe.v24i4.2467},
	journal = {Scalable Computing},
	keywords = {Collaborative filtering; Deep learning; Emotion Recognition; Factorization; Learning algorithms; Music; Search engines; Apriori algorithms; Associative rule minings; Content-based recommendation; Deep learning; Emotion; Matrix factorization collaborative filtering; Matrix factorizations; Mood; Music Recommendation System; Personalized contentbased recommendation; Recommender systems},
	note = {Cited by: 0; All Open Access, Bronze Open Access},
	number = {4},
	pages = {919 -- 939},
	publication_stage = {Final},
	source = {Scopus},
	title = {CONCOLLA - A SMART EMOTION-BASED MUSIC RECOMMENDATION SYSTEM FOR DRIVERS},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178478319&doi=10.12694%2fscpe.v24i4.2467&partnerID=40&md5=83823eb69a57c98c0d16e5ba19bc05a2},
	volume = {24},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178478319&doi=10.12694%2fscpe.v24i4.2467&partnerID=40&md5=83823eb69a57c98c0d16e5ba19bc05a2},
	bdsk-url-2 = {https://doi.org/10.12694/scpe.v24i4.2467}}

@article{lvarez2023168,
	abstract = {The music emotions can help to improve the personalization of services and contents offered by music streaming providers. Many research works based on the use of machine learning techniques have addressed the problem of recognising the music emotions during the last years. Nevertheless, the results obtained are only applied on small-size music repositories and do not consider what the users feel when they listen to the songs. These issues prevent the existing proposals to be integrated into the personalization mechanisms of the online music providers. In this paper, we present the RIADA infrastructure which is composed by a set of systems able to annotate emotionally the catalog of songs offered by Spotify based on the users' perception. RIADA works with the Spotify playlist miner and data services to build emotion recognition models that can solve the open challenges previously mentioned. Machine learning algorithms, music information retrieval techniques, architectures for parallelization of applications and cloud computing have been combined to develop a complex result of engineering able to integrate the music emotions into the Spotify-based applications. {\copyright} 2023, Universidad Internacional de la Rioja. All rights reserved.},
	author = {{\'A}lvarez, P. and de Quir{\'o}s, J. Garc{\'\i}a and Baldassarri, S.},
	author_keywords = {Affective Annotation; Cloud Computing; Emotion Recognition; Machine Learning; Music; Spotify},
	doi = {10.9781/ijimai.2022.04.002},
	journal = {International Journal of Interactive Multimedia and Artificial Intelligence},
	note = {Cited by: 4; All Open Access, Gold Open Access, Green Open Access},
	number = {2},
	pages = {168 -- 181},
	publication_stage = {Final},
	source = {Scopus},
	title = {RIADA: A Machine-Learning Based Infrastructure for Recognising the Emotions of Spotify Songs},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147947463&doi=10.9781%2fijimai.2022.04.002&partnerID=40&md5=8f61f829c998052047f5da67adcf48c9},
	volume = {8},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147947463&doi=10.9781%2fijimai.2022.04.002&partnerID=40&md5=8f61f829c998052047f5da67adcf48c9},
	bdsk-url-2 = {https://doi.org/10.9781/ijimai.2022.04.002}}

@article{Huang2023445,
	abstract = {It is urgent to solve the problem of music emotion classification. The stochastic forest algorithm is easy to operate and performs better than other single-layer classification models. Aiming at the problems of feature extraction and classification in conventional music emotion classification methods, music features are divided into long-term features and short-term features, and a two-layer music emotion classification model integrating a random forest (RF) algorithm is designed. The experimental results showed that the SVM model using the Gaussian radial basis kernel function had the highest classification accuracy of 90.78% in training the SVM model. The overall classification accuracy of the two-layer music emotion classification model was 98.92%, the recall rate was 97.63%, and its indicators in different emotion categories were the highest, with an average F1 value of 0.919. To sum up, the two-layer music emotion classification model based on the RF algorithm proposed in the research has excellent recognition and classification capabilities. Copyright {\copyright} 2023 Inderscience Enterprises Ltd.},
	author = {Huang, Linna},
	author_keywords = {double layer model; emotional classification; music characteristics; random forest; RF; SVM},
	doi = {10.1504/IJNVO.2023.133878},
	journal = {International Journal of Networking and Virtual Organisations},
	keywords = {Classification (of information); Emotion Recognition; Stochastic systems; Support vector machines; Classification models; Double layer models; Emotional classification; Music characteristic; Music emotion classifications; Random forest algorithm; Random forests; SVM; Two-layer; Stochastic models},
	note = {Cited by: 0},
	number = {2-4},
	pages = {445 -- 460},
	publication_stage = {Final},
	source = {Scopus},
	title = {The application and research of double-layer music emotion classification model based on random forest algorithm in digital music},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174308407&doi=10.1504%2fIJNVO.2023.133878&partnerID=40&md5=9a511d481ab7c25caee2871f387596ba},
	volume = {28},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174308407&doi=10.1504%2fIJNVO.2023.133878&partnerID=40&md5=9a511d481ab7c25caee2871f387596ba},
	bdsk-url-2 = {https://doi.org/10.1504/IJNVO.2023.133878}}

@article{Hu202319,
	abstract = {This work intends to help students perceive music, study music, create music, and realize the ``human-computer interaction'' music teaching mode. A distributed design pattern is adopted to design a gesture interactive robot suitable for music education. First, the client is designed. The client gesture acquisition module employs a dual-channel convolutional neural network (DCCNN) for gesture recognition. The convolutional layer of the constructed DCCNN contains convolution kernels with two sizes, which operate on the image. Second, the server is designed, which recognizes the collected gesture instruction data through two-stream convolutional neural network (CNN). This network cuts the gesture instruction data into K segments, and sparsely samples each segment into a short sequence. The optical flow algorithm is employed to extract the optical flow features of each short sequence. Finally, the performance of the robot is tested. The results show that the combination of convolution kernels with sizes of 5×5 and 7×7 has a recognition accuracy of 98%, suggesting that DCCNN can effectively collect gesture command data. After training, DCCNN's gesture recognition accuracy rate reaches 90%, which is higher than mainstream dynamic gesture recognition algorithms under the same conditions. In addition, the recognition accuracy of the gesture interactive robot is above 90%, suggesting that this robot can meet normal requirements and has good reliability and stability. It is also recommended to be utilized in music perception teaching to provide a basis for establishing a multi-sensory music teaching model. {\copyright} 2023 Institute of Information Science. All rights reserved.},
	author = {Hu, Jia-Xin and Song, Yu and Zhang, Yi-Yao},
	author_keywords = {DCCNN; deep learning; gesture recognition; robot; two-stream convolutional neural networks},
	doi = {10.6688/JISE.202301_39(1).0002},
	journal = {Journal of Information Science and Engineering},
	keywords = {Audio acoustics; Convolution; Convolutional neural networks; Deep learning; Emotion Recognition; Gesture recognition; Human computer interaction; Human robot interaction; Machine design; Optical flows; Convolutional neural network; Deep learning; Dual channel; Dual-channel convolutional neural network; Gestures recognition; Interactive robot; Recognition accuracy; Two-stream; Two-stream convolutional neural network; Music},
	note = {Cited by: 0},
	number = {1},
	pages = {19 -- 37},
	publication_stage = {Final},
	source = {Scopus},
	title = {Adoption of Gesture Interactive Robot in Music Perception Education with Deep Learning Approach},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164953976&doi=10.6688%2fJISE.202301_39%281%29.0002&partnerID=40&md5=2756243a7845b937254436b3c448ad11},
	volume = {39},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164953976&doi=10.6688%2fJISE.202301_39%281%29.0002&partnerID=40&md5=2756243a7845b937254436b3c448ad11},
	bdsk-url-2 = {https://doi.org/10.6688/JISE.202301_39(1).0002}}

@conference{Sana20224699,
	abstract = {Our face is amongst the most significant body organs. It is critical in determining a person's emotions and feelings. With the use of certain traits discernible on the face, the emotion of an individual can be approximated to a certain degree of precision. With new technology advances, recognizable features of the face can be retrieved as inputs utilizing a webcam. The gathered data helps in determining the mood and songs are played from a customized playlist. This eliminates that time-consuming procedure of physically selecting music or modifying playlists and allowed for the creation of an appropriate playlist dependent on the person's emotional level or mood. We will look at a variety of algorithms to come up with an automatic playlist generating methodology that uses emotion recognition to suggest songs. The facial expression-driven music player is set up in a way that allows you to listen to music based on your facial expression. In this work FER-2013, dataset and CNN algorithm are used for emotion recognition. {\copyright} 2022},
	author = {Sana, S.K. and Sruthi, G. and Suresh, D. and Rajesh, G. and Subba Reddy, G.V.},
	author_keywords = {CNN algorithm; Emotion Detection; Face Detection; Music Recommendation},
	doi = {10.1016/j.matpr.2022.03.131},
	journal = {Materials Today: Proceedings},
	keywords = {Convolutional neural networks; Face recognition; Speech recognition; CNN algorithm; Convolutional neural network; Degree of precision; Emotion detection; Emotion recognition; Faces detection; Facial emotions; Facial Expressions; Music recommendation; Technology advances; Emotion Recognition},
	note = {Cited by: 9},
	pages = {4699 -- 4706},
	publication_stage = {Final},
	source = {Scopus},
	title = {Facial emotion recognition based music system using convolutional neural networks},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127586340&doi=10.1016%2fj.matpr.2022.03.131&partnerID=40&md5=4417e06d71a5aac27e4eef5166880157},
	volume = {62},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127586340&doi=10.1016%2fj.matpr.2022.03.131&partnerID=40&md5=4417e06d71a5aac27e4eef5166880157},
	bdsk-url-2 = {https://doi.org/10.1016/j.matpr.2022.03.131}}

@article{Zhang20237319,
	abstract = {Music Emotion Recognition (MER) has attracted much interest in the past decades. Many deep learning methods have been applied to this field recently. However, the previous methods for MER mostly utilized simple convolutional layers to extract features from the original audio signals, in which representative emotion-related features cannot be extracted. In this paper, we propose a novel method named Modularized Composite Attention Network (MCAN) for continuous MER. A sample reconstruction technique is proposed to enhance the stability of the network. Specifically, a feature augmentation module is constructed to extract salient features and we design a weighted attention module to control the focus of the whole network. Furthermore, a style embedding module is introduced to enhance the detail processing capability of the network. We conduct experiments on two datasets, that is, the benchmark dataset DEAM and the newly proposed dataset PMEmo. The superior results prove the effectiveness of our proposed MCAN. Especially qualitative analyses are given to for explaining the performance of our model. {\copyright} 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	author = {Zhang, Meixian and Zhu, Yonghua and Zhang, Wenjun and Zhu, Yunwen and Feng, Tianyu},
	author_keywords = {Arousal; Filter bank output; Handcrafted features; Music emotion recognition; Valence},
	doi = {10.1007/s11042-022-13577-6},
	journal = {Multimedia Tools and Applications},
	keywords = {Deep learning; Filter banks; Learning systems; Music; Speech recognition; Arousal; Emotion recognition; Filter bank output; Filters bank; Handcrafted feature; Learning methods; Modularized; Music emotion recognition; Music emotions; Valence; Emotion Recognition},
	note = {Cited by: 0},
	number = {5},
	pages = {7319 -- 7341},
	publication_stage = {Final},
	source = {Scopus},
	title = {Modularized composite attention network for continuous music emotion recognition},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136483611&doi=10.1007%2fs11042-022-13577-6&partnerID=40&md5=052c485c3952429ad7b952a8d21461a8},
	volume = {82},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136483611&doi=10.1007%2fs11042-022-13577-6&partnerID=40&md5=052c485c3952429ad7b952a8d21461a8},
	bdsk-url-2 = {https://doi.org/10.1007/s11042-022-13577-6}}

@article{Singh2023707,
	abstract = {As more and more people access and consume music through streaming platforms and digital services, music recommendation has grown in importance within the music industry. Given the abundance of music at our disposal, music recommendation algorithms are essential for guiding users toward new music and for creating individualized listening experiences. People frequently seek out music that fits their current emotional state or desired emotional state, which means that emotions can have a big impact on music recommendations. Emotions can be taken into account by music recommendation algorithms when deciding which songs or playlists to recommend to listeners. Face expressions are frequently used to gauge a person's mood. By using a webcam or any other external device, recognizable facial traits can now be extracted as inputs thanks to modern technology. Transfer learning is a method that is increasingly in demand for enhancing emotion recognition and music recommendation systems in the modern world. Transfer learning has evolved into a potent method for utilizing prior knowledge to enhance model performance and lessen the requirement for massive volumes of labeled data as a result of the data explosion and the availability of big pre-trained models. Hence, the objective of this study is to understand how transfer learning impacts the accuracy of detecting emotions from facial expressions and how the music recommendations can be personalized based on the detected emotions. This study aims at recommending songs by detecting the facial expressions of users using the FER2013 dataset for emotion recognition which is further extended by adding own images to the categories in the dataset from Google. A basic CNN, finetuned pre-trained ResNet50V2, finetuned pre-trained VGG16, and finetuned pre-trained EfficientNet50 B0 are trained on the dataset for emotion detection and compared. The music recommendation system is developed using the Spotify songs dataset extracted using Spotify web API. It uses k-means clustering for grouping tracks based on emotions and getting song recommendations based on the emotion predictions using finetuned ResNet50-V2 model with the highest training accuracy of 77.16% and validation accuracy of 69.04%. The findings reveal that using a transfer learning approach may effectively identify emotions from facial expressions and can have a potential impact on recommending music. It improves duties related to music recommendations and might be a useful method for assisting users in finding new music that fits the intended emotional state. {\copyright} 2023 Krishna Kumar Singh and Payal Dembla. This open-access article is distributed under a Creative Commons Attribution (CC-BY) 4.0 license.},
	author = {Singh, Krishna Kumar and Dembla, Payal},
	author_keywords = {Emotion Prediction; Music Recommendation System; Transfer Learning},
	doi = {10.3844/jcssp.2023.707.726},
	journal = {Journal of Computer Science},
	note = {Cited by: 1; All Open Access, Hybrid Gold Open Access},
	number = {6},
	pages = {707 -- 726},
	publication_stage = {Final},
	source = {Scopus},
	title = {A Study on Emotion Analysis and Music Recommendation Using Transfer Learning},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162152487&doi=10.3844%2fjcssp.2023.707.726&partnerID=40&md5=7bcda4922bc4a0b04809ee7cbff8a472},
	volume = {19},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162152487&doi=10.3844%2fjcssp.2023.707.726&partnerID=40&md5=7bcda4922bc4a0b04809ee7cbff8a472},
	bdsk-url-2 = {https://doi.org/10.3844/jcssp.2023.707.726}}

@article{Liu2023,
	abstract = {In this paper, we present an approach to improve the effectiveness of automatic classification of music genres by integrating emotion and intelligent algorithms. We propose an automatic recognition and classification algorithm for music spectra, which takes into account emotional cues that can be extracted from music to improve classification accuracy. To achieve this goal, we set different weight coefficients, which are continuously adjusted based on the convergence process of the previous iteration. The size of each weighting coefficient is adaptively controlled to reduce the number of iterations of the reconstruction process, thereby reducing the algorithm's computational complexity and speeding up its convergence. We conducted several experiments to evaluate the effectiveness of our proposed method. The experimental results demonstrate that the automatic classification method of music genres, which integrates emotion and intelligent algorithms, can significantly improve the accuracy of automatic music genre classification. Moreover, our approach reduces the algorithm's computational complexity, resulting in a faster convergence speed. Our proposed approach provides a promising solution for automatic music genre classification that takes into account emotional cues. The integration of emotion and intelligent algorithms can help achieve higher accuracy and reduce computational complexity, making the proposed method applicable in various scenarios. {\copyright} 2023 The Author(s). Published with license by Taylor & Francis Group, LLC.},
	author = {Liu, Jianwen},
	doi = {10.1080/08839514.2023.2211458},
	journal = {Applied Artificial Intelligence},
	keywords = {Emotion Recognition; Iterative methods; Automatic classification; Automatic recognition; Classification accuracy; Classification algorithm; Classification methods; Intelligent Algorithms; Music genre; Music genre classification; MUSIC spectrum; Recognition algorithm; Computational complexity},
	note = {Cited by: 3; All Open Access, Gold Open Access},
	number = {1},
	publication_stage = {Final},
	source = {Scopus},
	title = {An Automatic Classification Method for Multiple Music Genres by Integrating Emotions and Intelligent Algorithms},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159193958&doi=10.1080%2f08839514.2023.2211458&partnerID=40&md5=043accf42b33035a4d0afc6801969664},
	volume = {37},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159193958&doi=10.1080%2f08839514.2023.2211458&partnerID=40&md5=043accf42b33035a4d0afc6801969664},
	bdsk-url-2 = {https://doi.org/10.1080/08839514.2023.2211458}}

@article{Wang2022116,
	abstract = {In a cross-cultural context, exploring musical elements' cultural specificity and universality that affect various types of music is conducive to personalised emotion recognition. In this study, high-level musical elements are introduced to explore their influence on emotional perception. By comparing music emotion recognition (MER) models of varied cultural music, musical elements with cultural universality and cultural specificity are further determined. Participants rated valence, tension arousal, and energy arousal on labelled nine-point analogical--categorical scales for four types of classical music: Chinese ensemble, Chinese solo, Western ensemble, and Western solo. Fifteen musical elements in five categories---timbre, rhythm, articulation, dynamics, and register were annotated through manual evaluation or the automatic algorithm. The relationship between music emotion and musical elements was analysed through partial least squares regression. Results showed that tempo, rhythm complexity, and articulation are culturally universal; musical elements related to timbre, register, and dynamics features are culturally specific. By increasing tempo, rhythm complexity, staccato, perception of valence, tension arousal, and energy arousal can be effectively improved. Based on the Partial least squares regression (PLSR) model's results for the datasets, the combination of manual and automatic annotation for musical elements can improve the MER system's performance. {\copyright} 2021 The Authors.},
	author = {Wang, Xin and Wei, Yujia and Yang, Dasheng},
	author_keywords = {cross‐culture; emotion perception; music emotion recognition; musical elements},
	doi = {10.1049/ccs2.12032},
	journal = {Cognitive Computation and Systems},
	keywords = {Behavioral research; Least squares approximations; Music; Speech recognition; Cross culture; Cultural analysis; Cultural context; Emotion perception; Emotion recognition; Energy; Music emotion recognition; Music emotions; Musical element; Personalized emotion recognition; Emotion Recognition},
	note = {Cited by: 3},
	number = {2},
	pages = {116 -- 129},
	publication_stage = {Final},
	source = {Scopus},
	title = {Cross‐cultural analysis of the correlation between musical elements and emotion},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134888595&doi=10.1049%2fccs2.12032&partnerID=40&md5=49613adc14e7b535b1cbdd5fb08a699d},
	volume = {4},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134888595&doi=10.1049%2fccs2.12032&partnerID=40&md5=49613adc14e7b535b1cbdd5fb08a699d},
	bdsk-url-2 = {https://doi.org/10.1049/ccs2.12032}}

@article{Zaman2023106620,
	abstract = {Deep learning can be used for audio signal classification in a variety of ways. It can be used to detect and classify various types of audio signals such as speech, music, and environmental sounds. Deep learning models are able to learn complex patterns of audio signals and can be trained on large datasets to achieve high accuracy. To employ deep learning for audio signal classification, the audio signal must first be represented in a suitable form. This can be done using signal representation techniques such as using spectrograms, Mel-frequency Cepstral coefficients, linear predictive coding, and wavelet decomposition. Once the audio signal is represented in a suitable form, it can then be fed into a deep learning model. Various deep learning models can be utilized for audio classification. We provide an extensive survey of current deep learning models used for a variety of audio classification tasks. In particular, we focus on works published under five different deep neural network architectures, namely Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Autoencoders, Transformers and Hybrid Models (hybrid deep learning models and hybrid deep learning models with traditional classifiers). CNNs can be used to classify audio signals into different categories such as speech, music, and environmental sounds. They can also be used for speech recognition, speaker identification, and emotion recognition. RNNs are widely used for audio classification and audio segmentation. RNN models can capture temporal patterns of audio signals and be used to classify audio segments into different categories. Another approach is to use autoencoders for learning the features of audio signals and then classifying the signals into different categories. Transformers are also well-suited for audio classification. In particular, temporal and frequency features can be extracted to identify the characteristics of the audio signals. Finally, hybrid models for audio classification either combine various deep learning architectures (i.e. CNN-RNN) or combine deep learning models with traditional machine learning techniques (i.e. CNN-Support Vector Machine). These hybrid models take advantage of the strengths of different architectures while avoiding their weaknesses. Existing literature under different categories of deep learning are summarized and compared in detail. {\copyright} 2013 IEEE.},
	author = {Zaman, Khalid and Sah, Melike and Direkoglu, Cem and Unoki, Masashi},
	author_keywords = {Audio; autoencoders; classification; CNNs; deep learning; emotion; hybrid models; music; noise; recognition; RNNs; speech; transformers},
	doi = {10.1109/ACCESS.2023.3318015},
	journal = {IEEE Access},
	keywords = {Audio acoustics; Classification (of information); Deep neural networks; Emotion Recognition; Feature extraction; Music; Network architecture; Recurrent neural networks; Wavelet decomposition; Audio; Auto encoders; Classification algorithm; Convolutional neural network; Deep learning; Emotion; Emotion recognition; Features extraction; Hidden-Markov models; Hybrid model; Noise; Recognition; Task analysis; Transformer; Speech recognition},
	note = {Cited by: 4},
	pages = {106620 -- 106649},
	publication_stage = {Final},
	source = {Scopus},
	title = {A Survey of Audio Classification Using Deep Learning},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173032328&doi=10.1109%2fACCESS.2023.3318015&partnerID=40&md5=03f7347b49aef324707d5714d974b460},
	volume = {11},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173032328&doi=10.1109%2fACCESS.2023.3318015&partnerID=40&md5=03f7347b49aef324707d5714d974b460},
	bdsk-url-2 = {https://doi.org/10.1109/ACCESS.2023.3318015}}

@article{Feng2022,
	abstract = {Children with Autism Spectrum Disorder (ASD) experience deficits in verbal and nonverbal communication skills including motor control, turn-taking, and emotion recognition. Innovative technology, such as socially assistive robots, has shown to be a viable method for Autism therapy. This paper presents a novel robot-based music-therapy platform for modeling and improving the social responses and behaviors of children with ASD. Our autonomous social interactive system consists of three modules. Module one provides an autonomous initiative positioning system for the robot, NAO, to properly localize and play the instrument (Xylophone) using the robot's arms. Module two allows NAO to play customized songs composed by individuals. Module three provides a real-life music therapy experience to the users. We adopted Short-time Fourier Transform and Levenshtein distance to fulfill the design requirements: 1) ``music detection'' and 2) ``smart scoring and feedback'', which allows NAO to understand music and provide additional practice and oral feedback to the users as applicable. We designed and implemented six Human-Robot-Interaction (HRI) sessions including four intervention sessions. Nine children with ASD and seven Typically Developing participated in a total of fifty HRI experimental sessions. Using our platform, we collected and analyzed data on social behavioral changes and emotion recognition using Electrodermal Activity (EDA) signals. The results of our experiments demonstrate most of the participants were able to complete motor control tasks with 70% accuracy. Six out of the nine ASD participants showed stable turn-taking behavior when playing music. The results of automated emotion classification using Support Vector Machines illustrates that emotional arousal in the ASD group can be detected and well recognized via EDA bio-signals. In summary, the results of our data analyses, including emotion classification using EDA signals, indicate that the proposed robot-music based therapy platform is an attractive and promising assistive tool to facilitate the improvement of fine motor control and turn-taking skills in children with ASD. Copyright {\copyright} 2022 Feng, Mahoor and Dino.},
	author = {Feng, Huanghao and Mahoor, Mohammad H. and Dino, Francesca},
	author_keywords = {autism; emotion classification; motor control; music therapy; social robotics; turn-taking},
	doi = {10.3389/frobt.2022.855819},
	journal = {Frontiers in Robotics and AI},
	note = {Cited by: 9; All Open Access, Gold Open Access},
	publication_stage = {Final},
	source = {Scopus},
	title = {A Music-Therapy Robotic Platform for Children With Autism: A Pilot Study},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131892650&doi=10.3389%2ffrobt.2022.855819&partnerID=40&md5=b26040075b7802c53c890c5039e3b4ee},
	volume = {9},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131892650&doi=10.3389%2ffrobt.2022.855819&partnerID=40&md5=b26040075b7802c53c890c5039e3b4ee},
	bdsk-url-2 = {https://doi.org/10.3389/frobt.2022.855819}}

@article{Dong2022,
	abstract = {Music and dance videos have been popular among researchers in recent years. Music is one of the most important forms of human communication; it carries a wealth of emotional information, and it is studied using computer tools. In the feature engineering process, most present machine learning approaches suffer from information loss or insufficient extracted features despite the relevance of computer interface and multimedia technologies in sound and music matching tasks. Multifeature fusion is widely utilized in education, aerospace, intelligent transportation, biomedicine, and other fields, and it plays a critical part in how humans get information. In this research, we offer an effective simulation method for matching dance technique movements with music based on multifeature fusion. The initial step is to use music beat extraction theory to segment the synchronized dance movements and music data, then locate mutation points in the music, and dynamically update the pheromones based on the merits of the dance motions. The audio feature sequence is obtained by extracting audio features from the dancing video's accompanying music. Then, we combine the two sequences to create an entropy value sequence based on audio variations. By comparing the consistency of several approaches for optimizing dance movement simulation trials, the optimized simulation method described in this research has an average consistency of 87%, indicating a high consistency. As a result, even though the background and the subject are readily confused, the algorithm in this research can keep a consistent recognition rate for more complicated dance background music, and the approach in this study can still guarantee a certain accuracy rate. {\copyright} 2022 Liusha Dong.},
	author = {Dong, Liusha},
	doi = {10.1155/2022/8679748},
	journal = {Computational Intelligence and Neuroscience},
	keywords = {Computer Simulation; Emotions; Humans; Movement; Music; Recognition, Psychology; Multimedia systems; Music; Audio features; Computer tools; Dance movement; Emotional information; Engineering process; Feature engineerings; Human communications; Multi-feature fusion; Music matching; Optimization-simulation; computer simulation; emotion; human; movement (physiology); music; psychology; Audio acoustics},
	note = {Cited by: 0; All Open Access, Gold Open Access, Green Open Access},
	publication_stage = {Final},
	source = {Scopus},
	title = {Optimization Simulation of Dance Technical Movements and Music Matching Based on Multifeature Fusion},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132245035&doi=10.1155%2f2022%2f8679748&partnerID=40&md5=c768ccd8579719eac06c0e4d0362f867},
	volume = {2022},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132245035&doi=10.1155%2f2022%2f8679748&partnerID=40&md5=c768ccd8579719eac06c0e4d0362f867},
	bdsk-url-2 = {https://doi.org/10.1155/2022/8679748}}

@article{Sivathasan2023,
	abstract = {In contrast with findings of reduced facial and vocal emotional recognition (ER) accuracy, children on the autism spectrum (AS) demonstrate comparable ER skills to those of typically-developing (TD) children using music. To understand the specificity of purported ER differences, the goal of this study was to examine ER from music compared with faces and voices among children on the AS and TD children. Twenty-five children on the AS and 23 TD children (6-13 years) completed an ER task, using categorical (happy, sad, fear) and dimensional (valence, arousal) ratings, of emotions presented via music, faces, or voices. Compared to the TD group, the AS group showed a relative ER strength from music, and comparable performance from faces and voices. Although both groups demonstrated greater vocal ER accuracy, the children on the AS performed equally well with music and faces, whereas the TD children performed better with faces than with music. Both groups performed comparably with dimensional ratings, except for greater variability by the children on the AS in valence ratings for happy emotions. These findings highlight a need to re-examine ER of children on the AS, and to consider how facilitating strengths-based approaches can re-shape our thinking about and support for persons on the AS. Copyright: {\copyright} 2023 Sivathasan et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
	author = {Sivathasan, Shalini and Dahary, Hadas and Burack, Jacob A. and Quintin, Eve-Marie},
	doi = {10.1371/journal.pone.0279002},
	journal = {PLoS ONE},
	keywords = {Autism Spectrum Disorder; Autistic Disorder; Child; Emotions; Facial Expression; Happiness; Humans; Music; accuracy; adolescent; arousal; Article; autism; child; childhood; clinical article; controlled study; emotion; emotion recognition; face; facial recognition; fear; female; happiness; human; male; music; recognition; sadness; stimulus response; valence (emotion); voice; voice recognition; emotion; facial expression; psychology},
	note = {Cited by: 2; All Open Access, Gold Open Access, Green Open Access},
	number = {1 January},
	publication_stage = {Final},
	source = {Scopus},
	title = {Basic emotion recognition of children on the autism spectrum is enhanced in music and typical for faces and voices},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146193313&doi=10.1371%2fjournal.pone.0279002&partnerID=40&md5=9d45ebe918bb93e6b123afa247540735},
	volume = {18},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146193313&doi=10.1371%2fjournal.pone.0279002&partnerID=40&md5=9d45ebe918bb93e6b123afa247540735},
	bdsk-url-2 = {https://doi.org/10.1371/journal.pone.0279002}}

@article{Long2024,
	abstract = {In this paper, starting from the fusion of audio-visual multisensory vocal music feature representation, the RAE algorithm is used to represent the music lyrics text in vocal singing teaching. Due to the heterogeneity of the feature space of the audio modality and the lyrics modality, which makes it exceptionally tricky to directly mine the correlation between these two modalities, it is necessary to optimize the research through the fusion of the audio and the textual modality for the representation of the implicit space of the music. According to the SVM's optimal classification, hyperplane is not limited by the data dimension. Combined with the kernel function parameters and emotional LFSM fusion, a visual multisensory vocal singing teaching emotion model based on SVM is constructed. The parameter environment and dataset are selected, and the comparison method and evaluation criteria are determined to analyze emotional research on vocal singing teaching in colleges and universities. The results show that in terms of model performance, the SVM model in this paper is 8.5% higher than model 6, reaching the highest 0.873, with stronger emotion extraction and recognition ability, greatly improving the emotion classification results of the model. The multimedia type of audio singing material is the least helpful for expression and physical performance in vocal singing teaching, with a total value of 67. This study provides a more comprehensive understanding of the emotional changes of students in teaching activities so as to identify problems, improve and optimize them, and give more students guidance and support in performing vocal singing.  {\copyright} 2023 Tao Long, published by Sciendo.},
	author = {Long, Tao},
	author_keywords = {Audiovisual multisensory; Kernel function; LFSM fusion; Optimal classification hyperplane; SVM; Vocal singing},
	doi = {10.2478/amns.2023.2.01468},
	journal = {Applied Mathematics and Nonlinear Sciences},
	keywords = {Audio acoustics; Emotion Recognition; Geometry; Students; Support vector machines; Audio-visual; Audiovisual multisensory; Colleges and universities; Kernel function; LFSM fusion; Multisensory; Optimal classification; Optimal classification hyperplane; SVM; Vocal singing; Music},
	note = {Cited by: 0; All Open Access, Gold Open Access},
	number = {1},
	publication_stage = {Final},
	source = {Scopus},
	title = {Exploring the diversified teaching mode of vocal singing in colleges and universities by integrating audio-visual and multi-sensory senses},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183766557&doi=10.2478%2famns.2023.2.01468&partnerID=40&md5=44012995547565ef39c0830f41ebab1a},
	volume = {9},
	year = {2024},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183766557&doi=10.2478%2famns.2023.2.01468&partnerID=40&md5=44012995547565ef39c0830f41ebab1a},
	bdsk-url-2 = {https://doi.org/10.2478/amns.2023.2.01468}}

@article{Wang2022,
	abstract = {For children with autism, music therapy has aroused great concern with its novelty and better influence. Music therapy, as one of the effective treatment methods, has an important influence on the social interaction, behavior, and emotion of autistic children. This study attempts to explore a form of applying highly specialized impromptu music therapy to the personal treatment of autistic children in schools for the disabled, as well as the design method of specific music activities. Based on music data mining, the machine learning method is introduced to model music emotion features, and various algorithms are compared to find a model with higher recognition rate, and, at the same time, the antinoise ability and generalization ability of the model are further improved. Finally, a music emotion cognitive model with better performance is established. The results show that the model can effectively promote the overall development of autistic children's cognitive movement, social communication, language communication, and cognition. {\copyright} 2022 Mingxun Wang et al.},
	author = {Wang, Mingxun and Luo, Gang and Chen, Hao},
	doi = {10.1155/2022/4576211},
	journal = {Mathematical Problems in Engineering},
	keywords = {Machine learning; Music; Autistic children; Children with autisms; Interaction behavior; Music data; Music emotions; Music therapy; Social behaviour; Social emotions; Social interactions; Treatment methods; Data mining},
	note = {Cited by: 0; All Open Access, Gold Open Access},
	publication_stage = {Final},
	source = {Scopus},
	title = {Practice of Music Therapy for Autistic Children Based on Music Data Mining},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128692602&doi=10.1155%2f2022%2f4576211&partnerID=40&md5=b4377eaefa6bb3e04d9b428c27eb664d},
	volume = {2022},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128692602&doi=10.1155%2f2022%2f4576211&partnerID=40&md5=b4377eaefa6bb3e04d9b428c27eb664d},
	bdsk-url-2 = {https://doi.org/10.1155/2022/4576211}}

@article{Cheng2022,
	abstract = {The art of music, which is a necessary component of daily life and an ideology older than language, reflects the emotions of human reality. Many new elements have been introduced into music as a result of the quick development of technology, gradually altering how people create, perform, and enjoy music. It is incredible to see how actively AI has been used in music applications and music education over the past few years and how significantly it has advanced. AI technology can efficiently pull in the course, stratify complex large-scale music or sections, simplify teaching, improve student understanding of music, solve challenging student problems in class, and simplify the tasks of teachers. The traditional music education model has been modified, and the music education model's audacious innovation has been made possible by reducing the distance between the teacher and the student. A classification algorithm based on spectrogram and NNS is proposed in light of the advantages in image processing. The abstract features on the spectrogram are automatically extracted using the NNS, which completes the end-to-end learning and avoids the tediousness and inaccuracy of manual feature extraction. This study, which uses experimental analysis to support its findings, demonstrates that different music teaching genres can be accurately classified at a rate of over 90%, which has a positive impact on recognition.  {\copyright} 2022 Chaozhi Cheng and Yujun Xiao.},
	author = {Cheng, Chaozhi and Xiao, Yujun},
	doi = {10.1155/2022/6440464},
	journal = {Journal of Environmental and Public Health},
	keywords = {Deep Learning; Humans; Image Processing, Computer-Assisted; Models, Theoretical; Music; Students; article; classification algorithm; deep learning; education; educational model; feature extraction; human; human experiment; image processing; learning; music; teacher; teaching; psychology; student; theoretical model},
	note = {Cited by: 1; All Open Access, Gold Open Access, Green Open Access},
	publication_stage = {Final},
	source = {Scopus},
	title = {Construction of AI Environmental Music Education Application Model Based on Deep Learning},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137055640&doi=10.1155%2f2022%2f6440464&partnerID=40&md5=64bb98297d03ed96e87847f21e549e38},
	volume = {2022},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137055640&doi=10.1155%2f2022%2f6440464&partnerID=40&md5=64bb98297d03ed96e87847f21e549e38},
	bdsk-url-2 = {https://doi.org/10.1155/2022/6440464}}

@article{He2022,
	abstract = {This paper proposes a new algorithm composition network from the perspective of machine learning, based on an in-depth study of related literature. At the same time, this paper examines the characteristics of music and develops a model for recognising musical emotions. Using the model's information entropy of pitch and intensity to extract the main melody track, note features are extracted from bar features. Finally, the cosine of the vector included angle is used to judge the similarity between feature vectors of several adjacent sections, allowing the music to be divided into several independent segments. The emotional model of music is used to analyze each segment's emotion. By quantifying music features, this paper classifies and quantifies music emotion based on the mapping relationship between music features and emotion. Music emotion can be accurately identified by the model. The model's emotion recognition accuracy is up to 93.78 percent, and the algorithm's recall rate is up to 96.3 percent, according to simulation results. The recognition method used in this paper has a higher recognition ability than other methods, and the emotion recognition result is more reliable. This paper can not only meet the composer's auxiliary creative needs, but it can also help intelligent music services. {\copyright} 2022 Jiao He.},
	author = {He, Jiao},
	doi = {10.1155/2022/1092383},
	journal = {Computational Intelligence and Neuroscience},
	keywords = {Algorithms; Emotions; Machine Learning; Music; Recognition, Psychology; Learning algorithms; Machine learning; Music; Speech recognition; Emotion recognition; Emotional models; Features vector; In-depth study; Information entropy; Machine-learning; Model informations; Music emotions; Musical emotion; On-machines; algorithm; emotion; machine learning; music; psychology; Emotion Recognition},
	note = {Cited by: 5; All Open Access, Gold Open Access, Green Open Access},
	publication_stage = {Final},
	source = {Scopus},
	title = {Algorithm Composition and Emotion Recognition Based on Machine Learning},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132146675&doi=10.1155%2f2022%2f1092383&partnerID=40&md5=c8b96f3849ce73b57928a3d94771a96c},
	volume = {2022},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132146675&doi=10.1155%2f2022%2f1092383&partnerID=40&md5=c8b96f3849ce73b57928a3d94771a96c},
	bdsk-url-2 = {https://doi.org/10.1155/2022/1092383}}

@article{Ospitia-Medina20231909,
	abstract = {This paper presents a novel dataset of songs by non-superstar artists in which a set of musical data is collected, identifying for each song its musical structure, and the emotional perception of the artist through a categorical emotional labeling process. The generation of this preliminary dataset is motivated by the existence of biases that have been detected in the analysis of the most used datasets in the field of emotion-based music recommendation. This new dataset contains 234 min of audio and 60 complete and labeled songs. In addition, an emotional analysis is carried out based on the representation of dynamic emotional perception through a time-series approach, in which the similarity values generated by the dynamic time warping (DTW) algorithm are analyzed and then used to implement a clustering process with the K-means algorithm. In the same way, clustering is also implemented with a Uniform Manifold Approximation and Projection (UMAP) technique, which is a manifold learning and dimension reduction algorithm. The algorithm HDBSCAN is applied for determining the optimal number of clusters. The results obtained from the different clustering strategies are compared and, in a preliminary analysis, a significant consistency is found between them. With the findings and experimental results obtained, a discussion is presented highlighting the importance of working with complete songs, preferably with a well-defined musical structure, considering the emotional variation that characterizes a song during the listening experience, in which the intensity of the emotion usually changes between verse, bridge, and chorus. {\copyright} 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.},
	author = {Ospitia-Medina, Yesid and Beltr{\'a}n, Jos{\'e} Ram{\'o}n and Baldassarri, Sandra},
	author_keywords = {MER (Music Emotion Recognition); MRS (Music Recommender Systems); Musical datasets; Non-superstar artists; Popularity bias; Time-series approach},
	doi = {10.1007/s00779-023-01721-4},
	journal = {Personal and Ubiquitous Computing},
	keywords = {Approximation algorithms; Behavioral research; Cluster analysis; Emotion Recognition; K-means clustering; Music; Time series analysis; Emotion recognition; Music emotion recognition; Music emotions; Music recommende system; Music recommender systems; Musical dataset; Non-superstar artist; Popularity bias; Time-series approach; Times series; Time series},
	note = {Cited by: 0; All Open Access, Green Open Access},
	number = {5},
	pages = {1909 -- 1925},
	publication_stage = {Final},
	source = {Scopus},
	title = {ENSA dataset: a dataset of songs by non-superstar artists tested with an emotional analysis based on time-series},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160831741&doi=10.1007%2fs00779-023-01721-4&partnerID=40&md5=74752ffaeafb746f7e2b41d905d15792},
	volume = {27},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160831741&doi=10.1007%2fs00779-023-01721-4&partnerID=40&md5=74752ffaeafb746f7e2b41d905d15792},
	bdsk-url-2 = {https://doi.org/10.1007/s00779-023-01721-4}}

@article{Tian2023719,
	abstract = {Purpose: Music sentiment analysis helps to promote the diversification of music information retrieval methods. Traditional music emotion classification tasks suffer from high manual workload and low classification accuracy caused by difficulty in feature extraction and inaccurate manual determination of hyperparameter. In this paper, the authors propose an optimized convolution neural network-random forest (CNN-RF) model for music sentiment classification which is capable of optimizing the manually selected hyperparameters to improve the accuracy of music sentiment classification and reduce labor costs and human classification errors. Design/methodology/approach: A CNN-RF music sentiment classification model is designed based on quantum particle swarm optimization (QPSO). First, the audio data are transformed into a Mel spectrogram, and feature extraction is conducted by a CNN. Second, the music features extracted are processed by RF algorithm to complete a preliminary emotion classification. Finally, to select the suitable hyperparameters for a CNN, the QPSO algorithm is adopted to extract the best hyperparameters and obtain the final classification results. Findings: The model has gone through experimental validations and achieved a classification accuracy of 97 per cent for different sentiment categories with shortened training time. The proposed method with QPSO achieved 1.2 and 1.6 per cent higher accuracy than that with particle swarm optimization and genetic algorithm, respectively. The proposed model had great potential for music sentiment classification. Originality/value: The dual contribution of this work comprises the proposed model which integrated two deep learning models and the introduction of a QPSO into model optimization. With these two innovations, the efficiency and accuracy of music emotion recognition and classification have been significantly improved. {\copyright} 2023, Emerald Publishing Limited.},
	author = {Tian, Rui and Yin, Ruheng and Gan, Feng},
	author_keywords = {Classification; CNN; Music; QPSO; RF},
	doi = {10.1108/DTA-07-2022-0267},
	journal = {Data Technologies and Applications},
	keywords = {Classification (of information); Deep learning; Emotion Recognition; Extraction; Feature extraction; Music; Particle swarm optimization (PSO); Wages; Classification accuracy; Convolution neural network; Emotion classification; Features extraction; Hyper-parameter; Particle swarm optimization models; Quanta particle swarm optimizations; Random forests; RF; Sentiment classification; Genetic algorithms},
	note = {Cited by: 1},
	number = {5},
	pages = {719 -- 733},
	publication_stage = {Final},
	source = {Scopus},
	title = {Music sentiment classification based on an optimized CNN-RF-QPSO model},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150013677&doi=10.1108%2fDTA-07-2022-0267&partnerID=40&md5=b0a35894329ea82150a07e0b87d4e6c6},
	volume = {57},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150013677&doi=10.1108%2fDTA-07-2022-0267&partnerID=40&md5=b0a35894329ea82150a07e0b87d4e6c6},
	bdsk-url-2 = {https://doi.org/10.1108/DTA-07-2022-0267}}

@article{Nag2022,
	abstract = {Music is often considered as the language of emotions. The way it stimulates the emotional appraisal across people from different communities, culture and demographics has long been known and hence categorizing on the basis of emotions is indeed an intriguing basic research area. Indian Classical Music (ICM) is famous for its ambiguous nature, i.e. its ability to evoke a number of mixed emotions through only a single musical narration, and hence classifying evoked emotions from ICM becomes a more challenging task. With the rapid advancements in the field of Deep Learning, this Music Emotion Recognition (MER) task is becoming more and more relevant and robust, hence can be applied to one of the most challenging test case i.e. classifying emotions elicited from ICM. In this paper we present a new dataset called JUMusEmoDB which presently has 1600 audio clips (approximately 30 s each) where 400 clips each correspond to happy, sad, calm and anxiety emotional scales. The initial annotations and emotional classification of the database was done based on an emotional rating test (5-point Likert scale) performed by 100 participants. The clips have been taken from different conventional `raga' renditions played in two Indian stringed instruments -- sitar and sarod by eminent maestros of ICM and digitized in 44.1 kHz frequency. The ragas, which are unique to ICM, are described as musical structures capable of inducing different moods or emotions. For supervised classification purposes, we have used Convolutional Neural Network (CNN) based architectures (resnet50, mobilenet v2.0, squeezenet v1.0 and a proposed ODE-Net) on corresponding music spectrograms of the 6400 sub-clips (where every clip was segmented into 4 sub-clips) which contain both time as well as frequency domain information. Along with emotion classification, instrument classification based response was also attempted on the same dataset using the CNN based architectures. In this context, a nonlinear technique, Multifractal Detrended Fluctuation Analysis (MFDFA) was also applied on the musical clips to classify them on the basis of complexity values extracted from the method. The initial classification accuracy obtained from the applied methods are quite inspiring and have been corroborated with ANOVA results to determine the statistical significance. This type of CNN based classification algorithm using a rich corpus of Indian Classical Music is unique even in the global perspective and can be replicated in other modalities of music also. The link to this newly developed dataset has been provided in the dataset description section of the paper. This dataset is still under development and we plan to include more data containing other emotional as well as instrumental entities into consideration. {\copyright} 2022 Elsevier B.V.},
	author = {Nag, Sayan and Basu, Medha and Sanyal, Shankha and Banerjee, Archi and Ghosh, Dipak},
	author_keywords = {Classification; CNN; Emotions; Indian Classical Music; Instruments; MFDFA},
	doi = {10.1016/j.physa.2022.127261},
	journal = {Physica A: Statistical Mechanics and its Applications},
	keywords = {Classification (of information); Convolutional neural networks; Deep learning; Frequency domain analysis; Music; Network architecture; Convolutional neural network; Emotion; Emotion recognition; Indian classical music; Learning techniques; Multifractal detrended fluctuation analysis; Multifractal technique; Music emotions; Network-based architectures; Research areas; Fractals},
	note = {Cited by: 19},
	publication_stage = {Final},
	source = {Scopus},
	title = {On the application of deep learning and multifractal techniques to classify emotions and instruments using Indian Classical Music},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127732290&doi=10.1016%2fj.physa.2022.127261&partnerID=40&md5=634cc77ee93e9ac834ca0dad8b815119},
	volume = {597},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127732290&doi=10.1016%2fj.physa.2022.127261&partnerID=40&md5=634cc77ee93e9ac834ca0dad8b815119},
	bdsk-url-2 = {https://doi.org/10.1016/j.physa.2022.127261}}

@article{Li2022,
	abstract = {With the continuous development of information technology and the arrival of the era of big data, music appreciation has also entered the digital development. Big data essence is highlighted by comparison with traditional data management and processing technologies. Under different requirements, the required time processing range is different. Music appreciation is an essential and important part of music lessons, which can enrich people's emotional experience, improve aesthetic ability, and cultivate noble sentiments. Data processing of music information resources will greatly facilitate the management, dissemination, and big data analysis and processing of music resources and improve the ability of music lovers to appreciate music. This paper aims to study the digital development of music in the environment of big data, making music appreciation more convenient and intelligent. This paper proposes an intelligent music recognition and appreciation model based on deep neural network (DNN) model. The use of DNN allows this study to have significant improvement over the traditional algorithm. This paper proposes an intelligent music recognition and appreciation model based on the DNN model and improves the traditional algorithm. The improved method in this paper refers to the Dropout method on the traditional DNN model. The DNN is trained on the database and tested on the data. The results show that, in the same database, the traditional DNN model is 114 and the RNN model is 120. The PPL of the improved DNN model in this paper is 98, i.e., the lowest value. The convergence speed is faster, which indicates that the model has stronger music recognition ability and it is more conducive to the digital development of music appreciation.  {\copyright} 2022 Yi Li.},
	author = {Li, Yi},
	doi = {10.1155/2022/7873636},
	journal = {Mobile Information Systems},
	keywords = {Big data; Data handling; Emotion Recognition; Information management; Music; Neural network models; Continuous development; Data environment; Information resource; Management technologies; Model-based OPC; Music recognition; Neural network model; Processing technologies; Required time; Time processing; Deep neural networks},
	note = {Cited by: 1; All Open Access, Gold Open Access},
	publication_stage = {Final},
	source = {Scopus},
	title = {Digital Development for Music Appreciation of Information Resources Using Big Data Environment},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138358215&doi=10.1155%2f2022%2f7873636&partnerID=40&md5=511e220b1fe520e2028f131f19bc400a},
	volume = {2022},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138358215&doi=10.1155%2f2022%2f7873636&partnerID=40&md5=511e220b1fe520e2028f131f19bc400a},
	bdsk-url-2 = {https://doi.org/10.1155/2022/7873636}}

@article{Ouyang2022,
	abstract = {The main semantic symbol systems for people to express their emotions include natural language and music. The analysis and establishment of semantic association between language and music is helpful to provide more accurate retrieval and recommendation services for text and music. Existing researches mainly focus on the surface symbolic features and association of natural language and music, which limits the performance and interpretability of applications based on semantic association of natural language and music. Emotion is the main meaning of music expression, and the semantic range of text expression includes emotion. In this paper, the semantic features of music are extracted from audio features, and the semantic matching model of audio emotion analysis is constructed to analyze ethnic music audio emotion through feature extraction ability of deep structure. The model is based on the framework of emotional semantic matching technology and realizes the emotional semantic matching of music fragments and words through semantic emotional recognition algorithm. Multiple experiments show that when W=0.65, the recognition rate of multichannel fusion model is 88.42%, and the model can reasonably realize audio emotion analysis. When the spatial dimension of music data changes, the classification accuracy reaches the highest when the spatial dimension is 25. Analysing the semantic association of audio promotes the application of folk music in occupational therapy. {\copyright} 2022 Wensi Ouyang.},
	author = {Ouyang, Wensi},
	doi = {10.1155/2022/6841445},
	journal = {Occupational Therapy International},
	keywords = {Algorithms; Emotions; Humans; Music; Occupational Therapy; Semantics; algorithm; article; emotion; feature extraction; human; human experiment; molecular recognition; music; occupational therapy; algorithm; emotion; psychology; semantics},
	note = {Cited by: 0; All Open Access, Gold Open Access, Green Open Access},
	publication_stage = {Final},
	source = {Scopus},
	title = {Design of Semantic Matching Model of Folk Music in Occupational Therapy Based on Audio Emotion Analysis},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133137200&doi=10.1155%2f2022%2f6841445&partnerID=40&md5=645a2a866a5fe1131adcf6e2277fa02f},
	volume = {2022},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133137200&doi=10.1155%2f2022%2f6841445&partnerID=40&md5=645a2a866a5fe1131adcf6e2277fa02f},
	bdsk-url-2 = {https://doi.org/10.1155/2022/6841445}}

@article{Millan-Castillo20222460,
	abstract = {In the last decade, soundscapes have become one of the most active topics in Acoustics, providing a holistic approach to the acoustic environment, which involves human perception and context. Soundscapes-elicited emotions are central and substantially subtle and unnoticed (compared to speech or music). Currently, soundscape emotion recognition is a very active topic in the literature. We provide an exhaustive variable selection study (i.e., a selection of the soundscapes indicators) to a well-known dataset (emo-soundscapes). We consider linear soundscape emotion models for two soundscapes descriptors: arousal and valence. Several ranking schemes and procedures for selecting the number of variables are applied. We have also performed an alternating optimization scheme for obtaining the best sequences keeping fixed a certain number of features. Furthermore, we have designed a novel technique based on Gibbs sampling, which provides a more complete and clear view of the relevance of each variable. Finally, we have also compared our results with the analysis obtained by the classical methods based on p-values. As a result of our study, we suggest two simple and parsimonious linear models of only 7 and 16 variables (within the 122 possible features) for the two outputs (arousal and valence), respectively. The suggested linear models provide very good and competitive performance, with R2>0.86 and R2>0.63 (values obtained after a cross-validation procedure), respectively.  {\copyright} 2014 IEEE.},
	author = {Millan-Castillo, Roberto San and Martino, Luca and Morgado, Eduardo and Llorente, Fernando},
	author_keywords = {Best sequence search; Gibbs sampling; MCMC algorithms; ranking methods; soundscape emotion; variable selection},
	doi = {10.1109/TASLP.2022.3192664},
	journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
	keywords = {Behavioral research; Emotion Recognition; Latexes; Music; Psychoacoustic; Best sequence search; Computational modelling; Gibbs sampling; Input variables; MCMC algorithms; Predictive models; Psychoacoustic model; Ranking methods; Soundscape emotion; Soundscapes; Support vectors machine; Variables selections; Support vector machines},
	note = {Cited by: 5; All Open Access, Green Open Access},
	pages = {2460 -- 2474},
	publication_stage = {Final},
	source = {Scopus},
	title = {An Exhaustive Variable Selection Study for Linear Models of Soundscape Emotions: Rankings and Gibbs Analysis},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135226593&doi=10.1109%2fTASLP.2022.3192664&partnerID=40&md5=05e14cd48d920d061909ab4f02046759},
	volume = {30},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135226593&doi=10.1109%2fTASLP.2022.3192664&partnerID=40&md5=05e14cd48d920d061909ab4f02046759},
	bdsk-url-2 = {https://doi.org/10.1109/TASLP.2022.3192664}}

@article{Xiang2022,
	abstract = {With the rapid development of the related computer industry, the use of computer-related technologies has become more and more frequent. The music industry is no exception. The research and analysis of music emotions has been a problem since ancient times. Due to the diversification of music emotions, people with different music in the same piece of music will have different feelings. The research topic of this article is to make a comprehensive analysis of the computer's automatic identification technology, combined with the powerful subcapacity of the computer, so that the research on music emotion can be developed rapidly. The article analyzes the technical research of the automatic recognition and analysis of music emotion in the computer, and conducts a comprehensive analysis of the music emotion through the research of the computer-related automatic recognition technology. This paper focuses on the computer automatic recognition model of music emotion, and successfully realizes the design and simulation of the automatic recognition system based on the MATLAB platform. An automatic identification model using BP neural network algorithm is proposed. By comparing it with the statistical classification algorithm, the experimental results verify the effectiveness of the designed BP network in music emotion recognition. {\copyright} 2022 Yuehua Xiang.},
	author = {Xiang, Yuehua},
	doi = {10.1155/2022/3145785},
	journal = {Mathematical Problems in Engineering},
	keywords = {Automation; Music; Neural networks; Simulation platform; Automatic identification; Automatic recognition; Comprehensive analysis; Computer analysis; Computer industry; Identification technology; Music emotions; Music industry; Research and analysis; Research topics; MATLAB},
	note = {Cited by: 7; All Open Access, Gold Open Access},
	publication_stage = {Final},
	source = {Scopus},
	title = {Computer Analysis and Automatic Recognition Technology of Music Emotion},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128135662&doi=10.1155%2f2022%2f3145785&partnerID=40&md5=3ee19512d1d7e1d34ae7e823a78310b3},
	volume = {2022},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128135662&doi=10.1155%2f2022%2f3145785&partnerID=40&md5=3ee19512d1d7e1d34ae7e823a78310b3},
	bdsk-url-2 = {https://doi.org/10.1155/2022/3145785}}

@article{Rezaee2022,
	abstract = {Today, in order to prevent chronic stress from causing irreparable damage, it is imperative to diagnose and treat it in its early stages. Using the Internet of Things (IoT) and automated learning methods in homes, creating an intelligent environment can help identify stress-related emotions. An approach to stress detection based on metaheuristic fuzzy inference system-based learning (fMFiS-L) and emotion recognition is presented in this paper. Accordingly, our study focuses on the use of fusion learning to diagnose stress using the healthcare system and the Internet of Medical Things (IoMT) for smart homes. Music videos were shown to participants in the first stage to arouse emotional states such as anger, anxiety, and depression. Volunteers were divided into two groups, with one group practicing Reiki meditation for two weeks. The EEG signals were recorded before and after meditation, stress levels were assessed using the Likert scale, and emotions were classified using the modified fusion fuzzy inference system. In addition, a method is presented for determining the optimal parameters in the fMFiS-L structure by optimizing the innovative gunner algorithm (AIG). We conclude that Reiki meditation can significantly reduce negative emotions and stress levels in the IoMT environment of smart homes. Furthermore, the fMFiS-L architecture was evaluated for generalization of emotion recognition based on unseen EEG data. Generally, the classification of emotions produced satisfactory results, with a 92% accuracy rate. {\copyright} 2022},
	author = {Rezaee, Khosro and Yang, Xuan and Khosravi, Mohammad R. and Zhang, Ruowei and Lin, Wenmin and Jeon, Gwanggil},
	author_keywords = {Fusion learning; Human emotion recognition; IoMT environment; Smart building; Smart homes; Stress treatment},
	doi = {10.1016/j.buildenv.2022.108988},
	journal = {Building and Environment},
	keywords = {Fuzzy inference; Fuzzy systems; Intelligent buildings; Internet of things; Learning systems; Optimization; Speech recognition; Chronic stress; Emotion recognition; Fusion learning; Fuzzy inference systems; Human emotion recognition; Internet of medical thing environment; Smart homes; Stress levels; Stress recognition; Stress treatment; algorithm; detection method; learning; mental disorder; mental health; technological development; Automation},
	note = {Cited by: 6},
	publication_stage = {Final},
	source = {Scopus},
	title = {Fusion-based learning for stress recognition in smart home: An IoMT framework},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127358728&doi=10.1016%2fj.buildenv.2022.108988&partnerID=40&md5=391decdebfedd42c1a936bddc3e9940a},
	volume = {216},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127358728&doi=10.1016%2fj.buildenv.2022.108988&partnerID=40&md5=391decdebfedd42c1a936bddc3e9940a},
	bdsk-url-2 = {https://doi.org/10.1016/j.buildenv.2022.108988}}

@article{Panwar20192986,
	abstract = {Music plays an important role in our society and has applications broader than just entertainment and pleasure due to its social and physiological effects. There has been recent interest in music, and two active research topics are music information retrieval and music emotion recognition, where data mining and machine learning techniques are integrated with music features and annotations to extract music information such as genres, instrument and its emotional content. In this paper, a machine learning music perception model is proposed to identify emotional content of a given audio stream and study the emotional effects of music. In fact, our developed model has the capability to determine the emotional state of a region (e.g., city) that could be utilized in applications such as marketing, and many other facets of the society such as cognitive development, education, therapy and security. This emotion recognition task is performed by mapping musical acoustic features to corresponding arousal and valence emotion indexes using a linear regression model. A radio-induced emotion dataset (RIED) is compiled from the songs broadcasted on radio in four US major cities (i.e., Houston, New York, Los Angeles and Miami) between October 21, 2017, and November 21, 2017. The RIED is then used as input to the proposed perception model to observe the regional music emotion propensity. {\copyright} 2018, Springer Science+Business Media, LLC, part of Springer Nature.},
	author = {Panwar, Sharaj and Rad, Paul and Choo, Kim-Kwang Raymond and Roopaei, Mehdi},
	author_keywords = {Categorical emotional models; Dimensional emotional models; Machine learning; Music information retrieval; Musical emotion},
	doi = {10.1007/s11227-018-2499-y},
	journal = {Journal of Supercomputing},
	keywords = {Artificial intelligence; Audio acoustics; Data mining; Information retrieval; Learning systems; Linear regression; Speech recognition; Cognitive development; Emotion recognition; Emotional models; Linear regression models; Machine learning techniques; Music information retrieval; Musical emotion; Physiological effects; Behavioral research},
	note = {Cited by: 31},
	number = {6},
	pages = {2986 -- 3009},
	publication_stage = {Final},
	source = {Scopus},
	title = {Are you emotional or depressed? Learning about your emotional state from your music using machine learning},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052067378&doi=10.1007%2fs11227-018-2499-y&partnerID=40&md5=a518ac5b093d89f75613fec37cef9e3b},
	volume = {75},
	year = {2019},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052067378&doi=10.1007%2fs11227-018-2499-y&partnerID=40&md5=a518ac5b093d89f75613fec37cef9e3b},
	bdsk-url-2 = {https://doi.org/10.1007/s11227-018-2499-y}}

@article{Jafari2017233,
	abstract = {BACKGROUND: Although extensive research has been published about the emotional consequences of stroke, most studies have focused on emotional words, speech prosody, voices, or facial expressions. The emotional processing of musical excerpts following stroke has been relatively unexplored. OBJECTIVE: The present study was conducted to investigate the effects of chronic stroke on the recognition of basic emotions in music. METHODS: Seventy persons, including 25 normal controls (NC), 25 persons with right brain damage (RBD) from stroke, and 20 persons with left brain damage (LBD) from stroke between the ages of 31-71 years were studied. The Musical Emotional Bursts (MEB) test, which consists of a set of short musical pieces expressing basic emotional states (happiness, sadness, and fear) and neutrality, was used to test musical emotional perception. RESULTS: Both stroke groups were significantly poorer than normal controls for the MEB total score and its subtests (p<0.001). The RBD group was significantly less able than the LBD group to recognize sadness (p = 0.047) and neutrality (p = 0.015). Negative correlations were found between age and MEB scores for all groups, particularly the NC and RBD groups. CONCLUSION: Our findings indicated that stroke affecting the auditory cerebrum can cause acquired amusia with greater severity in RBD than LBD. These results supported the "valence hypothesis" of right hemisphere dominance in processing negative emotions. {\copyright} 2017 IOS Press and the authors. All rights reserved.},
	author = {Jafari, Zahra and Esmaili, Mahdiye and Delbari, Ahmad and Mehrpour, Masoud and Mohajerani, Majid H.},
	author_keywords = {acquired amusia; aging; emotion recognition; Musical Emotional Bursts; Stroke},
	doi = {10.3233/NRE-161408},
	journal = {NeuroRehabilitation},
	keywords = {Adult; Aged; Cerebrum; Emotions; Facial Expression; Female; Humans; Male; Middle Aged; Music; Perceptual Disorders; Stroke; adult; aged; Article; brain injury; cerebrovascular accident; controlled study; emotion; fear; female; happiness; human; major clinical study; male; middle aged; music; perception; sadness; stroke patient; brain; comparative study; diagnostic imaging; emotion; facial expression; music; Perceptual Disorders; psychology; Stroke},
	note = {Cited by: 12},
	number = {2},
	pages = {233 -- 241},
	publication_stage = {Final},
	source = {Scopus},
	title = {Post-stroke acquired amusia: A comparison between right- A nd left-brain hemispheric damages},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016433073&doi=10.3233%2fNRE-161408&partnerID=40&md5=46de1228b92c3f60b4bc1aa5e2c30a81},
	volume = {40},
	year = {2017},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016433073&doi=10.3233%2fNRE-161408&partnerID=40&md5=46de1228b92c3f60b4bc1aa5e2c30a81},
	bdsk-url-2 = {https://doi.org/10.3233/NRE-161408}}

@article{Narme2016902,
	abstract = {Normal aging affects explicit memory while leaving implicit memory relatively spared. Normal aging also modifies how emotions are processed and experienced, with increasing evidence that older adults (OAs) focus more on positive information than younger adults (YAs). The aim of the present study was to investigate how age-related changes in emotion processing influence explicit and implicit memory. We used emotional melodies that differed in terms of valence (positive or negative) and arousal (high or low). Implicit memory was assessed with a preference task exploiting exposure effects, and explicit memory with a recognition task. Results indicated that effects of valence and arousal interacted to modulate both implicit and explicit memory in YAs. In OAs, recognition was poorer than in YAs; however, recognition of positive and high-arousal (happy) studied melodies was comparable. Insofar as socioemotional selectivity theory (SST) predicts a preservation of the recognition of positive information, our findings are not fully consistent with the extension of this theory to positive melodies since recognition of low-arousal (peaceful) studied melodies was poorer in OAs. In the preference task, YAs showed stronger exposure effects than OAs, suggesting an age-related decline of implicit memory. This impairment is smaller than the one observed for explicit memory (recognition), extending to the musical domain the dissociation between explicit memory decline and implicit memory relative preservation in aging. Finally, the disproportionate preference for positive material seen in OAs did not translate into stronger exposure effects for positive material suggesting no age-related emotional bias in implicit memory. {\copyright} 2016 American Psychological Association.},
	author = {Narme, Pauline and Peretz, Isabelle and Strub, Marie-Laure and Ergis, Anne-Marie},
	author_keywords = {Exposure effect; Music; Positivity effect; Socioemotional selectivity theory},
	doi = {10.1037/pag0000116},
	journal = {Psychology and Aging},
	keywords = {Adult; Aged; Aged, 80 and over; Aging; Emotions; Female; Humans; Male; Memory; Music; Young Adult; adult; age; aged; aging; arousal; Article; controlled study; dissociation; emotion; explicit memory; exposure; female; human; implicit memory; information; male; memory disorder; music; prediction; recognition; socioemotional selectivity theory; task performance; theory; very elderly; aging; emotion; memory; physiology; young adult},
	note = {Cited by: 11},
	number = {8},
	pages = {902 -- 913},
	publication_stage = {Final},
	source = {Scopus},
	title = {Emotion effects on implicit and explicit musical memory in normal aging},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984916923&doi=10.1037%2fpag0000116&partnerID=40&md5=effa6b2775dbc2d262f7e7713573cee4},
	volume = {31},
	year = {2016},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984916923&doi=10.1037%2fpag0000116&partnerID=40&md5=effa6b2775dbc2d262f7e7713573cee4},
	bdsk-url-2 = {https://doi.org/10.1037/pag0000116}}

@article{He2020872,
	abstract = {In recent years, with the further breakthrough of artificial intelligence theory and technology, as well as the further expansion of the Internet scale, the recognition of human emotions and the necessity for satisfying human psychological needs in future artificial intelligence technology development tendencies have been highlighted, in addition to physical task accomplishment. Musical emotion classification is an important research topic in artificial intelligence. The key premise of realizing music emotion classification is to construct a musical emotion model that conforms to the characteristics of music emotion recognition. Currently, three types of music emotion classification models are available: discrete category, continuous dimensional, and music emotion-specific models. The pleasure-arousal music emotion fuzzy model, which includes a wide range of emotions compared with other models, is selected as the emotional classification system in this study to investigate the influencing factor for musical emotion classification. Two representative emotional attributes, i.e., speed and strength, are used as variables. Based on test experiments involving music and non-music majors combined with questionnaire results, the relationship between music properties and emotional changes under the pleasure-arousal model is revealed quantitatively. {\copyright} 2020 Fuji Technology Press. All rights reserved.},
	author = {He, Jing-Xian and Zhou, Li and Liu, Zhen-Tao and Hu, Xin-Yue},
	author_keywords = {Affective computing; Classification; Fuzzy model; Music emotion; Pleasure-arousal emotion space},
	doi = {10.20965/JACIII.2020.P0872},
	journal = {Journal of Advanced Computational Intelligence and Intelligent Informatics},
	keywords = {Classification (of information); Artificial intelligence technologies; Emotional change; Emotional classification; Empirical research; Music emotion classifications; Psychological needs; Research topics; Task accomplishment; Artificial intelligence},
	note = {Cited by: 4; All Open Access, Gold Open Access},
	number = {7},
	pages = {872 -- 881},
	publication_stage = {Final},
	source = {Scopus},
	title = {Digital empirical research of influencing factors of musical emotion classification based on pleasure-arousal musical emotion fuzzy model},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098774308&doi=10.20965%2fJACIII.2020.P0872&partnerID=40&md5=a20634ec00d9b4bcc6e9980a4b708007},
	volume = {24},
	year = {2020},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098774308&doi=10.20965%2fJACIII.2020.P0872&partnerID=40&md5=a20634ec00d9b4bcc6e9980a4b708007},
	bdsk-url-2 = {https://doi.org/10.20965/JACIII.2020.P0872}}

@article{Paquette2018272,
	abstract = {Cochlear implants can successfully restore hearing in profoundly deaf individuals and enable speech comprehension. However, the acoustic signal provided is severely degraded and, as a result, many important acoustic cues for perceiving emotion in voices and music are unavailable. The deficit of cochlear implant users in auditory emotion processing has been clearly established. Yet, the extent to which this deficit and the specific cues that remain available to cochlear implant users are unknown due to several confounding factors. Here we assessed the recognition of the most basic forms of auditory emotion and aimed to identify which acoustic cues are most relevant to recognize emotions through cochlear implants. To do so, we used stimuli that allowed vocal and musical auditory emotions to be comparatively assessed while controlling for confounding factors. These stimuli were used to evaluate emotion perception in cochlear implant users (Experiment 1) and to investigate emotion perception in natural versus cochlear implant hearing in the same participants with a validated cochlear implant simulation approach (Experiment 2). Our results showed that vocal and musical fear was not accurately recognized by cochlear implant users. Interestingly, both experiments found that timbral acoustic cues (energy and roughness) correlate with participant ratings for both vocal and musical emotion bursts in the cochlear implant simulation condition. This suggests that specific attention should be given to these cues in the design of cochlear implant processors and rehabilitation protocols (especially energy, and roughness). For instance, music-based interventions focused on timbre could improve emotion perception and regulation, and thus improve social functioning, in children with cochlear implants during development. {\copyright} 2018 Elsevier B.V.},
	author = {Paquette, S. and Ahmed, G.D. and Goffi-Gomez, M.V. and Hoshino, A.C.H. and Peretz, I. and Lehmann, A.},
	author_keywords = {Cochlear implants; Cross-domain comparison; Emotional acoustic cues; Music; Timbre; Voice},
	doi = {10.1016/j.heares.2018.08.009},
	journal = {Hearing Research},
	keywords = {Acoustic Stimulation; Adult; Auditory Perception; Cochlear Implantation; Cochlear Implants; Cues; Electric Stimulation; Emotions; Female; Humans; Judgment; Male; Middle Aged; Music; Persons With Hearing Impairments; Voice Quality; Young Adult; acoustic cue; adult; arousal; Article; association; auditory discrimination; auditory emotion; auditory stimulation; clinical article; controlled study; emotion; fear; female; happiness; human; male; measurement accuracy; middle aged; music; musical emotion perception; perception; priority journal; regulatory mechanism; sadness; simulation; social interaction; vocal emotion perception; young adult; association; cochlea prosthesis; cochlear implantation; decision making; devices; electrostimulation; emotion; hearing; hearing impaired person; music; psychology; rehabilitation; voice},
	note = {Cited by: 38},
	pages = {272 -- 282},
	publication_stage = {Final},
	source = {Scopus},
	title = {Musical and vocal emotion perception for cochlear implants users},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052838987&doi=10.1016%2fj.heares.2018.08.009&partnerID=40&md5=c9dc902a65c24daa691f803e29963706},
	volume = {370},
	year = {2018},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052838987&doi=10.1016%2fj.heares.2018.08.009&partnerID=40&md5=c9dc902a65c24daa691f803e29963706},
	bdsk-url-2 = {https://doi.org/10.1016/j.heares.2018.08.009}}

@article{Hou2019,
	abstract = {Music can evoke a variety of emotions, which may be manifested by distinct signals on the electroencephalogram (EEG). Many previous studies have examined the associations between specific aspects of music, including the subjective emotions aroused, and EEG signal features. However, no study has comprehensively examined music-related EEG features and selected those with the strongest potential for discriminating emotions. So, this paper conducted a series of experiments to identify the most influential EEG features induced by music evoking different emotions (calm, joy, sad, and angry). We extracted 27-dimensional features from each of 12 electrode positions then used correlation-based feature selection method to identify the feature set most strongly related to the original features but with lowest redundancy. Several classifiers, including Support Vector Machine (SVM), C4.5, LDA, and BPNN, were then used to test the recognition accuracy of the original and selected feature sets. Finally, results are analyzed in detail and the relationships between selected feature set and human emotions are shown clearly. Through the classification results of 10 random examinations, it could be concluded that the selected feature sets of Pz are more effective than other features when using as the key feature set to classify human emotion statues. {\copyright} 2019 Yimin Hou and Shuaiqi Chen.},
	author = {Hou, Yimin and Chen, Shuaiqi},
	doi = {10.1155/2019/3191903},
	journal = {Computational Intelligence and Neuroscience},
	keywords = {Algorithms; Arousal; Auditory Perception; Brain; Electroencephalography; Emotions; Evoked Potentials; Humans; Music; Support Vector Machine; Electroencephalography; Man machine systems; Support vector machines; Classification results; Electro-encephalogram (EEG); Electroencephalographic signals; Feature selection methods; Feature sets; Human emotion; Key feature; Recognition accuracy; algorithm; arousal; brain; electroencephalography; emotion; evoked response; hearing; human; music; physiology; procedures; support vector machine; Biomedical signal processing},
	note = {Cited by: 23; All Open Access, Bronze Open Access, Green Open Access},
	publication_stage = {Final},
	source = {Scopus},
	title = {Distinguishing Different Emotions Evoked by Music via Electroencephalographic Signals},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063229122&doi=10.1155%2f2019%2f3191903&partnerID=40&md5=3f2f93a7ad6cfac7a64f379cd607b8cd},
	volume = {2019},
	year = {2019},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063229122&doi=10.1155%2f2019%2f3191903&partnerID=40&md5=3f2f93a7ad6cfac7a64f379cd607b8cd},
	bdsk-url-2 = {https://doi.org/10.1155/2019/3191903}}

@article{Goshvarpour2017355,
	abstract = {Background: The purpose of the current study was to examine the effectiveness of Matching Pursuit (MP) algorithm in emotion recognition. Methods: Electrocardiogram (ECG) and galvanic skin responses (GSR) of 11 healthy students were collected while subjects were listening to emotional music clips. Applying three dictionaries, including two wavelet packet dictionaries (Coiflet, and Daubechies) and discrete cosine transform, MP coefficients were extracted from ECG and GSR signals. Next, some statistical indices were calculated from the MP coefficients. Then, three dimensionality reduction methods, including Principal Component Analysis (PCA), Linear Discriminant Analysis, and Kernel PCA were applied. The dimensionality reduced features were fed into the Probabilistic Neural Network in subject-dependent and subject-independent modes. Emotion classes were described by a two-dimensional emotion space, including four quadrants of valence and arousal plane, valence based, and arousal based emotional states. Results: Using PCA, the highest recognition rate of 100% was achieved for sigma = 0.01 in all classification schemes. In addition, the classification performance of ECG features was evidently better than that of GSR features. Similar results were obtained for subject-dependent emotion classification mode. Conclusions: An accurate emotion recognition system was proposed using MP algorithm and wavelet dictionaries. {\copyright} 2018 Chang Gung University},
	author = {Goshvarpour, Atefeh and Abbasi, Ataollah and Goshvarpour, Ateke},
	author_keywords = {Electrocardiogram; Emotion recognition; Galvanic skin responses; Matching pursuit; Probabilistic neural network},
	doi = {10.1016/j.bj.2017.11.001},
	journal = {Biomedical Journal},
	keywords = {Adult; Algorithms; Electrocardiography; Emotions; Female; Galvanic Skin Response; Humans; Neural Networks (Computer); Principal Component Analysis; adult; arousal; classification; clinical article; discriminant analysis; electrocardiogram; electrodermal response; emotion; female; human; male; music; nervous system; principal component analysis; student; algorithm; artificial neural network; electrocardiography; emotion},
	note = {Cited by: 128; All Open Access, Gold Open Access, Green Open Access},
	number = {6},
	pages = {355 -- 368},
	publication_stage = {Final},
	source = {Scopus},
	title = {An accurate emotion recognition system using ECG and GSR signals and matching pursuit method},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039871409&doi=10.1016%2fj.bj.2017.11.001&partnerID=40&md5=fe0d73776ff98674221d24811b376ad6},
	volume = {40},
	year = {2017},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039871409&doi=10.1016%2fj.bj.2017.11.001&partnerID=40&md5=fe0d73776ff98674221d24811b376ad6},
	bdsk-url-2 = {https://doi.org/10.1016/j.bj.2017.11.001}}

@article{Al-Nafjan20201,
	abstract = {Brain--computer interface (BCI) technology provides a direct interface between the brain and an external device. BCIs have facilitated the monitoring of conscious brain electrical activity via electroencephalogram (EEG) signals and the detection of human emotion. Recently, great progress has been made in the development of novel paradigms for EEG-based emotion detection. These studies have also attempted to apply BCI research findings in varied contexts. Interestingly, advances in BCI technologies have increased the interest of scientists because such technologies' practical applications in human--machine relationships seem promising. This emphasizes the need for a building process for an EEG-based emotion detection system that is lightweight, in terms of a smaller EEG dataset size and no involvement of feature extraction methods. In this study, we investigated the feasibility of using a spiking neural network to build an emotion detection system from a smaller version of the DEAP dataset with no involvement of feature extraction methods while maintaining decent accuracy. The results showed that by using a NeuCube-based spiking neural network, we could detect the valence emotion level using only 60 EEG samples with 84.62% accuracy, which is a comparable accuracy to that of previous studies. {\copyright} 2020 by the authors. Licensee MDPI, Basel, Switzerland.},
	author = {Al-Nafjan, Abeer and Alharthi, Khulud and Kurdi, Heba},
	author_keywords = {Brain--computer interface (BCI); EEG-based emotion detection; Electroencephalogram (EEG); NeuCube; Spiking neural network},
	doi = {10.3390/brainsci10110781},
	journal = {Brain Sciences},
	keywords = {action potential; Article; artificial neural network; autism; Bayesian network; behavior; classification algorithm; cognition; comprehension; consciousness disorder; controlled study; convolutional neural network; data analysis; deep learning; electric activity; electroencephalogram; electroencephalography; emotion; emotion detection system; feasibility study; feature extraction; feature extraction algorithm; fuzzy system; human; human computer interaction; human experiment; k nearest neighbor; measurement accuracy; model; motor dysfunction; multilayer perceptron; music; neucube model; overall survival; Parkinson disease; pattern recognition; postsynaptic density; power spectrum; prediction; psychology; sample size; schizophrenia; signal noise ratio; software; spatiotemporal analysis; spiking neural network; support vector machine},
	note = {Cited by: 16; All Open Access, Gold Open Access, Green Open Access},
	number = {11},
	pages = {1 -- 17},
	publication_stage = {Final},
	source = {Scopus},
	title = {Lightweight building of an electroencephalogram-based emotion detection system},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094198226&doi=10.3390%2fbrainsci10110781&partnerID=40&md5=18f3d3a214aaf01c8abb182eb9205e01},
	volume = {10},
	year = {2020},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094198226&doi=10.3390%2fbrainsci10110781&partnerID=40&md5=18f3d3a214aaf01c8abb182eb9205e01},
	bdsk-url-2 = {https://doi.org/10.3390/brainsci10110781}}

@article{Cavallo20214471,
	abstract = {Future smart agents, like robots, should produce personalized behaviours based on user emotions and moods to fit more in ordinary users' activities. Besides, the emotions are also linked to human cognitive systems, thus their monitoring could be extremely useful in the case of neurodegenerative diseases such as dementia and Alzheimer. Literature works propose the use of music tracks and videos to stimulate emotions, and cameras to recorder the evoked reactions in human beings. However, these approaches may not be effective in everyday life, due to camera obstructions and different types of stimulation which can be related also with the interaction with other human beings. In this work, we investigate the Electrocardiogram, the ElectroDermal Activity and the Brain Activity signals as main informative channels, acquired through a wireless wearable sensor network. An experimental methodology was built to induce three different emotional states through social interaction. Collected data were classified with three supervised machine learning approaches with different kernels (Support Vector Machine, Decision Tree and k-nearest neighbour) considering the valence dimension and a combination of valence and arousal dimension evoked during the interaction. 34 healthy young participants were involved in the study and a total of 239 instances were analyzed. The supervised algorithms achieve an accuracy of 0.877 in the best case. {\copyright} 2019, Springer-Verlag GmbH Germany, part of Springer Nature.},
	author = {Cavallo, Filippo and Semeraro, Francesco and Mancioppi, Gianmaria and Betti, Stefano and Fiorini, Laura},
	author_keywords = {Machine learning; Mood recognition; Social robotics},
	doi = {10.1007/s12652-019-01595-6},
	journal = {Journal of Ambient Intelligence and Humanized Computing},
	keywords = {Behavioral research; Brain; Cameras; Cognitive systems; Decision trees; Nearest neighbor search; Neurodegenerative diseases; Physiological models; Support vector machines; Trees (mathematics); Wearable technology; Wireless sensor networks; Electrodermal activity; Experimental methodology; Human cognitive systems; K-nearest neighbours; Mood recognition; Physiological parameters; Social robotics; Supervised machine learning; Learning systems},
	note = {Cited by: 9; All Open Access, Green Open Access},
	number = {4},
	pages = {4471 -- 4484},
	publication_stage = {Final},
	source = {Scopus},
	title = {Mood classification through physiological parameters},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076226651&doi=10.1007%2fs12652-019-01595-6&partnerID=40&md5=449e166a087926fe7fc29a14a85870a0},
	volume = {12},
	year = {2021},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076226651&doi=10.1007%2fs12652-019-01595-6&partnerID=40&md5=449e166a087926fe7fc29a14a85870a0},
	bdsk-url-2 = {https://doi.org/10.1007/s12652-019-01595-6}}

@article{Goshvarpour2016,
	abstract = {The objective of this study is to propose an accurate emotion recognition methodology. To this end, a novel fusion framework based on wavelet transform (WT), and matching pursuit (MP) algorithm was offered. Electrocardiogram (ECG) and galvanic skin response (GSR) of 11 healthy students were collected while subjects listened to emotional music clips. In both fusion techniques, Coiflet wavelet (Coif5 at level 14) was chosen as a wavelet family and MP dictionary, respectively. After employing the proposed fusion framework, some statistical measures were extracted. To describe emotions, three schemes were adopted: two-dimensional model (five classes), valence-(three classes), and arousal-(three classes) based emotion categories. Subsequently, the probabilistic neural network (PNN) was applied to classify affective states. The experiments indicate that the MP-based fusion approach outperform the wavelet-based fusion technique or methods using only ECG or GSR indices. Considering the proposed fusion techniques, the maximum classification rate of 99.64% and 92.31% was reached for the fusion methodology based on the MP algorithm (five classes of emotion) and wavelet-based fusion technique (three classes of valence), respectively. {\copyright} 2016 National Taiwan University.},
	author = {Goshvarpour, Atefeh and Abbasi, Ataollah and Goshvarpour, Ateke and Daneshvar, Sabalan},
	author_keywords = {autonomic signals; Emotion recognition; fusion; matching pursuit; wavelet transform},
	doi = {10.4015/S101623721650040X},
	journal = {Biomedical Engineering - Applications, Basis and Communications},
	keywords = {Biomedical signal processing; Electrocardiography; Electrophysiology; Fusion reactions; Neural networks; Speech recognition; Wavelet analysis; Classification rates; Emotion recognition; Galvanic skin response; Matching pursuit; Matching pursuit algorithms; Probabilistic neural networks; Statistical measures; Two dimensional model; arousal; classification; clinical article; electrocardiogram; electrodermal response; family; human; model; music; nervous system; recognition; student; Wavelet transforms},
	note = {Cited by: 13},
	number = {6},
	publication_stage = {Final},
	source = {Scopus},
	title = {A novel signal-based fusion approach for accurate music emotion recognition},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007168400&doi=10.4015%2fS101623721650040X&partnerID=40&md5=816760b4e5bcdea5c14b6184c95701d4},
	volume = {28},
	year = {2016},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007168400&doi=10.4015%2fS101623721650040X&partnerID=40&md5=816760b4e5bcdea5c14b6184c95701d4},
	bdsk-url-2 = {https://doi.org/10.4015/S101623721650040X}}

@article{Goshvarpour2017617,
	abstract = {Designing an efficient automatic emotion recognition system based on physiological signals has attracted great interests within the research of human--machine interactions. This study was aimed to classify emotional responses by means of a simple dynamic signal processing technique and fusion frameworks. The electrocardiogram and finger pulse activity of 35 participants were recorded during rest condition and when subjects were listening to music intended to stimulate certain emotions. Four emotion categories, including happiness, sadness, peacefulness, and fear were chosen. Estimating heart rate variability (HRV) and pulse rate variability (PRV), 4 Poincare indices in 10 lags were extracted. The support vector machine classifier was used for emotion classification. Both feature level (FL) and decision level (DL) fusion schemes were examined. Significant differences have been observed between lag 1 Poincare plot indices and the other lagged measures. The mean accuracies of 84.1, 82.9, 79.68, and 76.05% were obtained for PRV, DL, FL, and HRV measures, respectively. However, DL outperformed others in discriminating sadness and peacefulness, using SD1 and total features, correspondingly. In both cases, the classification rates improved up to 92% (with the sensitivity of 95% and specificity of 83.33%). Totally, DL resulted in better performances compared to FL. In addition, the impact of the fusion rules on the classification performances has been confirmed. {\copyright} 2017, Australasian College of Physical Scientists and Engineers in Medicine.},
	author = {Goshvarpour, Atefeh and Abbasi, Ataollah and Goshvarpour, Ateke},
	author_keywords = {Classification; Emotion; Fusion; Lagged Poincare plot},
	doi = {10.1007/s13246-017-0571-1},
	journal = {Australasian Physical and Engineering Sciences in Medicine},
	keywords = {Algorithms; Emotions; Female; Heart Rate; Humans; Male; Pulse; Young Adult; Classification (of information); Fusion reactions; Heart; Speech recognition; Automatic emotion recognition; Classification performance; Dynamic signal processing; Emotion; Heart rate variability; Poincare plots; Pulse rate variability; Support vector machine classifiers; adult; Article; controlled study; electrocardiography; emotionality; female; heart rate variability; human; human experiment; male; music; normal human; pulse rate; pulse rate variability; sadness; sensitivity and specificity; support vector machine; algorithm; emotion; heart rate; physiology; young adult; Biomedical signal processing},
	note = {Cited by: 37},
	number = {3},
	pages = {617 -- 629},
	publication_stage = {Final},
	source = {Scopus},
	title = {Fusion of heart rate variability and pulse rate variability for emotion recognition using lagged poincare plots},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024505036&doi=10.1007%2fs13246-017-0571-1&partnerID=40&md5=595af2db595c4c27f2336e4ba3850fe0},
	volume = {40},
	year = {2017},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024505036&doi=10.1007%2fs13246-017-0571-1&partnerID=40&md5=595af2db595c4c27f2336e4ba3850fe0},
	bdsk-url-2 = {https://doi.org/10.1007/s13246-017-0571-1}}

@article{Giannakopoulos2015,
	abstract = {Audio information plays a rather important role in the increasing digital content that is available today, resulting in a need for methodologies that automatically analyze such content: audio event recognition for home automations and surveillance systems, speech recognition, music information retrieval, multimodal analysis (e.g. audio-visual analysis of online videos for content-based recommendation), etc. This paper presents pyAudioAnalysis, an open-source Python library that provides a wide range of audio analysis procedures including: feature extraction, classification of audio signals, supervised and unsupervised segmentation and content visualization. pyAudioAnalysis is licensed under the Apache License and is available at GitHub (https://github.com/tyiannak/pyAudioAnalysis/). Here we present the theoretical background behind the wide range of the implemented methodologies, along with evaluation metrics for some of the methods. pyAudioAnalysis has been already used in several audio analysis research applications: smart-home functionalities through audio event detection, speech emotion recognition, depression classification based on audiovisual features, music segmentation, multimodal content-based movie recommendation and health applications (e.g. monitoring eating habits). The feedback provided from all these particular audio applications has led to practical enhancement of the library. {\copyright} 2015 Theodoros Giannakopoulos.},
	author = {Giannakopoulos, Theodoros},
	doi = {10.1371/journal.pone.0144610},
	journal = {PLoS ONE},
	keywords = {Acoustic Stimulation; Animals; Humans; Information Storage and Retrieval; Libraries, Digital; Music; Pattern Recognition, Automated; Software; Sound; Speech; algorithm; Article; automatic speech recognition; computer analysis; computer graphics; computer interface; computer language; computer program; data extraction; information retrieval; multimedia; online analytical processing; signal detection; animal; auditory stimulation; automated pattern recognition; human; library; music; physiology; software; sound; speech},
	note = {Cited by: 326; All Open Access, Gold Open Access, Green Open Access},
	number = {12},
	publication_stage = {Final},
	source = {Scopus},
	title = {PyAudioAnalysis: An open-source python library for audio signal analysis},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961354674&doi=10.1371%2fjournal.pone.0144610&partnerID=40&md5=5b51e0800fac922c60ba6c7598037f07},
	volume = {10},
	year = {2015},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961354674&doi=10.1371%2fjournal.pone.0144610&partnerID=40&md5=5b51e0800fac922c60ba6c7598037f07},
	bdsk-url-2 = {https://doi.org/10.1371/journal.pone.0144610}}

@article{Liu2021,
	abstract = {This research uses facial expression recognition software (FaceReader) to explore the influence of different sound interventions on the emotions of older people with dementia. The field experiment was carried out in the public activity space of an older adult care facility. Three intervention sound sources were used, namely, music, stream, and birdsong. Data collected through the Self-Assessment Manikin Scale (SAM) were compared with facial expression recognition (FER) data. FaceReader identified differences in the emotional responses of older people with dementia to different sound interventions and revealed changes in facial expressions over time. The facial expression of the participants had significantly higher valence for all three sound interventions than in the intervention without sound (p < 0.01). The indices of sadness, fear, and disgust differed significantly between the different sound interventions. For example, before the start of the birdsong intervention, the disgust index initially increased by 0.06 from 0 s to about 20 s, followed by a linear downward trend, with an average reduction of 0.03 per 20 s. In addition, valence and arousal were significantly lower when the sound intervention began before, rather than concurrently with, the start of the activity (p < 0.01). Moreover, in the birdsong and stream interventions, there were significant differences between intervention days (p < 0.05 or p < 0.01). Furthermore, facial expression valence significantly differed by age and gender. Finally, a comparison of the SAM and FER results showed that, in the music intervention, the valence in the first 80 s helps to predict dominance (r = 0.600) and acoustic comfort (r = 0.545); in the stream sound intervention, the first 40 s helps to predict pleasure (r = 0.770) and acoustic comfort (r = 0.766); for the birdsong intervention, the first 20 s helps to predict dominance (r = 0.824) and arousal (r = 0.891). {\copyright} Copyright {\copyright} 2021 Liu, Wang and Yu.},
	author = {Liu, Ying and Wang, Zixuan and Yu, Ge},
	author_keywords = {elderly with dementia; emotion; facial expression recognition; sound intervention; type of sound source},
	doi = {10.3389/fpsyg.2021.707809},
	journal = {Frontiers in Psychology},
	note = {Cited by: 8; All Open Access, Gold Open Access, Green Open Access},
	publication_stage = {Final},
	source = {Scopus},
	title = {The Effectiveness of Facial Expression Recognition in Detecting Emotional Responses to Sound Interventions in Older Adults With Dementia},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114599537&doi=10.3389%2ffpsyg.2021.707809&partnerID=40&md5=a07d8da5947bbdf8f02f03e1b548c4fd},
	volume = {12},
	year = {2021},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114599537&doi=10.3389%2ffpsyg.2021.707809&partnerID=40&md5=a07d8da5947bbdf8f02f03e1b548c4fd},
	bdsk-url-2 = {https://doi.org/10.3389/fpsyg.2021.707809}}

@article{Gomez-Canon2021106,
	abstract = {Emotion is one of the main reasons why people engage and interact with music [1]. Songs can express our inner feelings, produce goosebumps, bring us to tears, share an emotional state with a composer or performer, or trigger specific memories. Interest in a deeper understanding of the relationship between music and emotion has motivated researchers from various areas of knowledge for decades [2], including computational researchers. Imagine an algorithm capable of predicting the emotions that a listener perceives in a musical piece, or one that dynamically generates music that adapts to the mood of a conversation in a film - a particularly fascinating and provocative idea. These algorithms typify music emotion recognition (MER), a computational task that attempts to automatically recognize either the emotional content in music or the emotions induced by music to the listener [3]. To do so, emotionally relevant features are extracted from music. The features are processed, evaluated, and then associated with certain emotions. MER is one of the most challenging high-level music description problems in music information retrieval (MIR), an interdisciplinary research field that focuses on the development of computational systems to help humans better understand music collections. MIR integrates concepts and methodologies from several disciplines, including music theory, music psychology, neuroscience, signal processing, and machine learning.  {\copyright} 1991-2012 IEEE.},
	author = {Gomez-Canon, Juan Sebastia and Cano, Estefania and Eerola, Tuomas and Herrera, Perfecto and Hu, Xiao and Yang, Yi-Hsuan and Gomez, Emilia},
	doi = {10.1109/MSP.2021.3106232},
	journal = {IEEE Signal Processing Magazine},
	keywords = {Behavioral research; Computation theory; Search engines; Signal processing; Speech recognition; Computational task; Context-sensitive applications; Emotion recognition; Emotional state; Inner feelings; Music and emotions; Music emotions; Music information retrieval; Musical pieces; Relevant features; Music},
	note = {Cited by: 40; All Open Access, Green Open Access},
	number = {6},
	pages = {106 -- 114},
	publication_stage = {Final},
	source = {Scopus},
	title = {Music Emotion Recognition: Toward new, robust standards in personalized and context-sensitive applications},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118593358&doi=10.1109%2fMSP.2021.3106232&partnerID=40&md5=65ab889060f2c2b20783e6691066b4d1},
	volume = {38},
	year = {2021},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118593358&doi=10.1109%2fMSP.2021.3106232&partnerID=40&md5=65ab889060f2c2b20783e6691066b4d1},
	bdsk-url-2 = {https://doi.org/10.1109/MSP.2021.3106232}}

@article{Hizlisoy2021760,
	abstract = {In this paper, we propose an approach for music emotion recognition based on convolutional long short term memory deep neural network (CLDNN) architecture. In addition, we construct a new Turkish emotional music database composed of 124 Turkish traditional music excerpts with a duration of 30 s each and the performance of the proposed approach is evaluated on the constructed database. We utilize features obtained by feeding convolutional neural network (CNN) layers with log-mel filterbank energies and mel frequency cepstral coefficients (MFCCs) in addition to standard acoustic features. Classification results show that the best performance is obtained when the new feature set is combined with the standard features using the long short term memory (LSTM) + deep neural network (DNN) classi fier. The overall accuracy of 99.19% is obtained using the proposed system with 10 fold cross-validation. Specifically, 6.45 points improvement is achieved. Additionally, the results also show that the LSTM + DNN classifier yields 1.61, 1.61 and 3.23 points improvements in music emotion recognition accuracies compared to k-nearest neighbor (k-NN), support vector machine (SVM), and Random Forest classifiers, respectively. {\copyright} 2020 Karabuk University},
	author = {Hizlisoy, Serhat and Yildirim, Serdar and Tufekci, Zekeriya},
	author_keywords = {Convolutional long short term memory deep neural networks; Music emotion recognition; Turkish emotional music database},
	doi = {10.1016/j.jestch.2020.10.009},
	journal = {Engineering Science and Technology, an International Journal},
	note = {Cited by: 85; All Open Access, Gold Open Access},
	number = {3},
	pages = {760 -- 767},
	publication_stage = {Final},
	source = {Scopus},
	title = {Music emotion recognition using convolutional long short term memory deep neural networks},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096857185&doi=10.1016%2fj.jestch.2020.10.009&partnerID=40&md5=a6d9948932df1065662495509f77bfe5},
	volume = {24},
	year = {2021},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096857185&doi=10.1016%2fj.jestch.2020.10.009&partnerID=40&md5=a6d9948932df1065662495509f77bfe5},
	bdsk-url-2 = {https://doi.org/10.1016/j.jestch.2020.10.009}}

@article{Chin2018541,
	abstract = {Computationally modeling the affective content of music has been intensively studied in recent years because of its wide applications in music retrieval and recommendation. Although significant progress has been made, this task remains challenging due to the difficulty in properly characterizing the emotion of a music piece. Music emotion perceived by people is subjective by nature and thus complicates the process of collecting the emotion annotations as well as developing the predictive model. Instead of assuming people can reach a consensus on the emotion of music, in this work we propose a novel machine learning approach that characterizes the music emotion as a probability distribution in the valence-Arousal (VA) emotion space, not only tackling the subjectivity but also precisely describing the emotions of a music piece. Specifically, we represent the emotion of a music piece as a probability density function (PDF) in the VA space via kernel density estimation from human annotations. To associate emotion with the audio features extracted from music pieces, we learn the combination coefficients by optimizing some objective functions of audio features, and then predict the emotion of an unseen piece by linearly combining the PDFs of the training pieces with the coefficients. Several algorithms for learning the coefficients are studied. Evaluations on the NTUMIR and MediaEval 2013 datasets validate the effectiveness of the proposed methods in predicting the probability distributions of emotion from audio features. We also demonstrate how to use the proposed approach in emotion-based music retrieval. {\copyright} 2010-2012 IEEE.},
	author = {Chin, Yu-Hao and Wang, Jia-Ching and Wang, Ju-Chiang and Yang, Yi-Hsuan},
	author_keywords = {Emotion in music; emotion recognition from audio; predictive model and algorithm; valence-Arousal space},
	doi = {10.1109/TAFFC.2016.2628794},
	journal = {IEEE Transactions on Affective Computing},
	keywords = {Audio acoustics; Behavioral research; Computer music; Forecasting; Learning algorithms; Learning systems; Probability density function; Emotion in music; Emotion recognition; Kernel Density Estimation; Machine learning approaches; Objective functions; Predictive modeling; Probability density function (pdf); valence-Arousal space; Probability distributions},
	note = {Cited by: 6},
	number = {4},
	pages = {541 -- 549},
	publication_stage = {Final},
	source = {Scopus},
	title = {Predicting the Probability Density Function of Music Emotion Using Emotion Space Mapping},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058048732&doi=10.1109%2fTAFFC.2016.2628794&partnerID=40&md5=a50a624a1dd2a6e53d5920e2499b1aca},
	volume = {9},
	year = {2018},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058048732&doi=10.1109%2fTAFFC.2016.2628794&partnerID=40&md5=a50a624a1dd2a6e53d5920e2499b1aca},
	bdsk-url-2 = {https://doi.org/10.1109/TAFFC.2016.2628794}}

@article{Beveridge2018411,
	abstract = {The voice plays a crucial role in expressing emotion in popular music. However, the importance of the voice in this context has not been systematically assessed. This study investigates the emotional effect of vocal features in popular music. In particular, it focuses on nonverbal characteristics, including vocal melody and rhythm. To determine the efficacy of these features, they are used to construct a computational Music Emotion Recognition (MER) system. The system is based on the circumplex model that expresses emotion in terms of arousal and valence. Two independent studies were used to develop the system. The first study established models for predicting arousal and valence based on a range of acoustical and nonverbal vocal features. The second study was used for independent validation of these models. Results show that features describing rhythmic qualities of the vocal line produce emotion models with a high level of generalizability. In particular these models reliably predict emotional valence, a well-known issue in existing Music Emotion Recognition systems. {\copyright} 2017, {\copyright} The Author(s) 2017.},
	author = {Beveridge, Scott and Knox, Don},
	author_keywords = {emotion; popular music; technology; voice},
	doi = {10.1177/0305735617713834},
	journal = {Psychology of Music},
	note = {Cited by: 5; All Open Access, Green Open Access},
	number = {3},
	pages = {411 -- 423},
	publication_stage = {Final},
	source = {Scopus},
	title = {Popular music and the role of vocal melody in perceived emotion},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044945735&doi=10.1177%2f0305735617713834&partnerID=40&md5=3744ad82265451b4157f28433eb61e30},
	volume = {46},
	year = {2018},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044945735&doi=10.1177%2f0305735617713834&partnerID=40&md5=3744ad82265451b4157f28433eb61e30},
	bdsk-url-2 = {https://doi.org/10.1177/0305735617713834}}

@article{Xu20211,
	abstract = {Melody and lyrics, reflecting two unique human cognitive abilities, are usually combined in music to convey emotions. Although psychologists and computer scientists have made considerable progress in revealing the association between musical structure and the perceived emotions of music, the features of lyrics are relatively less discussed. Using linguistic inquiry and word count (LIWC) technology to extract lyric features in 2,372 Chinese songs, this study investigated the effects of LIWC-based lyric features on the perceived arousal and valence of music. First, correlation analysis shows that, for example, the perceived arousal of music was positively correlated with the total number of lyric words and the mean number of words per sentence and was negatively correlated with the proportion of words related to the past and insight. The perceived valence of music was negatively correlated with the proportion of negative emotion words. Second, we used audio and lyric features as inputs to construct music emotion recognition (MER) models. The performance of random forest regressions reveals that, for the recognition models of perceived valence, adding lyric features can significantly improve the prediction effect of the model using audio features only; for the recognition models of perceived arousal, lyric features are almost useless. Finally, by calculating the feature importance to interpret the MER models, we observed that the audio features played a decisive role in the recognition models of both perceived arousal and perceived valence. Unlike the uselessness of the lyric features in the arousal recognition model, several lyric features, such as the usage frequency of words related to sadness, positive emotions, and tentativeness, played important roles in the valence recognition model. {\copyright} Copyright 2021 Xu et al. Distributed under Creative Commons CC-BY 4.0},
	author = {Xu, Liang and Sun, Zaoyi and Wen, Xin and Huang, Zhengxi and Chao, Chi-ju and Xu, Liuchang},
	author_keywords = {Audio signal processing; Chinese pop song; LIWC; Lyric feature extraction; Music emotion recognition},
	doi = {10.7717/peerj-cs.785},
	journal = {PeerJ Computer Science},
	keywords = {Audio acoustics; Audio signal processing; Behavioral research; Decision trees; Machine learning; Speech recognition; Audio features; Chinese pop song; Emotion recognition; Features extraction; Human cognitive abilities; Linguistic inquiry and word count; Lyric feature extraction; Music emotion recognition; Music emotions; Recognition models; Music},
	note = {Cited by: 5; All Open Access, Gold Open Access, Green Open Access},
	pages = {1 -- 23},
	publication_stage = {Final},
	source = {Scopus},
	title = {Using machine learning analysis to interpret the relationship between music emotion and lyric features},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122006685&doi=10.7717%2fpeerj-cs.785&partnerID=40&md5=5cb11dff1d360a27f98a2cf9832a0f0f},
	volume = {7},
	year = {2021},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122006685&doi=10.7717%2fpeerj-cs.785&partnerID=40&md5=5cb11dff1d360a27f98a2cf9832a0f0f},
	bdsk-url-2 = {https://doi.org/10.7717/peerj-cs.785}}

@article{Cunningham2021637,
	abstract = {The field of Music Emotion Recognition has become and established research sub-domain of Music Information Retrieval. Less attention has been directed towards the counterpart domain of Audio Emotion Recognition, which focuses upon detection of emotional stimuli resulting from non-musical sound. By better understanding how sounds provoke emotional responses in an audience, it may be possible to enhance the work of sound designers. The work in this paper uses the International Affective Digital Sounds set. A total of 76 features are extracted from the sounds, spanning the time and frequency domains. The features are then subjected to an initial analysis to determine what level of similarity exists between pairs of features measured using Pearson's r correlation coefficient before being used as inputs to a multiple regression model to determine their weighting and relative importance. The features are then used as the input to two machine learning approaches: regression modelling and artificial neural networks in order to determine their ability to predict the emotional dimensions of arousal and valence. It was found that a small number of strong correlations exist between the features and that a greater number of features contribute significantly to the predictive power of emotional valence, rather than arousal. Shallow neural networks perform significantly better than a range of regression models and the best performing networks were able to account for 64.4% of the variance in prediction of arousal and 65.4% in the case of valence. These findings are a major improvement over those encountered in the literature. Several extensions of this research are discussed, including work related to improving data sets as well as the modelling processes. {\copyright} 2020, The Author(s).},
	author = {Cunningham, Stuart and Ridley, Harrison and Weinel, Jonathan and Picking, Richard},
	author_keywords = {Affect; Arousal; Audio emotion recognition; Audio features; Emotion; IADS; Neural networks; Regression; Valence},
	doi = {10.1007/s00779-020-01389-0},
	journal = {Personal and Ubiquitous Computing},
	keywords = {Audio acoustics; Learning systems; Regression analysis; Speech recognition; Supervised learning; Correlation coefficient; Emotion recognition; Emotional dimensions; Multiple regression model; Music information retrieval; Regression modelling; Supervised machine learning; Time and frequency domains; Neural networks},
	note = {Cited by: 29; All Open Access, Green Open Access, Hybrid Gold Open Access},
	number = {4},
	pages = {637 -- 650},
	publication_stage = {Final},
	source = {Scopus},
	title = {Supervised machine learning for audio emotion recognition: Enhancing film sound design using audio features, regression models and artificial neural networks},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084087695&doi=10.1007%2fs00779-020-01389-0&partnerID=40&md5=bf41bee7590e3e61440ec10089e35937},
	volume = {25},
	year = {2021},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084087695&doi=10.1007%2fs00779-020-01389-0&partnerID=40&md5=bf41bee7590e3e61440ec10089e35937},
	bdsk-url-2 = {https://doi.org/10.1007/s00779-020-01389-0}}

@article{Kopylov2020907,
	abstract = {Separation of pre-recorded messages (Interactive Voice Response, IVR) from live speech fragments in real-time plays a significant role in speech emotion recognition (SER) systems, unwanted calls filtering, automatic detection of answering machine responses, reduction of stored record sizes, voice mail spam filtration, etc. The problem complexity is that, unlike with silent, music, and noise fragments studied by the conventional voice activity recognition (VAD), IVR usually contains speech. Three classifiers for live speech fragments detection in phone call records are considered: based on the support vector machine (SVM), gradient boosting (XGBoost) and convolutional neural network (CNN). The Geneva Minimalistic Acoustic Parameter Set for XGBoost and SVM, and log-spectrograms and gammatonegrams for CNN were used for feature representation of audio fragments. Experiments with a dataset of phone calls demonstrate comparable quality (around 0.96 according to the F1-averaged measure) of the considered algorithms with CNN having a advantage (0.98). {\copyright} 2020, Springer Science+Business Media, LLC, part of Springer Nature.},
	author = {Kopylov, Andrei and Seredin, Oleg and Filin, Andrei and Tyshkevich, Boris},
	author_keywords = {CNN; Gammatonegram; GeMAPS; Gradient boosting; IVR; Log-spectrogram; Speech analysis; SVM},
	doi = {10.1007/s10772-020-09754-3},
	journal = {International Journal of Speech Technology},
	keywords = {Audio acoustics; Convolutional neural networks; Personal communication systems; Real time systems; Support vector machines; Telephone sets; Acoustic parameters; Answering machines; Automatic Detection; Feature representation; Gradient boosting; Interactive voice response; Problem complexity; Speech emotion recognition; Speech recognition},
	note = {Cited by: 2},
	number = {4},
	pages = {907 -- 915},
	publication_stage = {Final},
	source = {Scopus},
	title = {Detection of interactive voice response (IVR) in phone call records},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096118230&doi=10.1007%2fs10772-020-09754-3&partnerID=40&md5=d0c9fd33fc018619fa7a8e4958da7fc3},
	volume = {23},
	year = {2020},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096118230&doi=10.1007%2fs10772-020-09754-3&partnerID=40&md5=d0c9fd33fc018619fa7a8e4958da7fc3},
	bdsk-url-2 = {https://doi.org/10.1007/s10772-020-09754-3}}

@article{Coutinho2017,
	abstract = {Music and speech exhibit striking similarities in the communication of emotions in the acoustic domain, in such a way that the communication of specific emotions is achieved, at least to a certain extent, by means of shared acoustic patterns. From an Affective Sciences points of view, determining the degree of overlap between both domains is fundamental to understand the shared mechanisms underlying such phenomenon. From a Machine learning perspective, the overlap between acoustic codes for emotional expression in music and speech opens new possibilities to enlarge the amount of data available to develop music and speech emotion recognition systems. In this article, we investigate time-continuous predictions of emotion (Arousal and Valence) in music and speech, and the Transfer Learning between these domains. We establish a comparative framework including intra- (i.e., models trained and tested on the same modality, either music or speech) and cross-domain experiments (i.e., models trained in one modality and tested on the other). In the cross-domain context, we evaluated two strategies---the direct transfer between domains, and the contribution of Transfer Learning techniques (feature-representation-transfer based on Denoising Auto Encoders) for reducing the gap in the feature space distributions. Our results demonstrate an excellent cross-domain generalisation performance with and without feature representation transfer in both directions. In the case of music, cross-domain approaches outperformed intra-domain models for Valence estimation, whereas for Speech intra-domain models achieve the best performance. This is the first demonstration of shared acoustic codes for emotional expression in music and speech in the time-continuous domain. {\copyright} 2017 Coutinho Schuller. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
	author = {Coutinho, Eduardo and Schuller, Bj{\"o}rn},
	doi = {10.1371/journal.pone.0179289},
	journal = {PLoS ONE},
	keywords = {Acoustics; Emotions; Female; Humans; Learning; Male; Music; Speech; arousal; human; human experiment; learning; model; music; prediction; speech; acoustics; emotion; female; learning; male; music; psychology; speech},
	note = {Cited by: 21; All Open Access, Gold Open Access, Green Open Access},
	number = {6},
	publication_stage = {Final},
	source = {Scopus},
	title = {Shared acoustic codes underlie emotional communication in music and speech---evidence from deep transfer learning},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021649937&doi=10.1371%2fjournal.pone.0179289&partnerID=40&md5=431c17e776a2fc6f937bdd7008b71812},
	volume = {12},
	year = {2017},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021649937&doi=10.1371%2fjournal.pone.0179289&partnerID=40&md5=431c17e776a2fc6f937bdd7008b71812},
	bdsk-url-2 = {https://doi.org/10.1371/journal.pone.0179289}}

@article{Zhang2021,
	abstract = {Emotional singing can affect vocal performance and the audience's engagement. Chinese universities use traditional training techniques for teaching theoretical and applied knowledge. Self-imagination is the predominant training method for emotional singing. Recently, virtual reality (VR) technologies have been applied in several fields for training purposes. In this empirical comparative study, a VR training task was implemented to elicit emotions from singers and further assist them with improving their emotional singing performance. The VR training method was compared against the traditional self-imagination method. By conducting a two-stage experiment, the two methods were compared in terms of emotions' elicitation and emotional singing performance. In the first stage, electroencephalographic (EEG) data were collected from the subjects. In the second stage, self-rating reports and third-party teachers' evaluations were collected. The EEG data were analyzed by adopting the max-relevance and min-redundancy algorithm for feature selection and the support vector machine (SVM) for emotion recognition. Based on the results of EEG emotion classification and subjective scale, VR can better elicit the positive, neutral, and negative emotional states from the singers than not using this technology (i.e., self-imagination). Furthermore, due to the improvement of emotional activation, VR brings the improvement of singing performance. The VR hence appears to be an effective approach that may improve and complement the available vocal music teaching methods. {\copyright} Copyright {\copyright} 2021 Zhang, Xu, Zhou, Wang, Fu, Xu and Zhang.},
	author = {Zhang, Jin and Xu, Ziming and Zhou, Yueying and Wang, Pengpai and Fu, Ping and Xu, Xijia and Zhang, Daoqiang},
	author_keywords = {electroencephalogram; emotion classification; self-imagination; singing emotion; virtual reality; vocal music teaching},
	doi = {10.3389/fnins.2021.693468},
	journal = {Frontiers in Neuroscience},
	keywords = {adult; article; comparative study; controlled study; electroencephalogram; emotion; feature selection; female; human; human experiment; imagination; male; music; singing; support vector machine; teacher; teaching; theoretical study; virtual reality},
	note = {Cited by: 3; All Open Access, Gold Open Access, Green Open Access},
	publication_stage = {Final},
	source = {Scopus},
	title = {An Empirical Comparative Study on the Two Methods of Eliciting Singers' Emotions in Singing: Self-Imagination and VR Training},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113442071&doi=10.3389%2ffnins.2021.693468&partnerID=40&md5=66bc319e8122006c5daa8f2feb92cc00},
	volume = {15},
	year = {2021},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113442071&doi=10.3389%2ffnins.2021.693468&partnerID=40&md5=66bc319e8122006c5daa8f2feb92cc00},
	bdsk-url-2 = {https://doi.org/10.3389/fnins.2021.693468}}

@article{Mohamed20214358,
	abstract = {Recently, the Recommender system is the most important research area with the advent of e-commerce and e-business on the web. Emotion-based music recovery will have extraordinary potential in catering nowadays, digital music archives quickly extending in the developing smartphones and ubiquitous environments. Many types of research are conducted to improve the music recommendation to users based on their emotions. Human emotions have much difficulty due to the subjective perception of emotions and accuracy challenges. In this paper, we need to solve the problem of recommending songs to the user based on his selection if it was bad, sad, or angry mood by using our system we will recommend to the user songs from pleasant mood to try changing him to the good mood and track if user listen to this song or scaped it. Our new algorithm, "Hybrid emotion-based music recommendation system," will recommend music to the next level, generating playlist which suits and matches your mood of listening to music. The user can try three choices to get the emotion by using face recognition, choosing three colors, and using the arousal map to select the emotion will appear to users then recommended songs according to his status we merge the output of the system to detect the right mood. Our new system has good novelty and diversity of songs recommended to users and changes the user's mood to the pleasure. At our experimental results We are using precision, recall and f-measure accuracy equations to calculate the effective of our system. To gain high results we apply different experiments detect users' emotions like using face only, colors, arousal map then let users select to types of emotion like face and colors or colors and arousal and finally apply hybrid emotions system. Every time we measure the accuracy of the results. Based on the experiments results using our new hybrid emotions model is best accuracy in surprised, anger, natural and relaxed. While user's emotion sadness using face. arousal map has high accuracy with happy emotions. {\copyright} 2021 Little Lion Scientific. All rights reserved.},
	author = {Mohamed, Marwa Hussien and Khafagy, Mohamed Helmy and Ibrahim, Mohamed Hasan and Elmenshawy, Khaled and Fadlallah, Haitham Rizk},
	author_keywords = {Collaborative filtering; Content-based filtering; Emotions; Face recognition; Recommender system},
	journal = {Journal of Theoretical and Applied Information Technology},
	note = {Cited by: 1},
	number = {17},
	pages = {4358 -- 4375},
	publication_stage = {Final},
	source = {Scopus},
	title = {Music recommendation system used emotions to track and change negative users' mood},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115224058&partnerID=40&md5=b6a6ab9b6fadfa2baf8f9e46ed5b8e4e},
	volume = {99},
	year = {2021},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115224058&partnerID=40&md5=b6a6ab9b6fadfa2baf8f9e46ed5b8e4e}}

@article{Nawaz2020910,
	abstract = {EEG-based emotion recognition is a challenging and active research area in affective computing. We used three-dimensional (arousal, valence and dominance) model of emotion to recognize the emotions induced by music videos. The participants watched a video (1 min long) while their EEG was recorded. The main objective of the study is to identify the features that can best discriminate the emotions. Power, entropy, fractal dimension, statistical features and wavelet energy are extracted from the EEG signals. The effects of these features are investigated and the best features are identified. The performance of the two feature selection methods, Relief based algorithm and principle component analysis (PCA), is compared. PCA is adopted because of its improved performance and the efficacies of the features are validated using support vector machine, K-nearest neighbors and decision tree classifiers. Our system achieves an overall best classification accuracy of 77.62%, 78.96% and 77.60% for valence, arousal and dominance respectively. Our results demonstrated that time-domain statistical characteristics of EEG signals can efficiently discriminate different emotional states. Also, the use of three-dimensional emotion model is able to classify similar emotions that were not correctly classified by two-dimensional model (e.g. anger and fear). The results of this study can be used to support the development of real-time EEG-based emotion recognition systems. {\copyright} 2020 Nalecz Institute of Biocybernetics and Biomedical Engineering of the Polish Academy of Sciences},
	author = {Nawaz, Rab and Cheah, Kit Hwa and Nisar, Humaira and Yap, Vooi Voon},
	author_keywords = {3D emotion model; EEG; Emotion recognition; Machine learning},
	doi = {10.1016/j.bbe.2020.04.005},
	journal = {Biocybernetics and Biomedical Engineering},
	keywords = {adult; anger; article; controlled study; decision tree; drug efficacy; electroencephalogram; entropy; fear; feature extraction; feature selection; fractal analysis; human; k nearest neighbor; music; support vector machine; videorecording},
	note = {Cited by: 115},
	number = {3},
	pages = {910 -- 926},
	publication_stage = {Final},
	source = {Scopus},
	title = {Comparison of different feature extraction methods for EEG-based emotion recognition},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085271984&doi=10.1016%2fj.bbe.2020.04.005&partnerID=40&md5=419431f6278c0facfda1d56fffc7faf7},
	volume = {40},
	year = {2020},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085271984&doi=10.1016%2fj.bbe.2020.04.005&partnerID=40&md5=419431f6278c0facfda1d56fffc7faf7},
	bdsk-url-2 = {https://doi.org/10.1016/j.bbe.2020.04.005}}

@article{Hsiao201714,
	abstract = {Traditionally, stage lighting regulations have required that professionally trained technicians operate the lighting equipment; however, contemporary demands for higher-quality performances require more preparation before a performance. Thus, technicians or club DJs now spend double to triple the time previously required before a show on matching the lighting control sequence musical instrument digital interface (MIDI) with the music, which is very time consuming. Thus, a methodology for automatic stage-lighting regulation would be very useful. Recently, the development of music emotion recognition (MER) and neural network algorithms has progressed significantly. Feelings related to music can be recognized and are even quantifiable using a supervised machine learning approach. In this study, a variety of music signal features from 2,087 song clips were captured, and then, a cross-validation test based on the support vector machine's (SVM) accuracy of classifying them into Thayer's emotion plane was applied to the main features related to music emotions, in order to produce linear quantitative values for describing music emotions. Music emotions and color preferences for stage lighting were subsequently studied. Using the experimental results, a support vector regression (SVR) model was trained to construct simulations. To increase the realism of the simulations, we developed an automatic music segment detection methodology based on music signal intensity to capture the different music strengths and feelings in each segment. Furthermore, music genres were studied as a factor for developing a comprehensive automatic stage lighting system based on feelings, genre, and the intensity of each segment of music. {\copyright} 2017 Elsevier Inc.},
	author = {Hsiao, Shih-Wen and Chen, Shih-Kai and Lee, Chu-Hsuan},
	author_keywords = {Automatic music segment detection; Automatic stage-lighting regulation; Lighting color regulation based on music emotions and genre; Music emotion recognition; Support vector regression (SVR)},
	doi = {10.1016/j.ins.2017.05.026},
	journal = {Information Sciences},
	keywords = {Electronic musical instruments; Learning systems; Speech recognition; Supervised learning; Support vector machines; Cross-validation tests; Music emotions; Music segments; Musical instrument digital interfaces; Neural network algorithm; Stage lighting; Supervised machine learning; Support vector regression (SVR); Lighting},
	note = {Cited by: 21},
	pages = {14 -- 35},
	publication_stage = {Final},
	source = {Scopus},
	title = {Methodology for stage lighting control based on music emotions},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019625713&doi=10.1016%2fj.ins.2017.05.026&partnerID=40&md5=273e15e75f4401f136d5826a8b70821e},
	volume = {412-413},
	year = {2017},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019625713&doi=10.1016%2fj.ins.2017.05.026&partnerID=40&md5=273e15e75f4401f136d5826a8b70821e},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2017.05.026}}

@article{Kim20211,
	abstract = {Emotion information represents a user's current emotional state and can be used in a variety of applications, such as cultural content services that recommend music according to user emotional states and user emotion monitoring. To increase user satisfaction, recommendation methods must understand and reflect user characteristics and circumstances, such as individual preferences and emotions. However, most recommendation methods do not reflect such characteristics accurately and are unable to increase user satisfaction. In this paper, six human emotions (neutral, happy, sad, angry, surprised, and bored) are broadly defined to consider user speech emotion information and recommend matching content. The ``genetic algorithms as a feature selection method'' (GAFS) algorithm was used to classify normalized speech according to speech emotion information. We used a support vector machine (SVM) algorithm and selected an optimal kernel function for recognizing the six target emotions. Performance evaluation results for each kernel function revealed that the radial basis function (RBF) kernel function yielded the highest emotion recognition accuracy of 86.98%. Additionally, content data (images and music) were classified based on emotion information using factor analysis, correspondence analysis, and Euclidean distance. Finally, speech information that was classified based on emotions and emotion information that was recognized through a collaborative filtering technique were used to predict user emotional preferences and recommend content that matched user emotions in a mobile application. {\copyright} 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
	author = {Kim, Tae-Yeun and Ko, Hoon and Kim, Sung-Hwan and Kim, Ho-Da},
	author_keywords = {Collaborative filtering; Emotion recognition; Speech emotion information; Support vector machine algorithm},
	doi = {10.3390/s21061997},
	journal = {Sensors},
	keywords = {Algorithms; Emotions; Humans; Music; Speech; Support Vector Machine; Classification (of information); Collaborative filtering; Genetic algorithms; Speech recognition; Support vector machines; Collaborative filtering techniques; Correspondence analysis; Emotional information; Feature selection methods; Individual preference; Radial Basis Function(RBF); Recommendation methods; Support vector machine algorithm; algorithm; emotion; human; music; speech; support vector machine; Information use},
	note = {Cited by: 29; All Open Access, Gold Open Access, Green Open Access},
	number = {6},
	pages = {1 -- 25},
	publication_stage = {Final},
	source = {Scopus},
	title = {Modeling of recommendation system based on emotional information and collaborative filtering},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102176699&doi=10.3390%2fs21061997&partnerID=40&md5=163e963e5ef610e4435953e01685a1f5},
	volume = {21},
	year = {2021},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102176699&doi=10.3390%2fs21061997&partnerID=40&md5=163e963e5ef610e4435953e01685a1f5},
	bdsk-url-2 = {https://doi.org/10.3390/s21061997}}

@article{Malheiro2018240,
	abstract = {This research addresses the role of lyrics in the music emotion recognition process. Our approach is based on several state of the art features complemented by novel stylistic, structural and semantic features. To evaluate our approach, we created a ground truth dataset containing 180 song lyrics, according to Russell's emotion model. We conduct four types of experiments: regression and classification by quadrant, arousal and valence categories. Comparing to the state of the art features (ngrams-baseline), adding other features, including novel features, improved the F-measure from 69.9, 82.7 and 85.6 percent to 80.1, 88.3 and 90 percent, respectively for the three classification experiments. To study the relation between features and emotions (quadrants) we performed experiments to identify the best features that allow to describe and discriminate each quadrant. To further validate these experiments, we built a validation set comprising 771 lyrics extracted from the AllMusic platform, having achieved 73.6 percent F-measure in the classification by quadrants. We also conducted experiments to identify interpretable rules that show the relation between features and emotions and the relation among features. Regarding regression, results show that, comparing to similar studies for audio, we achieve a similar performance for arousal and a much better performance for valence. {\copyright} 2010-2012 IEEE.},
	author = {Malheiro, Ricardo and Panda, Renato and Gomes, Paulo and Paiva, Rui Pedro},
	author_keywords = {Affective computing; Affective computing applications; Music retrieval and generation; Natural language processing; Recognition of group emotion},
	doi = {10.1109/TAFFC.2016.2598569},
	journal = {IEEE Transactions on Affective Computing},
	keywords = {Natural language processing systems; Semantics; Affective Computing; Emotion modeling; Ground-truth dataset; Group emotions; Interpretable rules; Music retrieval; Relevant features; Semantic features; Regression analysis},
	note = {Cited by: 42; All Open Access, Green Open Access},
	number = {2},
	pages = {240 -- 254},
	publication_stage = {Final},
	source = {Scopus},
	title = {Emotionally-relevant features for classification and regression of music lyrics},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047880602&doi=10.1109%2fTAFFC.2016.2598569&partnerID=40&md5=d8a4e5aabc1339f7a57f9f4f03d55d5d},
	volume = {9},
	year = {2018},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047880602&doi=10.1109%2fTAFFC.2016.2598569&partnerID=40&md5=d8a4e5aabc1339f7a57f9f4f03d55d5d},
	bdsk-url-2 = {https://doi.org/10.1109/TAFFC.2016.2598569}}

@article{Liu2021,
	abstract = {This study explored the behavioral and neural correlates of mindfulness meditation improvement in musical aesthetic emotion processing (MAEP) in young adults, using the revised across‐modal priming paradigm. Sixty‐two participants were selected from 652 college students who assessed their mindfulness traits using the Mindful Attention Awareness Scale (MAAS). According to the 27% ratio of the high and low total scores, participants were divided into two subgroups: high trait group (n =31) and low trait group (n =31). Participants underwent facial recognition and emotional arousal tasks while listening to music, and simultaneously recorded event‐related potentials (ERPs). The N400, P3, and late positive component (LPC) were investigated. The behavioral results showed that mindfulness meditation improved executive control abilities in emotional face processing and effectively regulated the emotional arousal of repeated listening to familiar music among young adults. These improvements were associated with positive changes in key neural signatures of facial recognition (smaller P3 and larger LPC effects) and emotional arousal (smaller N400 and larger LPC effects). Our results show that P3, N400, and LPC are important neural markers for the improvement of executive control and regulating emotional arousal in musical aesthetic emotion processing, providing new evidence for exploring attention training and emotional processing. We revised the affecting priming paradigm and E‐prime 3.0 procedure to fulfill the simultaneous measurement of music listening and experimental tasks and provide a new experimental paradigm to simultaneously detect the behavioral and neural correlates of mindfulnessbased musical aesthetic processing. {\copyright} 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
	author = {Liu, Xiaolin and Shi, Huijuan and Liu, Yong and Yuan, Hong and Zheng, Maoping},
	author_keywords = {Aesthetic emotion; ERPs; Executive control; Mindfulness meditation; Musical aesthetics},
	doi = {10.3390/ijerph182413045},
	journal = {International Journal of Environmental Research and Public Health},
	keywords = {Electroencephalography; Emotions; Esthetics; Evoked Potentials; Female; Humans; Male; Meditation; Mindfulness; Music; Young Adult; adult; esthetics; human behavior; music; psychology; adult; Article; biological trait; college student; emotionality; event related potential; executive function; facial recognition; female; hearing; human; male; Mindful Attention Awareness Scale; mindfulness meditation; musical aesthetic emotion processing; nervous system parameters; young adult; electroencephalography; emotion; esthetics; evoked response; meditation; mindfulness; music},
	note = {Cited by: 2; All Open Access, Gold Open Access, Green Open Access},
	number = {24},
	publication_stage = {Final},
	source = {Scopus},
	title = {Mindfulness meditation improves musical aesthetic emotion processing in young adults},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120827794&doi=10.3390%2fijerph182413045&partnerID=40&md5=84e2f017eaa49fa0c757a864bd899f72},
	volume = {18},
	year = {2021},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120827794&doi=10.3390%2fijerph182413045&partnerID=40&md5=84e2f017eaa49fa0c757a864bd899f72},
	bdsk-url-2 = {https://doi.org/10.3390/ijerph182413045}}

@article{Teo2019162,
	abstract = {This paper presents a deep learning approach to emotion recognition as applied to virtual reality and music predictive analytics. Firstly, it investigates the deep parameter tuning of the multi-hidden layer neural networks, which are also commonly referred to simply as deep networks that are used to conduct emotion detection in virtual reality (VR)-electroencephalography (EEG) predictive analytics. Deep networks have been studied extensively over the last decade and have shown to be among the most accurate methods for predictive analytics in image recognition and speech processing domains. However, most predictive analytics deep network studies focus on the shallow parameter tuning when attempting to boost prediction accuracies, which includes deep network tuning parameters such as number of hidden layers, number of hidden nodes per hidden layer and the types of activation functions used in the hidden nodes. Much less effort has been put into investigating the tuning of deep parameters such as input dropout ratios, L1 (lasso) regularization and L2 (ridge regularization) parameters of the deep networks. As such, the goal of this study is to perform a parameter tuning investigation on these deep parameters of the deep networks for predicting emotions in a virtual reality environment using electroencephalography (EEG) signal obtained when the user is exposed to immersive content. The results show that deep tuning of deep networks in VR-EEG can improve the accuracies of predicting emotions. The best emotion prediction accuracy was improved to over 96% after deep tuning was conducted on the deep network parameters of input dropout ratio, L1 and L2 regularization parameters. Secondly, it investigates a similar possible approach when applied to 4-quadrant music emotion recognition. Recent studies have been characterizing music based on music genres and various classification techniques have been used to achieve the best accuracy rate. Several researches on deep learning have shown outstanding results in relation to dimensional music emotion recognition. Yet, there is no concrete and concise description to express music. In regards to this research gap, a research using more detailed metadata on two-dimensional emotion annotations based on the Russell's model is conducted. Rather than applying music genres or lyrics into machine learning algorithm to MER, higher representation of music information, acoustic features are used. In conjunction with the four classes classification problem, an available dataset named AMG1608 is feed into a training model built from deep neural network. The dataset is first preprocessed to get full access of variables before any machine learning is done. The classification rate is then collected by running the scripts in R environment. The preliminary result showed a classification rate of 46.0%. Experiments on architecture and hyper-parameter tuning as well as instance reduction were designed and conducted. The tuned parameters that increased the accuracy for deep learners were hidden layer architecture, number of epochs, instance reduction, input dropout ratio and ℓ1 and ℓ2 regularization. The final best prediction accuracy obtained was 61.7%, giving an overall improvement of more than 15% for music emotion recognition which are based purely on the music's acoustical features. {\copyright} BEIESP.},
	author = {Teo, Jason and Chia, Jia Tian and Lee, Jie Yu},
	author_keywords = {Acoustic features; Deep learning; Electroencephalography; Emotion classification; Music emotion recognition; Neuroinformatics; Virtual reality},
	doi = {10.35940/ijrte.B1030.0782S219},
	journal = {International Journal of Recent Technology and Engineering},
	note = {Cited by: 1; All Open Access, Bronze Open Access},
	number = {2 Special Issue 2},
	pages = {162 -- 170},
	publication_stage = {Final},
	source = {Scopus},
	title = {Deep learning for emotion recognition in affective virtual reality and music applications},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071251566&doi=10.35940%2fijrte.B1030.0782S219&partnerID=40&md5=55349bfff59ab4f8778279d327f66e5a},
	volume = {8},
	year = {2019},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071251566&doi=10.35940%2fijrte.B1030.0782S219&partnerID=40&md5=55349bfff59ab4f8778279d327f66e5a},
	bdsk-url-2 = {https://doi.org/10.35940/ijrte.B1030.0782S219}}

@article{Dufour2021666,
	abstract = {The two commonly accepted models of affect used in affective computing are categorical and two-dimensional. However, categorical models are limited to datasets that only contain music for which human annotators fully agree upon, while two-dimensional models use descriptors to which users may not relate to (e.g., Valence and Arousal). This paper explores the hypothesis that the music emotion problem is circular, and shows how circular models can be used for automatic music emotion recognition. This hypothesis is tested through experiments on the two commonly accepted models of affect, as well as on an original circular model proposed by the authors. First, an original dataset was assembled and annotated as a way to investigate agreement among annotators. Then, polygonal approximations of circular regression are proposed as a practical method to investigate whether the circularity of the annotations can be exploited. Experiments with different polygons demonstrate consistent improvements over the categorical model on a dataset containing musical extracts for which the human annotators did not fully agree upon. Finally, a proposed multi-tagging strategy based on the circular predictions is put forward as a pragmatic method to automatically annotate music based on the circular models.  {\copyright} 2010-2012 IEEE.},
	author = {Dufour, Isabelle and Tzanetakis, George},
	author_keywords = {circular annotations; emotional model; Music emotion recognition},
	doi = {10.1109/TAFFC.2018.2885744},
	journal = {IEEE Transactions on Affective Computing},
	keywords = {Computer music; Job analysis; Speech recognition; Circular Annotations; Computational model; Emotion recognition; Emotional models; Mood; Music; Music emotions; Predictive models; Task analysis; Predictive analytics},
	note = {Cited by: 6},
	number = {3},
	pages = {666 -- 681},
	publication_stage = {Final},
	source = {Scopus},
	title = {Using Circular Models to Improve Music Emotion Recognition},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058188888&doi=10.1109%2fTAFFC.2018.2885744&partnerID=40&md5=5237f9b3b026d9356565a7f0dd3fc0b8},
	volume = {12},
	year = {2021},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058188888&doi=10.1109%2fTAFFC.2018.2885744&partnerID=40&md5=5237f9b3b026d9356565a7f0dd3fc0b8},
	bdsk-url-2 = {https://doi.org/10.1109/TAFFC.2018.2885744}}

@article{Wang2021,
	abstract = {To implement a mature music composition model for Chinese users, this paper analyzes the music composition and emotion recognition of composition content through big data technology and Neural Network (NN) algorithm. First, through a brief analysis of the current music composition style, a new Music Composition Neural Network (MCNN) structure is proposed, which adjusts the probability distribution of the Long Short-Term Memory (LSTM) generation network by constructing a reasonable Reward function. Meanwhile, the rules of music theory are used to restrict the generation of music style and realize the intelligent generation of specific style music. Afterward, the generated music composition signal is analyzed from the time-frequency domain, frequency domain, nonlinearity, and time domain. Finally, the emotion feature recognition and extraction of music composition content are realized. Experiments show that: When the iteration times of the function increase, the number of weight parameter adjustments and learning ability will increase, and thus the accuracy of the model for music composition can be greatly improved. Meanwhile, when the iteration times increases, the loss function will decrease slowly. Moreover, the music composition generated through the proposed model includes the following four aspects: Sadness, joy, loneliness, and relaxation. The research results can promote music composition intellectualization and impacts traditional music composition mode. {\copyright} 2021 Yu Wang.},
	author = {Wang, Yu},
	doi = {10.1155/2021/5398922},
	journal = {Computational Intelligence and Neuroscience},
	keywords = {Algorithms; Big Data; Emotions; Music; Neural Networks, Computer; Technology; Big data; Frequency domain analysis; Iterative methods; Long short-term memory; Music; Probability distributions; Time domain analysis; Composition content; Composition modeling; Data technologies; Emotion recognition; Iteration time; Music composition; Music emotions; Neural networks algorithms; Paper analysis; Technology network; algorithm; emotion; music; technology; Speech recognition},
	note = {Cited by: 3; All Open Access, Gold Open Access, Green Open Access},
	publication_stage = {Final},
	source = {Scopus},
	title = {Music Composition and Emotion Recognition Using Big Data Technology and Neural Network Algorithm},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122382786&doi=10.1155%2f2021%2f5398922&partnerID=40&md5=8526c913cb2f0b1c8962a1b7da8afb30},
	volume = {2021},
	year = {2021},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122382786&doi=10.1155%2f2021%2f5398922&partnerID=40&md5=8526c913cb2f0b1c8962a1b7da8afb30},
	bdsk-url-2 = {https://doi.org/10.1155/2021/5398922}}

@article{He2020,
	abstract = {Emotion recognition is helpful for human to enhance self-awareness and respond appropriately towards the happenings around them. Due to the complexity and diversity of emotions, EEG-based emotion recognition is still a challenging task in pattern recognition. In order to recognize diverse emotions, we propose a novel firefly integrated optimization algorithm (FIOA) in this paper. It can simultaneously accomplish multiple tasks, i.e. the optimal feature selection, parameter setting and classifier selection according to different EEG-based emotion datasets. The FIOA utilizes a ranking probability objection function to guarantee the high accuracy recognition with less features. Moreover, the hybrid encoding expression and the dual updating strategy are developed in the FIOA so as to realize the optimal selection of feature subset and classifier without stagnating in the local optimum. In addition to the public DEAP datasets, we also conducted an EEG-based music emotion experiment involving 20 subjects for the validation of the proposed FIOA. After filtering and segmentation, three categories of features were extracted from every EEG signal. Then FIOA was applied to every subject dataset for two pattern recognition of emotions. The results show that the FIOA can automatically find the optimal features, parameter and classifier for different emotion datasets, which greatly reduces the artificial selection workload. Furthermore, comparing with the binary particle swarm optimization (PSObinary) and the binary firefly (FAbinary), the FIOA can achieve the higher accuracy with less features in the emotion recognition. {\copyright} 2020 Elsevier B.V.},
	author = {He, Hong and Tan, Yonghong and Ying, Jun and Zhang, Wuxiong},
	author_keywords = {Classification; EEG; Emotion recognition; Feature selection; Firefly algorithm},
	doi = {10.1016/j.asoc.2020.106426},
	journal = {Applied Soft Computing Journal},
	keywords = {Bioluminescence; Biomedical signal processing; Feature extraction; Particle swarm optimization (PSO); Speech recognition; Artificial selection; Binary particle swarm optimization; Classifier selection; Emotion recognition; Integrated optimization; Objection functions; Optimal feature selections; Recognition of emotion; Classification (of information)},
	note = {Cited by: 48},
	publication_stage = {Final},
	source = {Scopus},
	title = {Strengthen EEG-based emotion recognition using firefly integrated optimization algorithm},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085937163&doi=10.1016%2fj.asoc.2020.106426&partnerID=40&md5=536ef66e1584b040e63724f7083e152d},
	volume = {94},
	year = {2020},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085937163&doi=10.1016%2fj.asoc.2020.106426&partnerID=40&md5=536ef66e1584b040e63724f7083e152d},
	bdsk-url-2 = {https://doi.org/10.1016/j.asoc.2020.106426}}

@article{Vidas2020906,
	abstract = {Previous research on the development of emotion recognition in music has focused on classical, rather than popular music. Such research does not consider the impact of lyrics on judgements of emotion in music, impact that may differ throughout development. We had 172 children, adolescents, and adults (7- to 20-year-olds) judge emotions in popular music. In song excerpts, the melody of the music and the lyrics had either congruent valence (e.g. happy lyrics and melody), or incongruent valence (e.g. scared lyrics, happy melody). We also examined participants' judgements of vocal bursts, and whether emotion identification was linked to emotion lexicon. Recognition of emotions in congruent music increased with age. For incongruent music, age was positively associated with judging the emotion in music by the melody. For incongruent music with happy or sad lyrics, younger participants were more likely to answer with the emotion of the lyrics. For scared incongruent music, older adolescents were more likely to answer with the lyrics than older and younger participants. Age groups did not differ on their emotion lexicons, nor recognition of emotion in vocal bursts. Whether children use lyrics or melody to determine the emotion of popular music may depend on the emotion conveyed. {\copyright} 2019, {\copyright} 2019 Informa UK Limited, trading as Taylor & Francis Group.},
	author = {Vidas, Dianna and Calligeros, Renee and Nelson, Nicole L. and Dingle, Genevieve A.},
	author_keywords = {development; Emotion recognition; lyrics; music; vocal bursts},
	doi = {10.1080/02699931.2019.1700482},
	journal = {Cognition and Emotion},
	keywords = {Adolescent; Adult; Age Factors; Auditory Perception; Child; Cues; Emotions; Female; Humans; Judgment; Male; Music; Recognition, Psychology; Singing; Young Adult; adolescent; article; child; decision making; female; groups by age; human; human experiment; major clinical study; male; music; adult; age; association; emotion; hearing; music; physiology; psychology; singing; young adult},
	note = {Cited by: 5},
	number = {5},
	pages = {906 -- 919},
	publication_stage = {Final},
	source = {Scopus},
	title = {Development of emotion recognition in popular music and vocal bursts},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076441712&doi=10.1080%2f02699931.2019.1700482&partnerID=40&md5=932297b7b8bb8abcf0a1c2e75e1e8209},
	volume = {34},
	year = {2020},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076441712&doi=10.1080%2f02699931.2019.1700482&partnerID=40&md5=932297b7b8bb8abcf0a1c2e75e1e8209},
	bdsk-url-2 = {https://doi.org/10.1080/02699931.2019.1700482}}

@article{Chen20171409,
	abstract = {Personalizing a music emotion recognition model is needed because the perception of music emotion is highly subjective, but it is a time-consuming process. In this paper, we consider how to expedite the personalization process that begins with a general model trained offline using a general user base and progressively adapts the model to a music listener using the emotion annotations of the listener. Specifically, we focus on reducing the number of user annotations needed for the personalization. We investigate and evaluate four component tying methods: Single group tying, quadrantwise tying, hierarchical tying, and random tying. These methods aim to exploit the available annotations by identifying related model parameters on-the-fly and updating them jointly. In the evaluation, we use the AMG1608 dataset, which contains the clip-level valence-arousal emotion ratings of 1608 30-s music clips annotated by 665 listeners. Also, we use the acoustic emotion Gaussians model as the general model that uses a mixture of Gaussian components to learn the mapping between the acoustic feature space and the emotion space. The results show that the model adaptation with component tying requires only 10-20 personal annotations to obtain the same level of prediction accuracy as the baseline model adaptation method that uses 50 personal annotations without component tying. {\copyright} 2014 IEEE.},
	author = {Chen, Yu-An and Wang, Ju-Chiang and Yang, Yi-Hsuan and Chen, Homer H.},
	author_keywords = {Component tying; mixture model; model adaptation; music emotion recognition; personalization},
	doi = {10.1109/TASLP.2017.2693565},
	journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
	keywords = {Mixtures; Speech recognition; Component tying; Mixture model; Model Adaptation; Music emotions; Personalizations; Behavioral research},
	note = {Cited by: 14},
	number = {7},
	pages = {1409 -- 1420},
	publication_stage = {Final},
	source = {Scopus},
	title = {Component tying for mixture model adaptation in personalization of music emotion recognition},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020698269&doi=10.1109%2fTASLP.2017.2693565&partnerID=40&md5=7b78196199dc279e0537b2270c4915bb},
	volume = {25},
	year = {2017},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020698269&doi=10.1109%2fTASLP.2017.2693565&partnerID=40&md5=7b78196199dc279e0537b2270c4915bb},
	bdsk-url-2 = {https://doi.org/10.1109/TASLP.2017.2693565}}

@article{Nguyen20171246,
	abstract = {This paper proposes an emotion detection method using a combination of dimensional approach and categorical approach. Thayer's model is divided into discrete emotion sections based on the level of arousal and valence. The main objective of the method is to increase the number of detected emotions which is used for emotion visualization. To evaluate the suggested method, we conducted various experiments with supervised learning and feature selection strategies. We collected 300 music clips with emotions annotated by music experts. Two feature sets are employed to create two training models for arousal and valence dimensions of Thayer's model. Finally, 36 music emotions are detected by proposed method. The results showed that the suggested algorithm achieved the highest accuracy when using RandomForest classifier with 70% and 57.3% for arousal and valence, respectively. These rates are better than previous studies. {\copyright} 2017 Institute of Advanced Engineering and Science.},
	author = {Nguyen, Van Loi and Kim, Donglim and Ho, Van Phi and Lim, Younghwan},
	author_keywords = {Feature extraction; Music emotion recognition algorithm; Music information retrieval; Music mood detection},
	doi = {10.11591/ijece.v7i3.pp1246-1254},
	journal = {International Journal of Electrical and Computer Engineering},
	note = {Cited by: 14},
	number = {3},
	pages = {1246 -- 1254},
	publication_stage = {Final},
	source = {Scopus},
	title = {A new recognition method for visualizing music emotion},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021156960&doi=10.11591%2fijece.v7i3.pp1246-1254&partnerID=40&md5=c3ac2cfa0f3a859b3232ebd8875ba59b},
	volume = {7},
	year = {2017},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021156960&doi=10.11591%2fijece.v7i3.pp1246-1254&partnerID=40&md5=c3ac2cfa0f3a859b3232ebd8875ba59b},
	bdsk-url-2 = {https://doi.org/10.11591/ijece.v7i3.pp1246-1254}}

@article{Thammasan20161234,
	abstract = {Research on emotion recognition using electroencephalogram (EEG) of subjects listening to music has become more active in the past decade. However, previous works did not consider emotional oscillations within a single musical piece. In this research, we propose a continuous music-emotion recognition approach based on brainwave signals. While considering the subject-dependent and changing-over-time characteristics of emotion, our experiment included self-reporting and continuous emotion annotation in the arousal-valence space. Fractal dimension (FD) and power spectral density (PSD) approaches were adopted to extract informative features from raw EEG signals and then we applied emotion classification algorithms to discriminate binary classes of emotion. According to our experimental results, FD slightly outperformed PSD approach both in arousal and valence classification, and FD was found to have the higher correlation with emotion reports than PSD. In addition, continuous emotion recognition during music listening based on EEG was found to be an effective method for tracking emotional reporting oscillations and provides an opportunity to better understand human emotional processes. Copyright {\copyright} 2016 The Institute of Electronics, Information and Communication Engineers.},
	author = {Thammasan, Nattapong and Moriyama, Koichi and Fukui, Ken-Ichi and Numao, Masayuki},
	author_keywords = {Electroencephalogram; Emotion; Music},
	doi = {10.1587/transinf.2015EDP7251},
	journal = {IEICE Transactions on Information and Systems},
	keywords = {Biomedical signal processing; Finite difference method; Fractal dimension; Power spectral density; Spectral density; Speech recognition; Electro-encephalogram (EEG); Emotion; Emotion classification; Emotion recognition; Music; Music emotions; Power spectral densities (PSD); Time characteristics; Electroencephalography},
	note = {Cited by: 59; All Open Access, Bronze Open Access},
	number = {4},
	pages = {1234 -- 1241},
	publication_stage = {Final},
	source = {Scopus},
	title = {Continuous music-emotion recognition based on electroencephalogram},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962784447&doi=10.1587%2ftransinf.2015EDP7251&partnerID=40&md5=ac54eb86b38014587ec15bc13a9b8a5b},
	volume = {E99D},
	year = {2016},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962784447&doi=10.1587%2ftransinf.2015EDP7251&partnerID=40&md5=ac54eb86b38014587ec15bc13a9b8a5b},
	bdsk-url-2 = {https://doi.org/10.1587/transinf.2015EDP7251}}

@article{Zhao201680,
	abstract = {Affective computing (AC) is a new field of emotion research along with the development of computing technology and human-machine interaction technology. Emotion recognition is a crucial part of the AC research framework. Emotion recognition based on physiological signals provides richer information without deception than other techniques such as facial expression, tone of voice, and gestures. Many studies of emotion recognition have been conducted, but the classification accuracy is diverse due to variability in stimuli, emotion categories, devices, feature extraction and machine learning algorithms. This paper reviews all works that cited DEAP dataset (a public available dataset which uses music video to induce emotion and record EEG and peripheral physiological signals) and introduces detailed methods and algorithms on feature extraction, normalization, dimension reduction, emotion classification, and cross validation. Eventually, this work presents the application of AC on game development, multimedia production, interactive experience, and social network as well as the current limitations and the direction of future investigation. {\copyright} 2016, Science Press. All right reserved.},
	author = {Zhao, Guozhen and Song, Jinjing and Ge, Yan and Liu, Yongjin and Yao, Lin and Wen, Tao},
	author_keywords = {Electroencephalograph (EEG); Emotion recognition; Feature extraction; Machine learning; Peripheral physiological signal},
	doi = {10.7544/issn1000-1239.2016.20150636},
	journal = {Jisuanji Yanjiu yu Fazhan/Computer Research and Development},
	keywords = {Artificial intelligence; Big data; Classification (of information); Computer music; Electroencephalography; Extraction; Face recognition; Feature extraction; Human computer interaction; Learning algorithms; Learning systems; Multimedia services; Physiology; Speech recognition; Classification accuracy; Computing technology; Electroencephalograph (EEG); Emotion classification; Emotion recognition; Human machine interaction; Multimedia productions; Physiological signals; Biomedical signal processing},
	note = {Cited by: 26},
	number = {1},
	pages = {80 -- 92},
	publication_stage = {Final},
	source = {Scopus},
	title = {Advances in emotion recognition based on physiological big data},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957011355&doi=10.7544%2fissn1000-1239.2016.20150636&partnerID=40&md5=504e9ac209dbaaea95b4655e1ce679dd},
	volume = {53},
	year = {2016},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957011355&doi=10.7544%2fissn1000-1239.2016.20150636&partnerID=40&md5=504e9ac209dbaaea95b4655e1ce679dd},
	bdsk-url-2 = {https://doi.org/10.7544/issn1000-1239.2016.20150636}}

@article{Bai201674,
	abstract = {Music emotion recognition (MER) is a challenging field of studies that has been addressed in multiple disciplines such as cognitive science, physiology, psychology, musicology, and arts. In this paper, music emotions are modeled as a set of continuous variables composed of valence and arousal (VA) values based on the Valence-Arousal model. MER is formulated as a regression problem where 548 dimensions of music features were extracted and selected. A wide range of methods including multivariate adaptive regression spline, support vector regression (S VR), radial basis function, random forest regression (RFR), and regression neural networks are adopted to recognize music emotions. Experimental results show that these regression algorithms have led to good regression effect for MER. The optimal R2 statistics and VA values are 29.3% and 62.5%, respectively, which are obtained by the RFR and SVR algorithms in the relief feature space. Copyright {\copyright} 2016, IGI Global.},
	author = {Bai, Junjie and Feng, Lixiao and Peng, Jun and Shi, Jinliang and Luo, Kan and Li, Zuojin and Liao, Lu and Wang, Yingxu},
	author_keywords = {Emotion regression; Feature extraction; Machine learning; Music emotion recognition; Pattern recognition; Valence arousal model},
	doi = {10.4018/IJCINI.2016100104},
	journal = {International Journal of Cognitive Informatics and Natural Intelligence},
	keywords = {Artificial intelligence; Decision trees; Feature extraction; Learning systems; Pattern recognition; Psychophysiology; Radial basis function networks; Speech recognition; Emotion regression; Multiple disciplines; Multivariate adaptive regression splines; Music emotions; Radial basis functions; Regression algorithms; Regression neural networks; Support vector regression (SVR); Regression analysis},
	note = {Cited by: 6},
	number = {4},
	pages = {74 -- 89},
	publication_stage = {Final},
	source = {Scopus},
	title = {Dimensional music emotion recognition by machine learning},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008950450&doi=10.4018%2fIJCINI.2016100104&partnerID=40&md5=b26d4a687047701ee7c157560cbdb7bb},
	volume = {10},
	year = {2016},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008950450&doi=10.4018%2fIJCINI.2016100104&partnerID=40&md5=b26d4a687047701ee7c157560cbdb7bb},
	bdsk-url-2 = {https://doi.org/10.4018/IJCINI.2016100104}}

@article{Hsu202085,
	abstract = {This paper presents an automatic ECG-based emotion recognition algorithm for human emotion recognition. First, we adopt a musical induction method to induce participants' real emotional states and collect their ECG signals without any deliberate laboratory setting. Afterward, we develop an automatic ECG-based emotion recognition algorithm to recognize human emotions elicited by listening to music. Physiological ECG features extracted from the time-, and frequency-domain, and nonlinear analyses of ECG signals are used to find emotion-relevant features and to correlate them with emotional states. Subsequently, we develop a sequential forward floating selection-kernel-based class separability-based (SFFS-KBCS-based) feature selection algorithm and utilize the generalized discriminant analysis (GDA) to effectively select significant ECG features associated with emotions and to reduce the dimensions of the selected features, respectively. Positive/negative valence, high/low arousal, and four types of emotions (joy, tension, sadness, and peacefulness) are recognized using least squares support vector machine (LS-SVM) recognizers. The results show that the correct classification rates for positive/negative valence, high/low arousal, and four types of emotion classification tasks are 82.78, 72.91, and 61.52 percent, respectively. {\copyright} 2010-2012 IEEE.},
	author = {Hsu, Yu-Liang and Wang, Jeen-Shing and Chiang, Wei-Chun and Hung, Chien-Han},
	author_keywords = {Electrocardiogram; emotion recognition; machine learning; music},
	doi = {10.1109/TAFFC.2017.2781732},
	journal = {IEEE Transactions on Affective Computing},
	keywords = {Computer music; Discriminant analysis; Electrocardiography; Feature extraction; Frequency domain analysis; Genetic algorithms; Learning systems; Machine learning; Physiology; Signal analysis; Speech recognition; Support vector machines; Algorithm design and analysis; Emotion recognition; Feature selection algorithm; Generalized discriminant analysis; Human emotion recognition; Least squares support vector machines; Multiple signal classification; Music; Biomedical signal processing},
	note = {Cited by: 128},
	number = {1},
	pages = {85 -- 99},
	publication_stage = {Final},
	source = {Scopus},
	title = {Automatic ECG-Based Emotion Recognition in Music Listening},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039791780&doi=10.1109%2fTAFFC.2017.2781732&partnerID=40&md5=e4b7189cd63ec52383c76360bdef2de4},
	volume = {11},
	year = {2020},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039791780&doi=10.1109%2fTAFFC.2017.2781732&partnerID=40&md5=e4b7189cd63ec52383c76360bdef2de4},
	bdsk-url-2 = {https://doi.org/10.1109/TAFFC.2017.2781732}}

@article{Grekow2018415,
	abstract = {Finding pieces with a similar emotional distribution throughout the same composition was the aim of this work. A comparative analysis of musical performances by using emotion tracking was proposed. A dimensional approach of dynamic music emotion recognition was used in the analysis. Music data annotation and regressor training were done. Values of arousal and valence, predicted by regressors, were used to compare performances. The obtained results confirm the validity of the assumption that tracking and analyzing the values of arousal and valence over time in different performances of the same composition can be used to indicate their similarities. Detailed results of analyzing different performances of Prelude No.1 by Fr{\'e}d{\'e}ric Chopin were presented. They enabled to find the most similar performances to the performance by Arthur Rubinstein, for example. The author found which performances of the same composition were closer to each other and which were quite distant in terms of the shaping of arousal and valence over time. The presented method gives access to knowledge on the shaping of emotions by a performer, which had previously been available only to music professionals. {\copyright} 2018, The Author(s).},
	author = {Grekow, Jacek},
	author_keywords = {Emotion tracking; Musical performances; Similarity},
	doi = {10.1007/s10844-018-0510-y},
	journal = {Journal of Intelligent Information Systems},
	keywords = {Information systems; Comparative analysis; Music data; Music emotions; Musical performance; Similarity; Artificial intelligence},
	note = {Cited by: 11; All Open Access, Hybrid Gold Open Access},
	number = {2},
	pages = {415 -- 437},
	publication_stage = {Final},
	source = {Scopus},
	title = {Musical performance analysis in terms of emotions it evokes},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048546394&doi=10.1007%2fs10844-018-0510-y&partnerID=40&md5=62c4d29844fa88dda289d26312eebe59},
	volume = {51},
	year = {2018},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048546394&doi=10.1007%2fs10844-018-0510-y&partnerID=40&md5=62c4d29844fa88dda289d26312eebe59},
	bdsk-url-2 = {https://doi.org/10.1007/s10844-018-0510-y}}

@article{Ntalampiras20171694,
	abstract = {Predicting the emotions evoked by generalized sound events is a relatively recent research domain which still needs attention. In this work a framework aiming to reveal potential similarities existing during the perception of emotions evoked by sound events and songs is presented. To this end the following are proposed: (a) the usage of temporal modulation features, (b) a transfer learning module based on an echo state network, and (c) a k-medoids clustering algorithm predicting valence and arousal measurements associated with generalized sound events. The effectiveness of the proposed solution is demonstrated after a thoroughly designed experimental phase employing both sound and music data. The results demonstrate the importance of transfer learning in the specific field and encourage further research on approaches which manage the problem in a synergistic way. {\copyright} 2017 Acoustical Society of America.},
	author = {Ntalampiras, Stavros},
	doi = {10.1121/1.4977749},
	journal = {Journal of the Acoustical Society of America},
	keywords = {Acoustic Stimulation; Adult; Algorithms; Auditory Perception; Cues; Emotions; Female; Humans; Male; Models, Theoretical; Music; Pattern Recognition, Physiological; Sound; Time Factors; Transfer (Psychology); Young Adult; Behavioral research; Forecasting; Echo state networks; K-medoids clustering; Music data; Recent researches; Sound events; Temporal modulations; Transfer learning; adult; algorithm; association; auditory stimulation; emotion; female; hearing; human; male; music; pattern recognition; sound; theoretical model; time factor; transfer of learning; young adult; Clustering algorithms},
	note = {Cited by: 23; All Open Access, Green Open Access},
	number = {3},
	pages = {1694 -- 1701},
	publication_stage = {Final},
	source = {Scopus},
	title = {A transfer learning framework for predicting the emotional content of generalized sound events},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015211289&doi=10.1121%2f1.4977749&partnerID=40&md5=744823c3bb1062a093db56fb31511f25},
	volume = {141},
	year = {2017},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015211289&doi=10.1121%2f1.4977749&partnerID=40&md5=744823c3bb1062a093db56fb31511f25},
	bdsk-url-2 = {https://doi.org/10.1121/1.4977749}}

@article{Zhang2017251,
	abstract = {Music emotion recognition is an important topic in music information retrieval area. A lot of acoustic features are used to train a music classification or regression emotion model. However, these existing features may not be efficient for classification or regression task. Furthermore, most works do not explain why these features do work for classification. In our work, eight features are extracted to represent the arousal dimension of music emotion, and various commonly used statistical learning methods such as Logistic Regression, and tree-based methods are applied to interpret important features. Then the shrinkage methods are applied to feature selection and classification in music emotion recognition for the first time. Our tests show that the proposed approaches are efficient for feature selection just as entropy-based filter methods, and better than wrapper methods. The shrinkage methods can produce more continuous and low variance model than wrapper methods. Then, we discover that the most useful features are low specific loudness sensation coefficients (low-SONE), root mean square and loudness-flux. Moreover, the shrinkage methods apply in logistic regression perform better for classification than most of other methods. We get an average accuracy rate of 83.8 %. {\copyright} 2015, Springer-Verlag Berlin Heidelberg.},
	author = {Zhang, Jiang Long and Huang, Xiang Lin and Yang, Li Fang and Xu, Ye and Sun, Shu Tao},
	author_keywords = {Features learning; Features selection; Music arousal dimension classification; Shrinkage method; Statistical learning},
	doi = {10.1007/s00530-015-0489-y},
	journal = {Multimedia Systems},
	keywords = {Feature extraction; Regression analysis; Shrinkage; Speech recognition; Feature selection and classification; Features learning; Features selection; Music classification; Music information retrieval; Shrinkage methods; Statistical learning; Statistical learning methods; Classification (of information)},
	note = {Cited by: 10},
	number = {2},
	pages = {251 -- 264},
	publication_stage = {Final},
	source = {Scopus},
	title = {Feature selection and feature learning in arousal dimension of music emotion by using shrinkage methods},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944937222&doi=10.1007%2fs00530-015-0489-y&partnerID=40&md5=f12764992ee29a4976644aacf6bbb43d},
	volume = {23},
	year = {2017},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944937222&doi=10.1007%2fs00530-015-0489-y&partnerID=40&md5=f12764992ee29a4976644aacf6bbb43d},
	bdsk-url-2 = {https://doi.org/10.1007/s00530-015-0489-y}}

@article{Marimpis2020170928,
	abstract = {A high number of studies have already demonstrated an electroencephalography (EEG)-based emotion recognition system with moderate results. Emotions are classified into discrete and dimensional models. We focused on the latter that incorporates valence and arousal dimensions. The mainstream methodology is the extraction of univariate measures derived from EEG activity from various frequencies classifying trials into low/high valence and arousal levels. Here, we evaluated brain connectivity within and between brain frequencies under the multiplexity framework. We analyzed an EEG database called DEAP that contains EEG responses to video stimuli and users' emotional self-Assessments.We adopted a dynamic functional connectivity analysis under the notion of our dominant coupling model (DoCM). DoCM detects the dominant coupling mode per pair of EEG sensors, which can be either within frequencies coupling (intra) or between frequencies coupling (cross-frequency). DoCM revealed an integrated dynamic functional connectivity graph (IDFCG) that keeps both the strength and the preferred dominant coupling mode. We aimed to create a connectomic mapping of valence-Arousal map via employing features derive from IDFCG. Our results outperformed previous findings succeeding to predict in a high accuracy participants' ratings in valence and arousal dimensions based on a fiexibility index of dominant coupling modes. {\copyright} 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.},
	author = {Marimpis, Avraam D. and Dimitriadi, Stavros I. and Goebel, Rainer},
	author_keywords = {Affective computing; computational neuroscience; emotion in human-computer interaction; graph theory; modeling from video; modeling human emotion; music; neuroscience; video},
	doi = {10.1109/ACCESS.2020.3025370},
	journal = {IEEE Access},
	keywords = {Electrophysiology; Brain connectivity; Coupling mode; Coupling modeling; Dimensional model; Emotion recognition; Emotional models; Functional connectivity; High-accuracy; Electroencephalography},
	note = {Cited by: 8; All Open Access, Gold Open Access, Green Open Access},
	pages = {170928 -- 170938},
	publication_stage = {Final},
	source = {Scopus},
	title = {A Multiplex connectivity map of valence-Arousal emotional model},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102813985&doi=10.1109%2fACCESS.2020.3025370&partnerID=40&md5=524564f301cda8ddd11f0901927a6a68},
	volume = {8},
	year = {2020},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102813985&doi=10.1109%2fACCESS.2020.3025370&partnerID=40&md5=524564f301cda8ddd11f0901927a6a68},
	bdsk-url-2 = {https://doi.org/10.1109/ACCESS.2020.3025370}}

@article{Wang2021,
	abstract = {In the complex system of music performance, there are differences in the expression of music emotions by listeners, so it is of great significance to study the classification of different emotions under different audio signals. In this paper, the research of human emotional intelligence recognition and classification algorithm in the complex system of music performance is proposed. Through the recognition of SVM, KNN, ANN, and ID3 classifiers, the accuracy of a single classifier is compared, and then the four classifiers are combined to compare the classification accuracy of audio signals before and after preprocessing. The results show that the accuracy of SVM and ANN fusion is the highest. Finally, recall and F1 are comprehensively compared in the fusion algorithm, and the fusion classification effect of SVM and ANN is better than that of the algorithm model.  {\copyright} 2021 Daliang Wang and Xiaowen Guo.},
	author = {Wang, Daliang and Guo, Xiaowen},
	doi = {10.1155/2021/4251827},
	journal = {Complexity},
	keywords = {Audio acoustics; Emotional intelligence; Support vector machines; Algorithm model; Classification accuracy; Classification algorithm; Fusion algorithms; Fusion classification; Intelligent recognition; Music emotions; Music performance; Computer music},
	note = {Cited by: 7; All Open Access, Gold Open Access},
	publication_stage = {Final},
	source = {Scopus},
	title = {Research on Intelligent Recognition and Classification Algorithm of Music Emotion in Complex System of Music Performance},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109435136&doi=10.1155%2f2021%2f4251827&partnerID=40&md5=b1866e5ba2f0b3b1ca5047cf2e22c97c},
	volume = {2021},
	year = {2021},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109435136&doi=10.1155%2f2021%2f4251827&partnerID=40&md5=b1866e5ba2f0b3b1ca5047cf2e22c97c},
	bdsk-url-2 = {https://doi.org/10.1155/2021/4251827}}

@article{Hsu2018195,
	abstract = {Emotion recognition of music objects is a promising and important research issues in the field of music information retrieval. Usually, music emotion recognition could be considered as a training/classification problem. However, even given a benchmark (a training data with ground truth) and using effective classification algorithms, music emotion recognition remains a challenging problem. Most previous relevant work focuses only on acoustic music content without considering individual difference (i.e., personalization issues). In addition, assessment of emotions is usually self-reported (e.g., emotion tags) which might introduce inaccuracy and inconsistency. Electroencephalography (EEG) is a non-invasive brain-machine interface which allows external machines to sense neurophysiological signals from the brain without surgery. Such unintrusive EEG signals, captured from the central nervous system, have been utilized for exploring emotions. This paper proposes an evidence-based and personalized model for music emotion recognition. In the training phase for model construction and personalized adaption, based on the IADS (the International Affective Digitized Sound system, a set of acoustic emotional stimuli for experimental investigations of emotion and attention), we construct two predictive and generic models ANN1 (``EEG recordings of standardized group vs. emotions'') and ANN2 (``music audio content vs. emotion''). Both models are trained by an artificial neural network. We then collect a subject's EEG recordings when listening the selected IADS samples, and apply the ANN1 to determine the subject's emotion vector. With the generic model and the corresponding individual differences, we construct the personalized model H by the projective transformation. In the testing phase, given a music object, the processing steps are: (1) to extract features from the music audio content, (2) to apply ANN2 to calculate the vector in the arousal-valence emotion space, and (3) to apply the transformation matrix H to determine the personalized emotion vector. Moreover, with respect to a moderate music object, we apply a sliding window on the music object to obtain a sequence of personalized emotion vectors, in which those predicted vectors will be fitted and organized as an emotion trail for revealing dynamics in the affective content of music object. Experimental results suggest the proposed approach is effective. {\copyright} 2017, Springer-Verlag Berlin Heidelberg.},
	author = {Hsu, Jia-Lien and Zhen, Yan-Lin and Lin, Tzu-Chieh and Chiu, Yi-Shiuan},
	author_keywords = {Affective analysis; EEG; Emotion trail; Music emotion recognition; Personalization},
	doi = {10.1007/s00530-017-0542-0},
	journal = {Multimedia Systems},
	keywords = {Audio recordings; Biomedical signal processing; Brain computer interface; Classification (of information); Computer music; Electroencephalography; Electrophysiology; Linear transformations; Neural networks; Neurophysiology; Speech recognition; Vector spaces; Vectors; Affective analysis; Classification algorithm; Emotion trail; Experimental investigations; Music emotions; Music information retrieval; Personalizations; Projective transformation; Audio acoustics},
	note = {Cited by: 31},
	number = {2},
	pages = {195 -- 210},
	publication_stage = {Final},
	source = {Scopus},
	title = {Affective content analysis of music emotion through EEG},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015176215&doi=10.1007%2fs00530-017-0542-0&partnerID=40&md5=ef140ebbf4cf24f774389b510e5add5c},
	volume = {24},
	year = {2018},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015176215&doi=10.1007%2fs00530-017-0542-0&partnerID=40&md5=ef140ebbf4cf24f774389b510e5add5c},
	bdsk-url-2 = {https://doi.org/10.1007/s00530-017-0542-0}}

@article{Bai201780,
	abstract = {Music emotions recognition (MER) is a challenging field of studies addressed in multiple disciplines such as musicology, cognitive science, physiology, psychology, arts and affective computing. In this article, music emotions are classified into four types known as those of pleasing, angry, sad and relaxing. MER is formulated as a classification problem in cognitive computing where 548 dimensions of music features are extracted and modeled. A set of classifications and machine learning algorithms are explored and comparatively studied for MER, which includes Support Vector Machine (SVM), k-Nearest Neighbors (KNN), Neuro-Fuzzy Networks Classification (NFNC), Fuzzy KNN (FKNN), Bayes classifier and Linear Discriminant Analysis (LDA). Experimental results show that the SVM, FKNN and LDA algorithms are the most effective methodologies that obtain more than 80% accuracy for MER. {\copyright} 2017 IGI Global.},
	author = {Bai, Junjie and Luo, Kan and Peng, Jun and Shi, Jinliang and Wu, Ying and Feng, Lixiao and Li, Jianqing and Wang, Yingxu},
	author_keywords = {Emotion Classification; Feature Extraction; Machine Learning; Music Emotion Recognition; Pattern Recognition},
	doi = {10.4018/IJCINI.2017100105},
	journal = {International Journal of Cognitive Informatics and Natural Intelligence},
	keywords = {Artificial intelligence; Arts computing; Barium compounds; Discriminant analysis; Feature extraction; Fuzzy neural networks; Image retrieval; Learning systems; Nearest neighbor search; Pattern recognition; Psychophysiology; Support vector machines; Affective Computing; Classification methodologies; Emotion classification; K nearest neighbor (KNN); Linear discriminant analysis; Multiple disciplines; Music emotions; Neuro-fuzzy network; Learning algorithms},
	note = {Cited by: 6},
	number = {4},
	pages = {80 -- 92},
	publication_stage = {Final},
	source = {Scopus},
	title = {Music emotions recognition by machine learning with cognitive classification methodologies},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038434665&doi=10.4018%2fIJCINI.2017100105&partnerID=40&md5=a2e1f1d69ab6ceb6f0cee7f2eb26985a},
	volume = {11},
	year = {2017},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038434665&doi=10.4018%2fIJCINI.2017100105&partnerID=40&md5=a2e1f1d69ab6ceb6f0cee7f2eb26985a},
	bdsk-url-2 = {https://doi.org/10.4018/IJCINI.2017100105}}

@article{Brown201755,
	abstract = {Background: Children with autism spectrum disorder (ASD) often struggle with social skills, including the ability to perceive emotions based on facial expressions. Research evidence suggests that many individuals with ASD can perceive emotion in music. Examining whether music can be used to enhance recognition of facial emotion by children with ASD would inform development of music therapy interventions. Objective: The purpose of this study was to investigate the influence of music with a strong emotional valance (happy; sad) on children with ASD's ability to label emotions depicted in facial photographs, and their response time. Methods: Thirty neurotypical children and 20 children with high-functioning ASD rated expressions of happy, neutral, and sad in 30 photographs under two music listening conditions (sad music; happy music). During each music listening condition, participants rated the 30 images using a 7-point scale that ranged from very sad to very happy. Response time data were also collected across both conditions. Results: A significant two-way interaction revealed that participants' ratings of happy and neutral faces were unaffected by music conditions, but sad faces were perceived to be sadder with sad music than with happy music. Across both conditions, neurotypical children rated the happy faces as happier and the sad faces as sadder than did participants with ASD. Response times of the neurotypical children were consistently shorter than response times of the children with ASD; both groups took longer to rate sad faces than happy faces. Response times of neurotypical children were generally unaffected by the valence of the music condition; however, children with ASD took longer to respond when listening to sad music. Conclusions: Music appears to affect perceptions of emotion in children with ASD, and perceptions of sad facial expressions seem to be more affected by emotionally congruent background music than are perceptions of happy or neutral faces. {\copyright} 2016 The American Music Therapy Association. All rights reserved.},
	author = {Brown, Laura S.},
	doi = {10.1093/jmt/thw017},
	journal = {Journal of Music Therapy},
	keywords = {Acoustic Stimulation; Adolescent; Autism Spectrum Disorder; Child; Face; Facial Expression; Female; Happiness; Humans; Male; Music; Music Therapy; Pattern Recognition, Visual; Photic Stimulation; Photography; Reaction Time; adolescent; anatomy and histology; auditory stimulation; autism; child; face; facial expression; female; happiness; human; male; music; music therapy; pattern recognition; photography; photostimulation; psychology; reaction time},
	note = {Cited by: 18},
	number = {1},
	pages = {55 -- 79},
	publication_stage = {Final},
	source = {Scopus},
	title = {The influence of music on facial emotion recognition in children with autism spectrum disorder and neurotypical children},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019942610&doi=10.1093%2fjmt%2fthw017&partnerID=40&md5=ec57292815967398a1f718798cba0333},
	volume = {54},
	year = {2017},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019942610&doi=10.1093%2fjmt%2fthw017&partnerID=40&md5=ec57292815967398a1f718798cba0333},
	bdsk-url-2 = {https://doi.org/10.1093/jmt/thw017}}

@article{Zhang2019964,
	abstract = {Music can trigger human emotion. This is a psychophysiological process. Therefore, using psychophysiological characteristics could be a way to understand individual music emotional experience. In this study, we explore a new method of personal music emotion recognition based on human physiological characteristics. First, we build up a database of features based on emotions related to music and a database based on physiological signals derived from music listening including EDA, PPG, SKT, RSP, and PD variation information. Then linear regression, ridge regression, support vector machines with three different kernels, decision trees, k-nearest neighbors, multi-layer perceptron, and Nu support vector regression (NuSVR) are used to recognize music emotions via a data synthesis of music features and human physiological features. NuSVR outperforms the other methods. The correlation coefficient values are 0.7347 for arousal and 0.7902 for valence, while the mean squared errors are 0.023 23 for arousal and 0.014 85 for valence. Finally, we compare the different data sets and find that the data set with all the features (music features and all physiological features) has the best performance in modeling. The correlation coefficient values are 0.6499 for arousal and 0.7735 for valence, while the mean squared errors are 0.029 32 for arousal and 0.015 76 for valence. We provide an effective way to recognize personal music emotional experience, and the study can be applied to personalized music recommendation. {\copyright} 2019, Zhejiang University and Springer-Verlag GmbH Germany, part of Springer Nature.},
	author = {Zhang, Le-kai and Sun, Shou-qian and Xing, Bai-xi and Luo, Rui-ming and Zhang, Ke-jun},
	author_keywords = {Emotion recognition; Music; Physiological signals; TP391.4; Wavelet transform},
	doi = {10.1631/FITEE.1800101},
	journal = {Frontiers of Information Technology and Electronic Engineering},
	keywords = {Decision trees; Mean square error; Nearest neighbor search; Physiology; Regression analysis; Speech recognition; Support vector machines; Trees (mathematics); Wavelet transforms; Correlation coefficient; Emotion recognition; Music; Physiological characteristics; Physiological signals; Psychophysiological measures; Support vector regression (SVR); TP391.4; Computer music},
	note = {Cited by: 15},
	number = {7},
	pages = {964 -- 974},
	publication_stage = {Final},
	source = {Scopus},
	title = {Using psychophysiological measures to recognize personal music emotional experience},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070413287&doi=10.1631%2fFITEE.1800101&partnerID=40&md5=8ad96b33c92d38227c9c2259052e1e11},
	volume = {20},
	year = {2019},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070413287&doi=10.1631%2fFITEE.1800101&partnerID=40&md5=8ad96b33c92d38227c9c2259052e1e11},
	bdsk-url-2 = {https://doi.org/10.1631/FITEE.1800101}}

@article{Naser2021,
	abstract = {In studies of emotions, music is usually used to induce the emotions that are measured on the arousal-valence/arousal-valence-dominance scales. However, the influence of music liking (that depends on individual preference and appraisal) on the induced emotions is often ignored. This work presents a novel study on the influence of liking on arousal, valence, and dominance using a signal processing and pattern recognition framework. Emotion recognition was performed using a feature-level fusion of three features together with feature selection method and a classifier. The features were derived from wavelet decomposition of EEG, pairwise functional connectivity, and graph-theoretic measures that reflect characteristics of an individual electrode, pair of electrodes, and topological properties of the brain networks, respectively. Here, classification is done between the high/low categories of each of the arousal, valence, and dominance scales under three different cases of music liking. The study shows that the classification performances of arousal, valence, and dominance were 22.50%, 14.87%, and 19.44% above the empirical chance-level, respectively. The fusion framework gave up to 5% relative improvement over individual features. The study indicates that liking influences classification performance and also the temporal dynamics of emotional experience across these scales. We observe an inverted U relationship between the level of liking and arousal and dominance classification performance. We also analyzed the feature and electrode usage and specific aspects of brain activity at different levels of liking. This reveals the importance of high-frequency bands and hemispheric features in emotion recognition. {\copyright} 2020},
	author = {Naser, Daimi Syed and Saha, Goutam},
	author_keywords = {Dual-tree complex wavelet packet transform (DT-CWPT); Electroencephalogram (EEG); Emotion recognition; Functional connectivity; Music liking},
	doi = {10.1016/j.bspc.2020.102251},
	journal = {Biomedical Signal Processing and Control},
	keywords = {Biomedical signal processing; Brain; Electrodes; Graph theory; Speech recognition; Wavelet decomposition; Classification performance; Emotional experiences; Feature level fusion; Feature selection methods; Functional connectivity; High frequency bands; Individual preference; Topological properties; arousal; Article; blind source separation; brain asymmetry; brain region; cognition; continuous wavelet transform; cross validation; electroencephalogram; electroencephalography; emotion; eye movement; feature extraction; feature selection; Fourier transform; functional connectivity; hemispheric dominance; human; machine learning; music; nerve cell network; priority journal; recognition; Classification (of information)},
	note = {Cited by: 43},
	publication_stage = {Final},
	source = {Scopus},
	title = {Influence of music liking on EEG based emotion recognition},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092634607&doi=10.1016%2fj.bspc.2020.102251&partnerID=40&md5=d1ef9292f92b764d9f7fb6c88ce27229},
	volume = {64},
	year = {2021},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092634607&doi=10.1016%2fj.bspc.2020.102251&partnerID=40&md5=d1ef9292f92b764d9f7fb6c88ce27229},
	bdsk-url-2 = {https://doi.org/10.1016/j.bspc.2020.102251}}

@article{Vignolo20161,
	abstract = {Mel-frequency cepstral coefficients introduced biologically-inspired features into speech technology, becoming the most commonly used representation for speech, speaker and emotion recognition, and even for applications in music. While this representation is quite popular, it is ambitious to assume that it would provide the best results for every application, as it is not designed for each specific objective. This work proposes a methodology to learn a speech representation from data by optimising a filter bank, in order to improve results in the classification of stressed speech. Since population-based metaheuristics have proved successful in related applications, an evolutionary algorithm is designed to search for a filter bank that maximises the classification accuracy. For the codification, spline functions are used to shape the filter banks, which allows reducing the number of parameters to optimise. The filter banks obtained with the proposed methodology improve the results in stressed and emotional speech classification. {\copyright} 2016 Elsevier B.V.},
	author = {Vignolo, Leandro D. and Prasanna, S.R. Mahadeva and Dandapat, Samarendra and Rufiner, H. Leonardo and Milone, Diego H.},
	author_keywords = {Cepstral coefficients; Emotional speech; Evolutionary algorithms; Speech processing; Stressed speech},
	doi = {10.1016/j.patrec.2016.07.017},
	journal = {Pattern Recognition Letters},
	keywords = {Evolutionary algorithms; Filter banks; Optimization; Speech; Speech analysis; Speech processing; Biologically inspired; Cepstral coefficients; Classification accuracy; Emotion recognition; Emotional speech; Mel frequency cepstral co-efficient; Stress recognition; Stressed speech; Speech recognition},
	note = {Cited by: 10},
	pages = {1 -- 7},
	publication_stage = {Final},
	source = {Scopus},
	title = {Feature optimisation for stress recognition in speech},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982684046&doi=10.1016%2fj.patrec.2016.07.017&partnerID=40&md5=6cd92f5324609ba595410d55ee59eb3a},
	volume = {84},
	year = {2016},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982684046&doi=10.1016%2fj.patrec.2016.07.017&partnerID=40&md5=6cd92f5324609ba595410d55ee59eb3a},
	bdsk-url-2 = {https://doi.org/10.1016/j.patrec.2016.07.017}}

@article{Hasanzadeh2021,
	abstract = {A controversial issue in artificial intelligence is human emotion recognition. This paper presents a fuzzy parallel cascades (FPC) model for predicting the continuous subjective emotional appraisal of music by time-varying spectral content of electroencephalogram (EEG) signals. The EEG, along with an emotional appraisal of 15 subjects, was recorded during listening to seven musical excerpts. The emotional appraisement was recorded along the valence and arousal emotional axes as a continuous signal. The FPC model was composed of parallel cascades with each cascade containing a fuzzy logic-based system. The FPC model performance was evaluated using linear regression (LR), support vector regression (SVR), and Long--Short-Term-Memory recurrent neural network (LSTM-RNN) models by 4 fold cross-validation. The root mean square error (RMSE) of the FPC was lower than other models in the estimation of both valence and arousal of all musical excerpts. The lowest obtained RMSE was 0.082, which was acquired by the FPC model. The analysis of mutual information of frontal EEG with the valence confirms the role of frontal channels in the theta frequency band in emotion recognition. Considering the dynamic variations of musical features during songs, employing a modeling approach to predict dynamic variations of the emotional appraisal can be a plausible substitute for the classification of musical excerpts into predefined labels. {\copyright} 2020 Elsevier B.V.},
	author = {Hasanzadeh, Fatemeh and Annabestani, Mohsen and Moghimi, Sahar},
	author_keywords = {Continuous emotion recognition; EEG; Fuzzy inference system; Musical emotions; System identification},
	doi = {10.1016/j.asoc.2020.107028},
	journal = {Applied Soft Computing},
	keywords = {Electroencephalography; Fuzzy logic; Long short-term memory; Mean square error; Speech recognition; Support vector regression; Dynamic variations; Electroencephalogram signals; Emotion recognition; Human emotion recognition; Mutual informations; Root mean square errors; Short term memory; Support vector regression (SVR); Biomedical signal processing},
	note = {Cited by: 18; All Open Access, Green Open Access},
	publication_stage = {Final},
	source = {Scopus},
	title = {Continuous emotion recognition during music listening using EEG signals: A fuzzy parallel cascades model},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099050273&doi=10.1016%2fj.asoc.2020.107028&partnerID=40&md5=5663ee4dc34c35a351823ac78978e347},
	volume = {101},
	year = {2021},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099050273&doi=10.1016%2fj.asoc.2020.107028&partnerID=40&md5=5663ee4dc34c35a351823ac78978e347},
	bdsk-url-2 = {https://doi.org/10.1016/j.asoc.2020.107028}}

@article{Bravo2017,
	abstract = {Previous neuroimaging studies have shown an increased sensory cortical response (i.e., heightened weight on sensory evidence) under higher levels of predictive uncertainty. The signal enhancement theory proposes that attention improves the quality of the stimulus representation, and therefore reduces uncertainty by increasing the gain of the sensory signal. The present study employed functional magnetic resonance imaging (fMRI) to investigate the neural correlates for ambiguous valence inferences signaled by auditory information within an emotion recognition paradigm. Participants categorized sound stimuli of three distinct levels of consonance/dissonance controlled by interval content. Separate behavioural and neuroscientific experiments were conducted. Behavioural results revealed that, compared with the consonance condition (perfect fourths, fifths and octaves) and the strong dissonance condition (minor/major seconds and tritones), the intermediate dissonance condition (minor thirds) was the most ambiguous, least salient and more cognitively demanding category (slowest reaction times). The neuroscientific findings were consistent with a heightened weight on sensory evidence whilst participants were evaluating intermediate dissonances, which was reflected in an increased neural response of the right Heschl's gyrus. The results support previous studies that have observed enhanced precision of sensory evidence whilst participants attempted to represent and respond to higher degrees of uncertainty, and converge with evidence showing preferential processing of complex spectral information in the right primary auditory cortex. These findings are discussed with respect to music-theoretical concepts and recent Bayesian models of perception, which have proposed that attention may heighten the weight of information coming from sensory channels to stimulate learning about unknown predictive relationships. {\copyright} 2017 Bravo et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
	author = {Bravo, Fernando and Cross, Ian and Stamatakis, Emmanuel Andreas and Rohrmeier, Martin},
	doi = {10.1371/journal.pone.0175991},
	journal = {PLoS ONE},
	keywords = {Acoustic Stimulation; Adult; Attention; Auditory Cortex; Auditory Perception; Bayes Theorem; Brain Mapping; Cues; Female; Frustration; Humans; Learning; Magnetic Resonance Imaging; Male; Music; Pleasure; Reaction Time; Uncertainty; attention; controlled study; functional magnetic resonance imaging; human; human experiment; learning; music; nerve potential; participant observation; perception; primary auditory cortex; reaction time; sound; stimulus; theoretical model; uncertainty; adult; anatomy and histology; association; auditory cortex; auditory stimulation; Bayes theorem; brain mapping; female; frustration; hearing; male; nuclear magnetic resonance imaging; physiology; pleasure; psychology; uncertainty},
	note = {Cited by: 2; All Open Access, Gold Open Access, Green Open Access},
	number = {4},
	publication_stage = {Final},
	source = {Scopus},
	title = {Sensory cortical response to uncertainty and low salience during recognition of affective cues in musical intervals},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018517487&doi=10.1371%2fjournal.pone.0175991&partnerID=40&md5=bc4de1649aa92d585f991d81da38a629},
	volume = {12},
	year = {2017},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018517487&doi=10.1371%2fjournal.pone.0175991&partnerID=40&md5=bc4de1649aa92d585f991d81da38a629},
	bdsk-url-2 = {https://doi.org/10.1371/journal.pone.0175991}}

@article{Hong2017304,
	abstract = {Music emotion recognition and recommendation systems often use a simplified 4-quadrant model with categories such as Happy, Sad, Angry, and Calm. Previous research has shown that both listeners and automated systems often have difficulty distinguishing low-arousal categories such as Calm and Sad. This paper seeks to explore what makes the categories Calm and Sad so difficult to distinguish. We used 300 low-arousal excerpts from the classical piano repertoire to determine the coverage of the categories Calm and Sad in the low-arousal space, their overlap, and their balance to one another. Our results show that Calm was 40% bigger in terms of coverage than Sad, but that on average Sad excerpts were significantly more negative in mood than Calm excerpts were positive. Calm and Sad overlapped in nearly 20% of the excerpts, meaning 20% of the excerpts were about equally Calm and Sad. Calm and Sad covered about 92% of the low-arousal space, where 8% of the space were holes that were not-at-all Calm or Sad. The largest holes were for excerpts considered Mysterious and Doubtful, but there were smaller holes among positive excerpts as well. Due to the holes in the coverage, the overlaps, and imbalances the Calm-Sad model adds about 6% more errors when compared to asking users directly whether the mood of the music is positive or negative. Nevertheless, the Calm-Sad model is still useful and appropriate for applications in music emotion recognition and recommendation such as when a simple and intuitive interface is preferred or when categorization is more important than precise differentiation.},
	author = {Hong, Yu and Chau, Chuck-Jee and Horner, Andrew},
	doi = {10.17743/jaes.2017.0001},
	journal = {AES: Journal of the Audio Engineering Society},
	keywords = {Automation; Automated systems; Intuitive interfaces; Music emotions; Piano music; Speech recognition},
	note = {Cited by: 14; All Open Access, Bronze Open Access},
	number = {4},
	pages = {304 -- 320},
	publication_stage = {Final},
	source = {Scopus},
	title = {An analysis of low-arousal piano music ratings to uncover what makes calm and sad music so difficult to distinguish in music emotion recognition},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019993531&doi=10.17743%2fjaes.2017.0001&partnerID=40&md5=e66e2e892e26fd861e70e06d45bf06b9},
	volume = {65},
	year = {2017},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019993531&doi=10.17743%2fjaes.2017.0001&partnerID=40&md5=e66e2e892e26fd861e70e06d45bf06b9},
	bdsk-url-2 = {https://doi.org/10.17743/jaes.2017.0001}}

@article{Huang2021,
	abstract = {The music performance system works by identifying the emotional elements of music to control the lighting changes. However, if there is a recognition error, a good stage effect will not be able to create. Therefore, this paper proposes an intelligent music emotion recognition and classification algorithm in the music performance system. The first part of the algorithm is to analyze the emotional features of music, including acoustic features, melody features, and audio features. Then, the three kinds of features are combined together to form a feature vector set. In the latter part of the algorithm, it divides the feature vector set into training samples and test samples. The training samples are trained by using recognition and classification model based on the neural network. And then, the testing samples are input into the trained model, which is aiming to realize the intelligent recognition and classification of music emotion. The result shows that the kappa coefficient k values calculated by the proposed algorithm are greater than 0.75, which indicates that the recognition and classification results are consistent with the actual results, and the accuracy of recognition and classification is high. So, the research purpose is achieved.  {\copyright} 2021 Chun Huang and Diao Shen.},
	author = {Huang, Chun and Shen, Diao},
	doi = {10.1155/2021/7886570},
	journal = {Scientific Programming},
	keywords = {Audio acoustics; Sampling; Speech recognition; Classification algorithm; Features vector; Intelligent classification; Intelligent recognition; Music emotions; Music performance; Performance system; Recognition algorithm; Recognition error; Training sample; Music},
	note = {Cited by: 3; All Open Access, Gold Open Access},
	publication_stage = {Final},
	source = {Scopus},
	title = {Research on Music Emotion Intelligent Recognition and Classification Algorithm in Music Performance System},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119913107&doi=10.1155%2f2021%2f7886570&partnerID=40&md5=04d9fb372c6e843a9daf134c1e0885f8},
	volume = {2021},
	year = {2021},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119913107&doi=10.1155%2f2021%2f7886570&partnerID=40&md5=04d9fb372c6e843a9daf134c1e0885f8},
	bdsk-url-2 = {https://doi.org/10.1155/2021/7886570}}

@article{Pesek2017246,
	abstract = {This paper presents a new multimodal dataset Moodo that can aid the development of affective music information retrieval systems. Moodo's main novelties are a multimodal approach that links emotional and color perception to music and the inclusion of user context. Analysis of the dataset reveals notable differences in emotion-color associations and their valence-arousal ratings in non-music and music context. We also show differences in ratings of perceived and induced emotions, especially for those with perceived negative connotation, as well as the influence of genre and user context on perception of emotions. By applying an intermediate data fusion model, we demonstrate the importance of user profiles for predictive modeling in affective music information retrieval scenarios. {\copyright} 2017 Informa UK Limited, trading as Taylor & Francis Group.},
	author = {Pesek, Matev{\v z} and Strle, Gregor and Kav{\v c}i{\v c}, Alenka and Marolt, Matija},
	author_keywords = {affective computing; music datasets; music emotion recognition; music information retrieval; user context},
	doi = {10.1080/09298215.2017.1333518},
	journal = {Journal of New Music Research},
	note = {Cited by: 16},
	number = {3},
	pages = {246 -- 260},
	publication_stage = {Final},
	source = {Scopus},
	title = {The Moodo dataset: Integrating user context with emotional and color perception of music for affective music information retrieval},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020180643&doi=10.1080%2f09298215.2017.1333518&partnerID=40&md5=dbadb4824ccdcf58dfceb3c0fdd61675},
	volume = {46},
	year = {2017},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020180643&doi=10.1080%2f09298215.2017.1333518&partnerID=40&md5=dbadb4824ccdcf58dfceb3c0fdd61675},
	bdsk-url-2 = {https://doi.org/10.1080/09298215.2017.1333518}}

@article{Panda2020614,
	abstract = {This work advances the music emotion recognition state-of-the-art by proposing novel emotionally-relevant audio features. We reviewed the existing audio features implemented in well-known frameworks and their relationships with the eight commonly defined musical concepts. This knowledge helped uncover musical concepts lacking computational extractors, to which we propose algorithms - namely related with musical texture and expressive techniques. To evaluate our work, we created a public dataset of 900 audio clips, with subjective annotations following Russell's emotion quadrants. The existent audio features (baseline) and the proposed features (novel) were tested using 20 repetitions of 10-fold cross-validation. Adding the proposed features improved the F1-score to 76.4 percent (by 9 percent), when compared to a similar number of baseline-only features. Moreover, analysing the features relevance and results uncovered interesting relations, namely the weight of specific features and musical concepts to each emotion quadrant, and warrant promising new directions for future research in the field of music emotion recognition, interactive media, and novel music interfaces. {\copyright} 2010-2012 IEEE.},
	author = {Panda, Renato and Malheiro, Ricardo and Paiva, Rui Pedro},
	author_keywords = {Affective computing; audio databases; emotion recognition; feature extraction; music information retrieval},
	doi = {10.1109/TAFFC.2018.2820691},
	journal = {IEEE Transactions on Affective Computing},
	keywords = {Feature extraction; Speech recognition; Textures; 10-fold cross-validation; Affective Computing; Audio database; Emotion recognition; Interactive media; Music information retrieval; Music interfaces; Musical concepts; Audio acoustics},
	note = {Cited by: 83; All Open Access, Green Open Access},
	number = {4},
	pages = {614 -- 626},
	publication_stage = {Final},
	source = {Scopus},
	title = {Novel Audio Features for Music Emotion Recognition},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044777756&doi=10.1109%2fTAFFC.2018.2820691&partnerID=40&md5=6ff59839d539ee19db6d2a75a843d7d6},
	volume = {11},
	year = {2020},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044777756&doi=10.1109%2fTAFFC.2018.2820691&partnerID=40&md5=6ff59839d539ee19db6d2a75a843d7d6},
	bdsk-url-2 = {https://doi.org/10.1109/TAFFC.2018.2820691}}

@article{Xiao2018,
	abstract = {We used a novel intermodal association task to examine whether infants associate own- and other-race faces with music of different emotional valences. Three- to 9-month-olds saw a series of neutral own- or other-race faces paired with happy or sad musical excerpts. Three- to 6-month-olds did not show any specific association between face race and music. At 9 months, however, infants looked longer at own-race faces paired with happy music than at own-race faces paired with sad music. Nine-month-olds also looked longer at other-race faces paired with sad music than at other-race faces paired with happy music. These results indicate that infants with nearly exclusive own-race face experience develop associations between face race and music emotional valence in the first year of life. The potential implications of such associations for developing racial biases in early childhood are discussed. {\copyright} 2017 John Wiley & Sons Ltd},
	author = {Xiao, Naiqi G. and Quinn, Paul C. and Liu, Shaoying and Ge, Liezhong and Pascalis, Olivier and Lee, Kang},
	doi = {10.1111/desc.12537},
	journal = {Developmental Science},
	keywords = {Age Factors; Child; Emotions; Facial Recognition; Female; Happiness; Humans; Infant; Male; Music; Racism; age; child; emotion; facial recognition; female; happiness; human; infant; male; music; psychology; racism},
	note = {Cited by: 65},
	number = {2},
	publication_stage = {Final},
	source = {Scopus},
	title = {Older but not younger infants associate own-race faces with happy music and other-race faces with sad music},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020860676&doi=10.1111%2fdesc.12537&partnerID=40&md5=4ff39f8409f3a6f90d1ccd9525362bb3},
	volume = {21},
	year = {2018},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020860676&doi=10.1111%2fdesc.12537&partnerID=40&md5=4ff39f8409f3a6f90d1ccd9525362bb3},
	bdsk-url-2 = {https://doi.org/10.1111/desc.12537}}

@article{Han2021102,
	abstract = {In order to solve the numerical prediction problem in PAD (pleasure, arousal and dominance) dimensional emotion prediction, a PAD dimensional emotion prediction model integrating heart rate variability (HRV) based on principal component analysis (PCA) and support vector regression (SVR) is proposed in this paper. The heart rate and heart interval data of 12 volunteers in two emotion states with relaxation and anxiety induced by music and video were collected by flexible iontronic sensing, and labeled on a PAD emotion scale. The time-domain, frequency-domain and nonlinear features of HRV were then extracted by different statistical methods, namely mean and variance, Welch power spectrum and Poincar{\'e} scatter diagram, respectively. Moreover, the PCA model was used to reduce the dimension of HRV features. The HRV features after dimensionality reduction were used as the input features of the SVR model for training and prediction. The experimental results show that the PCA-SVR model combined with HRV features had good prediction effects for the three dimensions of PAD, and its average consistency correlation coefficient (CCC) reached 0.51. The three prediction methods of the SVR, extreme learning machine (ELM) and the ELM based the PCA were compared, and the results showed that the proposed method resulted in improvements in CCC of 0.14, 0.10, and 0.04, respectively. Furthermore, the proposed method can divide emotions in detail, and has a certain complementary role in emotion recognition and analysis. Thus using the method in combination with wearable devices, it is possible to identify and predict emotions in daily life. {\copyright} 2021, Editorial Board of Journal of Beijing University of Chemical Technology (Natural Science Edition). All right reserved.},
	author = {Han, Yongming and Zhang, Mingxing and Geng, Zhiqiang},
	author_keywords = {Heart rate variability; PAD dimensional emotion; Principal component analysis; Support vector regression},
	doi = {10.13543/j.bhxbzr.2021.05.013},
	journal = {Beijing Huagong Daxue Xuebao (Ziran Kexueban)/Journal of Beijing University of Chemical Technology (Natural Science Edition)},
	note = {Cited by: 0},
	number = {5},
	pages = {102 -- 110},
	publication_stage = {Final},
	source = {Scopus},
	title = {Heart rate variability features for emotion dimensional prediction by using a principal component analysis-support vector regression (PCA-SVR) model; [基于心率变异性特征和PCA-SVR的PAD维度情感预测分析]},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117879673&doi=10.13543%2fj.bhxbzr.2021.05.013&partnerID=40&md5=dc0712cdfb4b191fcf90be3ac2f857b2},
	volume = {48},
	year = {2021},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117879673&doi=10.13543%2fj.bhxbzr.2021.05.013&partnerID=40&md5=dc0712cdfb4b191fcf90be3ac2f857b2},
	bdsk-url-2 = {https://doi.org/10.13543/j.bhxbzr.2021.05.013}}

@article{Vrysis202066,
	abstract = {Semantic audio analysis has become a fundamental task in modern audio applications, making the improvement and optimization of classification algorithms a necessity. Standard frame-based audio classification methods have been optimized and modern approaches introduce engineering methodologies that capture the temporal dependency between successive feature observations, following the process of temporal feature integration. Moreover, the deployment of the convolutional neural networks defined a new era on semantic audio analysis. The current paper attempts a thorough comparison between standard feature-based classification strategies, state-of-the-art temporal feature integration tactics and 1D/2D deep convolutional neural network setups, on typical audio classification tasks. Experiments focus on optimizing a lightweight configuration for convolutional network topologies on a Speech/Music/Other classification scheme that can be deployed on various audio information retrieval tasks, such as voice activity detection, speaker diarization, or speech emotion recognition. The outmost target of this work is to establish an optimized protocol for constructing deep convolutional topologies on general audio detection classification schemes, minimizing complexity and computational needs. {\copyright} 2020 Audio Engineering Society. All rights reserved.},
	author = {Vrysis, Lazaros and Tsipas, Nikolaos and Thoidis, Iordanis and Dimoulas, Charalampos},
	doi = {10.17743/JAES.2019.0058},
	journal = {AES: Journal of the Audio Engineering Society},
	keywords = {Audio acoustics; Audio systems; Convolution; Convolutional neural networks; Deep neural networks; Integration; Semantics; Speech recognition; Topology; Audio information retrievals; Classification algorithm; Convolutional networks; Engineering methodology; Feature-based classification; Speech emotion recognition; Temporal feature integrations; Voice activity detection; Classification (of information)},
	note = {Cited by: 34},
	number = {1-2},
	pages = {66 -- 77},
	publication_stage = {Final},
	source = {Scopus},
	title = {1D/2D Deep CNNs vs. Temporal Feature Integration for General Audio Classification},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084651899&doi=10.17743%2fJAES.2019.0058&partnerID=40&md5=b861aa6d7390598e18fbf677e859398b},
	volume = {68},
	year = {2020},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084651899&doi=10.17743%2fJAES.2019.0058&partnerID=40&md5=b861aa6d7390598e18fbf677e859398b},
	bdsk-url-2 = {https://doi.org/10.17743/JAES.2019.0058}}

@article{Kim2018245,
	abstract = {Emotional evaluation of video clips is the difficult task because it includes not only stationary objects as the background but also dynamic objects as the foreground. In addition, there are many video analysis problems to be solved beforehand to properly address the emotionrelated tasks. Recently, however, the convolutional neural network (CNN)-based deep learning approach, opens the possibility by solving the action recognition problem. Inspired by the CNN-based action recognition technology, this paper challenges to evaluate the emotion of video clips. In the paper, we propose a deep learning model to capture the video features and evaluate the emotion of a video clip on Thayer 2D emotion space. In the model, the pre-trained convolutional 3D neural network (C3D) generates short-term spatiotemporal features of the video, LSTM accumulates those consecutive time-varying features to characterize long-term dynamic behaviors, and multilayer perceptron (MLP) evaluates emotion of a video clip by regression on the emotion space. Due to the limited number of labeled data, the C3D is employed to extract diverse spatiotemporal from various layers by transfer learning technique. The pre-trained C3D on the Sports-1M dataset and long short term memory (LSTM) followed by the MLP for regression are trained in end-to-end manner to fine-tune the C3D, and to adjust weights of LSTM and the MLP-type emotion estimator. The proposed method achieves the concordance correlation coefficient values of 0.6024 for valence and 0.6460 for arousal, respectively. We believe this emotional evaluation of video could be easily associated with appropriate music recommendation, once the music is emotionally evaluated in the same high-level emotional space. {\copyright} The Korean Institute of Intelligent Systems.},
	author = {Kim, Byoungjun and Lee, Joonwhoan},
	author_keywords = {C3D; LSTM; Transfer learning; Video emotion analysis},
	doi = {10.5391/IJFIS.2018.18.4.245},
	journal = {International Journal of Fuzzy Logic and Intelligent Systems},
	note = {Cited by: 14; All Open Access, Gold Open Access},
	number = {4},
	pages = {245 -- 253},
	publication_stage = {Final},
	source = {Scopus},
	title = {A deep-learning based model for emotional evaluation of video clips},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062981746&doi=10.5391%2fIJFIS.2018.18.4.245&partnerID=40&md5=e70171a3f300bc0ac98989abde260b6c},
	volume = {18},
	year = {2018},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062981746&doi=10.5391%2fIJFIS.2018.18.4.245&partnerID=40&md5=e70171a3f300bc0ac98989abde260b6c},
	bdsk-url-2 = {https://doi.org/10.5391/IJFIS.2018.18.4.245}}

@article{Agarwal202198,
	abstract = {Music is the art of `language of emotions'. Recently, music mood recognition is an emerging task. An efficient supervised framework for music mood recognition using autoencoder-based optimised support vector regression (SVR) model is developed for the music emotion recognition. Our main intention is to increase the accuracy of emotion classification of music by considering text-dependent and non-text-dependent features. For the high level feature representation, stacked autoencoder is used with two hidden layers. Modified K-Medoid-based brain storm optimisation-based support vector regression (SVR_KMBSO) model is utilised for the emotion classification. Using the K-Medoid-based brain storm algorithm, the optimal parameters of the SVR are selected. The proposed framework utilises ISMIR2012 dataset and NJU_V1 dataset for English and for Hindi; online songs are also gathered and used for the music mood recognition. All the three datasets include songs based on four emotions like happy, angry, relax and sad. The experimental results are evaluated and compared with the existing classifiers including SVR, deep belief network (DBN) and Recurrent neural network (RNN). The proposed method SVR_KMBSO achieved high accuracy using three different datasets. {\copyright} 2021 The Authors. IET Signal Processing published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.},
	author = {Agarwal, Gaurav and Om, Hari},
	doi = {10.1049/sil2.12015},
	journal = {IET Signal Processing},
	keywords = {Character recognition; Emotion Recognition; Learning systems; Recurrent neural networks; Regression analysis; Storms; Auto encoders; Emotion classification; Emotion recognition; Feature representation; Hidden layers; High-level features; K-medoid; Music emotions; Support vector regression models; Support vector regressions; Music},
	note = {Cited by: 23},
	number = {2},
	pages = {98 -- 121},
	publication_stage = {Final},
	source = {Scopus},
	title = {An efficient supervised framework for music mood recognition using autoencoder-based optimised support vector regression model},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116929889&doi=10.1049%2fsil2.12015&partnerID=40&md5=4e078ee8b1c740d21d4b7d8dbf7f6cd8},
	volume = {15},
	year = {2021},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116929889&doi=10.1049%2fsil2.12015&partnerID=40&md5=4e078ee8b1c740d21d4b7d8dbf7f6cd8},
	bdsk-url-2 = {https://doi.org/10.1049/sil2.12015}}

@article{Ferreri20173721,
	abstract = {Music represents a special type of reward involving the recruitment of the mesolimbic dopaminergic system. According to recent theories on episodic memory formation, as dopamine strengthens the synaptic potentiation produced by learning, stimuli triggering dopamine release could result in long-term memory improvements. Here, we behaviourally test whether music-related reward responses could modulate episodic memory performance. Thirty participants rated (in terms of arousal, familiarity, emotional valence, and reward) and encoded unfamiliar classical music excerpts. Twenty-four hours later, their episodic memory was tested (old/new recognition and remember/know paradigm). Results revealed an influence of music-related reward responses on memory: excerpts rated as more rewarding were significantly better recognized and remembered. Furthermore, inter-individual differences in the ability to experience musical reward, measured through the Barcelona Music Reward Questionnaire, positively predicted memory performance. Taken together, these findings shed new light on the relationship between music, reward and memory, showing for the first time that music-driven reward responses are directly implicated in higher cognitive functions and can account for individual differences in memory performance. {\copyright} 2017, Springer-Verlag GmbH Germany.},
	author = {Ferreri, Laura and Rodriguez-Fornells, Antoni},
	author_keywords = {Episodic memory; Music; Musical hedonia; Reward},
	doi = {10.1007/s00221-017-5095-0},
	journal = {Experimental Brain Research},
	keywords = {Acoustic Stimulation; Adolescent; Adult; Analysis of Variance; Arousal; Emotions; Female; Humans; Male; Memory, Episodic; Mental Recall; Music; Recognition (Psychology); Reward; Young Adult; adult; arousal; Article; barcelona music reward questionnaire; cognition; emotion; episodic memory; experience; female; human; human experiment; male; mental task; music; neuromodulation; normal human; priority journal; questionnaire; recognition; reward; university student; adolescent; analysis of variance; auditory stimulation; music; physiology; psychology; recall; young adult},
	note = {Cited by: 28},
	number = {12},
	pages = {3721 -- 3731},
	publication_stage = {Final},
	source = {Scopus},
	title = {Music-related reward responses predict episodic memory performance},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029758928&doi=10.1007%2fs00221-017-5095-0&partnerID=40&md5=1792cc6f0a0d64add5f64e4718cb99ab},
	volume = {235},
	year = {2017},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029758928&doi=10.1007%2fs00221-017-5095-0&partnerID=40&md5=1792cc6f0a0d64add5f64e4718cb99ab},
	bdsk-url-2 = {https://doi.org/10.1007/s00221-017-5095-0}}

@article{Fernandez2021,
	abstract = {Congenital amusia in its most common form is a disorder characterized by a musical pitch processing deficit. Although pitch is involved in conveying emotion in music, the implications for pitch deficits on musical emotion judgements is still under debate. Relatedly, both limited and spared musical emotion recognition was reported in amusia in conditions where emotion cues were not determined by musical mode or dissonance. Additionally, assumed links between musical abilities and visuo-spatial attention processes need further investigation in congenital amusics. Hence, we here test to what extent musical emotions can influence attentional performance. Fifteen congenital amusic adults and fifteen healthy controls matched for age and education were assessed in three attentional conditions: executive control (distractor inhibition), alerting, and orienting (spatial shift) while music expressing either joy, tenderness, sadness, or tension was presented. Visual target detection was in the normal range for both accuracy and response times in the amusic relative to the control participants. Moreover, in both groups, music exposure produced facilitating effects on selective attention that appeared to be driven by the arousal dimension of musical emotional content, with faster correct target detection during joyful compared to sad music. These findings corroborate the idea that pitch processing deficits related to congenital amusia do not impede other cognitive domains, particularly visual attention. Furthermore, our study uncovers an intact influence of music and its emotional content on the attentional abilities of amusic individuals. The results highlight the domain-selectivity of the pitch disorder in congenital amusia, which largely spares the development of visual attention and affective systems. {\copyright} Copyright {\copyright} 2021 Fernandez, Vuilleumier, Gosselin and Peretz.},
	author = {Fernandez, Natalia B. and Vuilleumier, Patrik and Gosselin, Nathalie and Peretz, Isabelle},
	author_keywords = {congenital amusia; emotion; executive control; music exposure; selective attention},
	doi = {10.3389/fnhum.2020.566841},
	journal = {Frontiers in Human Neuroscience},
	keywords = {adult; arousal; article; controlled study; education; emotion; executive function; female; human; human experiment; male; music; pitch; reaction time; sadness; selective attention; tension; visual attention},
	note = {Cited by: 3; All Open Access, Gold Open Access, Green Open Access},
	publication_stage = {Final},
	source = {Scopus},
	title = {Influence of Background Musical Emotions on Attention in Congenital Amusia},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100679612&doi=10.3389%2ffnhum.2020.566841&partnerID=40&md5=08cef00f3012a04e714cdc4da213fe16},
	volume = {14},
	year = {2021},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100679612&doi=10.3389%2ffnhum.2020.566841&partnerID=40&md5=08cef00f3012a04e714cdc4da213fe16},
	bdsk-url-2 = {https://doi.org/10.3389/fnhum.2020.566841}}

@article{Doma2020,
	abstract = {Emotion recognition using brain signals has the potential to change the way we identify and treat some health conditions. Difficulties and limitations may arise in general emotion recognition software due to the restricted number of facial expression triggers, dissembling of emotions, or among people with alexithymia. Such triggers are identified by studying the continuous brainwaves generated by human brain. Electroencephalogram (EEG) signals from the brain give us a more diverse insight on emotional states that one may not be able to express. Brainwave EEG signals can reflect the changes in electrical potential resulting from communications networks between neurons. This research involves analyzing the epoch data from EEG sensor channels and performing comparative analysis of multiple machine learning techniques [namely Support Vector Machine (SVM), K-nearest neighbor, Linear Discriminant Analysis, Logistic Regression and Decision Trees each of these models] were tested with and without principal component analysis (PCA) for dimensionality reduction. Grid search was also utilized for hyper-parameter tuning for each of the tested machine learning models over Spark cluster for lowered execution time. The DEAP Dataset was used in this study, which is a multimodal dataset for the analysis of human affective states. The predictions were based on the labels given by the participants for each of the 40 1-min long excerpts of music. music. Participants rated each video in terms of the level of arousal, valence, like/dislike, dominance and familiarity. The binary class classifiers were trained on the time segmented, 15 s intervals of epoch data, individually for each of the 4 classes. PCA with SVM performed the best and produced an F1-score of 84.73% with 98.01% recall in the 30th to 45th interval of segmentation. For each of the time segments and ``a binary training class'' a different classification model converges to a better accuracy and recall than others. The results prove that different classification models must be used to identify different emotional states. {\copyright} 2020, The Author(s).},
	author = {Doma, Vikrant and Pirouz, Matin},
	author_keywords = {Classification; Emotion recognition; Machine learning; Multi-channel EEG},
	doi = {10.1186/s40537-020-00289-7},
	journal = {Journal of Big Data},
	keywords = {Biomedical signal processing; Decision trees; Discriminant analysis; Electroencephalography; Learning systems; Nearest neighbor search; Principal component analysis; Speech recognition; Support vector machines; Brain wave; Comparative analyzes; Electroencephalogram signals; Emotion recognition; Emotional state; Machine-learning; Multi channel; Multi-channel electroencephalogram; Principal-component analysis; Support vectors machine; Emotion Recognition},
	note = {Cited by: 86},
	number = {1},
	publication_stage = {Final},
	source = {Scopus},
	title = {A comparative analysis of machine learning methods for emotion recognition using EEG and peripheral physiological signals},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081367885&doi=10.1186%2fs40537-020-00289-7&partnerID=40&md5=33c7246874faa195268c6d262f9047e8},
	volume = {7},
	year = {2020},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081367885&doi=10.1186%2fs40537-020-00289-7&partnerID=40&md5=33c7246874faa195268c6d262f9047e8},
	bdsk-url-2 = {https://doi.org/10.1186/s40537-020-00289-7}}

@article{Bo20192439,
	abstract = {Electroencephalographic (EEG) based emotion recognition has attracted increasing attention in the field of human-computer interaction (HCI). But, how to use the cognitive principles to enhance the emotion recognition model is still a challenge. The purpose of this research paper is to investigate the emotion cognitive process and its application. Firstly, to evoke the response emotions, a three-stage experimental paradigm of long-time music stimuli was designed. The EEG signals were recorded in 15 healthy adults during the listening of 16 music clips respectively. Then, the time course analysis method of music-evoked emotions was proposed to examine the differences of brain activities. There was great increase of the spectral power in alpha band and a slight decrease in high-frequency beta and gamma bands during music listening. Through time analysis, the characteristics of the inspiring--keeping--fading were also found in different emotional states. After that, the most relevant EEG features were selected based on the time correlation analysis between EEG and music features. Finally, based on the cognitive principles inspired EEG features, an emotional prediction system was built. From the results, the accuracies of binary classification were 66.8% for valence and 59.5% for arousal. The accuracies of 3-classes classification performed as 45.9% for valence and 45.1% for arousal. These results suggest that with the help of cognitive principles, a better emotional recognition system could be built. Understanding the cognitive process could promote the development of artificial intelligence. {\copyright} 2018, Springer-Verlag GmbH Germany, part of Springer Nature.},
	author = {Bo, Hongjian and Ma, Lin and Liu, Quansheng and Xu, Ruifeng and Li, Haifeng},
	author_keywords = {Affective computing; Electroencephalography (EEG); Music-evoked emotions; Temporal-spectral analysis},
	doi = {10.1007/s13042-018-0880-z},
	journal = {International Journal of Machine Learning and Cybernetics},
	keywords = {Brain; Cognitive systems; Electrophysiology; Human computer interaction; Spectrum analysis; Speech recognition; Affective Computing; Binary classification; Cognitive principles; Electroencephalographic (EEG); Emotional recognition; Human computer interaction (HCI); Music-evoked emotions; Time-course analysis; Electroencephalography},
	note = {Cited by: 23},
	number = {9},
	pages = {2439 -- 2448},
	publication_stage = {Final},
	source = {Scopus},
	title = {Music-evoked emotion recognition based on cognitive principles inspired EEG temporal and spectral features},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070927419&doi=10.1007%2fs13042-018-0880-z&partnerID=40&md5=e6b7e05f2e063717043bcf71ee5f2984},
	volume = {10},
	year = {2019},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070927419&doi=10.1007%2fs13042-018-0880-z&partnerID=40&md5=e6b7e05f2e063717043bcf71ee5f2984},
	bdsk-url-2 = {https://doi.org/10.1007/s13042-018-0880-z}}

@article{Belfi202134,
	abstract = {Famous musical melodies, such as ``Row, Row, Row Your Boat'' and ``Hot Cross Buns,'' are frequently used in psychological research. Such melodies have been used to assess the degree of cognitive impairments in various neurological disorders, and to investigate differences between ``naming'' vs. ``knowing.'' Despite their utility as an experimental stimulus, there is currently no standardized, openly available set of famous musical melodies based on a United States population, as prior work on the topic has primarily relied on creating stimuli in an ad hoc manner. Therefore, the goal of the present work was to create a set of famous musical melodies. Here, we describe the development of the Famous Melodies Stimulus Set, a set of 107 melodies. We provide normative data for the melodies on five dimensions: familiarity, age of acquisition, emotional valence, emotional arousal, and naming ability. Participants (N = 397) rated the melodies on these five variables, validating that most melodies were highly familiar and reliably named. While familiarity ratings were skewed, all other rating scales covered a relatively broad range, allowing for researchers to select melodies for future work based on particular attributes. {\copyright} 2020, The Psychonomic Society, Inc.},
	author = {Belfi, Amy M. and Kacirek, Kaelyn},
	author_keywords = {Arousal; Familiarity; Music; Naming; Valence},
	doi = {10.3758/s13428-020-01411-6},
	journal = {Behavior Research Methods},
	keywords = {Arousal; Emotions; Humans; Music; Recognition, Psychology; adult; arousal; article; human; human experiment; major clinical study; music; rating scale; arousal; emotion},
	note = {Cited by: 8; All Open Access, Bronze Open Access},
	number = {1},
	pages = {34 -- 48},
	publication_stage = {Final},
	source = {Scopus},
	title = {The famous melodies stimulus set},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086733915&doi=10.3758%2fs13428-020-01411-6&partnerID=40&md5=1f82768fc612de27263c38beb00876dc},
	volume = {53},
	year = {2021},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086733915&doi=10.3758%2fs13428-020-01411-6&partnerID=40&md5=1f82768fc612de27263c38beb00876dc},
	bdsk-url-2 = {https://doi.org/10.3758/s13428-020-01411-6}}

@article{Sorussa202053,
	abstract = {Music selection is diffcult without effcient organization based on metadata or tags, and one effective tag scheme is based on the emotion expressed by the music. However, manual annotation is labor intensive and unstable because the perception of music emotion varies from person to person. This paper presents an emotion classi cation system for digital music with a resolution of eight emotional classes. Russell's emotion model was adopted as common ground for emotional annotation. The music information retrieval (MIR) toolbox was employed to extract acoustic features from audio les. The classi cation system utilized a supervised machine learning technique to recognize acoustic features and create predictive models. Four predictive models were proposed and compared. The models were composed by crossmatching two types of neural networks, the Levenberg-Marquardt (LM) and resilient backpropagation (Rprop), with two types of structures: a traditional multiclass model and the cascaded structure of a binary-class model. The performance of each model was evaluated via the MediaEval Database for Emotional Analysis (DEAM) benchmark. The best result was achieved by the model trained with the cascaded Rprop neural network (accuracy of 89.5%). In addition, correlation coeffcient analysis showed that timbre features were the most impactful for prediction. Our work offers an opportunity for a competitive advantage in music classi cation because only a few music providers currently tag music with emotional terms. {\copyright} 2020, ECTI Association Sirindhon International Institute of Technology. All rights reserved.},
	author = {Sorussa, Kanawat and Choksuriwong, Anant and Karnjanadecha, Montri},
	author_keywords = {Articial neural networks; Classi cation algorithms; Emotion recognition; Music information retrieval},
	doi = {10.37936/ecti-cit.2020141.205317},
	journal = {ECTI Transactions on Computer and Information Technology},
	note = {Cited by: 4; All Open Access, Gold Open Access},
	number = {1},
	pages = {53 -- 66},
	publication_stage = {Final},
	source = {Scopus},
	title = {Emotion classi cation system for digital music with a cascaded technique},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084006024&doi=10.37936%2fecti-cit.2020141.205317&partnerID=40&md5=508dd81e9a9120b0c6e87d44e90a2d5a},
	volume = {14},
	year = {2020},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084006024&doi=10.37936%2fecti-cit.2020141.205317&partnerID=40&md5=508dd81e9a9120b0c6e87d44e90a2d5a},
	bdsk-url-2 = {https://doi.org/10.37936/ecti-cit.2020141.205317}}

@article{Aljanaki2017,
	abstract = {Music emotion recognition (MER) field rapidly expanded in the last decade. Many new methods and new audio features are developed to improve the performance of MER algorithms. However, it is very difficult to compare the performance of the new methods because of the data representation diversity and scarcity of publicly available data. In this paper, we address these problems by creating a data set and a benchmark for MER. The data set that we release, a MediaEval Database for Emotional Analysis in Music (DEAM), is the largest available data set of dynamic annotations (valence and arousal annotations for 1,802 songs and song excerpts licensed under Creative Commons with 2Hz time resolution). Using DEAM, we organized the 'Emotion in Music' task at MediaEval Multimedia Evaluation Campaign from 2013 to 2015. The benchmark attracted, in total, 21 active teams to participate in the challenge. We analyze the results of the benchmark: the winning algorithms and feature-sets. We also describe the design of the benchmark, the evaluation procedures and the data cleaning and transformations that we suggest. The results from the benchmark suggest that the recurrent neural network based approaches combined with large feature-sets work best for dynamic MER. {\copyright} 2017 Aljanaki et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
	author = {Aljanaki, Anna and Yang, Yi-Hsuan and Soleymani, Mohammad},
	doi = {10.1371/journal.pone.0173392},
	journal = {PLoS ONE},
	keywords = {Algorithms; Arousal; Benchmarking; Emotions; Humans; Music; acoustics; algorithm; Article; artificial neural network; auditory feedback; auditory stimulation; benchmarking; data base; data processing; emotion recognition; evaluation study; machine learning; music; music emotion recognition; recognition; arousal; emotion; human},
	note = {Cited by: 120; All Open Access, Gold Open Access, Green Open Access},
	number = {3},
	publication_stage = {Final},
	source = {Scopus},
	title = {Developing a benchmark for emotional analysis of music},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015192152&doi=10.1371%2fjournal.pone.0173392&partnerID=40&md5=96e017e2254d9bcb08be87ef837b69a0},
	volume = {12},
	year = {2017},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015192152&doi=10.1371%2fjournal.pone.0173392&partnerID=40&md5=96e017e2254d9bcb08be87ef837b69a0},
	bdsk-url-2 = {https://doi.org/10.1371/journal.pone.0173392}}

@article{Zhang2016333,
	abstract = {Music emotion recognition (MER) is an important topic in music understanding, recommendation, retrieval and human computer interaction. Great success has been achieved by machine learning methods in estimating human emotional response to music. However, few of them pay much attention in semantic interpret for emotion response. In our work, we first train an interpretable model between acoustic audio and emotion. Filter, wrapper and shrinkage methods are applied to select important features. We then apply statistical models to build and explain the emotion model. Extensive experimental results reveal that the shrinkage methods outperform the wrapper methods and the filter methods in arousal emotion. In addition, we observed that only a small set of the extracted features have the key effects to arousal. While, most of our extracted features have small contribution to valence music perception. Ultimately, we obtain a higher average accuracy rate in arousal, compared to that in valence. {\copyright} 2016 Elsevier B.V.},
	author = {Zhang, JiangLong and Huang, XiangLin and Yang, Lifang and Nie, Liqiang},
	author_keywords = {Feature selection; Interpretable model; Music emotion; Semantic gap; Shrinkage methods},
	doi = {10.1016/j.neucom.2016.01.099},
	journal = {Neurocomputing},
	keywords = {Artificial intelligence; Feature extraction; Human computer interaction; Learning systems; Semantics; Shrinkage; Acoustic features; Emotional response; Important features; Machine learning methods; Music emotions; Music understanding; Semantic gap; Shrinkage methods; acoustics; arousal; Article; brain computer interface; controlled study; emotion; human; limit of quantitation; machine learning; measurement accuracy; music; music emotion recognition; principal component analysis; priority journal; semantic gap; semantics; statistical model; Audio acoustics},
	note = {Cited by: 18},
	pages = {333 -- 341},
	publication_stage = {Final},
	source = {Scopus},
	title = {Bridge the semantic gap between pop music acoustic feature and emotion: Build an interpretable model},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973570901&doi=10.1016%2fj.neucom.2016.01.099&partnerID=40&md5=778337b1db0cfb023e3a004a166bc9dc},
	volume = {208},
	year = {2016},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973570901&doi=10.1016%2fj.neucom.2016.01.099&partnerID=40&md5=778337b1db0cfb023e3a004a166bc9dc},
	bdsk-url-2 = {https://doi.org/10.1016/j.neucom.2016.01.099}}

@article{Chen2021,
	abstract = {Recently, emotion classification from electroencephalogram (EEG) data has attracted much attention. As EEG is an unsteady and rapidly changing voltage signal, the features extracted from EEG usually change dramatically, whereas emotion states change gradually. Most existing feature extraction approaches do not consider these differences between EEG and emotion. Microstate analysis could capture important spatio-temporal properties of EEG signals. At the same time, it could reduce the fast-changing EEG signals to a sequence of prototypical topographical maps. While microstate analysis has been widely used to study brain function, few studies have used this method to analyze how brain responds to emotional auditory stimuli. In this study, the authors proposed a novel feature extraction method based on EEG microstates for emotion recognition. Determining the optimal number of microstates automatically is a challenge for applying microstate analysis to emotion. This research proposed dual-threshold-based atomize and agglomerate hierarchical clustering (DTAAHC) to determine the optimal number of microstate classes automatically. By using the proposed method to model the temporal dynamics of auditory emotion process, we extracted microstate characteristics as novel temporospatial features to improve the performance of emotion recognition from EEG signals. We evaluated the proposed method on two datasets. For public music-evoked EEG Dataset for Emotion Analysis using Physiological signals, the microstate analysis identified 10 microstates which together explained around 86% of the data in global field power peaks. The accuracy of emotion recognition achieved 75.8% in valence and 77.1% in arousal using microstate sequence characteristics as features. Compared to previous studies, the proposed method outperformed the current feature sets. For the speech-evoked EEG dataset, the microstate analysis identified nine microstates which together explained around 85% of the data. The accuracy of emotion recognition achieved 74.2% in valence and 72.3% in arousal using microstate sequence characteristics as features. The experimental results indicated that microstate characteristics can effectively improve the performance of emotion recognition from EEG signals. {\copyright} Copyright {\copyright} 2021 Chen, Li, Ma, Bo, Soong and Shi.},
	author = {Chen, Jing and Li, Haifeng and Ma, Lin and Bo, Hongjian and Soong, Frank and Shi, Yaohui},
	author_keywords = {auditory emotion process; dual-threshold-based AAHC; EEG; emotion recognition; microstate characteristics},
	doi = {10.3389/fnins.2021.689791},
	journal = {Frontiers in Neuroscience},
	keywords = {arousal; article; brain function; electroencephalogram; emotion; feature extraction; hierarchical clustering; human; human experiment; molecular recognition; music; speech},
	note = {Cited by: 14; All Open Access, Gold Open Access, Green Open Access},
	publication_stage = {Final},
	source = {Scopus},
	title = {Dual-Threshold-Based Microstate Analysis on Characterizing Temporal Dynamics of Affective Process and Emotion Recognition From EEG Signals},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111878700&doi=10.3389%2ffnins.2021.689791&partnerID=40&md5=c62afefc481168f124c0f8c8261c7877},
	volume = {15},
	year = {2021},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111878700&doi=10.3389%2ffnins.2021.689791&partnerID=40&md5=c62afefc481168f124c0f8c8261c7877},
	bdsk-url-2 = {https://doi.org/10.3389/fnins.2021.689791}}

@article{Clerico2018,
	abstract = {The quantity of music content is rapidly increasing and automated affective tagging of music video clips can enable the development of intelligent retrieval, music recommendation, automatic playlist generators, and music browsing interfaces tuned to the users' current desires, preferences, or affective states. To achieve this goal, the field of affective computing has emerged, in particular the development of so-called affective brain-computer interfaces, which measure the user's affective state directly from measured brain waves using non-invasive tools, such as electroencephalography (EEG). Typically, conventional features extracted from the EEG signal have been used, such as frequency subband powers and/or inter-hemispheric power asymmetry indices. More recently, the coupling between EEG and peripheral physiological signals, such as the galvanic skin response (GSR), have also been proposed. Here, we show the importance of EEG amplitude modulations and propose several new features that measure the amplitude-amplitude cross-frequency coupling per EEG electrode, as well as linear and non-linear connections between multiple electrode pairs. When tested on a publicly available dataset of music video clips tagged with subjective affective ratings, support vector classifiers trained on the proposed features were shown to outperform those trained on conventional benchmark EEG features by as much as 6, 20, 8, and 7% for arousal, valence, dominance and liking, respectively. Moreover, fusion of the proposed features with EEG-GSR coupling features showed to be particularly useful for arousal (feature-level fusion) and liking (decision-level fusion) prediction. Together, these findings show the importance of the proposed features to characterize human affective states during music clip watching. {\copyright} 2018 Clerico, Tiwari, Gupta, Jayaraman and Falk.},
	author = {Clerico, Andrea and Tiwari, Abhishek and Gupta, Rishabh and Jayaraman, Srinivasan and Falk, Tiago H.},
	author_keywords = {Affective computing; Electroencephalography; Emotion classification; Multimedia content; Pattern classification; Physiological signals; Signal processing},
	doi = {10.3389/fncom.2017.00115},
	journal = {Frontiers in Computational Neuroscience},
	keywords = {Amplitude modulation; Brain computer interface; Classification (of information); Electrodes; Electrophysiology; Human computer interaction; Interface states; Interfaces (computer); Modulation; Pattern recognition; Physiological models; Physiology; Signal processing; User interfaces; Video cameras; Affective Computing; Crossfrequency couplings (CFC); Emotion classification; Galvanic skin response; Intelligent retrieval; Multimedia contents; Physiological signals; Support vector classifiers; adult; affect; amplitude modulation; amplitude modulation coherence; amplitude modulation energy; amplitude modulation interaction; arousal; Article; brain computer interface; brain region; breathing; computer prediction; electrodermal response; electroencephalogram; electroencephalography; female; frontal cortex; human; information processing; male; music; neurophysiological monitoring; non invasive measurement; pulse rate; skin temperature; stimulus response; support vector machine; videorecording; Electroencephalography},
	note = {Cited by: 20; All Open Access, Gold Open Access, Green Open Access},
	publication_stage = {Final},
	source = {Scopus},
	title = {Electroencephalography amplitude modulation analysis for automated affective tagging of music video clips},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041100606&doi=10.3389%2ffncom.2017.00115&partnerID=40&md5=c92d8ecc61c786b6b72105c9b47639ad},
	volume = {11},
	year = {2018},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041100606&doi=10.3389%2ffncom.2017.00115&partnerID=40&md5=c92d8ecc61c786b6b72105c9b47639ad},
	bdsk-url-2 = {https://doi.org/10.3389/fncom.2017.00115}}

@article{Li2016762,
	abstract = {In the present paper,wavelet transform and empirical mode decomposition(EMD)are combined to extracted the features of electroencephalogram(EEG)signal with music intervention,and to achieve a better classification accuracy rate and reliability in emotional assessment in order to provide a support for music therapy.The data were from Database for Emotion Analysis using Physiological Signals(DEAP).Based on wavelet transformα,βandθrhythms were extracted at frontal(F3,F4),temporal(T7,T8)and central regions(C3,C4).Based on the EMD,the intrinsic mode function(IMF)was analyzed and extracted.Furthermore,average energy and amplitude difference of IMF were analyzed and obtained.The support vector machine was used to assess the state of emotion in order to support music therapy.According to this algorithm,the classification accuracy rate could reach 100% between no emotions,positive emotions and negative emotions,which made a 10%improvement between positive and negative emotion recognition.Effective evaluation result between positive and negative emotions was achieved.The states of emotion would influence the effect of music therapy,undoubtedly,the classification accuracy rate increasing of emotional assessment will further help improve the effect of music therapy and provide a better support to the therapy.},
	author = {Li, Xin and Tian, Yanxiu and Hou, Yongjie and Qi, Xiaoying and Sun, Xiaoqi and Fan, MEngdi and Cai, Erjuan},
	journal = {Sheng wu yi xue gong cheng xue za zhi = Journal of biomedical engineering = Shengwu yixue gongchengxue zazhi},
	keywords = {Algorithms; Electroencephalography; Emotions; Humans; Music Therapy; Reproducibility of Results; Signal Processing, Computer-Assisted; Support Vector Machine; Wavelet Analysis; algorithm; electroencephalography; emotion; human; music therapy; reproducibility; signal processing; support vector machine; wavelet analysis},
	note = {Cited by: 1},
	number = {4},
	pages = {762 -- 769},
	publication_stage = {Final},
	source = {Scopus},
	title = {Applications of Wavelet Transform Combining Empirical Mode Decomposition in EEG Analysis with Music Intervention},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049117812&partnerID=40&md5=9c6fbda5a1a79391f2f6a5ede2fba382},
	volume = {33},
	year = {2016},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049117812&partnerID=40&md5=9c6fbda5a1a79391f2f6a5ede2fba382}}

@article{Czepiel2021,
	abstract = {While there is an increasing shift in cognitive science to study perception of naturalistic stimuli, this study extends this goal to naturalistic contexts by assessing physiological synchrony across audience members in a concert setting. Cardiorespiratory, skin conductance, and facial muscle responses were measured from participants attending live string quintet performances of full-length works from Viennese Classical, Contemporary, and Romantic styles. The concert was repeated on three consecutive days with different audiences. Using inter-subject correlation (ISC) to identify reliable responses to music, we found that highly correlated responses depicted typical signatures of physiological arousal. By relating physiological ISC to quantitative values of music features, logistic regressions revealed that high physiological synchrony was consistently predicted by faster tempi (which had higher ratings of arousing emotions and engagement), but only in Classical and Romantic styles (rated as familiar) and not the Contemporary style (rated as unfamiliar). Additionally, highly synchronised responses across all three concert audiences occurred during important structural moments in the music---identified using music theoretical analysis---namely at transitional passages, boundaries, and phrase repetitions. Overall, our results show that specific music features induce similar physiological responses across audience members in a concert context, which are linked to arousal, engagement, and familiarity. {\copyright} 2021, The Author(s).},
	author = {Czepiel, Anna and Fink, Lauren K. and Fink, Lea T. and Wald-Fuhrmann, Melanie and Tr{\"o}ndle, Martin and Merrill, Julia},
	doi = {10.1038/s41598-021-00492-3},
	journal = {Scientific Reports},
	keywords = {Acoustic Stimulation; Adolescent; Adult; Aged; Aged, 80 and over; Arousal; Auditory Perception; Emotions; Facial Muscles; Female; Heart Rate; Humans; Logistic Models; Male; Middle Aged; Music; Recognition, Psychology; Respiratory Rate; Young Adult; adult; arousal; article; case report; clinical article; emotion; face muscle; female; human; human experiment; male; music; perception; physiological coregulation; psychology; quantitative analysis; skin conductance; adolescent; aged; auditory stimulation; breathing rate; hearing; heart rate; middle aged; music; physiology; procedures; psychology; statistical model; very elderly; young adult},
	note = {Cited by: 21; All Open Access, Gold Open Access, Green Open Access},
	number = {1},
	publication_stage = {Final},
	source = {Scopus},
	title = {Synchrony in the periphery: inter-subject correlation of physiological responses during live music concerts},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119294610&doi=10.1038%2fs41598-021-00492-3&partnerID=40&md5=b6bf4b482127d12f8785e58a63a25445},
	volume = {11},
	year = {2021},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119294610&doi=10.1038%2fs41598-021-00492-3&partnerID=40&md5=b6bf4b482127d12f8785e58a63a25445},
	bdsk-url-2 = {https://doi.org/10.1038/s41598-021-00492-3}}

@article{Yang2021,
	abstract = {Music plays an extremely important role in people's production and life. The amount of music is growing rapidly. At the same time, the demand for music organization, classification, and retrieval is also increasing. Paying more attention to the emotional expression of creators and the psychological characteristics of music are also indispensable personalized needs of users. The existing music emotion recognition (MER) methods have the following two challenges. First, the emotional color conveyed by the first music is constantly changing with the playback of the music, and it is difficult to accurately express the ups and downs of music emotion based on the analysis of the entire music. Second, it is difficult to analyze music emotions based on the pitch, length, and intensity of the notes, which can hardly reflect the soul and connotation of music. In this paper, an improved back propagation (BP) algorithm neural network is used to analyze music data. Because the traditional BP network tends to fall into local solutions, the selection of initial weights and thresholds directly affects the training effect. This paper introduces artificial bee colony (ABC) algorithm to improve the structure of BP neural network. The output value of the ABC algorithm is used as the weight and threshold of the BP neural network. The ABC algorithm is responsible for adjusting the weights and thresholds, and feeds back the optimal weights and thresholds to the BP neural network system. BP neural network with ABC algorithm can improve the global search ability of the BP network, while reducing the probability of the BP network falling into the local optimal solution, and the convergence speed is faster. Through experiments on public music data sets, the experimental results show that compared with other comparative models, the MER method used in this paper has better recognition effect and faster recognition speed. {\copyright} Copyright {\copyright} 2021 Yang.},
	author = {Yang, Jing},
	author_keywords = {ABC algorithm; BP neural network; emotion recognition; MediaEval Emotion in Music data set; music},
	doi = {10.3389/fpsyg.2021.760060},
	journal = {Frontiers in Psychology},
	note = {Cited by: 14; All Open Access, Gold Open Access, Green Open Access},
	publication_stage = {Final},
	source = {Scopus},
	title = {A Novel Music Emotion Recognition Model Using Neural Network Technology},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116855247&doi=10.3389%2ffpsyg.2021.760060&partnerID=40&md5=d8babd4e6050f14375d2b4632dfdb587},
	volume = {12},
	year = {2021},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116855247&doi=10.3389%2ffpsyg.2021.760060&partnerID=40&md5=d8babd4e6050f14375d2b4632dfdb587},
	bdsk-url-2 = {https://doi.org/10.3389/fpsyg.2021.760060}}

@article{Ziegler202150,
	abstract = {ODO is a journey through worlds of imagination inspired by Antoine de Saint-Exup{\'e}ry's Little Prince and Stanley Kubrick's ``HAL9000'' in 2001: A Space Odyssey. An AI character lives on stage as in Plato's Cave. ODO can't leave the stage; ODO is offline {\ldots} Every visitor to the installation means the world to him/her/it. It collects stories and narratives to understand how our world works. ODO is theatre, opera and choreographic architecture. The stage is the orchestra pit of an ancient Greek tragedy where a chorus of three to five audience members interrogates the main character. ODO is a world builder who creates imaginary worlds on stage using a robotic light matrix, moving LEDs like pixels in space. ODO continues through the storyline of the piece with verbal and physical dialogues. ODO uses AI algorithms--Natural Language Processing (NLP)--to conduct natural conversations with the audience and Deep Learning to create Haiku poems and music. ODO has sensors to hear and see the audience. ODO uses Face Recognition Algorithms and Crowd Cluster Tools to understand emotions and physical behaviour. With all means possible, ODO tries to get ``in touch'' with us!. {\copyright} 2021 Informa UK Limited, trading as Taylor & Francis Group.},
	author = {Ziegler, Chris},
	doi = {10.1080/23322551.2021.1940682},
	journal = {Theatre and Performance Design},
	note = {Cited by: 0},
	number = {1-2},
	pages = {50 -- 60},
	publication_stage = {Final},
	source = {Scopus},
	title = {No Body lives here (ODO). 2020: An Interactive Theater Performance for five people + one AI characterMuffatwerk, Munich, Germany/Centre for Art and Media ZKM, Karlsruhe},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113744629&doi=10.1080%2f23322551.2021.1940682&partnerID=40&md5=fa53f9dba3b9a6e54d8ee86a3dd1d96f},
	volume = {7},
	year = {2021},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113744629&doi=10.1080%2f23322551.2021.1940682&partnerID=40&md5=fa53f9dba3b9a6e54d8ee86a3dd1d96f},
	bdsk-url-2 = {https://doi.org/10.1080/23322551.2021.1940682}}

@article{Zhang2016321,
	abstract = {Based on the researches and improvement of the main melody recognition technology of music, this paper constructs the music feature space model, and the characteristic parameters of the model feature space are marked, furthermore on this basis, the music emotion computer automatic recognition model is studied, and design and simulation of automatic recognition system are achieved based on the MATLAB platform. This paper also proposes an automatic recognition model by using BP neural network algorithm. Experimental results show that the proposed BP neural network is effective in music emotion recognition after comparing it with the statistical classification algorithm. {\copyright} AISTI 2016.},
	author = {Zhang, Yang and Liu, Heng},
	author_keywords = {Computer analysis; Computer recognition technology; Emotion analysis; Music production},
	journal = {RISTI - Revista Iberica de Sistemas e Tecnologias de Informacao},
	note = {Cited by: 0},
	number = {E13},
	pages = {321 -- 331},
	publication_stage = {Final},
	source = {Scopus},
	title = {Research on music production based on computer analysis and recognition technology},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016046810&partnerID=40&md5=7c6f1412441ed987b75e88238a9e53ce},
	volume = {2016},
	year = {2016},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016046810&partnerID=40&md5=7c6f1412441ed987b75e88238a9e53ce}}

@article{Feradov2020,
	abstract = {There is a strong correlation between the like/dislike responses to audio--visual stimuli and the emotional arousal and valence reactions of a person. In the present work, our attention is focused on the automated detection of dislike responses based on EEG activity when music videos are used as audio--visual stimuli. Specifically, we investigate the discriminative capacity of the Logarithmic Energy (LogE), Linear Frequency Cepstral Coefficients (LFCC), Power Spectral Density (PSD) and Discrete Wavelet Transform (DWT)-based EEG features, computed with and without segmentation of the EEG signal, on the dislike detection task. We carried out a comparative evaluation with eighteen modifications of the above-mentioned EEG features that cover different frequency bands and use different energy decomposition methods and spectral resolutions. For that purpose, we made use of Na{\"\i}ve Bayes classifier (NB), Classification and regression trees (CART), k-Nearest Neighbors (kNN) classifier, and support vector machines (SVM) classifier with a radial basis function (RBF) kernel trained with the Sequential Minimal Optimization (SMO) method. The experimental evaluation was performed on the well-known and widely used DEAP dataset. A classification accuracy of up to 98.6% was observed for the best performing combination of pre-processing, EEG features and classifier. These results support that the automated detection of like/dislike reactions based on EEG activity is feasible in a personalized setup. This opens opportunities for the incorporation of such functionality in entertainment, healthcare and security applications. {\copyright} 2020 by the authors. Licensee MDPI, Basel, Switzerland.},
	author = {Feradov, Firgan and Mporas, Iosif and Ganchev, Todor},
	author_keywords = {Classification and regression threes (CART); Detection of negative emotional states; Discrete Wavelet Transform (DWT); Electroencephalography (EEG); Emotion recognition; K-Nearest Neighbors classifier (kNN); Linear Frequency Cepstral Coefficients (LFCC); Logarithmic Energy (LogE); Na{\"\i}ve Bayes classification (NB); Physiological signals; Power Spectral Density (PSD); Support Vector Machine (SVM)},
	doi = {10.3390/computers9020033},
	journal = {Computers},
	note = {Cited by: 9; All Open Access, Gold Open Access, Green Open Access},
	number = {2},
	publication_stage = {Final},
	source = {Scopus},
	title = {Evaluation of features in detection of dislike responses to audio--visual stimuli from EEG signals},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084117098&doi=10.3390%2fcomputers9020033&partnerID=40&md5=22df05ec158a97455852c23605d714e8},
	volume = {9},
	year = {2020},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084117098&doi=10.3390%2fcomputers9020033&partnerID=40&md5=22df05ec158a97455852c23605d714e8},
	bdsk-url-2 = {https://doi.org/10.3390/computers9020033}}

@article{Goshvarpour2019,
	abstract = {Recently, developing an accurate automatic emotion recognition system using a minimum number of bio-signals has become a challenging issue in ``affective computing.'' This study aimed to propose a reliable system by examining nonlinear dynamics of photoplethysmogram (PPG) and galvanic skin response (GSR). To address this goal, two strategies were adopted. First, the efficiency of each signal in valence/arousal based emotion categorization was examined. Then, the proficiency of a hybrid feature, by combining both GSR and PPG features was studied. Lyapunov exponents, lagged Poincare's measures, and approximate entropy were extracted to characterize the irregularity and chaotic behavior of the phase space. To discriminate two levels of arousal and two levels of the valence, a probabilistic neural network (PNN) with different sigma adjustment parameter was examined. The results showed that the phase space geometry and consequently, the signal dynamics are influenced by the emotional music video. Additionally, distinctive patterns of the phase space behavior were observed under the influence of different lags. For both signals, the most irregularity was observed during the high valence, and the least irregularity was seen during the low valence. Consequently, signals' irregularity is affected by the valence dimension. The results showed that the fusion has more potential for emotion recognition than that of using each signal separately. For sigma = 0.1, the highest recognition rate was 100% in a subject-dependent mode. In a subject-independent mode, the maximum accuracies of 88.57 and 86.8% were obtained for arousal and valence dimensions, respectively. {\copyright} 2019, Australasian College of Physical Scientists and Engineers in Medicine.},
	author = {Goshvarpour, Atefeh and Goshvarpour, Ateke},
	author_keywords = {Emotion recognition; Fusion; Galvanic skin response; Nonlinear dynamics; Photoplethysmogram},
	doi = {10.1007/s13246-019-00825-7},
	journal = {Australasian Physical and Engineering Sciences in Medicine},
	note = {Cited by: 14},
	publication_stage = {Article in press},
	source = {Scopus},
	title = {The potential of photoplethysmogram and galvanic skin response in emotion recognition using nonlinear features},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075936030&doi=10.1007%2fs13246-019-00825-7&partnerID=40&md5=1b454d5d9f4d68acbbe68e976afcdc29},
	year = {2019},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075936030&doi=10.1007%2fs13246-019-00825-7&partnerID=40&md5=1b454d5d9f4d68acbbe68e976afcdc29},
	bdsk-url-2 = {https://doi.org/10.1007/s13246-019-00825-7}}

@article{Hausmann201658,
	abstract = {After decades of research, it remains unclear whether emotion lateralization occurs because one hemisphere is dominant for processing the emotional content of the stimuli, or whether emotional stimuli activate lateralised networks associated with the subjective emotional experience. By using emotion-induction procedures, we investigated the effect of listening to happy and sad music on three well-established lateralization tasks. In a prestudy, Mozart's piano sonata (K. 448) and Beethoven's Moonlight Sonata were rated as the most happy and sad excerpts, respectively. Participants listened to either one emotional excerpt, or sat in silence before completing an emotional chimeric faces task (Experiment 1), visual line bisection task (Experiment 2) and a dichotic listening task (Experiment 3 and 4). Listening to happy music resulted in a reduced right hemispheric bias in facial emotion recognition (Experiment 1) and visuospatial attention (Experiment 2) and increased left hemispheric bias in language lateralization (Experiments 3 and 4). Although Experiments 1-3 revealed an increased positive emotional state after listening to happy music, mediation analyses revealed that the effect on hemispheric asymmetries was not mediated by music-induced emotional changes. The direct effect of music listening on lateralization was investigated in Experiment 4 in which tempo of the happy excerpt was manipulated by controlling for other acoustic features. However, the results of Experiment 4 made it rather unlikely that tempo is the critical cue accounting for the effects. We conclude that listening to music can affect functional cerebral asymmetries in well-established emotional and cognitive laterality tasks, independent of music-induced changes in the emotion state. {\copyright} 2016 Elsevier Inc.},
	author = {Hausmann, Markus and Hodgetts, Sophie and Eerola, Tuomas},
	author_keywords = {Brain asymmetries; Emotion induction; Emotional valence; Lateralization; Music},
	doi = {10.1016/j.bandc.2016.03.001},
	journal = {Brain and Cognition},
	keywords = {Adolescent; Attention; Auditory Perception; Brain; Dichotic Listening Tests; Emotions; Facial Recognition; Female; Functional Laterality; Happiness; Humans; Male; Music; Task Performance and Analysis; Young Adult; adult; Article; brain asymmetry; controlled study; emotionality; facial expression; female; hearing; hemispheric dominance; human; human experiment; male; mental task; music; priority journal; task performance; adolescent; attention; brain; dichotic listening; emotion; facial recognition; happiness; physiology; psychology; young adult},
	note = {Cited by: 17; All Open Access, Green Open Access},
	pages = {58 -- 71},
	publication_stage = {Final},
	source = {Scopus},
	title = {Music-induced changes in functional cerebral asymmetries},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960347235&doi=10.1016%2fj.bandc.2016.03.001&partnerID=40&md5=01c1c5770924cba000201aa62c170760},
	volume = {104},
	year = {2016},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960347235&doi=10.1016%2fj.bandc.2016.03.001&partnerID=40&md5=01c1c5770924cba000201aa62c170760},
	bdsk-url-2 = {https://doi.org/10.1016/j.bandc.2016.03.001}}

@article{Juremi2017259,
	abstract = {It is necessary to find the human inter-rater agreement in emotion recognition research especially when handling with publicly available database. This paper discusses the Cohen's Kappa coefficient technique to verify the actual tagged emotion categories for hybrid emotion model using music video as stimulus. This method has been done by finding the degree of inter-rater reliability between the five selected raters. As the results, the values of Cohen's Kappa coefficients are over 0.87 for four actual tagged emotion categories which are happy, relaxed, sad and angry. These values demonstrate that the degree of inter-rater agreement are excellent. The actual tagged emotion categories are selected based on the division of average value of arousal-valence rating. {\copyright} 2005 - 2017 JATIT & LLS. All rights reserved.},
	author = {Juremi, Nor Rashidah Md and Zulkifley, Mohd Asyraf and Hussain, Aini and Zaki, Wan Mimi Diyana Wan},
	author_keywords = {Cohen's Kappa coefficient; Emotion recognition},
	journal = {Journal of Theoretical and Applied Information Technology},
	note = {Cited by: 8},
	number = {2},
	pages = {259 -- 264},
	publication_stage = {Final},
	source = {Scopus},
	title = {Inter-rater reliability of actual tagged emotion categories validation using Cohen's Kappa coefficient},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011659640&partnerID=40&md5=b3c6804a58c78ab147e87dc60d3f986a},
	volume = {95},
	year = {2017},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011659640&partnerID=40&md5=b3c6804a58c78ab147e87dc60d3f986a}}

@article{Fawcett2021784,
	abstract = {Infants show remarkable skills for processing music in the first year of life. Such skills are believed to foster social and communicative development, yet little is known about how infants' own preferences for music develop and whether social information plays a role. Here, we investigate whether the reactions of another person influence infants' responses to music. Specifically, 12-month-olds (N = 33) saw an actor react positively or negatively after listening to clips of instrumental music. Arousal (measured via pupil dilation) and attention (measured via looking time) were assessed when infants later heard the clips without the actor visible. Results showed greater pupil dilation when listening to music clips that had previously been reacted to negatively than those that had been reacted to positively (Exp. 1). This effect was not replicated when a similar, rather than identical, clip from the piece of music was used in the test phase (Exp. 2, N = 35 12-month-olds). There were no effects of the actor's positive or negative reaction on looking time. Together, our findings suggest that infants are sensitive to others' positive and negative reactions not only for concrete objects, such as food or toys, but also for more abstract stimuli including music. {\copyright} 2021 The Authors. Infancy published by Wiley Periodicals LLC on behalf of International Congress of Infant Studies.},
	author = {Fawcett, Christine and Kreutz, Gunter},
	doi = {10.1111/infa.12415},
	journal = {Infancy},
	keywords = {Attention; Auditory Perception; Humans; Infant; Music; noradrenalin; prolactin; aphasia; Article; circadian rhythm; clinical article; electroencephalography; emotion; executive function; facial recognition; female; human; infant; male; physiological coregulation; predictive value; pupillometry; skin conductance; task performance; visual attention; visual field; waveform; attention; hearing; music},
	note = {Cited by: 2; All Open Access, Green Open Access},
	number = {6},
	pages = {784 -- 797},
	publication_stage = {Final},
	source = {Scopus},
	title = {Twelve-month-old infants' physiological responses to music are affected by others' positive and negative reactions},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107752072&doi=10.1111%2finfa.12415&partnerID=40&md5=f21a126dfe30ac256ad6d0b79b929ee8},
	volume = {26},
	year = {2021},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107752072&doi=10.1111%2finfa.12415&partnerID=40&md5=f21a126dfe30ac256ad6d0b79b929ee8},
	bdsk-url-2 = {https://doi.org/10.1111/infa.12415}}

@article{Sheykhivand2020139332,
	abstract = {Emotion is considered to be critical for the actual interpretation of actions and relationships. Recognizing emotions from EEG signals is also becoming an important computer-aided method for diagnosing emotional disorders in neurology and psychiatry. Another advantage of this approach is recognizing emotions without clinical and medical examination, which plays a major role in completing the Brain-Computer Interface (BCI) structure. Emotions recognition ability, without traditional utilization strategies such as self-assessment tests, is of paramount importance. EEG signals are considered the most reliable technique for emotions recognition because of the non-invasive nature. Manual analysis of EEG signals is impossible for emotions recognition, so an automatic method of EEG signals should be provided for emotions recognition. One problem with automatic emotions recognition is the extraction and selection of discriminative features that generally lead to high computational complexity. This paper was design to prepare a new approach to automatic two-stage classification (negative and positive) and three-stage classification (negative, positive, and neutral) of emotions from EEG signals. In the proposed method, directly apply the raw EEG signal to the convolutional neural network and long short-term memory network (CNN-LSTM), without involving feature extraction/selection. In prior literature, this is a challenging method. The suggested deep neural network architecture includes 10-convolutional layers with 3-LSTM layers followed by 2-fully connected layers. The LSTM network in a fusion of the CNN network has been used to increase stability and reduce oscillation. In the present research, we also recorded the EEG signals of 14 subjects with music stimulation for the process. The simulation results of the proposed algorithm for two-stage classification (negative and positive) and three-stage classification (negative, neutral and positive) of emotion for 12 active channels showed 97.42% and 96.78% accuracy and Kappa coefficient of 0.94 and 0.93 respectively. We also compared our proposed LSTM-CNN network (end-to-end) with other hand-crafted methods based on MLP and DBM classifiers and achieved promising results in comparison with similar approaches. According to the high accuracy of the proposed method, it can be used to develop the human-computer interface system.  {\copyright} 2013 IEEE.},
	author = {Sheykhivand, Sobhan and Mousavi, Zohreh and Rezaii, Tohid Yousefi and Farzamnia, Ali},
	author_keywords = {CNN; EEG; Emotions Recognition; LSTM},
	doi = {10.1109/ACCESS.2020.3011882},
	journal = {IEEE Access},
	keywords = {Biomedical signal processing; Brain computer interface; Convolutional neural networks; Deep neural networks; Diagnosis; Extraction; Feature extraction; Lead compounds; Multilayer neural networks; Network architecture; Signal analysis; Computer aided methods; Discriminative features; Emotions recognition; Human computer interfaces; Kappa coefficient; Recognizing emotions; Short term memory; Utilization strategy; Long short-term memory},
	note = {Cited by: 74; All Open Access, Gold Open Access},
	pages = {139332 -- 139345},
	publication_stage = {Final},
	source = {Scopus},
	title = {Recognizing Emotions Evoked by Music Using CNN-LSTM Networks on EEG Signals},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089604796&doi=10.1109%2fACCESS.2020.3011882&partnerID=40&md5=0a89bc479ae6925f3ca89f0a18bed53d},
	volume = {8},
	year = {2020},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089604796&doi=10.1109%2fACCESS.2020.3011882&partnerID=40&md5=0a89bc479ae6925f3ca89f0a18bed53d},
	bdsk-url-2 = {https://doi.org/10.1109/ACCESS.2020.3011882}}

@article{Grekow2021531,
	abstract = {The article presents conducted experiments using recurrent neural networks for emotion detection in musical segments. Trained regression models were used to predict the continuous values of emotions on the axes of Russell's circumplex model. A process of audio feature extraction and creating sequential data for learning networks with long short-term memory (LSTM) units is presented. Models were implemented using the WekaDeeplearning4j package and a number of experiments were carried out with data with different sets of features and varying segmentation. The usefulness of dividing the data into sequences as well as the point of using recurrent networks to recognize emotions in music, the results of which have even exceeded the SVM algorithm for regression, were demonstrated. The author analyzed the effect of the network structure and the set of used features on the results of the regressors recognizing values on two axes of the emotion model: arousal and valence. Finally, the use of a pretrained model for processing audio features and training a recurrent network with new sequences of features is presented. {\copyright} 2021, The Author(s).},
	author = {Grekow, Jacek},
	author_keywords = {Audio features; Emotion detection; Recurrent neural networks; Sequential data},
	doi = {10.1007/s10844-021-00658-5},
	journal = {Journal of Intelligent Information Systems},
	keywords = {Regression analysis; Audio feature extraction; Circumplex models; Continuous value; Emotion detection; Learning network; Network structures; Recurrent networks; Regression model; Long short-term memory},
	note = {Cited by: 12; All Open Access, Hybrid Gold Open Access},
	number = {3},
	pages = {531 -- 546},
	publication_stage = {Final},
	source = {Scopus},
	title = {Music emotion recognition using recurrent neural networks and pretrained models},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112649812&doi=10.1007%2fs10844-021-00658-5&partnerID=40&md5=94cc218543e20b1cedaaad1edf9fb339},
	volume = {57},
	year = {2021},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112649812&doi=10.1007%2fs10844-021-00658-5&partnerID=40&md5=94cc218543e20b1cedaaad1edf9fb339},
	bdsk-url-2 = {https://doi.org/10.1007/s10844-021-00658-5}}

@article{Griffiths2021355,
	abstract = {Making the link between human emotion and music is challenging. Our aim was to produce an efficient system that emotionally rates songs from multiple genres. To achieve this, we employed a series of online self-report studies, utilising Russell's circumplex model. The first study (n = 44) identified audio features that map to arousal and valence for 20 songs. From this, we constructed a set of linear regressors. The second study (n = 158) measured the efficacy of our system, utilising 40 new songs to create a ground truth. Results show our approach may be effective at emotionally rating music, particularly in the prediction of valence. {\copyright} 2021 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.},
	author = {Griffiths, Darryl and Cunningham, Stuart and Weinel, Jonathan and Picking, Richard},
	author_keywords = {Arousal; emotion; MER; music; perception; regression; valence},
	doi = {10.1080/09298215.2021.1977336},
	journal = {Journal of New Music Research},
	note = {Cited by: 7; All Open Access, Hybrid Gold Open Access},
	number = {4},
	pages = {355 -- 372},
	publication_stage = {Final},
	source = {Scopus},
	title = {A multi-genre model for music emotion recognition using linear regressors},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115253186&doi=10.1080%2f09298215.2021.1977336&partnerID=40&md5=c7bc9b870ae841bd1e3a465372ed0278},
	volume = {50},
	year = {2021},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115253186&doi=10.1080%2f09298215.2021.1977336&partnerID=40&md5=c7bc9b870ae841bd1e3a465372ed0278},
	bdsk-url-2 = {https://doi.org/10.1080/09298215.2021.1977336}}

@article{Sharma2019324,
	abstract = {In this paper, a machine learning algorithm is proposed for emotional pattern recognition during audio-visual stimuli (music videos) using Electrodermal Activity (EDA). For emotion prediction apart from conventional time domain features of EDA signal, various features in different signal representation i.e. frequency and wavelet were analysed. The comparative result indicated that the wavelet features subset outperformed the conventional time domain features in term of classification accuracy. For identification of optimal network configuration, various combination of optimization algorithms (i.e. backpropagation algorithms) and error function were explored. The best performance of 79% for arousal, 69.8% for valence and 71.2% for dominance were obtained for emotion recognition respectively. {\copyright} 2018 Elsevier Ltd},
	author = {Sharma, Vivek and Prakash, Neelam R. and Kalra, Parveen},
	author_keywords = {Affective computing; Electrodermal activity; Multilayer neural networks; Music videos},
	doi = {10.1016/j.bspc.2018.08.024},
	journal = {Biomedical Signal Processing and Control},
	keywords = {Audio acoustics; Backpropagation; Biomedical signal processing; Electrodes; Pattern recognition; Affective Computing; Audio-visual stimulus; Classification accuracy; Combination of optimizations; Electrodermal activity; Music video; Optimal network configuration; Signal representations; arousal; article; human; human experiment; machine learning; music; pattern recognition; prediction; stimulus; videorecording; Multilayer neural networks},
	note = {Cited by: 22},
	pages = {324 -- 333},
	publication_stage = {Final},
	source = {Scopus},
	title = {Audio-video emotional response mapping based upon Electrodermal Activity},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053082851&doi=10.1016%2fj.bspc.2018.08.024&partnerID=40&md5=f8f8f55b31f0cfe71e8238bbbd12cc9f},
	volume = {47},
	year = {2019},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053082851&doi=10.1016%2fj.bspc.2018.08.024&partnerID=40&md5=f8f8f55b31f0cfe71e8238bbbd12cc9f},
	bdsk-url-2 = {https://doi.org/10.1016/j.bspc.2018.08.024}}

@article{Fabio2019,
	abstract = {After the introduction of mobile computing devices, the way people listen to music has changed considerably. Although there is a broad scientific consensus on the fact that people show music preferences and make music choices based on their feelings and emotions, the sources of such preferences and choices are still debated. The main aim of this study is to understand whether listening in ecological (mobile) contexts differs from listening in non-mobile contexts in terms of the elicited emotive response. A total of 328 participants listen to 100 classical music tracks, available through an ad-hoc mobile application for mobile devices. The participants were asked to report their self-evaluation of each of the tracks, according to the Pleasure-Arousal-Dominance model and filled out a questionnaire about their listening behaviour. Our findings show that the same factors that affect music listening in non-mobile contexts also affect it in a mobile context. {\copyright} 2019, {\copyright} 2019 The Author(s). This open access article is distributed under a Creative Commons Attribution (CC-BY) 4.0 license.},
	author = {Fabio, Rosa Angela and Iannizzotto, Giancarlo and Nucita, Andrea and Capr{\`\i}, Tindara},
	author_keywords = {emotion recognition; mobile context; music; music listening in ecological contexts},
	doi = {10.1080/23311916.2019.1597666},
	journal = {Cogent Engineering},
	note = {Cited by: 7; All Open Access, Gold Open Access, Green Open Access},
	number = {1},
	publication_stage = {Final},
	source = {Scopus},
	title = {Adult listening behaviour, music preferences and emotions in the mobile context. Does mobile context affect elicited emotions?},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064259930&doi=10.1080%2f23311916.2019.1597666&partnerID=40&md5=66927d75f900041fc51c968baa4ac66c},
	volume = {6},
	year = {2019},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064259930&doi=10.1080%2f23311916.2019.1597666&partnerID=40&md5=66927d75f900041fc51c968baa4ac66c},
	bdsk-url-2 = {https://doi.org/10.1080/23311916.2019.1597666}}

@article{Ayata2018196,
	abstract = {Most of the existing music recommendation systems use collaborative or content based recommendation engines. However, the music choice of a user is not only dependent to the historical preferences or music contents. But also dependent to the mood of that user. This paper proposes an emotion based music recommendation framework that learns the emotion of a user from the signals obtained via wearable physiological sensors. In particular, the emotion of a user is classified by a wearable computing device which is integrated with a galvanic skin response (GSR) and photo plethysmography (PPG) physiological sensors. This emotion information is feed to any collaborative or content based recommendation engine as a supplementary data. Thus, existing recommendation engine performances can be increased using these data. Therefore, in this paper emotion recognition problem is considered as arousal and valence prediction from multi-channel physiological signals. Experimental results are obtained on 32 subjects' GSR and PPG signal data with/out feature fusion using decision tree, random forest, support vector machine and k-nearest neighbors algorithms. The results of comprehensive experiments on real data confirm the accuracy of the proposed emotion classification system that can be integrated to any recommendation engine. {\copyright} 2018 IEEE.},
	author = {Ayata, Deger and Yaslan, Yusuf and Kamasak, Mustafa E.},
	author_keywords = {Emotion aware recommendation engine; emotion recognition; galvanic skin response; machine learning; photo plethysmography; physiological signals},
	doi = {10.1109/TCE.2018.2844736},
	journal = {IEEE Transactions on Consumer Electronics},
	keywords = {Biomedical signal processing; Data mining; Decision trees; Electrophysiology; Engines; Learning systems; Nearest neighbor search; Physiological models; Plethysmography; Speech recognition; Wearable sensors; Content-based recommendation; Emotion classification systems; Emotion recognition; Galvanic skin response; Music recommendation; Music Recommendation System; Physiological sensors; Physiological signals; Recommender systems},
	note = {Cited by: 172},
	number = {2},
	pages = {196 -- 203},
	publication_stage = {Final},
	source = {Scopus},
	title = {Emotion Based Music Recommendation System Using Wearable Physiological Sensors},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048199534&doi=10.1109%2fTCE.2018.2844736&partnerID=40&md5=764a9166266e1a799c73bbfb7643411d},
	volume = {64},
	year = {2018},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048199534&doi=10.1109%2fTCE.2018.2844736&partnerID=40&md5=764a9166266e1a799c73bbfb7643411d},
	bdsk-url-2 = {https://doi.org/10.1109/TCE.2018.2844736}}

@article{Ren20194491,
	abstract = {This paper proposes a method to evaluate the degree of emotion being motivated in continuous music videos based on asymmetry index (AsI). By collecting two groups of electroencephalogram (EEG) signals from 6 channels (Fp1, Fp2, Fz and AF3, AF4, Fz) in the left and right hemispheres, multidimensional directed information is used to measure the mutual information shared between two frontal lobes, and then, we get AsI to estimate the degree of emotional induction. In order to evaluate the effect of AsI processing on physiological emotion recognition, 32-channel EEG signals, 2-channel EEG signals and 2-channel EMG signals are selected for each subject from the DEAP dataset, and different sub-bands are extracted using wavelet packet transform. k-means algorithm is used to cluster the wavelet packet coefficients of each sub-band, and the probability distribution of the coefficients under each cluster is calculated. Finally, the probability distribution value of each sample is sent as the original features into echo state network for unsupervised intrinsic plasticity training; the reservoir state nodes are selected as the final feature vector and fed into the support vector machine. The experimental results show that the proposed algorithm can achieve an average recognition rate of 70.5% when the subjects are independent. Compared with the case without AsI, the recognition rate is increased by 8.73%. On the other hand, the ESN is adopted for the original physiological feature refinement which can significantly reduce feature dimensions and be more beneficial to the emotion classification. Therefore, this study can effectively improve the performance of human--machine interface systems based on emotion recognition. {\copyright} 2018, The Natural Computing Applications Forum.},
	author = {Ren, Fuji and Dong, Yindong and Wang, Wei},
	author_keywords = {Brain asymmetry index; Echo state network; Emotion recognition; Physiological signals},
	doi = {10.1007/s00521-018-3664-1},
	journal = {Neural Computing and Applications},
	keywords = {Electroencephalography; Image retrieval; Physiology; Probability distributions; Speech recognition; Wavelet analysis; Echo state networks; Electroencephalogram signals; Emotion recognition; Multidimensional directed informations; Physiological features; Physiological signals; Wavelet packet coefficient; Wavelet packet transforms; Biomedical signal processing},
	note = {Cited by: 35},
	number = {9},
	pages = {4491 -- 4501},
	publication_stage = {Final},
	source = {Scopus},
	title = {Emotion recognition based on physiological signals using brain asymmetry index and echo state network},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051487502&doi=10.1007%2fs00521-018-3664-1&partnerID=40&md5=f4f813981d0b98faf66f5b573a6ecc4e},
	volume = {31},
	year = {2019},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051487502&doi=10.1007%2fs00521-018-3664-1&partnerID=40&md5=f4f813981d0b98faf66f5b573a6ecc4e},
	bdsk-url-2 = {https://doi.org/10.1007/s00521-018-3664-1}}

@article{Drapeau2017221,
	abstract = {Objectives: To assess emotion recognition from dynamic facial, vocal and musical expressions in sub-groups of adults with traumatic brain injuries (TBI) of different severities and identify possible common underlying mechanisms across domains.Methods: Forty-one adults participated in this study: 10 with moderate--severe TBI, nine with complicated mild TBI, 11 with uncomplicated mild TBI and 11 healthy controls, who were administered experimental (emotional recognition, valence-arousal) and control tasks (emotional and structural discrimination) for each domain.Results: Recognition of fearful faces was significantly impaired in moderate--severe and in complicated mild TBI sub-groups, as compared to those with uncomplicated mild TBI and controls. Effect sizes were medium--large. Participants with lower GCS scores performed more poorly when recognizing fearful dynamic facial expressions. Emotion recognition from auditory domains was preserved following TBI, irrespective of severity. All groups performed equally on control tasks, indicating no perceptual disorders. Although emotional recognition from vocal and musical expressions was preserved, no correlation was found across auditory domains.Conclusions: This preliminary study may contribute to improving comprehension of emotional recognition following TBI. Future studies of larger samples could usefully include measures of functional impacts of recognition deficits for fearful facial expressions. These could help refine interventions for emotional recognition following a brain injury. {\copyright} 2017 Taylor & Francis Group, LLC.},
	author = {Drapeau, Joanie and Gosselin, Nathalie and Peretz, Isabelle and McKerral, Michelle},
	author_keywords = {complicated mild; emotions; faces; mild; moderate; music; non-linguistic vocalizations; severe; Traumatic brain injury},
	doi = {10.1080/02699052.2016.1208846},
	journal = {Brain Injury},
	keywords = {Adult; Brain Concussion; Brain Injuries, Traumatic; Emotions; Facial Expression; Facial Recognition; Humans; Middle Aged; Music; Neuropsychological Tests; Recognition (Psychology); Young Adult; adult; arousal; Article; auditory system; Beck Depression Inventory; clinical article; comprehension; controlled study; effect size; emotion; facial expression; fear; female; Glasgow coma scale; happiness; human; injury severity; male; music; postconcussion symptoms score; sadness; scoring system; traumatic brain injury; vocalization; brain concussion; facial recognition; middle aged; music; neuropsychological test; psychology; recognition; traumatic brain injury; young adult},
	note = {Cited by: 17},
	number = {2},
	pages = {221 -- 229},
	publication_stage = {Final},
	source = {Scopus},
	title = {Emotional recognition from dynamic facial, vocal and musical expressions following traumatic brain injury},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992193842&doi=10.1080%2f02699052.2016.1208846&partnerID=40&md5=d42ef3e0ebc52fab54283d958b1c181b},
	volume = {31},
	year = {2017},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992193842&doi=10.1080%2f02699052.2016.1208846&partnerID=40&md5=d42ef3e0ebc52fab54283d958b1c181b},
	bdsk-url-2 = {https://doi.org/10.1080/02699052.2016.1208846}}

@article{Xie2020,
	abstract = {This paper presents a method for extracting novel spectral features based on a sinusoidal model. The method is focused on characterizing the spectral shapes of audio signals using spectral peaks in frequency sub-bands. The extracted features are evaluated for predicting the levels of emotional dimensions, namely arousal and valence. Principal component regression, partial least squares regression, and deep convolutional neural network (CNN) models are used as prediction models for the levels of the emotional dimensions. The experimental results indicate that the proposed features include additional spectral information that common baseline features may not include. Since the quality of audio signals, especially timbre, plays a major role in affecting the perception of emotional valence in music, the inclusion of the presented features will contribute to decreasing the prediction error rate. {\copyright} 2020 by the authors.},
	author = {Xie, Baijun and Kim, Jonathan C. and Park, Chung Hyuk},
	author_keywords = {Deep learning; Machine learning; Musical emotion recognition; Principal component regression; Sinusoidal model; Spectral feature extraction},
	doi = {10.3390/app10030902},
	journal = {Applied Sciences (Switzerland)},
	note = {Cited by: 7; All Open Access, Gold Open Access, Green Open Access},
	number = {3},
	publication_stage = {Final},
	source = {Scopus},
	title = {Musical emotion recognition with spectral feature extraction based on a sinusoidal model with model-based and deep-learning approaches},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081594504&doi=10.3390%2fapp10030902&partnerID=40&md5=267262fee705d9db0ed7d14948780f94},
	volume = {10},
	year = {2020},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081594504&doi=10.3390%2fapp10030902&partnerID=40&md5=267262fee705d9db0ed7d14948780f94},
	bdsk-url-2 = {https://doi.org/10.3390/app10030902}}

@article{Cowen2019698,
	abstract = {Emotional vocalizations are central to human social life. Recent studies have documented that people recognize at least 13 emotions in brief vocalizations. This capacity emerges early in development, is preserved in some form across cultures, and informs how people respond emotionally to music. What is poorly understood is how emotion recognition from vocalization is structured within what we call a semantic space, the study of which addresses questions critical to the field: How many distinct kinds of emotions can be expressed? Do expressions convey emotion categories or affective appraisals (e.g., valence, arousal)? Is the recognition of emotion expressions discrete or continuous? Guided by a new theoretical approach to emotion taxonomies, we apply large-scale data collection and analysis techniques to judgments of 2,032 emotional vocal bursts produced in laboratory settings (Study 1) and 48 found in the real world (Study 2) by U.S. English speakers (N = 1,105). We find that vocal bursts convey at least 24 distinct kinds of emotion. Emotion categories (sympathy, awe), more so than affective appraisals (including valence and arousal), organize emotion recognition. In contrast to discrete emotion theories, the emotion categories conveyed by vocal bursts are bridged by smooth gradients with continuously varying meaning. We visualize the complex, highdimensional space of emotion conveyed by brief human vocalization within an online interactive map. {\copyright} 2019 American Psychological Association.},
	author = {Cowen, Alan S. and Elfenbein, Hillary Anger and Laukka, Petri and Keltner, Dacher},
	author_keywords = {Affect; Computational methods; Emotion; Semantic space; Voice},
	doi = {10.1037/amp0000399},
	journal = {American Psychologist},
	keywords = {Adolescent; Adult; Aged; Communication; Emotions; Female; Humans; Male; Middle Aged; Recognition, Psychology; Semantics; Social Perception; Voice; Young Adult; adolescent; adult; aged; classification; emotion; female; human; interpersonal communication; male; middle aged; perception; physiology; semantics; voice; young adult},
	note = {Cited by: 99; All Open Access, Green Open Access},
	number = {6},
	pages = {698 -- 712},
	publication_stage = {Final},
	source = {Scopus},
	title = {Mapping 24 emotions conveyed by brief human vocalization},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058859152&doi=10.1037%2famp0000399&partnerID=40&md5=9dfbdd8f376be00a34e13affc06d9e33},
	volume = {74},
	year = {2019},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058859152&doi=10.1037%2famp0000399&partnerID=40&md5=9dfbdd8f376be00a34e13affc06d9e33},
	bdsk-url-2 = {https://doi.org/10.1037/amp0000399}}

@article{Goshvarpour2016163,
	abstract = {Introduction To extract and combine information from different modalities, fusion techniques are commonly applied to promote system performance. In this study, we aimed to examine the effectiveness of fusion techniques in emotion recognition. Materials and Methods Electrocardiogram (ECG) and galvanic skin responses (GSR) of 11 healthy female students (mean age: 22.73$\pm$1.68 years) were collected while the subjects were listening to emotional music clips. For multi-resolution analysis of signals, wavelet transform (Coiflets 5 at level 14) was used. Moreover, a novel feature-level fusion method was employed, in which low-frequency sub-band coefficients of GSR signals and high-frequency sub-band coefficients of ECG signals were fused to reconstruct a new feature. To reduce the dimensionality of the feature vector, the absolute value of some statistical indices was calculated and considered as input of PNN classifier. To describe emotions, two-dimensional models (four quadrants of valence and arousal dimensions), valence-based emotional states, and emotional arousal were applied. Results The highest recognition rates were obtained from sigma=0.01. Mean classification rate of 100% was achieved through applying the proposed fusion methodology. However, the accuracy rates of 97.90% and 97.20% were attained for GSR and ECG signals, respectively. Conclusion Compared to the previously published articles in the field of emotion recognition using musical stimuli, promising results were obtained through application of the proposed methodology.},
	author = {Goshvarpour, Atefeh and Abbasi, Ataollah and Goshvarpour, Ateke and Daneshvar, Sabalan},
	author_keywords = {Electrocardiogram; Emotion; Galvanic skin responses; Neural networks; Wavelet analyses},
	doi = {10.22038/ijmp.2016.7960},
	journal = {Iranian Journal of Medical Physics},
	keywords = {Electrocardiography; Electrophysiology; Face recognition; Neural networks; Speech recognition; Wavelet analysis; Wavelet transforms; Emotion; Emotion recognition; Feature level fusion; Fusion methodology; Galvanic skin response; Mean classification; Statistical indices; Two dimensional model; Biomedical signal processing},
	note = {Cited by: 7},
	number = {3},
	pages = {163 -- 173},
	publication_stage = {Final},
	source = {Scopus},
	title = {Fusion framework for emotional electrocardiogram and galvanic skin response recognition: Applying wavelet transform},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010918197&doi=10.22038%2fijmp.2016.7960&partnerID=40&md5=a84db768b9832db789d7daa80302d122},
	volume = {13},
	year = {2016},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010918197&doi=10.22038%2fijmp.2016.7960&partnerID=40&md5=a84db768b9832db789d7daa80302d122},
	bdsk-url-2 = {https://doi.org/10.22038/ijmp.2016.7960}}

@article{Hendry2019255,
	abstract = {Currently, many recommendation systems propose the breakthrough of traditional single recommendation. Many items usually belong to more than one label at a time, for example, genres of music, categories of the products and emotions. One data point could be labeled more than one tag which is a problem for many classification algorithms. Clustering analysis is a primary task of data mining, which works by dividing the dataset into the partitions based on the distance of data points. Clustering is an unsupervised learning model, which is suitable to learn multi-label classification problem. The technique is commonly used in machine learning, pattern recognition, and many others. K-means is one of the simple and widely used clustering algorithms. In this paper, we propose the collaboration between business and user-item reviews to predict the multi-label classification. We implement the combination of k-means between business and user-items review. We found that the value of k equal to three will have the best multi-label classification results for business categories and business rating. {\copyright} 2019, ICIC International. All rights reserved.},
	author = {Hendry and Chen, Rung-Ching},
	author_keywords = {K-means; Multi-label classification; User-item reviews},
	doi = {10.24507/icicel.13.03.255},
	journal = {ICIC Express Letters},
	note = {Cited by: 2},
	number = {3},
	pages = {255 -- 262},
	publication_stage = {Final},
	source = {Scopus},
	title = {Predicting business category with multi-label classification from user-item review and business data based on K-means},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062332918&doi=10.24507%2ficicel.13.03.255&partnerID=40&md5=6cc587c4901a00bacb697bc0f8ad4151},
	volume = {13},
	year = {2019},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062332918&doi=10.24507%2ficicel.13.03.255&partnerID=40&md5=6cc587c4901a00bacb697bc0f8ad4151},
	bdsk-url-2 = {https://doi.org/10.24507/icicel.13.03.255}}

@article{Pralus202078,
	abstract = {For the hemispheric laterality of emotion processing in the brain, two competing hypotheses are currently still debated. The first hypothesis suggests a greater involvement of the right hemisphere in emotion perception whereas the second hypothesis suggests different involvements of each hemisphere as a function of the valence of the emotion. These hypotheses are based on findings for facial and prosodic emotion perception. Investigating emotion perception for other stimuli, such as music, should provide further insight and potentially help to disentangle between these two hypotheses. The present study investigated musical emotion perception in patients with unilateral right brain damage (RBD, n = 16) or left brain damage (LBD, n = 16), as well as in matched healthy comparison participants (n = 28). The experimental task required explicit recognition of musical emotions as well as ratings on the perceived intensity of the emotion. Compared to matched comparison participants, musical emotion recognition was impaired only in LBD participants, suggesting a potential specificity of the left hemisphere for explicit emotion recognition in musical material. In contrast, intensity ratings of musical emotions revealed that RBD patients underestimated the intensity of negative emotions compared to positive emotions, while LBD patients and comparisons did not show this pattern. To control for a potential generalized emotion deficit for other types of stimuli, we also tested facial emotion recognition in the same patients and their matched healthy comparisons. This revealed that emotion recognition after brain damage might depend on the stimulus category or modality used. These results are in line with the hypothesis of a deficit of emotion perception depending on lesion laterality and valence in brain-damaged participants. The present findings provide critical information to disentangle the currently debated competing hypotheses and thus allow for a better characterization of the involvement of each hemisphere for explicit emotion recognition and their perceived intensity. {\copyright} 2020 Elsevier Ltd},
	author = {Pralus, Agathe and Belfi, Amy and Hirel, Catherine and L{\'e}v{\^e}que, Yohana and Fornoni, Lesly and Bigand, Emmanuel and Jung, Julien and Tranel, Daniel and Nighoghossian, Norbert and Tillmann, Barbara and Caclin, Anne},
	author_keywords = {Brain lesion; Emotion perception; Music; Right hemisphere hypothesis; Valence hypothesis},
	doi = {10.1016/j.cortex.2020.05.015},
	journal = {Cortex},
	keywords = {adult; Article; audiometry; brain damage; clinical article; clinical assessment; comparative study; controlled study; emotion; female; functional dissociation; hemispheric dominance; human; left hemisphere; male; middle aged; music; neuropsychological test; outcome assessment; perception; recognition; right hemisphere; task performance},
	note = {Cited by: 5; All Open Access, Hybrid Gold Open Access},
	pages = {78 -- 93},
	publication_stage = {Final},
	source = {Scopus},
	title = {Recognition of musical emotions and their perceived intensity after unilateral brain damage},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087421145&doi=10.1016%2fj.cortex.2020.05.015&partnerID=40&md5=c1629b258d735cfa9a743ac6b05f7d3c},
	volume = {130},
	year = {2020},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087421145&doi=10.1016%2fj.cortex.2020.05.015&partnerID=40&md5=c1629b258d735cfa9a743ac6b05f7d3c},
	bdsk-url-2 = {https://doi.org/10.1016/j.cortex.2020.05.015}}

@article{Nineuil2020,
	abstract = {The influence of emotional dimensions such as arousal and valence on memory has been a topic of particularly intense inquiry. As stimuli go, music is capable of provoking strong emotional responses from listeners, which can in turn influence memory. However, few studies have examined the effect of musical emotions on memory, and even fewer the effect of valence and arousal. In order to shed light on the ways in which emotional dimensions affect musical memory as study-test delay intervals increase, we tested recognition after a short delay and after a long delay. In line with the literature, we hypothesized an emotional enhancement of music memory induced by post-encoding processes leading to better recognition of musical excerpts in delayed condition, as compared to the immediate condition. The effects of arousal and valence were expected to become exaggerated after a long delay. We also predicted that the two emotional dimensions would be differently affected by the study-test intervals. Our results showed that the emotional enhancement of memory depends upon the valence, with remembering of positive and negative stimuli being differently affected by the duration of the study-test delay interval. Furthermore, our data demonstrated that musical excerpts were better recognized after a long delay than after a short delay, illustrating that memory consolidation for musical information is taking place during the long study-test interval. Moreover, musical memory consolidation is strongly related to the characteristics of the positive valence, which have been discussed in relation to its pleasantness. This original finding provides new insights into the modulatory effects of emotional valence on memory consolidation and could offer promising therapeutic possibilities for the rehabilitation of memory disorders. {\copyright} Copyright {\copyright} 2020 Nineuil, Dellacherie and Samson.},
	author = {Nineuil, Cl{\'e}mence and Dellacherie, Delphine and Samson, S{\'e}verine},
	author_keywords = {arousal; consolidation; emotion; musical memory; valence},
	doi = {10.3389/fpsyg.2020.02110},
	journal = {Frontiers in Psychology},
	note = {Cited by: 4; All Open Access, Gold Open Access, Green Open Access},
	publication_stage = {Final},
	source = {Scopus},
	title = {The Impact of Emotion on Musical Long-Term Memory},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090939559&doi=10.3389%2ffpsyg.2020.02110&partnerID=40&md5=2a30fe7b4b898f744f86b754dbd5c569},
	volume = {11},
	year = {2020},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090939559&doi=10.3389%2ffpsyg.2020.02110&partnerID=40&md5=2a30fe7b4b898f744f86b754dbd5c569},
	bdsk-url-2 = {https://doi.org/10.3389/fpsyg.2020.02110}}

@article{Raheel2019,
	abstract = {Human emotions are recognized in response to content engaging one (audio music) or two human senses (videos). An enhanced sensation with a more realistic feel could be achievable by engaging more than two human senses. In this study, olfaction enhanced multimedia content is generated by synchronizing traditional multimedia content with an olfaction dispenser for engaging olfactory sense in addition to vision and auditory senses. Brain activity of 20 participants (10 males and 10 females) is recorded with a commercially available EEG headband, while engaging with traditional and olfaction enhanced multimedia content. The human brain activity is used to analyze and differentiate the content engaging two (traditional multimedia content) or more than two (olfaction enhanced multimedia content) human senses. For brain activity analysis, we apply a t-test on the power spectra of five frequency sub-bands (delta, theta, alpha, beta, and gamma) of the acquired EEG data in response to traditional and olfaction enhanced multimedia. We observe that alpha, theta, and delta bands are significant in discriminating the response to traditional and olfaction enhanced multimedia content. High brain activity is observed in alpha, theta, and delta bands of frontal channels, while experiencing the olfaction enhanced multimedia content. A user-independent pleasantness classification based on human brain activity is also presented, where classification performance is measured using 10-fold cross validation. We extract features in frequency domain i.e., rational asymmetry (RASM) and differential asymmetry (DASM) from five EEG bands to classify two pleasantness states based on their valence scores using support vector machine (SVM) classifier. Features are further selected based on EEG electrode pair positions and sub-bands. We observed that RASM and DASM features selected from delta band (olfaction enhanced content), and alpha or gamma bands (traditional multimedia content) gives best classification accuracy. We achieved an accuracy of 75%, sensitivity of 77.7%, and specificity of 72.7% in response to olfaction enhanced multimedia content and an accuracy of 68.7%, sensitivity of 71.4%, and specificity of 69.2% in response to traditional multimedia content in classifying pleasant and unpleasant states using SVM. We observed that classification of pleasant state was comparatively better with olfaction enhanced multimedia content than traditional multimedia content. {\copyright} 2019 Elsevier Ltd},
	author = {Raheel, Aasim and Majid, Muhammad and Anwar, Syed Muhammad},
	author_keywords = {Brain activity; Classification; Electroencephalography; Emotion recognition; Olfaction enhanced multimedia},
	doi = {10.1016/j.compbiomed.2019.103469},
	journal = {Computers in Biology and Medicine},
	keywords = {Adolescent; Adult; Brain; Electroencephalography; Emotions; Female; Humans; Male; Multimedia; Sensitivity and Specificity; Smell; Support Vector Machine; Young Adult; Audio acoustics; Classification (of information); Electroencephalography; Electrophysiology; Frequency domain analysis; Information analysis; Neurophysiology; Support vector machines; 10-fold cross-validation; Brain activity; Brain activity analysis; Classification accuracy; Classification performance; Emotion recognition; Multimedia contents; Olfaction enhanced multimedia; adult; alpha rhythm; Article; beta rhythm; classifier; delta rhythm; electroencephalogram; emotion; feature selection; female; gamma rhythm; human; human experiment; male; normal human; priority journal; smelling; support vector machine; theta rhythm; adolescent; brain; electroencephalography; multimedia; odor; physiology; sensitivity and specificity; young adult; Brain},
	note = {Cited by: 18},
	publication_stage = {Final},
	source = {Scopus},
	title = {A study on the effects of traditional and olfaction enhanced multimedia on pleasantness classification based on brain activity analysis},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072708828&doi=10.1016%2fj.compbiomed.2019.103469&partnerID=40&md5=da3e9b55256bb2a1e08ae7c3c3050394},
	volume = {114},
	year = {2019},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072708828&doi=10.1016%2fj.compbiomed.2019.103469&partnerID=40&md5=da3e9b55256bb2a1e08ae7c3c3050394},
	bdsk-url-2 = {https://doi.org/10.1016/j.compbiomed.2019.103469}}

@article{Kopec2014436,
	abstract = {Emotion perception deficits are commonly observed in individuals with autism spectrum disorders (ASD). Numerous studies have documented deficits in emotional recognition of social stimuli among those with ASD, such as faces and voices, while far fewer have investigated emotional recognition of nonsocial stimuli in this population. In this study, participants with ASD and a comparison group of typically developing (TD) control participants listened to song clips that varied in levels of pleasantness (valence) and arousal. Participants then rated emotions they felt or perceived in the music, using a list of eight emotion words for each song. Results showed that individuals with ASD gave significantly lower ratings of negative emotions in both the felt and perceived categories compared to TD controls, but did not show significant differences in ratings of positive emotions. These findings suggest that deficits in processing emotions in music among those with ASD may be valence specific. {\copyright} 2014 BY THE REGENTS OF THE UNIVERSITY OF CALIFORNIA ALL RIGHTS RESERVED.},
	author = {Kopec, Justin and Hillier, Ashleigh and Frye, Alice},
	author_keywords = {Asperger's syndrom; Autism; Emotional response; Music perception; Valence},
	doi = {10.1525/MP.2014.31.5.436},
	journal = {Music Perception},
	note = {Cited by: 8},
	number = {5},
	pages = {436 -- 443},
	publication_stage = {Final},
	source = {Scopus},
	title = {The valency of music has different effects on the emotional responses of those with autism spectrum disorders and a comparison group},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902205412&doi=10.1525%2fMP.2014.31.5.436&partnerID=40&md5=cdcf920cd0956389e070d5cff2f15031},
	volume = {31},
	year = {2014},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902205412&doi=10.1525%2fMP.2014.31.5.436&partnerID=40&md5=cdcf920cd0956389e070d5cff2f15031},
	bdsk-url-2 = {https://doi.org/10.1525/MP.2014.31.5.436}}

@article{Drossos2015139,
	abstract = {Sound is a prominent element in the communication of humans with their environment. It can be either in an organized form, e.g., speech or music, or in a non-organized form, i.e., Sound Events (SEs). There are many studies focusing on emotion recognition from music content, but the research on emotion recognition from SEs is a rather new field. This works falls within this field and concludes that the rhythmic characteristics of an SE have a major impact on the listener's arousal. This result verifies the empirical knowledge that the rhythm of sound affects the listener's activation state. Moreover, we also investigate whether the above characteristics also affect the pleasure of the listener. Toward this aim, we have utilized a well known data set of emotionally annotated SEs, extracted various rhythm-related technical cues, and conducted a series of machine learning experiments. The overall results indicate a relation of the rhythm and listener's valence with accuracy results reaching up to 63%.},
	author = {Drossos, Konstantinos and Floros, Andreas and Kermanidis, Katia-Lida},
	doi = {10.17743/jaes.2015.0010},
	journal = {AES: Journal of the Audio Engineering Society},
	keywords = {Artificial intelligence; Activation state; Data set; Emotion recognition; Empirical knowledge; Music contents; Sound events; Learning systems},
	note = {Cited by: 2},
	number = {3},
	pages = {139 -- 153},
	publication_stage = {Final},
	source = {Scopus},
	title = {Evaluating the impact of sound events' rhythm characteristics to listener's valence},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925390260&doi=10.17743%2fjaes.2015.0010&partnerID=40&md5=1e4e814c6c6ccea563e1f4eb8e680e79},
	volume = {63},
	year = {2015},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925390260&doi=10.17743%2fjaes.2015.0010&partnerID=40&md5=1e4e814c6c6ccea563e1f4eb8e680e79},
	bdsk-url-2 = {https://doi.org/10.17743/jaes.2015.0010}}

@article{Sereno2015,
	abstract = {Visual emotion word processing has been in the focus of recent psycholinguistic research. In general, emotion words provoke differential responses in comparison to neutral words. However, words are typically processed within a context rather than in isolation. For instance, how does one's inner emotional state influence the comprehension of emotion words? To address this question, the current study examined lexical decision responses to emotionally positive, negative, and neutral words as a function of induced mood as well as their word frequency. Mood was manipulated by exposing participants to different types of music. Participants were randomly assigned to one of three conditions---no music, positive music, and negative music. Participants' moods were assessed during the experiment to confirm the mood induction manipulation. Reaction time results confirmed prior demonstrations of an interaction between a word's emotionality and its frequency. Results also showed a significant interaction between participant mood and word emotionality. However, the pattern of results was not consistent with mood-congruency effects. Although positive and negative mood facilitated responses overall in comparison to the control group, neither positive nor negative mood appeared to additionally facilitate responses to mood-congruent words. Instead, the pattern of findings seemed to be the consequence of attentional effects arising from induced mood. Positive mood broadens attention to a global level, eliminating the category distinction of positive-negative valence but leaving the high-low arousal dimension intact. In contrast, negative mood narrows attention to a local level, enhancing within-category distinctions, in particular, for negative words, resulting in less effective facilitation. {\copyright} Copyright {\copyright} 2015 Sereno, Scott, Yao, Thaden and O'Donnell.},
	author = {Sereno, Sara C. and Scott, Graham G. and Yao, Bo and Thaden, Elske J. and O'Donnell, Patrick J.},
	author_keywords = {arousal; emotion; lexical decision; mood induction; valence; visual word recognition; word frequency},
	doi = {10.3389/fpsyg.2015.01191},
	journal = {Frontiers in Psychology},
	note = {Cited by: 34; All Open Access, Gold Open Access, Green Open Access},
	publication_stage = {Final},
	source = {Scopus},
	title = {Emotion word processing: does mood make a difference?},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031307913&doi=10.3389%2ffpsyg.2015.01191&partnerID=40&md5=88a383188172c052192b2ac071083bff},
	volume = {6},
	year = {2015},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031307913&doi=10.3389%2ffpsyg.2015.01191&partnerID=40&md5=88a383188172c052192b2ac071083bff},
	bdsk-url-2 = {https://doi.org/10.3389/fpsyg.2015.01191}}

@article{Deng2015,
	abstract = {We present several interrelated technical and empirical contributions to the problem of emotion-based music recommendation and show how they can be applied in a possible usage scenario. The contributions are (1) a new three-dimensional resonance-arousal-valence model for the representation of emotion expressed in music, together with methods for automatically classifying a piece of music in terms of this model, using robust regression methods applied to musical/acoustic features; (2) methods for predicting a listener's emotional state on the assumption that the emotional state has been determined entirely by a sequence of pieces of music recently listened to, using conditional random fields and taking into account the decay of emotion intensity over time; and (3) a method for selecting a ranked list of pieces of music that match a particular emotional state, using a minimization iteration method. A series of experiments yield information about the validity of our operationalizations of these contributions. Throughout the article, we refer to an illustrative usage scenario in which all of these contributions can be exploited, where it is assumed that (1) a listener's emotional state is being determined entirely by the music that he or she has been listening to and (2) the listener wants to hear additional music that matches his or her current emotional state. The contributions are intended to be useful in a variety of other scenarios as well. {\copyright} 2015 ACM.},
	author = {Deng, James J. and Leung, Clement H.C. and Milani, Alfredo and Chen, Li},
	author_keywords = {Affective computing; Conditional random fields; Emotional state; Music emotion recognition; Music recommendation; Musical emotion},
	doi = {10.1145/2723575},
	journal = {ACM Transactions on Interactive Intelligent Systems},
	keywords = {Random processes; Regression analysis; Affective Computing; Conditional random field; Emotional state; Music emotions; Music recommendation; Musical emotion; Iterative methods},
	note = {Cited by: 48},
	number = {1},
	publication_stage = {Final},
	source = {Scopus},
	title = {Emotional states associated with music: Classification, prediction of changes, and consideration in recommendation},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967332968&doi=10.1145%2f2723575&partnerID=40&md5=776b6e30deb692db07d6683305959a89},
	volume = {5},
	year = {2015},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967332968&doi=10.1145%2f2723575&partnerID=40&md5=776b6e30deb692db07d6683305959a89},
	bdsk-url-2 = {https://doi.org/10.1145/2723575}}

@article{Greene20141327,
	abstract = {The two dimensions of emotion, mood valence and arousal, have independent effects on recognition memory. At present, however, it is not clear how those effects are reflected in the human brain. Previous research in this area has generally dealt with memory for emotionally valenced or arousing stimuli, but the manner in which interacting mood and arousal states modulate responses in memory substrates remains poorly understood. We investigated memory for emotionally neutral items while independently manipulating mood valence and arousal state by means of music exposure. Four emotional conditions were created: positive mood/high arousal, positive mood/low arousal, negative mood/high arousal, and negative mood/low arousal. We observed distinct effects of mood valence and arousal in parietal substrates of recognition memory. Positive mood increased activity in ventral posterior parietal cortex (PPC) and orbitofrontal cortex, whereas arousal condition modulated activity in dorsal PPC and the posterior cingulate. An interaction between valence and arousal was observed in left ventral PPC, notably in a parietal area distinct from the those identified for the main effects, with a stronger effect of mood on recognition memory responses here under conditions of relative high versus low arousal. We interpreted the PPC activations in terms of the attention-to-memory hypothesis: Increased arousal may lead to increased top-down control of memory, and hence dorsal PPC activation, whereas positive mood valence may result in increased activity in ventral PPC regions associated with bottom-up attention to memory. These findings indicate that distinct parietal sites mediate the influences of mood, arousal, and their interplay during recognition memory. {\copyright} 2014, Psychonomic Society, Inc.},
	author = {Greene, Ciara M. and Flannery, Oliver and Soto, David},
	author_keywords = {Emotion; Parietal cortex; Prefrontal cortex; Recollection},
	doi = {10.3758/s13415-014-0266-y},
	journal = {Cognitive, Affective and Behavioral Neuroscience},
	keywords = {Acoustic Stimulation; Affect; Analysis of Variance; Arousal; Brain Mapping; Female; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Music; Oxygen; Parietal Lobe; Reaction Time; Recognition (Psychology); Visual Analog Scale; Young Adult; oxygen; affect; analysis of variance; arousal; auditory stimulation; blood; brain mapping; female; human; image processing; male; music; nuclear magnetic resonance imaging; parietal lobe; physiology; reaction time; recognition; vascularization; visual analog scale; young adult},
	note = {Cited by: 11; All Open Access, Green Open Access},
	number = {4},
	pages = {1327 -- 1339},
	publication_stage = {Final},
	source = {Scopus},
	title = {Distinct parietal sites mediate the influences of mood, arousal, and their interaction on human recognition memory},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894882843&doi=10.3758%2fs13415-014-0266-y&partnerID=40&md5=e99354f94b019e106fae30595eb646e1},
	volume = {14},
	year = {2014},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894882843&doi=10.3758%2fs13415-014-0266-y&partnerID=40&md5=e99354f94b019e106fae30595eb646e1},
	bdsk-url-2 = {https://doi.org/10.3758/s13415-014-0266-y}}

@article{Abadi2015209,
	abstract = {In this work, we present DECAF - a multimodal data set for decoding user physiological responses to affective multimedia content. Different from data sets such as DEAP [15] and MAHNOB-HCI [31], DECAF contains (1) brain signals acquired using the Magnetoencephalogram (MEG) sensor, which requires little physical contact with the user's scalp and consequently facilitates naturalistic affective response, and (2) explicit and implicit emotional responses of 30 participants to 40 one-minute music video segments used in [15] and 36 movie clips, thereby enabling comparisons between the EEG versus MEG modalities as well as movie versus music stimuli for affect recognition. In addition to MEG data, DECAF comprises synchronously recorded near-infra-red (NIR) facial videos, horizontal Electrooculogram (hEOG), Electrocardiogram (ECG), and trapezius-Electromyogram (tEMG) peripheral physiological responses. To demonstrate DECAF's utility, we present (i) a detailed analysis of the correlations between participants' self-assessments and their physiological responses and (ii) single-trial classification results for valence, arousal and dominance, with performance evaluation against existing data sets. DECAF also contains time-continuous emotion annotations for movie clips from seven users, which we use to demonstrate dynamic emotion prediction. {\copyright} 2010-2012 IEEE.},
	author = {Abadi, Mojtaba Khomami and Subramanian, Ramanathan and Kia, Seyed Mostafa and Avesani, Paolo and Patras, Ioannis and Sebe, Nicu},
	author_keywords = {Affective computing; Emotion recognition; MEG; Single-trial classification; User physiological responses},
	doi = {10.1109/TAFFC.2015.2392932},
	journal = {IEEE Transactions on Affective Computing},
	keywords = {Classification (of information); Decoding; Electrocardiography; Human computer interaction; Motion pictures; Physiology; Affective Computing; Emotion recognition; Magnetoencephalogram (MEG); Multimedia contents; Multimodal database; Performance evaluations; Physiological response; Single-trial classifications; Physiological models},
	note = {Cited by: 230; All Open Access, Green Open Access},
	number = {3},
	pages = {209 -- 222},
	publication_stage = {Final},
	source = {Scopus},
	title = {DECAF: MEG-Based Multimodal Database for Decoding Affective Physiological Responses},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940987310&doi=10.1109%2fTAFFC.2015.2392932&partnerID=40&md5=9f3978933830d0fec198fc1232b4a3fb},
	volume = {6},
	year = {2015},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940987310&doi=10.1109%2fTAFFC.2015.2392932&partnerID=40&md5=9f3978933830d0fec198fc1232b4a3fb},
	bdsk-url-2 = {https://doi.org/10.1109/TAFFC.2015.2392932}}

@article{Markov2014688,
	abstract = {Gaussian Processes (GPs) are Bayesian nonparametric models that are becoming more and more popular for their superior capabilities to capture highly nonlinear data relationships in various tasks, such as dimensionality reduction, time series analysis, novelty detection, as well as classical regression and classification tasks. In this paper, we investigate the feasibility and applicability of GP models for music genre classification and music emotion estimation. These are two of the main tasks in the music information retrieval (MIR) field. So far, the support vector machine (SVM) has been the dominant model used in MIR systems. Like SVM, GP models are based on kernel functions and Gram matrices; but, in contrast, they produce truly probabilistic outputs with an explicit degree of prediction uncertainty. In addition, there exist algorithms for GP hyperparameter learning - something the SVM framework lacks. In this paper, we built two systems, one for music genre classification and another for music emotion estimation using both SVM and GP models, and compared their performances on two databases of similar size. In all cases, the music audio signal was processed in the same way, and the effects of different feature extraction methods and their various combinations were also investigated. The evaluation experiments clearly showed that in both music genre classification and music emotion estimation tasks the GP performed consistently better than the SVM. The GP achieved a 13.6% relative genre classification error reduction and up to an 11% absolute increase of the coefficient of determination in the emotion estimation task. {\copyright} 2014 IEEE.},
	author = {Markov, Konstantin and Matsui, Tomoko},
	author_keywords = {Gaussian processes; Music emotion estimation; Music genre classification},
	doi = {10.1109/ACCESS.2014.2333095},
	journal = {IEEE Access},
	keywords = {Classification (of information); Feature extraction; Gaussian distribution; Gaussian noise (electronic); Image retrieval; Support vector machines; Time series analysis; Coefficient of determination; Dimensionality reduction; Feature extraction methods; Gaussian Processes; Music emotions; Music genre classification; Music information retrieval; Prediction uncertainty; Audio acoustics},
	note = {Cited by: 73; All Open Access, Gold Open Access},
	pages = {688 -- 697},
	publication_stage = {Final},
	source = {Scopus},
	title = {Music genre and emotion recognition using Gaussian processes},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923317754&doi=10.1109%2fACCESS.2014.2333095&partnerID=40&md5=91033029d14599965aafb73fa7175273},
	volume = {2},
	year = {2014},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923317754&doi=10.1109%2fACCESS.2014.2333095&partnerID=40&md5=91033029d14599965aafb73fa7175273},
	bdsk-url-2 = {https://doi.org/10.1109/ACCESS.2014.2333095}}

@article{Parncutt2014324,
	abstract = {The association between major/minor tonality and positive/negative emotional valence is psychologically robust, but without a single accepted explanation. I compare six partially related theories. Dissonance: On average, passages in minor keys are more dissonant because, on average, the minor triad is more dissonant (rougher, less harmonic) or because tonal structure is more complex. Alterity and markedness: Major triads and scales are more common than minor, and positive valence is more common than negative. Major and positive valence are the norm; minor and negative are marked Others. Uncertainty: The minor triad has a more ambiguous (less salient) root than the major, and the minor scale has more variable form and a more ambiguous (less stable) tonic; uncertainty is associated with anger, sadness, distress, and grief. Speech: By comparison to major triads and scales, minor contain pitch(es) that are lower than expected -- just as sad speech is lower than expected. Salience: In diatonic chord progressions, flattened diatonic scale degrees are more salient than sharpened because their harmonics better match the prevailing scale. Scale degrees 3 and 6 are more likely to destabilize tonality in minor than major tonalities. Familiarity: Arbitrary emotional differences between major and minor were reinforced in a historical process of cultural differentiation. For each theory, there are credible arguments and evidence for and against. All theories are broadly consistent with Terhardt's pattern-recognition model of pitch perception (non-musical perceptual familiarity with the harmonic series), Schenker's concept of prolongation (specifically, tonal voice leading as a prolongation of the tonic triad), evolutionary explanations of the emotional connotations of alterity, and a psychohistory of tonality in which melody, polyphony, leading tones, and the major--minor system emerged at different times, explicable by different psychological principles. {\copyright} The Author(s) 2014.},
	author = {Parncutt, Richard},
	author_keywords = {emotion; leading tone; major; minor; music and evolution; pitch; prolongation; salience; Schenker; speech; uncertainty},
	doi = {10.1177/1029864914542842},
	journal = {Musicae Scientiae},
	note = {Cited by: 46},
	number = {3},
	pages = {324 -- 353},
	publication_stage = {Final},
	source = {Scopus},
	title = {The emotional connotations of major versus minor tonality: One or more origins?},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910098161&doi=10.1177%2f1029864914542842&partnerID=40&md5=b02c97a8a9bc1bfcfb13706023e565ff},
	volume = {18},
	year = {2014},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910098161&doi=10.1177%2f1029864914542842&partnerID=40&md5=b02c97a8a9bc1bfcfb13706023e565ff},
	bdsk-url-2 = {https://doi.org/10.1177/1029864914542842}}

@article{Wang201556,
	abstract = {Modeling the association between music and emotion has been considered important for music information retrieval and affective human computer interaction. This paper presents a novel generative model called acoustic emotion Gaussians (AEG) for computational modeling of emotion. Instead of assigning a music excerpt with a deterministic (hard) emotion label, AEG treats the affective content of music as a (soft) probability distribution in the valence-arousal space and parameterizes it with a Gaussian mixture model (GMM). In this way, the subjective nature of emotion perception is explicitly modeled. Specifically, AEG employs two GMMs to characterize the audio and emotion data. The fitting algorithm of the GMM parameters makes the model learning process transparent and interpretable. Based on AEG, a probabilistic graphical structure for predicting the emotion distribution from music audio data is also developed. A comprehensive performance study over two emotion-labeled datasets demonstrates that AEG offers new insights into the relationship between music and emotion (e.g., to assess the "affective diversity" of a corpus) and represents an effective means of emotion modeling. Readers can easily implement AEG via the publicly available codes. As the AEG model is generic, it holds the promise of analyzing any signal that carries affective or other highly subjective information. {\copyright} 2010-2012 IEEE.},
	author = {Wang, Ju-Chiang and Yang, Yi-Hsuan and Wang, Hsin-Min and Jeng, Shyh-Kang},
	author_keywords = {Arousal; Gaussian mixture model; Music emotion recognition; Music information retrieval; Subjectivity; Valence},
	doi = {10.1109/TAFFC.2015.2397457},
	journal = {IEEE Transactions on Affective Computing},
	keywords = {Association reactions; Audio acoustics; Behavioral research; Gaussian distribution; Human computer interaction; Information retrieval; Learning algorithms; Probability distributions; Arousal; Gaussian Mixture Model; Music emotions; Music information retrieval; Subjectivity; Valence; Computer music},
	note = {Cited by: 47},
	number = {1},
	pages = {56 -- 68},
	publication_stage = {Final},
	source = {Scopus},
	title = {Modeling the affective content of music with a Gaussian mixture model},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924020940&doi=10.1109%2fTAFFC.2015.2397457&partnerID=40&md5=3f9ed3c409c2857abf2d2048c43fc492},
	volume = {6},
	year = {2015},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924020940&doi=10.1109%2fTAFFC.2015.2397457&partnerID=40&md5=3f9ed3c409c2857abf2d2048c43fc492},
	bdsk-url-2 = {https://doi.org/10.1109/TAFFC.2015.2397457}}

@article{Gebauer2014,
	abstract = {Music is a potent source for eliciting emotions, but not everybody experience emotions in the same way. Individuals with autism spectrum disorder (ASD) show difficulties with social and emotional cognition. Impairments in emotion recognition are widely studied in ASD, and have been associated with atypical brain activation in response to emotional expressions in faces and speech. Whether these impairments and atypical brain responses generalize to other domains, such as emotional processing of music, is less clear. Using functional magnetic resonance imaging, we investigated neural correlates of emotion recognition in music in high-functioning adults with ASD and neurotypical adults. Both groups engaged similar neural networks during processing of emotional music, and individuals with ASD rated emotional music comparable to the group of neurotypical individuals. However, in the ASD group, increased activity in response to happy compared to sad music was observed in dorsolateral prefrontal regions and in the rolandic operculum/insula, and we propose that this reflects increased cognitive processing and physiological arousal in response to emotional musical stimuli in this group. {\copyright} 2014 Gebauer, Skewes, Westphael, Heaton and Vuust.},
	author = {Gebauer, Line and Skewes, Joshua and Westphael, Gitte and Heaton, Pamela and Vuust, Peter},
	author_keywords = {Autism spectrum disorder; Emotion; FMRI; Music},
	doi = {10.3389/fnins.2014.00192},
	journal = {Frontiers in Neuroscience},
	keywords = {adult; anterior cingulate; arousal; article; autism; brain function; cingulate gyrus; clinical article; cognition; Cognitive load; controlled study; corpus striatum; emotion; emotionality; female; functional magnetic resonance imaging; human; insula; male; mesencephalon; middle frontal gyrus; music; nerve cell network; operculum (brain); orbital cortex; parahippocampal gyrus; postcentral gyrus; prefrontal cortex; primary motor cortex; stimulus response; superior frontal gyrus},
	note = {Cited by: 60; All Open Access, Gold Open Access, Green Open Access},
	number = {8 JUL},
	publication_stage = {Final},
	source = {Scopus},
	title = {Intact brain processing of musical emotions in autism spectrum disorder, but more cognitive load and arousal in happy vs. sad music},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905925105&doi=10.3389%2ffnins.2014.00192&partnerID=40&md5=e37ef39814e060d74736eba867d87352},
	year = {2014},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905925105&doi=10.3389%2ffnins.2014.00192&partnerID=40&md5=e37ef39814e060d74736eba867d87352},
	bdsk-url-2 = {https://doi.org/10.3389/fnins.2014.00192}}

@article{Kim201483,
	abstract = {In this research we implement an EEG based music therapy. Music therapy can help the student deal with the stress, anxiety and depression problems. To do so we will develop EEG-based human emotion recognition algorithm. Proposed training program works as a therapist. The music choice and duration of the music is adjusted based on the student's current emotion recognized automatically from EEG. If the happy emotion is not induced by the current music, the system would automatically switch to another one until he or she feel happy. Proposed system is personalized brain music treatment that is making a brain training application running on smart phone or pad. That overcomes the critical problems of time and space constraints of existing brain training program. By using this brain training program, student can manage the stress easily without the help of expert. {\copyright} 2014 SERSC.},
	author = {Kim, Ga-Hyung and Kim, Byung-Joo},
	author_keywords = {Electroencephlagraphy; Personalized Music Training},
	doi = {10.14257/ijmue.2014.9.5.08},
	journal = {International Journal of Multimedia and Ubiquitous Engineering},
	keywords = {Occupational therapy; Critical problems; Electroencephlagraphy; Human emotion recognition; Music therapy; Space constraints; Training applications; Training program; Students},
	note = {Cited by: 7},
	number = {5},
	pages = {83 -- 91},
	publication_stage = {Final},
	source = {Scopus},
	title = {Reducing the student's stress from studying by personalized brain music training},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901941545&doi=10.14257%2fijmue.2014.9.5.08&partnerID=40&md5=d77a346c913347af9de9adee24016ce6},
	volume = {9},
	year = {2014},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901941545&doi=10.14257%2fijmue.2014.9.5.08&partnerID=40&md5=d77a346c913347af9de9adee24016ce6},
	bdsk-url-2 = {https://doi.org/10.14257/ijmue.2014.9.5.08}}

@article{Borella2014,
	abstract = {There are evidences showing that music can affect cognitive performance by improving our emotional state. The aim of the current study was to analyze whether age-related differences between young and older adults in a Working Memory (WM) Span test in which the stimuli to be recalled have a different valence (i.e., neutral, positive, or negative words), are sensitive to exposure to music. Because some previous studies showed that emotional words can sustain older adults' performance in WM, we examined whether listening to music could enhance the benefit of emotional material, with respect to neutral words, on WM performance decreasing the age-related difference between younger and older adults. In particular, the effect of two types of music (Mozart vs. Albinoni), which differ in tempo, arousal and mood induction, on age-related differences in an affective version of the Operation WM Span task were analyzed. Results showed no effect of music on the WM test regardless of the emotional content of the music (Mozart vs. Albinoni). However, as in previous studies, a valence effect for the words in the WM task was found with a higher number of negative words recalled with respect to positive and neutral ones in both younger and older adults. When individual differences, in terms of accuracy in the processing phase of the Operation Span task, were considered, only younger low-performing participants were affected by the type music, with the Albinoni condition that lowered their performance with respect to the Mozart condition. Such a result suggests that individual differences in WM performance, at least when young adults are considered, could be affected by the type of music. Altogether, these findings suggest that complex span tasks, such as WM tasks, along with age-related differences are less sensitive to music effects. {\copyright} 2014 Borella, Carretti, Grassi, Nucci and Sciore.},
	author = {Borella, Erika and Carretti, Barbara and Grassi, Massimo and Nucci, Massimo and Sciore, Roberta},
	author_keywords = {Aging; Emotions; Individual differences; Music; Working memory},
	doi = {10.3389/fnagi.2014.00298},
	journal = {Frontiers in Aging Neuroscience},
	keywords = {accuracy; adult; affective working memory test; aged; aging; arousal; Article; controlled study; groups by age; human; human experiment; learning and memory test; mood; music; normal human; performance; randomized controlled trial; word recognition; working memory; young adult},
	note = {Cited by: 9; All Open Access, Gold Open Access, Green Open Access},
	number = {OCT},
	publication_stage = {Final},
	source = {Scopus},
	title = {Are age-related differences between young and older adults in an affective working memory test sensitive to the music effects?},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949125827&doi=10.3389%2ffnagi.2014.00298&partnerID=40&md5=3894646f04579cdbad496ff64ddaaa9d},
	volume = {6},
	year = {2014},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949125827&doi=10.3389%2ffnagi.2014.00298&partnerID=40&md5=3894646f04579cdbad496ff64ddaaa9d},
	bdsk-url-2 = {https://doi.org/10.3389/fnagi.2014.00298}}

@article{Gingras20141428,
	abstract = {Emotions in music are conveyed by a variety of acoustic cues. Notably, the positive association between sound intensity and arousal has particular biological relevance. However, although amplitude normalization is a common procedure used to control for intensity in music psychology research, direct comparisons between emotional ratings of original and amplitude-normalized musical excerpts are lacking.In this study, 30 nonmusicians retrospectively rated the subjective arousal and pleasantness induced by 84 six-second classical music excerpts, and an additional 30 nonmusicians rated the same excerpts normalized for amplitude. Following the cue-redundancy and Brunswik lens models of acoustic communication, we hypothesized that arousal and pleasantness ratings would be similar for both versions of the excerpts, and that arousal could be predicted effectively by other acoustic cues besides intensity.Although the difference in mean arousal and pleasantness ratings between original and amplitude-normalized excerpts correlated significantly with the amplitude adjustment, ratings for both sets of excerpts were highly correlated and shared a similar range of values, thus validating the use of amplitude normalization in music emotion research. Two acoustic parameters, spectral flux and spectral entropy, accounted for 65% of the variance in arousal ratings for both sets, indicating that spectral features can effectively predict arousal. Additionally, we confirmed that amplitude-normalized excerpts were adequately matched for loudness. Overall, the results corroborate our hypotheses and support the cue-redundancy and Brunswik lens models. {\copyright} 2013 The Experimental Psychology Society.},
	author = {Gingras, Bruno and Marin, Manuela M. and Fitch, W. Tecumseh},
	author_keywords = {Arousal; Brunswik lens model; Emotion; Intensity; Music},
	doi = {10.1080/17470218.2013.863954},
	journal = {Quarterly Journal of Experimental Psychology},
	keywords = {Acoustic Stimulation; Acoustics; Adult; Arousal; Emotions; Female; Humans; Linear Models; Loudness Perception; Male; Music; Predictive Value of Tests; Questionnaires; Recognition (Psychology); Young Adult; acoustics; adult; arousal; auditory stimulation; emotion; female; hearing; human; male; music; physiology; predictive value; psychology; questionnaire; recognition; statistical model; young adult},
	note = {Cited by: 32},
	number = {7},
	pages = {1428 -- 1446},
	publication_stage = {Final},
	source = {Scopus},
	title = {Beyond intensity: Spectral features effectively predict music-induced subjective arousal},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902376903&doi=10.1080%2f17470218.2013.863954&partnerID=40&md5=021d634d0f679aaff17d6770894daad5},
	volume = {67},
	year = {2014},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902376903&doi=10.1080%2f17470218.2013.863954&partnerID=40&md5=021d634d0f679aaff17d6770894daad5},
	bdsk-url-2 = {https://doi.org/10.1080/17470218.2013.863954}}

@article{Chin2014,
	abstract = {For music emotion detection, this paper presents a music emotion verification system based on hierarchical sparse kernel machines. With the proposed system, we intend to verify if a music clip possesses happiness emotion or not. There are two levels in the hierarchical sparse kernel machines. In the first level, a set of acoustical features are extracted, and principle component analysis (PCA) is implemented to reduce the dimension. The acoustical features are utilized to generate the first-level decision vector, which is a vector with each element being a significant value of an emotion. The significant values of eight main emotional classes are utilized in this paper. To calculate the significant value of an emotion, we construct its 2-class SVM with calm emotion as the global (non-target) side of the SVM. The probability distributions of the adopted acoustical features are calculated and the probability product kernel is applied in the first-level SVMs to obtain first-level decision vector feature. In the second level of the hierarchical system, we merely construct a 2-class relevance vector machine (RVM) with happiness as the target side and other emotions as the background side of the RVM. The first-level decision vector is used as the feature with conventional radial basis function kernel. The happiness verification threshold is built on the probability value. In the experimental results, the detection error tradeoff (DET) curve shows that the proposed system has a good performance on verifying if a music clip reveals happiness emotion. {\copyright} 2014 Yu-Hao Chin et al.},
	author = {Chin, Yu-Hao and Lin, Chang-Hong and Siahaan, Ernestasia and Wang, Jia-Ching},
	doi = {10.1155/2014/270378},
	journal = {The Scientific World Journal},
	keywords = {Algorithms; Auditory Perception; Biomimetics; Emotions; Humans; Music; Pattern Recognition, Automated; Sound Spectrography; Support Vector Machines; algorithm; article; emotion; happiness; kernel method; music; principal component analysis; probability; relevance vector machine; support vector machine; algorithm; automated pattern recognition; biomimetics; emotion; hearing; human; physiology; procedures; sound detection; support vector machine},
	note = {Cited by: 4; All Open Access, Gold Open Access, Green Open Access},
	publication_stage = {Final},
	source = {Scopus},
	title = {Music emotion detection using hierarchical sparse kernel machines},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896904854&doi=10.1155%2f2014%2f270378&partnerID=40&md5=48fe84315e318b35b7b402c797cb9a66},
	volume = {2014},
	year = {2014},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896904854&doi=10.1155%2f2014%2f270378&partnerID=40&md5=48fe84315e318b35b7b402c797cb9a66},
	bdsk-url-2 = {https://doi.org/10.1155/2014/270378}}

@article{Xing2015619,
	abstract = {In this study, we attempt to explore cross-media retrieval between music and image data based on the emotional correlation. Emotion feature analytic could be the bridge of cross-media retrieval, since emotion represents the user's perspective and effectively meets the user's retrieval need. Currently, there is little research about the emotion correlation of different multimedia data (e.g. image or music). We propose a promising model based on Differential Evolutionary-Support Vector Machine (DE-SVM) to build up the emotion-driven cross-media retrieval system between Chinese folk image and Chinese folk music. In this work, we first build up the Chinese Folk Music Library and Chinese Folk Image Library.Second, we compare Back Propagation(BP), Linear Regression(LR) and Differential Evolutionary-Support Vector Machine (DE-SVM), and find that DE-SVM has the best performance. Then we conduct DE-SVM to build the optimal model for music/image emotion recognition. Finally, an Emotion-driven Chinese Folk Music-Image Exploring System based on DE-SVM is developed and experiment results show our method is effective in terms of retrieval performance. {\copyright} 2014 Elsevier B.V.},
	author = {Xing, Baixi and Zhang, Kejun and Sun, Shouqian and Zhang, Lekai and Gao, Zenggui and Wang, Jiaxi and Chen, Shi},
	author_keywords = {Back propagation; Cross-media information retrieval; Differential Evolutionary algorithm; Image emotion recognition; Music emotion recognition; Support vector machine},
	doi = {10.1016/j.neucom.2014.08.007},
	journal = {Neurocomputing},
	keywords = {Cross-media information retrieval; Differential evolutionary algorithm; Emotion recognition; Music emotions; accuracy; Article; back propagation; calculation; Chinese; classification algorithm; cultural period; Differential Evolutionary Support Vector Machine algorithm; emotion; image retrieval; library; linear system; mathematical analysis; mathematical model; music; nonlinear system; support vector machine; Support vector machines},
	note = {Cited by: 28},
	pages = {619 -- 627},
	publication_stage = {Final},
	source = {Scopus},
	title = {Emotion-driven Chinese folk music-image retrieval based on DE-SVM},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908032200&doi=10.1016%2fj.neucom.2014.08.007&partnerID=40&md5=62cf1f46b87868a59e2d0da9852809db},
	volume = {148},
	year = {2015},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908032200&doi=10.1016%2fj.neucom.2014.08.007&partnerID=40&md5=62cf1f46b87868a59e2d0da9852809db},
	bdsk-url-2 = {https://doi.org/10.1016/j.neucom.2014.08.007}}

@article{Niu2014147,
	abstract = {In this paper, we first regard emotion recognition as a pattern recognition problem, a novel feature selection method was presented to recognize human emotional state from four physiological signals. Electrocardiogram (ECG), electromyogram (EMG), skin conductance (SC) and respiration (RSP). The raw training data was collected from four sensors, ECG, EMG, SC, RSP, when a single subject intentionally expressed four different affective states, joy, anger, sadness, pleasure. The total 193 features were extracted for the recognition. A music induction method was used to elicit natural emotional reactions from the subject, after calculating a sufficient amount of features from the raw signals, the genetic algorithm and the K-neighbor methods were tested to extract a new feature set consisting of the most significant features which represents exactly the relevant emotional state for improving classification performance. The numerical results demonstrate that there is significant information in physiological signals for recognizing the affective state. It also turned out that it was much easier to separate emotions along the arousal axis than along the valence axis. {\copyright} 2014 by IFSA Publishing, S. L.},
	author = {Niu, Xiaowei and Chen, Liwan and Xie, Hui and Chen, Qiang and Li, Hongbing},
	author_keywords = {Affective state; Feature selection; Pattern recognition; Physiological sensor; Physiological signals},
	journal = {Sensors and Transducers},
	keywords = {Computer music; Electrocardiography; Feature extraction; Genetic algorithms; Pattern recognition; Physiology; Affective state; Classification performance; Emotion recognition; Emotional reactions; Feature selection methods; Pattern recognition problems; Physiological sensors; Physiological signals; Signal processing},
	note = {Cited by: 10},
	number = {6},
	pages = {147 -- 156},
	publication_stage = {Final},
	source = {Scopus},
	title = {Emotion pattern recognition using physiological signals},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924376515&partnerID=40&md5=8c6eb06b70e87d57c4568d424b16d809},
	volume = {172},
	year = {2014},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924376515&partnerID=40&md5=8c6eb06b70e87d57c4568d424b16d809}}

@article{Yeh20142103,
	abstract = {This paper proposes a popular music representation strategy based on the song's emotion. First, a piece of popular music is decomposed into chorus and verse segments through the proposed chorus detection algorithm. Three descriptive features: intensity, frequency band and rhythm regularity are extracted from the structured segments for emotion detection. A hierarchical Adaboost classifier is employed to recognize the emotion of a piece of popular music. The general emotion of the music is classified according to Thayer's model into four emotions: happy, angry, depressed and relaxed. Experiments conducted on a 350-popular-music database show the average recall and precision of our proposed chorus detection are approximately 95 % and 84 %, respectively; and the average precision rate of emotion detection is 92 %. Additional tests are performed on songs with cover versions in different lyrics and languages, and the resultant precision rate is 90 %. The proposes approaches have been tested and proven by the professional online music company, KKBOX Inc. and show promising performance for effectively and efficiently identifying the emotions of a variety of popular music. {\copyright} 2013, Springer Science+Business Media New York.},
	author = {Yeh, Chia-Hung and Tseng, Wen-Yu and Chen, Chia-Yen and Lin, Yu-Dun and Tsai, Yi-Ren and Bi, Hsuan-I and Lin, Yu-Ching and Lin, Ho-Yi},
	author_keywords = {Adaboost; Chorus; Emotion; MFCCs; Popular music; Rhythm; Verse},
	doi = {10.1007/s11042-013-1687-2},
	journal = {Multimedia Tools and Applications},
	keywords = {Multimedia systems; Chorus; Emotion; MFCCs; Popular music; Rhythm; Verse; Adaptive boosting},
	note = {Cited by: 11},
	number = {3},
	pages = {2103 -- 2128},
	publication_stage = {Final},
	source = {Scopus},
	title = {Popular music representation: chorus detection & emotion recognition},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84912027522&doi=10.1007%2fs11042-013-1687-2&partnerID=40&md5=6ce0fe7a0b38a6e2037403e140bd0a60},
	volume = {73},
	year = {2014},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84912027522&doi=10.1007%2fs11042-013-1687-2&partnerID=40&md5=6ce0fe7a0b38a6e2037403e140bd0a60},
	bdsk-url-2 = {https://doi.org/10.1007/s11042-013-1687-2}}

@article{Naji20151365,
	abstract = {Emotion recognition systems are helpful in human--machine interactions and clinical applications. This paper investigates the feasibility of using 3-channel forehead biosignals (left temporalis, frontalis, and right temporalis channel) as informative channels for emotion recognition during music listening. Classification of four emotional states (positive valence/low arousal, positive valence/high arousal, negative valence/high arousal, and negative valence/low arousal) in arousal--valence space was performed by employing two parallel cascade-forward neural networks as arousal and valence classifiers. The inputs of the classifiers were obtained by applying a fuzzy rough model feature evaluation criterion and sequential forward floating selection algorithm. An averaged classification accuracy of 87.05 % was achieved, corresponding to average valence classification accuracy of 93.66 % and average arousal classification accuracy of 93.29 %. {\copyright} 2013, Springer-Verlag London.},
	author = {Naji, Mohsen and Firoozabadi, Mohammd and Azadfallah, Parviz},
	author_keywords = {Arousal; Emotion recognition; Forehead biosignals; Valence},
	doi = {10.1007/s11760-013-0591-6},
	journal = {Signal, Image and Video Processing},
	keywords = {Speech recognition; Arousal; Arousal classifications; Biosignals; Classification accuracy; Clinical application; Emotion classification; Emotion recognition; Valence; Classification (of information)},
	note = {Cited by: 42},
	number = {6},
	pages = {1365 -- 1375},
	publication_stage = {Final},
	source = {Scopus},
	title = {Emotion classification during music listening from forehead biosignals},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939651639&doi=10.1007%2fs11760-013-0591-6&partnerID=40&md5=a205a7fa7a593add1a57871772ebc62f},
	volume = {9},
	year = {2015},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939651639&doi=10.1007%2fs11760-013-0591-6&partnerID=40&md5=a205a7fa7a593add1a57871772ebc62f},
	bdsk-url-2 = {https://doi.org/10.1007/s11760-013-0591-6}}

@article{Drossos201527,
	abstract = {Emotion recognition from sound signals represents an emerging field of recent research. Although many existing works focus on emotion recognition from music, there seems to be a relative scarcity of research on emotion recognition from general sounds. One of the key characteristics of sound events is the sound source spatial position, i.e. the location of the source relatively to the acoustic receiver. Existing studies that aim to investigate the relation of the latter source placement and the elicited emotions are limited to distance, front and back spatial localization and/or specific emotional categories. In this paper we analytically investigate the effect of the source angular position on the listener's emotional state, modeled in the well-established valence/arousal affective space. Towards this aim, we have developed an annotated sound events dataset using binaural processed versions of the available International Affective Digitized Sound (IADS) sound events library. All subjective affective annotations were obtained using the Self Assessment Manikin (SAM) approach. Preliminary results obtained by processing these annotation scores are likely to indicate a systematic change in the listener affective state as the sound source angular position changes. This trend is more obvious when the sound source is located outside of the visible field of the listener. {\copyright} 2010-2012 IEEE.},
	author = {Drossos, Konstantinos and Floros, Andreas and Giannakoulopoulos, Andreas and Kanellopoulos, Nikolaos},
	author_keywords = {affective acoustic ecology; affective state; Binaural processing; emotions; Sound events},
	doi = {10.1109/TAFFC.2015.2392768},
	journal = {IEEE Transactions on Affective Computing},
	keywords = {Computer programming; Affective acoustic ecologies; Affective state; Binaural processing; emotions; Sound events; Acoustic generators},
	note = {Cited by: 13},
	number = {1},
	pages = {27 -- 42},
	publication_stage = {Final},
	source = {Scopus},
	title = {Investigating the impact of sound angular position on the listener affective state},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924037825&doi=10.1109%2fTAFFC.2015.2392768&partnerID=40&md5=1f0afb878c1b00e4074d4870cb2897c4},
	volume = {6},
	year = {2015},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924037825&doi=10.1109%2fTAFFC.2015.2392768&partnerID=40&md5=1f0afb878c1b00e4074d4870cb2897c4},
	bdsk-url-2 = {https://doi.org/10.1109/TAFFC.2015.2392768}}

@article{Kragel20141437,
	abstract = {Understanding how emotions are represented neurally is a central aim of affective neuroscience. Despite decades of neuroimaging efforts addressing this question, it remains unclear whether emotions are represented as distinct entities, as predicted by categorical theories, or are constructed from a smaller set of underlying factors, as predicted by dimensional accounts. Here, we capitalize on multivariate statistical approaches and computational modeling to directly evaluate these theoretical perspectives. We elicited discrete emotional states using music and films during functional magnetic resonance imaging scanning. Distinct patterns of neural activation predicted the emotion category of stimuli and tracked subjective experience. Bayesian model comparison revealed that combining dimensional and categorical models of emotion best characterized the information content of activation patterns. Surprisingly, categorical and dimensional aspects of emotion experience captured unique and opposing sources of neural information. These results indicate that diverse emotional states are poorly differentiated by simple models of valence and arousal, and that activity within separable neural systems can be mapped to unique emotion categories. {\copyright} The Author (2015). Published by Oxford University Press.},
	author = {Kragel, Philip A. and LaBar, Kevin S.},
	author_keywords = {Affect; Bayesian model comparison; Emotion; Functional magnetic resonance imaging; Multi-voxel pattern analysis; Pattern classification},
	doi = {10.1093/scan/nsv032},
	journal = {Social Cognitive and Affective Neuroscience},
	keywords = {Adult; Bayes Theorem; Biomarkers; Brain; Brain Mapping; Emotions; Female; Humans; Magnetic Resonance Imaging; Male; Motion Pictures; Music; Pattern Recognition, Automated; Young Adult; biological marker; adult; automated pattern recognition; Bayes theorem; brain; brain mapping; emotion; female; human; male; movie; music; nuclear magnetic resonance imaging; physiology; procedures; young adult},
	note = {Cited by: 128; All Open Access, Green Open Access},
	number = {11},
	pages = {1437 -- 1448},
	publication_stage = {Final},
	source = {Scopus},
	title = {Multivariate neural biomarkers of emotional states are categorically distinct},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947706348&doi=10.1093%2fscan%2fnsv032&partnerID=40&md5=cce5ddb245e9f926d92aa921e9580a00},
	volume = {10},
	year = {2014},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947706348&doi=10.1093%2fscan%2fnsv032&partnerID=40&md5=cce5ddb245e9f926d92aa921e9580a00},
	bdsk-url-2 = {https://doi.org/10.1093/scan/nsv032}}

@article{Alonso2015,
	abstract = {The emotions evoked by music can enhance recognition of excerpts. It has been suggested that memory is better for high than for low arousing music (Eschrich et al., 2005; Samson et al., 2009), but it remains unclear whether positively (Eschrich et al., 2008) or negatively valenced music (Aub{\'e} et al., 2013; Vieillard and Gilet, 2013) may be better recognized. Moreover, we still know very little about the influence of age on emotional memory for music. To address these issues, we tested emotional memory for music in young and older adults using musical excerpts varying in terms of arousal and valence. Participants completed immediate and 24h delayed recognition tests. We predicted highly arousing excerpts to be better recognized by both groups in immediate recognition. We hypothesized that arousal may compensate consolidation deficits in aging, thus showing more prominent benefit of high over low arousing stimuli in older than younger adults on delayed recognition. We also hypothesized worst retention of negative excerpts for the older group, resulting in a recognition benefit for positive over negative excerpts specific to older adults. Our results suggest that although older adults had worse recognition than young adults overall, effects of emotion on memory do not seem to be modified by aging. Results on immediate recognition suggest that recognition of low arousing excerpts can be affected by valence, with better memory for positive relative to negative low arousing music. However, 24h delayed recognition results demonstrate effects of emotion on memory consolidation, with a recognition benefit for high arousal and for negatively valenced music. The present study highlights the role of emotion on memory consolidation. Findings are examined in light of the literature on emotional memory for music and for other stimuli. We finally discuss the implication of the present results for potential music interventions in aging and dementia. {\copyright} 2015 Alonso, Dellacherie and Samson.},
	author = {Alonso, Irene and Dellacherie, Delphine and Samson, S{\'e}verine},
	author_keywords = {Aging; Consolidation; Emotion; Memory; Music},
	doi = {10.3389/fnagi.2015.00023},
	journal = {Frontiers in Aging Neuroscience},
	keywords = {adult; aged; aging; anxiety; arousal; Article; cognition; dementia; depression; emotion; emotional memory; fatigue; female; human; male; memory consolidation; memory disorder; Mini Mental State Examination; music therapy; normal human; prediction; Profile of Mood States; recognition; senescence; therapy effect},
	note = {Cited by: 11; All Open Access, Gold Open Access, Green Open Access},
	number = {FEB},
	publication_stage = {Final},
	source = {Scopus},
	title = {Emotional memory for musical excerpts in young and older adults},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924532933&doi=10.3389%2ffnagi.2015.00023&partnerID=40&md5=c9f90971e41b74961897ba469383434d},
	volume = {7},
	year = {2015},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924532933&doi=10.3389%2ffnagi.2015.00023&partnerID=40&md5=c9f90971e41b74961897ba469383434d},
	bdsk-url-2 = {https://doi.org/10.3389/fnagi.2015.00023}}

@article{Naji2014241,
	abstract = {Emotion recognition systems have been developed to assess human emotional states during different experiences. In this paper, an approach is proposed for recognizing music-induced emotions through the fusion of three-channel forehead biosignals (the left temporalis, frontalis, and right temporalis channels) and an electrocardiogram. The classification of four emotional states in an arousal-valence space (positive valence/low arousal, positive valence/high arousal, negative valence/high arousal, and negative valence/low arousal) was performed by employing two parallel support vector machines as arousal and valence classifiers. The inputs of the classifiers were obtained by applying a fuzzy-rough model feature evaluation criterion and sequential forward floating selection algorithm. An average classification accuracy of 88.78 % was achieved, corresponding to an average valence classification accuracy of 94.91 % and average arousal classification accuracy of 93.63 %. The proposed emotion recognition system may be useful for interactive multimedia applications or music therapy. {\copyright} 2013 Springer Science+Business Media New York.},
	author = {Naji, Mohsen and Firoozabadi, Mohammd and Azadfallah, Parviz},
	author_keywords = {Arousal; ECG; Emotion classification; Forehead biosignals; Valence},
	doi = {10.1007/s12559-013-9239-7},
	journal = {Cognitive Computation},
	keywords = {Interactive computer systems; Multimedia systems; Arousal; Arousal classifications; Biosignals; Classification accuracy; Emotion classification; Emotion recognition; Interactive multimedia; Valence; Electrocardiography},
	note = {Cited by: 41},
	number = {2},
	pages = {241 -- 252},
	publication_stage = {Final},
	source = {Scopus},
	title = {Classification of Music-Induced Emotions Based on Information Fusion of Forehead Biosignals and Electrocardiogram},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901198056&doi=10.1007%2fs12559-013-9239-7&partnerID=40&md5=23054f30e2f6e999d97a6f9c852da30c},
	volume = {6},
	year = {2014},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901198056&doi=10.1007%2fs12559-013-9239-7&partnerID=40&md5=23054f30e2f6e999d97a6f9c852da30c},
	bdsk-url-2 = {https://doi.org/10.1007/s12559-013-9239-7}}

@article{Platz2015327,
	abstract = {Songs heard between the ages of 15 and 24 should be remembered better and have a stronger relationship to autobiographical memories when compared with music from other phases of life (``reminiscence bump effect''). Additionally, the proportion of music-evoked autobiographical memories (MEAMs) is at a maximum in these years of early adolescence and then declines up to the age of 60. In our study we tried both to replicate these important findings based on a German sample and to further investigate the influence of the affective characteristics of the songs on the frequency of participants' autobiographical memories. In Experiment 1 a group of adults (N = 48, Mage = 67.1 years) listened to excerpts from 80, number-one, popular music hits from 1930 to 2010 and gave written self-reports on MEAMs. In Experiment 2 the affective characteristics were rated by another group of adults (N = 22, Mage = 66 years) and were used to predict the frequency of MEAMs. As a main result of Experiment 1, we confirmed the reminiscence bump and decline effect with a small effect size for the ratings of feelings evoked by the song and with a medium effect size for the song recognition performance of those songs released during the participants' age range of 15 to 24 years. The total number of MEAMs was only marginally influenced by a memory bump and decline effect, and participants showed a significant proportion of MEAMs up to the fifth decade. Experiment 2 revealed that the affective ratings of the songs were unequally distributed over the two-dimensional emotion space unlike the average rate of MEAMs which was nearly equally distributed. In contrast to previous research, we therefore conclude that popular songs can be associated with autobiographical memory over five decades of life -- independent of the affective character of the music. {\copyright} The Author(s) 2015.},
	author = {Platz, Friedrich and Kopiez, Reinhard and Hasselhorn, Johannes and Wolf, Anna},
	author_keywords = {arousal; music-evoked autobiographical memory; popular music; reminiscence bumb effect; song-specific age; valence},
	doi = {10.1177/1029864915597567},
	journal = {Musicae Scientiae},
	note = {Cited by: 29},
	number = {4},
	pages = {327 -- 349},
	publication_stage = {Final},
	source = {Scopus},
	title = {The impact of song-specific age and affective qualities of popular songs on music-evoked autobiographical memories (MEAMs)},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948457436&doi=10.1177%2f1029864915597567&partnerID=40&md5=c440e95dc369005c70e0304f0b0b49f6},
	volume = {19},
	year = {2015},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948457436&doi=10.1177%2f1029864915597567&partnerID=40&md5=c440e95dc369005c70e0304f0b0b49f6},
	bdsk-url-2 = {https://doi.org/10.1177/1029864915597567}}

@article{Justel2015247,
	abstract = {In the last decades, different neuroscientific investigations have shown that emotions can be determinant in memory storage and consolidation. Events with emotional content are remembered more easily than neutral events. There are several factors that could affect memory consolidation for emotional events, strengthening or deteriorating them. Stress is one of them, since investigations indicate that moderate levels of stress improve the memory of emotional events, while high or low levels have the opposite effect (Justel, Psyrdellis, & Ruetti, 2013, 2014). Multiple studies showed that the exposition to different musical pieces could modulate memory. Activating music, both positive and negative valence, increases de levels of arousal and strengthen memory consolidation (Judde & Rickard, 2010), while relaxing music has the opposite effect and deteriorate the capacity of emotional memory (Rickard, Wing Wong, & Velik, 2012). However, there are not studies with older adults. The goal of this study was to evaluate how different musical pieces, arousing and relaxing ones, modulate memory consolidation in older adults. 27 participants were included, divided in three groups. 12 slides with emotional content and 12 neutral were presented in a computer, selected from the International Affective Picture System (Lang, Bradley, & Cuthbert, 1995). The older adults watched the emotional and neutral images, and evaluated the arousal /emotionality degree of the images as they filled in a table of five choices, from not exciting to very exciting. According to the assignment group, they were exposed to different musical stimuli: activating or relaxing music for the experimental groups or white noise in the control group. The auditive stimulies were selected according to the previous literature. For the activating musical stimuli, Symphony No. 70 in D major of Joseph Haydn was chosen (Kreutz, Ott, Teichmann, Osawa, & Vaitl, 2008); for the relaxing musical stimuli, Pachebel's canon in D major was chosen (Knight & Rickard, 2001); for the control stimuli, white noise was chosen (Rickard et al., 2012). Then free recall and recognition test were performed, immediately and deferred (one week later). In free recall, each subject briefly listed the images he remembered, mentioning them with a word or a short sentence. For the recognition test, the 24 images were mixed up with 24 new images, and the participants had to indicate if they had seen each image or not, as they filled in a table. Regarding to the evaluation of the arousal / emotionality degree of the images, the results indicate that older adults, no matter the group, punctuated the emotional images as more activating tan neutral images, consolidating, as other studies, the validity and reliability of IAPS. The participants exposed to relaxing music had a worse performance in free recall and recognition, compared with the other two groups. On the other hand, it was expected that participants exposed to activating music have a better performance in free recall and recognition test. In free recall, both immediate and deferred, there is a tendency for the activating group to perform better than the control group, but no significant statistical data was found. Regarding to the recognition test, both immediate and deferred, no significant differences were found between the activating and the control groups. This may be because the sample was formed by a reduced number of participants, but if the sample is extended, the results may change. These results allow to conclude that music modulate the consolidation of visual emotional memory in older adults, being music a useful tool in memory stimulation and a possible therapeutic resource for patients with memory dysfunctions.},
	author = {Justel, Nadia and O'Conor, Jaime and Rubinstein, Wanda},
	author_keywords = {Emotion; Memory; Modulation; Music; Older adults},
	journal = {Interdisciplinaria},
	note = {Cited by: 13},
	number = {2},
	pages = {247 -- 259},
	publication_stage = {Final},
	source = {Scopus},
	title = {Modulation of emotional memory through music in older adults: A preliminary study; [Modulaci{\'o}n de la memoria emocional a trav{\'e}s de la m{\'u}sica en adultos mayores: Un estudio preliminar]},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971513893&partnerID=40&md5=4423b15fe24ccd7cec68d4b87ea5de6c},
	volume = {32},
	year = {2015},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971513893&partnerID=40&md5=4423b15fe24ccd7cec68d4b87ea5de6c}}

@article{Winters201460,
	abstract = {Emotion is a word not often heard in sonification, though advances in affective computing make the data type imminent. At times the relationship between emotion and sonification has been contentious due to an implied overlap with music. This paper clarifies the relationship, demonstrating how it can be mutually beneficial. After identifying contexts favourable to auditory display of emotion, and the utility of its development to research in musical emotion, the current state of the field is addressed, reiterating the necessary conditions for sound to qualify as a sonification of emotion. With this framework, strategies for display are presented that use acoustic and structural cues designed to target select auditory-cognitive mechanisms of musical emotion. Two sonifications are then described using these strategies to convey arousal and valence though differing in design methodology: one designed ecologically, the other computationally. Each model is sampled at 15-second intervals at 49 evenly distributed points on the AV space, and evaluated using a publically available tool for computational music emotion recognition. The computational design performed 65 times better in this test, but the ecological design is argued to be more useful for emotional communication. Conscious of these limitations, computational design and evaluation is supported for future development. {\copyright} 2014 Cambridge University Press.},
	author = {Winters, R. Michael and Wanderley, Marcelo M.},
	doi = {10.1017/S1355771813000411},
	journal = {Organised Sound},
	note = {Cited by: 12},
	number = {1},
	pages = {60 -- 69},
	publication_stage = {Final},
	source = {Scopus},
	title = {Sonification of Emotion: Strategies and results from the intersection with music},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896367343&doi=10.1017%2fS1355771813000411&partnerID=40&md5=61581876533d8231c214659c80642b60},
	volume = {19},
	year = {2014},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896367343&doi=10.1017%2fS1355771813000411&partnerID=40&md5=61581876533d8231c214659c80642b60},
	bdsk-url-2 = {https://doi.org/10.1017/S1355771813000411}}
