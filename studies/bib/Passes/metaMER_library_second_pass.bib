%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Tuomas Eerola at 2024-05-13 14:50:12 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@Article{bai2016di,
  author        = {Bai, Junjie and Feng, Lixiao and Peng, Jun and Shi, Jinliang and Luo, Kan and Li, Zuojin and Liao, Lu and Wang, Yingxu},
  journal       = {INTERNATIONAL JOURNAL OF COGNITIVE INFORMATICS AND NATURAL INTELLIGENCE},
  title         = {Dimensional Music Emotion Recognition by Machine Learning},
  year          = {2016},
  issn          = {1557-3958},
  month         = {OCT-DEC},
  number        = {4},
  pages         = {74-89},
  volume        = {10},
  bdsk-url-1    = {https://doi.org/10.4018/IJCINI.2016100104},
  doi           = {10.4018/IJCINI.2016100104},
  eissn         = {1557-3966},
  hasabstract   = {N},
  orcid-numbers = {Wang, Yingxu/0000-0003-0445-3632 Peng, Jun/0000-0001-6800-0064 luo, kan/0000-0003-2317-6714},
  priority      = {prio1},
  unique-id     = {WOS:000389803200004},
}

@Article{naji2015em,
  author               = {Naji, Mohsen and Firoozabadi, Mohammd and Azadfallah, Parviz},
  journal              = {SIGNAL IMAGE AND VIDEO PROCESSING},
  title                = {Emotion classification during music listening from forehead biosignals},
  year                 = {2015},
  issn                 = {1863-1703},
  month                = {SEP},
  number               = {6},
  pages                = {1365-1375},
  volume               = {9},
  bdsk-url-1           = {https://doi.org/10.1007/s11760-013-0591-6},
  doi                  = {10.1007/s11760-013-0591-6},
  eissn                = {1863-1711},
  hasabstract          = {N},
  orcid-numbers        = {Azadfallah, Parviz/0000-0003-2810-762X Firoozabadi, S. Mohammad P./0000-0002-0607-257X},
  priority             = {prio3},
  researcherid-numbers = {Azadfallah, Parviz/HJX-9931-2023 Firoozabadi, S. Mohammad P./A-2290-2009},
  unique-id            = {WOS:000360078000014},
}

@Article{andrade2017as,
  author               = {Andrade, Paulo E. and Vanzella, Patricia and Andrade, Olga V. C. A. and Schellenberg, E. Glenn},
  journal              = {PSYCHOLOGY OF MUSIC},
  title                = {Associating emotions with Wagner's music: A developmental perspective},
  year                 = {2017},
  issn                 = {0305-7356},
  month                = {SEP},
  number               = {5},
  pages                = {752-760},
  volume               = {45},
  bdsk-url-1           = {https://doi.org/10.1177/0305735616678056},
  doi                  = {10.1177/0305735616678056},
  eissn                = {1741-3087},
  hasabstract          = {N},
  orcid-numbers        = {Vanzella, Patricia M/0000-0002-7709-1495 Schellenberg, Glenn/0000-0003-3681-6020},
  priority             = {prio1},
  researcherid-numbers = {Vanzella, Patricia M/K-6184-2016},
  unique-id            = {WOS:000407938800010},
}

@Article{griffiths2021am,
  author               = {Griffiths, Darryl and Cunningham, Stuart and Weinel, Jonathan and Picking, Richard},
  journal              = {JOURNAL OF NEW MUSIC RESEARCH},
  title                = {A multi-genre model for music emotion recognition using linear regressors},
  year                 = {2021},
  issn                 = {0929-8215},
  month                = {AUG 8},
  number               = {4},
  pages                = {355-372},
  volume               = {50},
  bdsk-url-1           = {https://doi.org/10.1080/09298215.2021.1977336},
  doi                  = {10.1080/09298215.2021.1977336},
  earlyaccessdate      = {SEP 2021},
  eissn                = {1744-5027},
  hasabstract          = {N},
  orcid-numbers        = {Cunningham, Stuart/0000-0002-5348-7700 Weinel, Jonathan/0000-0001-5347-3897},
  priority             = {prio1},
  researcherid-numbers = {Cunningham, Stuart/HSH-5303-2023},
  unique-id            = {WOS:000697651500001},
}

@Article{thammasan2016co,
  author      = {Thammasan, Nattapong and Moriyama, Koichi and Fukui, Ken-ichi and Numao, Masayuki},
  journal     = {IEICE TRANSACTIONS ON INFORMATION AND SYSTEMS},
  title       = {Continuous Music-Emotion Recognition Based on Electroencephalogram},
  year        = {2016},
  issn        = {0916-8532},
  month       = {APR},
  number      = {4},
  pages       = {1234-1241},
  volume      = {E99D},
  bdsk-url-1  = {https://doi.org/10.1587/transinf.2015EDP7251},
  doi         = {10.1587/transinf.2015EDP7251},
  eissn       = {1745-1361},
  hasabstract = {N},
  priority    = {prio3},
  unique-id   = {WOS:000375973800048},
}

@Article{shashikumar2023cl,
  author               = {Shashi Kumar, G. and Sampathila, Niranjana and Martis, Roshan Joy},
  journal              = {JOURNAL OF MEDICAL SIGNALS \& SENSORS},
  title                = {Classification of human emotional states based on valence-arousal scale using electroencephalogram},
  year                 = {2023},
  issn                 = {2228-7477},
  month                = {APR-JUN},
  number               = {2},
  pages                = {173-182},
  volume               = {13},
  bdsk-url-1           = {https://doi.org/10.4103/jmss.jmss%5C_169%5C_21},
  doi                  = {10.4103/jmss.jmss\_169\_21},
  hasabstract          = {N},
  orcid-numbers        = {S/0000-0002-3345-360X},
  priority             = {prio3},
  researcherid-numbers = {S/AAQ-8318-2020 Martis, Roshan J/E-7529-2016},
  unique-id            = {WOS:001013826100013},
}

@Article{du2023va,
  author          = {Du, Ruoyu and Zhu, Shujin and Ni, Huangjing and Mao, Tianyi and Li, Jiajia and Wei, Ran},
  journal         = {MULTIMEDIA TOOLS AND APPLICATIONS},
  title           = {Valence-arousal classification of emotion evoked by Chinese ancient-style music using 1D-CNN-BiLSTM model on EEG signals for college students},
  year            = {2023},
  issn            = {1380-7501},
  month           = {APR},
  number          = {10},
  pages           = {15439-15456},
  volume          = {82},
  bdsk-url-1      = {https://doi.org/10.1007/s11042-022-14011-7},
  doi             = {10.1007/s11042-022-14011-7},
  earlyaccessdate = {OCT 2022},
  eissn           = {1573-7721},
  hasabstract     = {N},
  priority        = {prio1},
  unique-id       = {WOS:000863811400005},
}

@Article{naser2021in,
  author               = {Naser, Daimi Syed and Saha, Goutam},
  journal              = {BIOMEDICAL SIGNAL PROCESSING AND CONTROL},
  title                = {Influence of music liking on EEG based emotion recognition},
  year                 = {2021},
  issn                 = {1746-8094},
  month                = {FEB},
  volume               = {64},
  article-number       = {102251},
  bdsk-url-1           = {https://doi.org/10.1016/j.bspc.2020.102251},
  doi                  = {10.1016/j.bspc.2020.102251},
  eissn                = {1746-8108},
  hasabstract          = {N},
  orcid-numbers        = {Saha, Goutam/0000-0001-6187-1684},
  priority             = {prio3},
  researcherid-numbers = {Saha, Goutam/AAH-6281-2020},
  unique-id            = {WOS:000600894700020},
}

@Article{chin2018pr,
  author      = {Chin, Yu-Hao and Wang, Jia-Ching and Wang, Ju-Chiang and Yang, Yi-Hsuan},
  journal     = {IEEE TRANSACTIONS ON AFFECTIVE COMPUTING},
  title       = {Predicting the Probability Density Function of Music Emotion Using Emotion Space Mapping},
  year        = {2018},
  issn        = {1949-3045},
  month       = {OCT-DEC},
  number      = {4},
  pages       = {541-549},
  volume      = {9},
  bdsk-url-1  = {https://doi.org/10.1109/TAFFC.2016.2628794},
  doi         = {10.1109/TAFFC.2016.2628794},
  hasabstract = {N},
  priority    = {prio1},
  unique-id   = {WOS:000451918200011},
}

@Article{dufour2021us,
  author               = {Dufour, Isabelle and Tzanetakis, George},
  journal              = {IEEE TRANSACTIONS ON AFFECTIVE COMPUTING},
  title                = {Using Circular Models to Improve Music Emotion Recognition},
  year                 = {2021},
  issn                 = {1949-3045},
  month                = {JUL-SEP},
  number               = {3},
  pages                = {666-681},
  volume               = {12},
  bdsk-url-1           = {https://doi.org/10.1109/TAFFC.2018.2885744},
  doi                  = {10.1109/TAFFC.2018.2885744},
  hasabstract          = {N},
  orcid-numbers        = {Tzanetakis, George/0000-0002-6844-7912},
  priority             = {prio1},
  researcherid-numbers = {Tzanetakis, George/I-6593-2013},
  unique-id            = {WOS:000692232000010},
}

@Article{naji2014cl,
  author               = {Naji, Mohsen and Firoozabadi, Mohammd and Azadfallah, Parviz},
  journal              = {COGNITIVE COMPUTATION},
  title                = {Classification of Music-Induced Emotions Based on Information Fusion of Forehead Biosignals and Electrocardiogram},
  year                 = {2014},
  issn                 = {1866-9956},
  month                = {JUN},
  number               = {2},
  pages                = {241-252},
  volume               = {6},
  bdsk-url-1           = {https://doi.org/10.1007/s12559-013-9239-7},
  doi                  = {10.1007/s12559-013-9239-7},
  eissn                = {1866-9964},
  hasabstract          = {N},
  orcid-numbers        = {Firoozabadi, S. Mohammad P./0000-0002-0607-257X Azadfallah, Parviz/0000-0003-2810-762X},
  priority             = {prio3},
  researcherid-numbers = {Firoozabadi, S. Mohammad P./A-2290-2009 Azadfallah, Parviz/HJX-9931-2023},
  unique-id            = {WOS:000336313700009},
}

@Article{qiao2024mu,
  author         = {Qiao, Yinghao and Mu, Jiajia and Xie, Jialan and Hu, Binghui and Liu, Guangyuan},
  journal        = {FRONTIERS IN HUMAN NEUROSCIENCE},
  title          = {Music emotion recognition based on temporal convolutional attention network using EEG},
  year           = {2024},
  issn           = {1662-5161},
  month          = {MAR 28},
  volume         = {18},
  article-number = {1324897},
  bdsk-url-1     = {https://doi.org/10.3389/fnhum.2024.1324897},
  doi            = {10.3389/fnhum.2024.1324897},
  hasabstract    = {N},
  priority       = {prio3},
  unique-id      = {WOS:001199606100001},
}

@Article{hofbauer2023em,
  author               = {Hofbauer, Lena M. and Rodriguez, Francisca S.},
  journal              = {INTERNATIONAL JOURNAL OF PSYCHOLOGY},
  title                = {Emotional valence perception in music and subjective arousal: Experimental validation of stimuli},
  year                 = {2023},
  issn                 = {0020-7594},
  month                = {OCT},
  number               = {5},
  pages                = {465-475},
  volume               = {58},
  bdsk-url-1           = {https://doi.org/10.1002/ijop.12922},
  doi                  = {10.1002/ijop.12922},
  earlyaccessdate      = {MAY 2023},
  eissn                = {1464-066X},
  hasabstract          = {N},
  orcid-numbers        = {Rodriguez, Francisca/0000-0003-2919-5510 Hofbauer, Lena/0000-0003-1789-711X},
  priority             = {prio1},
  researcherid-numbers = {Rodriguez, Francisca/AAD-6803-2019 Hofbauer, Lena/AAT-3664-2021},
  unique-id            = {WOS:000999320600001},
}

@Article{hsu2020au,
  author               = {Hsu, Yu-Liang and Wang, Jeen-Shing and Chiang, Wei-Chun and Hung, Chien-Han},
  journal              = {IEEE TRANSACTIONS ON AFFECTIVE COMPUTING},
  title                = {Automatic ECG-Based Emotion Recognition in Music Listening},
  year                 = {2020},
  issn                 = {1949-3045},
  month                = {JAN-MAR},
  number               = {1},
  pages                = {85-99},
  volume               = {11},
  bdsk-url-1           = {https://doi.org/10.1109/TAFFC.2017.2781732},
  doi                  = {10.1109/TAFFC.2017.2781732},
  hasabstract          = {N},
  priority             = {prio3},
  researcherid-numbers = {Wang, Jeen-Shing/I-8464-2012},
  unique-id            = {WOS:000521989700006},
}

@Article{zhang2023mo,
  author               = {Zhang, Meixian and Zhu, Yonghua and Zhang, Wenjun and Zhu, Yunwen and Feng, Tianyu},
  journal              = {MULTIMEDIA TOOLS AND APPLICATIONS},
  title                = {Modularized composite attention network for continuous music emotion recognition},
  year                 = {2023},
  issn                 = {1380-7501},
  month                = {FEB},
  number               = {5},
  pages                = {7319-7341},
  volume               = {82},
  bdsk-url-1           = {https://doi.org/10.1007/s11042-022-13577-6},
  doi                  = {10.1007/s11042-022-13577-6},
  earlyaccessdate      = {AUG 2022},
  eissn                = {1573-7721},
  hasabstract          = {N},
  orcid-numbers        = {Zhang, Meixian/0000-0002-6696-2814},
  priority             = {prio1},
  researcherid-numbers = {Zhu, Yonghua/HSI-1360-2023 Feng, Tian-Yu/AAX-3892-2020},
  unique-id            = {WOS:000842724400002},
}

@Article{orjesek2022en,
  author               = {Orjesek, Richard and Jarina, Roman and Chmulik, Michal},
  journal              = {MULTIMEDIA TOOLS AND APPLICATIONS},
  title                = {End-to-end music emotion variation detection using iteratively reconstructed deep features},
  year                 = {2022},
  issn                 = {1380-7501},
  month                = {FEB},
  number               = {4},
  pages                = {5017-5031},
  volume               = {81},
  bdsk-url-1           = {https://doi.org/10.1007/s11042-021-11584-7},
  doi                  = {10.1007/s11042-021-11584-7},
  earlyaccessdate      = {JAN 2022},
  eissn                = {1573-7721},
  hasabstract          = {N},
  orcid-numbers        = {Chmulik, Michal/0000-0002-0513-5129 Jarina, Roman/0000-0002-0478-5808},
  priority             = {prio1},
  researcherid-numbers = {Chmulik, Michal/IQW-1183-2023 Jarina, Roman/E-2541-2018},
  unique-id            = {WOS:000740429700032},
}

@Article{beveridge2018po,
  author        = {Beveridge, Scott and Knox, Don},
  journal       = {PSYCHOLOGY OF MUSIC},
  title         = {Popular music and the role of vocal melody in perceived emotion},
  year          = {2018},
  issn          = {0305-7356},
  month         = {MAY},
  number        = {3},
  pages         = {411-423},
  volume        = {46},
  bdsk-url-1    = {https://doi.org/10.1177/0305735617713834},
  doi           = {10.1177/0305735617713834},
  eissn         = {1741-3087},
  hasabstract   = {N},
  orcid-numbers = {Knox, Don/0000-0003-1303-1183},
  priority      = {prio1},
  unique-id     = {WOS:000429920400007},
}

@Article{chen2017co,
  author      = {Chen, Yu-An and Wang, Ju-Chiang and Yang, Yi-Hsuan and Chen, Homer H.},
  journal     = {IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING},
  title       = {Component Tying for Mixture Model Adaptation in Personalization of Music Emotion Recognition},
  year        = {2017},
  issn        = {2329-9290},
  month       = {JUL},
  number      = {7},
  pages       = {1409-1420},
  volume      = {25},
  bdsk-url-1  = {https://doi.org/10.1109/TASLP.2017.2693565},
  doi         = {10.1109/TASLP.2017.2693565},
  eissn       = {2329-9304},
  hasabstract = {N},
  priority    = {prio1},
  unique-id   = {WOS:000403311100001},
}

@Article{hsu2018af,
  author               = {Hsu, Jia-Lien and Zhen, Yan-Lin and Lin, Tzu-Chieh and Chiu, Yi-Shiuan},
  journal              = {MULTIMEDIA SYSTEMS},
  title                = {Affective content analysis of music emotion through EEG},
  year                 = {2018},
  issn                 = {0942-4962},
  month                = {MAR},
  number               = {2},
  pages                = {195-210},
  volume               = {24},
  bdsk-url-1           = {https://doi.org/10.1007/s00530-017-0542-0},
  doi                  = {10.1007/s00530-017-0542-0},
  eissn                = {1432-1882},
  hasabstract          = {N},
  orcid-numbers        = {Hsu, Jia-Lien/0000-0002-4818-3198},
  priority             = {prio3},
  researcherid-numbers = {Hsu, Jia-Lien/K-7271-2015},
  unique-id            = {WOS:000427153500005},
}

@Article{christensen2014en,
  author               = {Christensen, Julia F. and Gaigg, Sebastian B. and Gomila, Antoni and Oke, Peter and Calvo-Merino, Beatriz},
  journal              = {FRONTIERS IN HUMAN NEUROSCIENCE},
  title                = {Enhancing emotional experiences to dance through music: the role of valence and arousal in the cross-modal bias},
  year                 = {2014},
  issn                 = {1662-5161},
  month                = {OCT 6},
  volume               = {8},
  bdsk-url-1           = {https://doi.org/10.3389/fnhum.2014.00757},
  comment              = {physiological, GSR},
  doi                  = {10.3389/fnhum.2014.00757},
  hasabstract          = {N},
  orcid-numbers        = {Christensen, Julia F./0000-0003-0381-5101 Calvo-Merino, Beatriz/0000-0003-4669-4573 Gaigg, Sebastian/0000-0003-2644-7145},
  priority             = {prio3},
  researcherid-numbers = {Christensen, Julia F./AAK-4477-2021 Gomila, Antoni/I-1342-2012 Calvo-Merino, Beatriz/V-8924-2018},
  unique-id            = {WOS:000342554900001},
}

@Article{yang2023ex,
  author        = {Yang, Simin and Reed, Courtney N. and Chew, Elaine and Barthet, Mathieu},
  journal       = {IEEE TRANSACTIONS ON AFFECTIVE COMPUTING},
  title         = {Examining Emotion Perception Agreement in Live Music Performance},
  year          = {2023},
  issn          = {1949-3045},
  month         = {APR-JUN},
  number        = {2},
  pages         = {1442-1460},
  volume        = {14},
  bdsk-url-1    = {https://doi.org/10.1109/TAFFC.2021.3093787},
  doi           = {10.1109/TAFFC.2021.3093787},
  hasabstract   = {N},
  orcid-numbers = {Barthet, Mathieu/0000-0002-9869-1668 Reed, Courtney Nicole/0000-0003-0893-9277 Chew, Elaine/0000-0002-8342-1024},
  priority      = {prio1},
  unique-id     = {WOS:001000299100044},
}

@Article{park2019ac,
  author               = {Park, Hye Young and Chong, Hyun Ju},
  journal              = {PSYCHOLOGY OF MUSIC},
  title                = {A comparative study of the perception of music emotion between adults with and without visual impairment},
  year                 = {2019},
  issn                 = {0305-7356},
  month                = {MAR},
  number               = {2},
  pages                = {225-240},
  volume               = {47},
  bdsk-url-1           = {https://doi.org/10.1177/0305735617745148},
  doi                  = {10.1177/0305735617745148},
  eissn                = {1741-3087},
  hasabstract          = {N},
  orcid-numbers        = {Chong, Hyun Ju/0000-0001-9788-0320},
  priority             = {prio3},
  researcherid-numbers = {Campailla, Jasmin/AAK-2420-2021},
  unique-id            = {WOS:000460326900006},
}

@Article{jandaghian2023mu,
  author               = {Jandaghian, Maryam and Setayeshi, Saeed and Razzazi, Farbod and Sharifi, Arash},
  journal              = {MULTIMEDIA TOOLS AND APPLICATIONS},
  title                = {Music emotion recognition based on a modified brain emotional learning model},
  year                 = {2023},
  issn                 = {1380-7501},
  month                = {JUL},
  number               = {17},
  pages                = {26037-26061},
  volume               = {82},
  bdsk-url-1           = {https://doi.org/10.1007/s11042-023-14345-w},
  comment              = {neuroscientific},
  doi                  = {10.1007/s11042-023-14345-w},
  earlyaccessdate      = {JAN 2023},
  eissn                = {1573-7721},
  hasabstract          = {N},
  orcid-numbers        = {Setayeshi, Saeed/0000-0002-1415-222X},
  priority             = {prio3},
  researcherid-numbers = {Setayeshi, Saeed/JPY-2228-2023},
  unique-id            = {WOS:000913487000001},
}

@Article{harding2023mu,
  author               = {Harding, Eleanor E. and Gaudrain, Etienne and Hrycyk, Imke J. and Harris, Robert L. and Tillmann, Barbara and Maat, Bert and Free, Rolien H. and Baskent, Deniz},
  journal              = {TRENDS IN HEARING},
  title                = {Musical Emotion Categorization with Vocoders of Varying Temporal and Spectral Content},
  year                 = {2023},
  issn                 = {2331-2165},
  volume               = {27},
  article-number       = {23312165221141142},
  bdsk-url-1           = {https://doi.org/10.1177/23312165221141142},
  comment              = {cochlear implants},
  doi                  = {10.1177/23312165221141142},
  hasabstract          = {N},
  orcid-numbers        = {Baskent, Deniz/0000-0002-6560-1451 Tillmann, Barbara/0000-0001-9676-5822 Harding, Eleanor/0000-0002-3244-9625 Maat, Bert/0000-0001-9856-7010 Gaudrain, Etienne/0000-0003-0490-0295},
  priority             = {prio3},
  researcherid-numbers = {Hrycyk, Imke/HSD-8229-2023 Gaudrain, Etienne/C-6713-2013},
  unique-id            = {WOS:000998856200001},
}

@Article{grekow2018au,
  author               = {Grekow, Jacek},
  journal              = {JOURNAL OF INFORMATION AND TELECOMMUNICATION},
  title                = {Audio features dedicated to the detection and tracking of arousal and valence in musical compositions},
  year                 = {2018},
  issn                 = {2475-1839},
  month                = {JUL 3},
  number               = {3},
  pages                = {322-333},
  volume               = {2},
  bdsk-url-1           = {https://doi.org/10.1080/24751839.2018.1463749},
  doi                  = {10.1080/24751839.2018.1463749},
  eissn                = {2475-1847},
  hasabstract          = {N},
  priority             = {prio1},
  researcherid-numbers = {Grekow, Jacek/M-9500-2015},
  unique-id            = {WOS:000668121800006},
}

@Article{koh2023me,
  author               = {Koh, En Yan and Cheuk, Kin Wai and Heung, Kwan Yee and Agres, Kat R. and Herremans, Dorien},
  journal              = {SENSORS},
  title                = {MERP: A Music Dataset with Emotion Ratings and Raters' Profile Information},
  year                 = {2023},
  month                = {JAN},
  number               = {1},
  volume               = {23},
  article-number       = {382},
  bdsk-url-1           = {https://doi.org/10.3390/s23010382},
  doi                  = {10.3390/s23010382},
  eissn                = {1424-8220},
  hasabstract          = {N},
  modificationdate     = {2024-05-16T05:46:30},
  orcid-numbers        = {Herremans, Dorien/0000-0001-8607-1640 Cheuk, Kin Wai/0000-0003-3213-8242 Agres, Kathleen/0000-0001-7260-2447},
  priority             = {prio1},
  researcherid-numbers = {Herremans, Dorien/G-9599-2018 Cheuk, Kin Wai/HZJ-8015-2023},
  unique-id            = {WOS:000908918500001},
}

@Article{bo2019mu,
  author               = {Bo, Hongjian and Ma, Lin and Liu, Quansheng and Xu, Ruifeng and Li, Haifeng},
  journal              = {INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS},
  title                = {Music-evoked emotion recognition based on cognitive principles inspired EEG temporal and spectral features},
  year                 = {2019},
  issn                 = {1868-8071},
  month                = {SEP},
  number               = {9},
  pages                = {2439-2448},
  volume               = {10},
  bdsk-url-1           = {https://doi.org/10.1007/s13042-018-0880-z},
  comment              = {eeg},
  doi                  = {10.1007/s13042-018-0880-z},
  eissn                = {1868-808X},
  hasabstract          = {N},
  modificationdate     = {2024-05-16T05:47:20},
  orcid-numbers        = {Bo, Hongjian/0000-0002-2850-6199},
  priority             = {prio3},
  researcherid-numbers = {xu, rui/GRX-5734-2022 Bo, Hongjian/HRC-9086-2023},
  unique-id            = {WOS:000481418600015},
}

@Article{cunningham2021su,
  author               = {Cunningham, Stuart and Ridley, Harrison and Weinel, Jonathan and Picking, Richard},
  journal              = {PERSONAL AND UBIQUITOUS COMPUTING},
  title                = {Supervised machine learning for audio emotion recognition Enhancing film sound design using audio features, regression models and artificial neural networks},
  year                 = {2021},
  issn                 = {1617-4909},
  month                = {AUG},
  number               = {4},
  pages                = {637-650},
  volume               = {25},
  bdsk-url-1           = {https://doi.org/10.1007/s00779-020-01389-0},
  doi                  = {10.1007/s00779-020-01389-0},
  earlyaccessdate      = {APR 2020},
  eissn                = {1617-4917},
  hasabstract          = {N},
  orcid-numbers        = {Cunningham, Stuart/0000-0002-5348-7700 Weinel, Jonathan/0000-0001-5347-3897},
  priority             = {prio1},
  researcherid-numbers = {Cunningham, Stuart/HSH-5303-2023},
  unique-id            = {WOS:000528157400001},
}

@Article{xu2021us,
  author         = {Xu, Liang and Sun, Zaoyi and Wen, Xin and Huang, Zhengxi and Chao, Chi-ju and Xu, Liuchang},
  journal        = {PEERJ COMPUTER SCIENCE},
  title          = {Using machine learning analysis to interpret the relationship between music emotion and lyric features},
  year           = {2021},
  month          = {NOV 15},
  volume         = {7},
  article-number = {e785},
  bdsk-url-1     = {https://doi.org/10.7717/peerj-cs.785},
  doi            = {10.7717/peerj-cs.785},
  eissn          = {2376-5992},
  hasabstract    = {N},
  orcid-numbers  = {Xu, Liang/0000-0003-3889-927X Sun, Zaoyi/0000-0002-4551-3606},
  priority       = {prio1},
  unique-id      = {WOS:000720396400001},
}

@Article{kragness2021ev,
  author               = {Kragness, Haley E. and Eitel, Matthew J. and Baksh, Ammaarah M. and Trainor, Laurel J.},
  journal              = {DEVELOPMENTAL SCIENCE},
  title                = {Evidence for early arousal-based differentiation of emotions in children's musical production},
  year                 = {2021},
  issn                 = {1363-755X},
  month                = {JAN},
  number               = {1},
  volume               = {24},
  article-number       = {e12982},
  bdsk-url-1           = {https://doi.org/10.1111/desc.12982},
  comment              = {developmental},
  doi                  = {10.1111/desc.12982},
  earlyaccessdate      = {MAY 2020},
  eissn                = {1467-7687},
  hasabstract          = {N},
  modificationdate     = {2024-05-16T05:48:21},
  orcid-numbers        = {Trainor, Laurel/0000-0003-3397-2079 Kragness, Haley/0000-0003-3137-7655},
  priority             = {prio3},
  researcherid-numbers = {Eitel, Matthew/IAQ-1306-2023 Eitel, Matthew/JOK-2437-2023},
  unique-id            = {WOS:000534965200001},
}

@Article{sulun2022sy,
  author               = {Sulun, Serkan and Davies, Matthew E. P. and Viana, Paula},
  journal              = {IEEE ACCESS},
  title                = {Symbolic Music Generation Conditioned on Continuous-Valued Emotions},
  year                 = {2022},
  issn                 = {2169-3536},
  pages                = {44617-44626},
  volume               = {10},
  bdsk-url-1           = {https://doi.org/10.1109/ACCESS.2022.3169744},
  comment              = {No relevant task, discussed},
  doi                  = {10.1109/ACCESS.2022.3169744},
  hasabstract          = {N},
  modificationdate     = {2024-05-16T12:11:13},
  orcid-numbers        = {Viana, Paula/0000-0001-8447-2360 Davies, Matthew/0000-0002-1315-3992 Sulun, Serkan/0000-0003-1365-1884},
  priority             = {prio3},
  researcherid-numbers = {Davies, Matthew/K-1701-2014},
  unique-id            = {WOS:000790769400001},
}

@Article{wang2015mo,
  author               = {Wang, Ju-Chiang and Yang, Yi-Hsuan and Wang, Hsin-Min and Jeng, Shyh-Kang},
  journal              = {IEEE TRANSACTIONS ON AFFECTIVE COMPUTING},
  title                = {Modeling the Affective Content of Music with a Gaussian Mixture Model},
  year                 = {2015},
  issn                 = {1949-3045},
  month                = {JAN-MAR},
  number               = {1},
  pages                = {56-68},
  volume               = {6},
  bdsk-url-1           = {https://doi.org/10.1109/TAFFC.2015.2397457},
  doi                  = {10.1109/TAFFC.2015.2397457},
  hasabstract          = {N},
  orcid-numbers        = {Wang, Hsin-Min/0000-0003-3599-5071},
  priority             = {prio1},
  researcherid-numbers = {Wang, Hsin-Min/ABA-8747-2020},
  unique-id            = {WOS:000350741300005},
}

@Article{ghafoor2023te,
  author           = {Ghafoor, Yusra and Jinping, Shi and Calderon, Fernando H. and Huang, Yen-Hao and Chen, Kuan-Ta and Chen, Yi-Shin},
  journal          = {APPLIED INTELLIGENCE},
  title            = {TERMS: textual emotion recognition in multidimensional space},
  year             = {2023},
  issn             = {0924-669X},
  month            = {FEB},
  number           = {3},
  pages            = {2673-2693},
  volume           = {53},
  bdsk-url-1       = {https://doi.org/10.1007/s10489-022-03567-4},
  comment          = {no music},
  doi              = {10.1007/s10489-022-03567-4},
  earlyaccessdate  = {MAY 2022},
  eissn            = {1573-7497},
  hasabstract      = {N},
  modificationdate = {2024-05-16T05:50:11},
  orcid-numbers    = {Huang, Yen-Hao/0000-0003-2103-7235},
  priority         = {prio3},
  unique-id        = {WOS:000793639100005},
}

@Article{nineuil2020th,
  author           = {Nineuil, Clemence and Dellacherie, Delphine and Samson, Severine},
  journal          = {FRONTIERS IN PSYCHOLOGY},
  title            = {The Impact of Emotion on Musical Long-Term Memory},
  year             = {2020},
  issn             = {1664-1078},
  month            = {AUG 31},
  volume           = {11},
  article-number   = {2110},
  bdsk-url-1       = {https://doi.org/10.3389/fpsyg.2020.02110},
  comment          = {emotion as IV},
  doi              = {10.3389/fpsyg.2020.02110},
  hasabstract      = {N},
  modificationdate = {2024-05-16T05:51:25},
  orcid-numbers    = {Nineuil, Clemence/0000-0002-6140-3861},
  priority         = {prio3},
  unique-id        = {WOS:000570566100001},
}

@Article{goshvarpour2016an,
  author               = {Goshvarpour, Atefeh and Abbasi, Ataollah and Goshvarpour, Ateke and Daneshvar, Sabalan},
  journal              = {BIOMEDICAL ENGINEERING-APPLICATIONS BASIS COMMUNICATIONS},
  title                = {A NOVEL SIGNAL-BASED FUSION APPROACH FOR ACCURATE MUSIC EMOTION RECOGNITION},
  year                 = {2016},
  issn                 = {1016-2372},
  month                = {DEC},
  number               = {6},
  volume               = {28},
  article-number       = {1650040},
  bdsk-url-1           = {https://doi.org/10.4015/S101623721650040X},
  comment              = {ECG, GSR},
  doi                  = {10.4015/S101623721650040X},
  eissn                = {1793-7132},
  hasabstract          = {N},
  modificationdate     = {2024-05-16T05:52:09},
  orcid-numbers        = {Goshvarpour, Ateke/0000-0002-5185-5645 Goshvarpour, Atefeh/0000-0002-0343-4344 Goshvarpour, Ateke/0000-0002-5185-5645 Danishvar, Sebelan/0000-0002-8258-0437},
  priority             = {prio3},
  researcherid-numbers = {Danishvar, Sebelan/AAA-1884-2020 Goshvarpour, Ateke/IZD-8225-2023 Goshvarpour, Atefeh/AAC-3145-2020 Goshvarpour, Ateke/AAB-5275-2019 Danishvar, Sebelan/L-8995-2017},
  unique-id            = {WOS:000390886800002},
}

@Article{fuentessanchez2023in,
  author               = {Fuentes-Sanchez, Nieves and Carmen Pastor, M. and Eerola, Tuomas and Pastor, Raul},
  journal              = {MUSICAE SCIENTIAE},
  title                = {Individual differences in music reward sensitivity influence the perception of emotions represented by music},
  year                 = {2023},
  issn                 = {1029-8649},
  month                = {JUN},
  number               = {2},
  pages                = {313-331},
  volume               = {27},
  article-number       = {10298649211060028},
  bdsk-url-1           = {https://doi.org/10.1177/10298649211060028},
  comment              = {no audio prediction, discussed},
  doi                  = {10.1177/10298649211060028},
  earlyaccessdate      = {DEC 2021},
  eissn                = {2045-4147},
  hasabstract          = {N},
  modificationdate     = {2024-05-16T11:49:49},
  orcid-numbers        = {Fuentes S{\'a}nchez, Nieves/0000-0002-3508-3358 Eerola, Tuomas/0000-0002-2896-929X Pastor, M. Carmen/0000-0001-9909-3646},
  priority             = {prio3},
  researcherid-numbers = {Medall, Ra{\'u}l Pastor/L-2813-2017 Fuentes S{\'a}nchez, Nieves/AAE-2492-2022 Eerola, Tuomas/K-7596-2019 Pastor, M. Carmen/N-1692-2019},
  unique-id            = {WOS:000730995100001},
}

@Article{fuentessanchez2021sp,
  author               = {Fuentes-Sanchez, Nieves and Pastor, Raul and Eerola, Tuomas and Pastor, M. Carmen},
  journal              = {PSYCHOLOGY OF MUSIC},
  title                = {Spanish adaptation of a film music stimulus set (FMSS): Cultural and gender differences in the perception of emotions prompted by music excerpts},
  year                 = {2021},
  issn                 = {0305-7356},
  month                = {SEP},
  number               = {5},
  pages                = {1242-1260},
  volume               = {49},
  article-number       = {0305735620958464},
  bdsk-url-1           = {https://doi.org/10.1177/0305735620958464},
  comment              = {no audio prediction},
  doi                  = {10.1177/0305735620958464},
  earlyaccessdate      = {OCT 2020},
  eissn                = {1741-3087},
  hasabstract          = {N},
  orcid-numbers        = {Eerola, Tuomas/0000-0002-2896-929X Fuentes S{\'a}nchez, Nieves/0000-0002-3508-3358 Pastor, M. Carmen/0000-0001-9909-3646},
  priority             = {prio3},
  researcherid-numbers = {Eerola, Tuomas/K-7596-2019 Medall, Ra{\'u}l Pastor/L-2813-2017 Fuentes S{\'a}nchez, Nieves/AAE-2492-2022 Pastor, M. Carmen/N-1692-2019},
  unique-id            = {WOS:000582031400001},
}

@Article{hasanzadeh2021co,
  author          = {Hasanzadeh, Fatemeh and Annabestani, Mohsen and Moghimi, Sahar},
  journal         = {APPLIED SOFT COMPUTING},
  title           = {Continuous emotion recognition during music listening using EEG signals: A fuzzy parallel cascades model},
  year            = {2021},
  issn            = {1568-4946},
  month           = {MAR},
  volume          = {101},
  article-number  = {107028},
  bdsk-url-1      = {https://doi.org/10.1016/j.asoc.2020.107028},
  doi             = {10.1016/j.asoc.2020.107028},
  earlyaccessdate = {JAN 2021},
  eissn           = {1872-9681},
  hasabstract     = {N},
  priority        = {prio3},
  unique-id       = {WOS:000621420300005},
}

@Article{alonso2015em,
  author               = {Alonso, Irene and Dellacherie, Delphine and Samson, Severine},
  journal              = {FRONTIERS IN AGING NEUROSCIENCE},
  title                = {Emotional memory for musical excerpts in young and older adults},
  year                 = {2015},
  issn                 = {1663-4365},
  month                = {MAR 12},
  volume               = {7},
  article-number       = {23},
  bdsk-url-1           = {https://doi.org/10.3389/fnagi.2015.00023},
  comment              = {emotion as IV},
  doi                  = {10.3389/fnagi.2015.00023},
  hasabstract          = {N},
  modificationdate     = {2024-05-16T05:54:02},
  orcid-numbers        = {Samson, S{\'e}verine/0000-0002-4507-9440 Alonso, Irene/0000-0002-7460-7708},
  priority             = {prio3},
  researcherid-numbers = {Samson, S{\'e}verine/P-7417-2019},
  unique-id            = {WOS:000352353000002},
}

@Article{jiang2016th,
  author               = {Jiang, Jun and Rickson, Daphne and Jiang, Cunmei},
  journal              = {ARTS IN PSYCHOTHERAPY},
  title                = {The mechanism of music for reducing psychological stress: Music preference as a mediator},
  year                 = {2016},
  issn                 = {0197-4556},
  month                = {APR},
  pages                = {62-68},
  volume               = {48},
  bdsk-url-1           = {https://doi.org/10.1016/j.aip.2016.02.002},
  comment              = {emotion as IV},
  doi                  = {10.1016/j.aip.2016.02.002},
  eissn                = {1873-5878},
  hasabstract          = {N},
  modificationdate     = {2024-05-16T05:55:14},
  orcid-numbers        = {Jiang, Jun/0000-0002-0063-3225 Jiang, Cunmei/0000-0002-0264-5924},
  priority             = {prio3},
  researcherid-numbers = {Jiang, Cunmei/IUM-3917-2023 Jiang, Jun/AAQ-1216-2021 Jiang, Jan/ABF-1730-2020},
  unique-id            = {WOS:000377627600009},
}

@Article{greco2017ar,
  author               = {Greco, Alberto and Valenza, Gaetano and Citi, Luca and Scilingo, Enzo Pasquale},
  journal              = {IEEE SENSORS JOURNAL},
  title                = {Arousal and Valence Recognition of Affective Sounds Based on Electrodermal Activity},
  year                 = {2017},
  issn                 = {1530-437X},
  month                = {FEB 1},
  number               = {3},
  pages                = {716-725},
  volume               = {17},
  bdsk-url-1           = {https://doi.org/10.1109/JSEN.2016.2623677},
  doi                  = {10.1109/JSEN.2016.2623677},
  eissn                = {1558-1748},
  hasabstract          = {N},
  orcid-numbers        = {Citi, Luca/0000-0001-8702-5654 Greco, Alberto/0000-0002-4822-5562 Valenza, Gaetano/0000-0001-6574-1879},
  priority             = {prio3},
  researcherid-numbers = {Citi, Luca/AAE-3192-2022},
  unique-id            = {WOS:000393998700022},
}

@Article{ambertdahan2015ju,
  author               = {Ambert-Dahan, Emmanuele and Giraud, Anne-Lise and Sterkers, Olivier and Samson, Severine},
  journal              = {FRONTIERS IN PSYCHOLOGY},
  title                = {Judgment of musical emotions after cochlear implantation in adults with progressive deafness},
  year                 = {2015},
  issn                 = {1664-1078},
  month                = {MAR 12},
  volume               = {6},
  article-number       = {181},
  bdsk-url-1           = {https://doi.org/10.3389/fpsyg.2015.00181},
  doi                  = {10.3389/fpsyg.2015.00181},
  hasabstract          = {N},
  orcid-numbers        = {Samson, S{\'e}verine/0000-0002-4507-9440},
  priority             = {prio3},
  researcherid-numbers = {Samson, S{\'e}verine/P-7417-2019},
  unique-id            = {WOS:000350793600001},
}

@Article{pearce2015ag,
  author               = {Pearce, Marcus T. and Halpern, Andrea R.},
  journal              = {PSYCHOLOGY OF AESTHETICS CREATIVITY AND THE ARTS},
  title                = {Age-Related Patterns in Emotions Evoked by Music},
  year                 = {2015},
  issn                 = {1931-3896},
  month                = {AUG},
  number               = {3},
  pages                = {248-253},
  volume               = {9},
  bdsk-url-1           = {https://doi.org/10.1037/a0039279},
  comment              = {age differences},
  doi                  = {10.1037/a0039279},
  eissn                = {1931-390X},
  hasabstract          = {N},
  modificationdate     = {2024-05-16T05:56:18},
  orcid-numbers        = {Pearce, Marcus/0000-0002-1282-431X Halpern, Andrea/0000-0001-6384-6490},
  priority             = {prio3},
  researcherid-numbers = {Pearce, Marcus/KHW-6509-2024 Halpern, Andrea/B-7099-2009},
  unique-id            = {WOS:000359379300007},
}

@Article{cohrdes2020th,
  author               = {Cohrdes, Caroline and Wrzus, Cornelia and Wald-Fuhrmann, Melanie and Riediger, Michaela},
  journal              = {MUSICAE SCIENTIAE},
  title                = {``The sound of affect{''}: Age differences in perceiving valence and arousal in music and their relation to music characteristics and momentary mood},
  year                 = {2020},
  issn                 = {1029-8649},
  month                = {MAR},
  number               = {1},
  pages                = {21-43},
  volume               = {24},
  bdsk-url-1           = {https://doi.org/10.1177/1029864918765613},
  doi                  = {10.1177/1029864918765613},
  eissn                = {2045-4147},
  hasabstract          = {N},
  orcid-numbers        = {Riediger, Michaela/0000-0001-5206-3626 Cohrdes, Caroline/0000-0003-0063-4145 Wald-Fuhrmann, Melanie/0000-0002-3659-4731},
  priority             = {prio3},
  researcherid-numbers = {Riediger, Michaela/ABC-3345-2021 Cohrdes, Caroline/ABL-5334-2022 Wald-Fuhrmann, Melanie/HSB-9699-2023},
  unique-id            = {WOS:000523163500002},
}

@Article{zhang2016br,
  author        = {Zhang, JiangLong and Huang, XiangLin and Yang, Lifang and Nie, Liqiang},
  journal       = {NEUROCOMPUTING},
  title         = {Bridge the semantic gap between pop music acoustic feature and emotion: Build an interpretable model},
  year          = {2016},
  issn          = {0925-2312},
  month         = {OCT 5},
  number        = {SI},
  pages         = {333-341},
  volume        = {208},
  bdsk-url-1    = {https://doi.org/10.1016/j.neucom.2016.01.099},
  doi           = {10.1016/j.neucom.2016.01.099},
  eissn         = {1872-8286},
  hasabstract   = {N},
  orcid-numbers = {Zhang, Jianglong/0000-0002-9079-2499},
  priority      = {prio1},
  unique-id     = {WOS:000382794300034},
}

@Article{nawaz2020co,
  author               = {Nawaz, Rab and Cheah, Kit Hwa and Nisar, Humaira and Yap, Vooi Voon},
  journal              = {BIOCYBERNETICS AND BIOMEDICAL ENGINEERING},
  title                = {Comparison of different feature extraction methods for EEG-based emotion recognition},
  year                 = {2020},
  issn                 = {0208-5216},
  month                = {JUL-SEP},
  number               = {3},
  pages                = {910-926},
  volume               = {40},
  bdsk-url-1           = {https://doi.org/10.1016/j.bbe.2020.04.005},
  doi                  = {10.1016/j.bbe.2020.04.005},
  hasabstract          = {N},
  orcid-numbers        = {Nawaz, Rab/0000-0003-1120-0844 Nawaz, Rab/0000-0002-7622-9632 Nisar, Humaira/0000-0003-2026-5666},
  priority             = {prio3},
  researcherid-numbers = {Nawaz, Rab/IUP-3023-2023 Nawaz, Rab/AAF-8789-2019 Nisar, Humaira/A-5188-2009 Nawaz, Rab/HSF-5318-2023},
  unique-id            = {WOS:000580657600003},
}

@Article{balasubramanian2018mu,
  author               = {Balasubramanian, Geethanjali and Kanagasabai, Adalarasu and Mohan, Jagannath and Seshadri, N. P. Guhan},
  journal              = {BIOMEDICAL SIGNAL PROCESSING AND CONTROL},
  title                = {Music induced emotion using wavelet packet decomposition-An EEG study},
  year                 = {2018},
  issn                 = {1746-8094},
  month                = {APR},
  pages                = {115-128},
  volume               = {42},
  bdsk-url-1           = {https://doi.org/10.1016/j.bspc.2018.01.015},
  doi                  = {10.1016/j.bspc.2018.01.015},
  eissn                = {1746-8108},
  hasabstract          = {N},
  orcid-numbers        = {Adalarasu, Kanagasabai/0000-0003-1247-5743 N P, Guhan Seshadri/0000-0002-7506-9104 Mohan, Jagannath/0000-0001-8953-118X},
  priority             = {prio3},
  researcherid-numbers = {B, Geethanjali/HNJ-2140-2023 Jagannath, M./X-9706-2018 Adalarasu, Kanagasabai/AIB-1061-2022 N P, Guhan Seshadri/AAU-5698-2020},
  unique-id            = {WOS:000429390700013},
}

@Article{wang2022cr,
  author               = {Wang, Xin and Wei, Yujia and Yang, Dasheng},
  journal              = {COGNITIVE COMPUTATION AND SYSTEMS},
  title                = {Cross-cultural analysis of the correlation between musical elements and emotion},
  year                 = {2022},
  month                = {JUN},
  number               = {2, SI},
  pages                = {116-129},
  volume               = {4},
  bdsk-url-1           = {https://doi.org/10.1049/ccs2.12032},
  doi                  = {10.1049/ccs2.12032},
  eissn                = {2517-7567},
  hasabstract          = {N},
  priority             = {prio1},
  researcherid-numbers = {Wei, Yujia/IAQ-8917-2023},
  unique-id            = {WOS:001062678000004},
}

@Article{marimpis2020am,
  author               = {Marimpis, Avraam D. and Dimitriadis, Stavros I. and Goebel, Rainer},
  journal              = {IEEE ACCESS},
  title                = {A Multiplex Connectivity Map of Valence-Arousal Emotional Model},
  year                 = {2020},
  issn                 = {2169-3536},
  pages                = {170928-170938},
  volume               = {8},
  bdsk-url-1           = {https://doi.org/10.1109/ACCESS.2020.3025370},
  doi                  = {10.1109/ACCESS.2020.3025370},
  hasabstract          = {N},
  orcid-numbers        = {DIMITRIADIS, STAVROS I/0000-0002-0000-5392 Goebel, Rainer/0000-0003-1780-2467 Marimpis, Avraam/0000-0003-1551-9940},
  priority             = {prio1},
  researcherid-numbers = {DIMITRIADIS, STAVROS I/AAN-8992-2020 Goebel, Rainer/JXX-5301-2024},
  unique-id            = {WOS:000573786200001},
}

@Article{ayata2018em,
  author               = {Ayata, Deger and Yaslan, Yusuf and Kamasak, Mustafa E.},
  journal              = {IEEE TRANSACTIONS ON CONSUMER ELECTRONICS},
  title                = {Emotion Based Music Recommendation System Using Wearable Physiological Sensors},
  year                 = {2018},
  issn                 = {0098-3063},
  month                = {MAY},
  number               = {2},
  pages                = {196-203},
  volume               = {64},
  bdsk-url-1           = {https://doi.org/10.1109/TCE.2018.2844736},
  doi                  = {10.1109/TCE.2018.2844736},
  eissn                = {1558-4127},
  hasabstract          = {N},
  orcid-numbers        = {, kamasak/0000-0002-5050-3357 Yaslan, Yusuf/0000-0001-8038-948X},
  priority             = {prio3},
  researcherid-numbers = {, kamasak/N-9461-2013 Yaslan, Yusuf/ABA-8190-2020},
  unique-id            = {WOS:000438123000007},
}

@Article{zhang2019us,
  author               = {Zhang, Le-kai and Sun, Shou-qian and Xing, Bai-xi and Luo, Rui-ming and Zhang, Ke-jun},
  journal              = {FRONTIERS OF INFORMATION TECHNOLOGY \& ELECTRONIC ENGINEERING},
  title                = {Using psychophysiological measures to recognize personal music emotional experience},
  year                 = {2019},
  issn                 = {2095-9184},
  month                = {JUL},
  number               = {7},
  pages                = {964-974},
  volume               = {20},
  bdsk-url-1           = {https://doi.org/10.1631/FITEE.1800101},
  doi                  = {10.1631/FITEE.1800101},
  eissn                = {2095-9230},
  hasabstract          = {N},
  priority             = {prio1},
  researcherid-numbers = {zhang, ke/AAH-8217-2019},
  unique-id            = {WOS:000480501000006},
}

@Article{egermann2015mu,
  author               = {Egermann, Hauke and Fernando, Nathalie and Chuen, Lorraine and McAdams, Stephen},
  journal              = {FRONTIERS IN PSYCHOLOGY},
  title                = {Music induces universal emotion-related psychophysiological responses: comparing Canadian listeners to Congolese Pygmies},
  year                 = {2015},
  issn                 = {1664-1078},
  month                = {JAN 7},
  volume               = {5},
  article-number       = {1341},
  bdsk-url-1           = {https://doi.org/10.3389/fpsyg.2014.01341},
  comment              = {physiological, facial},
  doi                  = {10.3389/fpsyg.2014.01341},
  hasabstract          = {N},
  modificationdate     = {2024-05-16T05:57:19},
  orcid-numbers        = {McAdams, Stephen/0000-0002-6744-9035},
  priority             = {prio3},
  researcherid-numbers = {McAdams, Stephen/GQB-0225-2022},
  unique-id            = {WOS:000348125600001},
}

@Article{liu2022en,
  author               = {Liu, Jun and Sun, Lechan and Liu, Jun and Huang, Min and Xu, Yichen and Li, Rihui},
  journal              = {FRONTIERS IN NEUROSCIENCE},
  title                = {Enhancing Emotion Recognition Using Region-Specific Electroencephalogram Data and Dynamic Functional Connectivity},
  year                 = {2022},
  month                = {MAY 2},
  volume               = {16},
  article-number       = {884475},
  bdsk-url-1           = {https://doi.org/10.3389/fnins.2022.884475},
  doi                  = {10.3389/fnins.2022.884475},
  eissn                = {1662-453X},
  hasabstract          = {N},
  orcid-numbers        = {Li, Rihui/0000-0001-8006-7333},
  priority             = {prio3},
  researcherid-numbers = {Xu, Yi Chen/HII-8724-2022 Li, Rihui/AGH-1020-2022},
  unique-id            = {WOS:000834830700001},
}

@Article{grekow2021mu,
  author               = {Grekow, Jacek},
  journal              = {JOURNAL OF INTELLIGENT INFORMATION SYSTEMS},
  title                = {Music emotion recognition using recurrent neural networks and pretrained models},
  year                 = {2021},
  issn                 = {0925-9902},
  month                = {DEC},
  number               = {3},
  pages                = {531-546},
  volume               = {57},
  bdsk-url-1           = {https://doi.org/10.1007/s10844-021-00658-5},
  doi                  = {10.1007/s10844-021-00658-5},
  earlyaccessdate      = {AUG 2021},
  eissn                = {1573-7675},
  hasabstract          = {N},
  orcid-numbers        = {Grekow, Jacek/0000-0003-2094-0107},
  priority             = {prio1},
  researcherid-numbers = {Grekow, Jacek/M-9500-2015},
  unique-id            = {WOS:000682651100001},
}

@Article{cespedesguevara2018mu,
  author               = {Cespedes-Guevara, Julian and Eerola, Tuomas},
  journal              = {FRONTIERS IN PSYCHOLOGY},
  title                = {Music Communicates Affects, Not Basic Emotions - A Constructionist Account of Attribution of Emotional Meanings to Music},
  year                 = {2018},
  issn                 = {1664-1078},
  month                = {FEB 28},
  volume               = {9},
  article-number       = {215},
  bdsk-url-1           = {https://doi.org/10.3389/fpsyg.2018.00215},
  doi                  = {10.3389/fpsyg.2018.00215},
  hasabstract          = {N},
  orcid-numbers        = {Eerola, Tuomas/0000-0002-2896-929X Cespedes Guevara, Julian/0000-0002-8816-3650},
  priority             = {prio3},
  researcherid-numbers = {Eerola, Tuomas/K-7596-2019},
  unique-id            = {WOS:000426262300001},
}

@Article{pesek2017th,
  author               = {Pesek, Matevz and Strle, Gregor and Kavcic, Alenka and Marolt, Matija},
  journal              = {JOURNAL OF NEW MUSIC RESEARCH},
  title                = {The Moodo dataset: Integrating user context with emotional and color perception of music for affective music information retrieval},
  year                 = {2017},
  issn                 = {0929-8215},
  number               = {3},
  pages                = {246-260},
  volume               = {46},
  bdsk-url-1           = {https://doi.org/10.1080/09298215.2017.1333518},
  comment              = {Insufficient reporting, no outcome measures, discussed},
  doi                  = {10.1080/09298215.2017.1333518},
  eissn                = {1744-5027},
  hasabstract          = {N},
  modificationdate     = {2024-05-16T12:15:49},
  orcid-numbers        = {Strle, Gregor/0000-0003-1076-6707 Strle, Gregor/0000-0003-1076-6707 Pesek, Matevz/0000-0001-9101-0471},
  priority             = {prio3},
  researcherid-numbers = {Strle, Gregor/O-7525-2019 Strle, Gregor/C-7255-2015},
  unique-id            = {WOS:000410880300003},
}

@Article{cao2023th,
  author           = {Cao, Yujing and Park, Jinwan},
  journal          = {IEEE ACCESS},
  title            = {The Analysis of Music Emotion and Visualization Fusing Long Short-Term Memory Networks Under the Internet of Things},
  year             = {2023},
  issn             = {2169-3536},
  pages            = {141192-141204},
  volume           = {11},
  bdsk-url-1       = {https://doi.org/10.1109/ACCESS.2023.3341926},
  doi              = {10.1109/ACCESS.2023.3341926},
  hasabstract      = {N},
  modificationdate = {2024-05-16T05:58:48},
  priority         = {prio1},
  unique-id        = {WOS:001127418600001},
}

@Article{goshvarpour2020th,
  author               = {Goshvarpour, Atefeh and Goshvarpour, Ateke},
  journal              = {PHYSICAL AND ENGINEERING SCIENCES IN MEDICINE},
  title                = {The potential of photoplethysmogram and galvanic skin response in emotion recognition using nonlinear features},
  year                 = {2020},
  issn                 = {2662-4729},
  month                = {MAR},
  number               = {1},
  pages                = {119-134},
  volume               = {43},
  bdsk-url-1           = {https://doi.org/10.1007/s13246-019-00825-7},
  doi                  = {10.1007/s13246-019-00825-7},
  earlyaccessdate      = {NOV 2019},
  eissn                = {2662-4737},
  hasabstract          = {N},
  orcid-numbers        = {Goshvarpour, Atefeh/0000-0002-0343-4344 Goshvarpour, Ateke/0000-0002-5185-5645 Goshvarpour, Ateke/0000-0002-5185-5645},
  priority             = {prio3},
  researcherid-numbers = {Goshvarpour, Atefeh/AAC-3145-2020 Goshvarpour, Ateke/IZD-8225-2023 Goshvarpour, Ateke/AAB-5275-2019},
  unique-id            = {WOS:000498942300001},
}

@Article{ramirezmelendez2022mu,
  author               = {Ramirez-Melendez, Rafael and Matamoros, Elisabet and Hernandez, Davinia and Mirabel, Julia and Sanchez, Elisabet and Escude, Nuria},
  journal              = {BRAIN SCIENCES},
  title                = {Music-Enhanced Emotion Identification of Facial Emotions in Autistic Spectrum Disorder Children: A Pilot EEG Study},
  year                 = {2022},
  month                = {JUN},
  number               = {6},
  volume               = {12},
  article-number       = {704},
  bdsk-url-1           = {https://doi.org/10.3390/brainsci12060704},
  doi                  = {10.3390/brainsci12060704},
  eissn                = {2076-3425},
  hasabstract          = {N},
  orcid-numbers        = {Hern{\'a}ndez-Leo, Davinia/0000-0003-0548-7455},
  priority             = {prio3},
  researcherid-numbers = {Hern{\'a}ndez-Leo, Davinia/C-2929-2011 Ramirez-Melendez, Rafael/C-9827-2014},
  unique-id            = {WOS:000816333900001},
}

@Article{hu2022de,
  author               = {Hu, Xiao and Li, Fanjie and Liu, Ruilun},
  journal              = {APPLIED SCIENCES-BASEL},
  title                = {Detecting Music-Induced Emotion Based on Acoustic Analysis and Physiological Sensing: A Multimodal Approach},
  year                 = {2022},
  month                = {SEP},
  number               = {18},
  volume               = {12},
  article-number       = {9354},
  bdsk-url-1           = {https://doi.org/10.3390/app12189354},
  comment              = {includes audio-only model},
  doi                  = {10.3390/app12189354},
  eissn                = {2076-3417},
  hasabstract          = {N},
  modificationdate     = {2024-05-16T06:00:23},
  orcid-numbers        = {Hu, Xiao/0000-0003-3994-0385 Li, Fanjie/0000-0001-7016-6354},
  priority             = {prio1},
  researcherid-numbers = {Hu, Xiao/AAD-8405-2020},
  unique-id            = {WOS:000857371900001},
}

@Article{goshvarpour2017an,
  author               = {Goshvarpour, Atefeh and Abbasi, Ataollah and Goshvarpour, Ateke},
  journal              = {BIOMEDICAL JOURNAL},
  title                = {An accurate emotion recognition system using ECG and GSR signals and matching pursuit method},
  year                 = {2017},
  issn                 = {2319-4170},
  month                = {DEC},
  number               = {6},
  pages                = {355-368},
  volume               = {40},
  bdsk-url-1           = {https://doi.org/10.1016/j.bj.2017.11.001},
  doi                  = {10.1016/j.bj.2017.11.001},
  eissn                = {2320-2890},
  hasabstract          = {N},
  orcid-numbers        = {Goshvarpour, Ateke/0000-0002-5185-5645 Goshvarpour, Atefeh/0000-0002-0343-4344 Goshvarpour, Ateke/0000-0002-5185-5645},
  priority             = {prio3},
  researcherid-numbers = {Goshvarpour, Ateke/IZD-8225-2023 Goshvarpour, Atefeh/AAC-3145-2020 Goshvarpour, Ateke/AAB-5275-2019},
  unique-id            = {WOS:000425244500008},
}

@Article{kopec2014th,
  author      = {Kopec, Justin and Hillier, Ashleigh and Frye, Alice},
  journal     = {MUSIC PERCEPTION},
  title       = {THE VALENCY OF MUSIC HAS DIFFERENT EFFECTS ON THE EMOTIONAL RESPONSES OF THOSE WITH AUTISM SPECTRUM DISORDERS AND A COMPARISON GROUP},
  year        = {2014},
  issn        = {0730-7829},
  month       = {JUN},
  number      = {5},
  pages       = {436-443},
  volume      = {31},
  bdsk-url-1  = {https://doi.org/10.1525/MP.2014.31.5.436},
  doi         = {10.1525/MP.2014.31.5.436},
  hasabstract = {N},
  priority    = {prio3},
  unique-id   = {WOS:000337212900003},
}

@Article{dibben2018do,
  author               = {Dibben, Nicola and Coutinho, Eduardo and Vilar, Jose A. and Estevez-Perez, Graciela},
  journal              = {FRONTIERS IN BEHAVIORAL NEUROSCIENCE},
  title                = {Do Individual Differences Influence Moment-by-Moment Reports of Emotion Perceived in Music and Speech Prosody?},
  year                 = {2018},
  issn                 = {1662-5153},
  month                = {AUG 27},
  volume               = {12},
  article-number       = {184},
  bdsk-url-1           = {https://doi.org/10.3389/fnbeh.2018.00184},
  comment              = {acoustic modeling reported elsewhere, discussed},
  doi                  = {10.3389/fnbeh.2018.00184},
  hasabstract          = {N},
  modificationdate     = {2024-05-16T11:23:44},
  orcid-numbers        = {Coutinho, Eduardo/0000-0001-5234-1497 Estevez Perez, Graciela/0000-0002-6021-5236 Dibben, Nicola/0000-0002-9250-5035 Vilar, Jose/0000-0001-5494-171X},
  priority             = {prio3},
  researcherid-numbers = {Coutinho, Eduardo/K-1391-2019 Vilar, Jose/B-9729-2015},
  unique-id            = {WOS:000442758800001},
}

@Article{greene2014di,
  author               = {Greene, Ciara M. and Flannery, Oliver and Soto, David},
  journal              = {COGNITIVE AFFECTIVE \& BEHAVIORAL NEUROSCIENCE},
  title                = {Distinct parietal sites mediate the influences of mood, arousal, and their interaction on human recognition memory},
  year                 = {2014},
  issn                 = {1530-7026},
  month                = {DEC},
  number               = {4},
  pages                = {1327-1339},
  volume               = {14},
  bdsk-url-1           = {https://doi.org/10.3758/s13415-014-0266-y},
  doi                  = {10.3758/s13415-014-0266-y},
  eissn                = {1531-135X},
  hasabstract          = {N},
  orcid-numbers        = {Soto, David/0000-0003-0205-7513 Greene, Ciara/0000-0002-8833-9046},
  priority             = {prio3},
  researcherid-numbers = {Soto, David/R-3711-2019},
  unique-id            = {WOS:000344648000013},
}

@Article{medina2020em,
  author               = {Medina, Yesid Ospitia and Beltran, Jose Ramon and Baldassarri, Sandra},
  journal              = {PERSONAL AND UBIQUITOUS COMPUTING},
  title                = {Emotional classification of music using neural networks with the MediaEval dataset},
  year                 = {2020},
  issn                 = {1617-4909},
  month                = {2020 APR 15},
  bdsk-url-1           = {https://doi.org/10.1007/s00779-020-01393-4},
  doi                  = {10.1007/s00779-020-01393-4},
  earlyaccessdate      = {APR 2020},
  eissn                = {1617-4917},
  hasabstract          = {N},
  orcid-numbers        = {Ospitia, Yesid/0000-0002-5494-2787 Beltran, Jose Ramon/0000-0002-7500-4650 Baldassarri, Sandra/0000-0002-9315-6391},
  priority             = {prio1},
  researcherid-numbers = {Ospitia, Yesid/AAD-6729-2021 Beltran, Jose Ramon/K-7693-2015 Baldassarri, Sandra/L-6033-2014},
  unique-id            = {WOS:000526254100001},
}

@Article{sereno2015em,
  author               = {Sereno, Sara C. and Scott, Graham G. and Yao, Bo and Thaden, Elske J. and O'Donnell, Patrick J.},
  journal              = {FRONTIERS IN PSYCHOLOGY},
  title                = {Emotion word processing: does mood make a difference?},
  year                 = {2015},
  issn                 = {1664-1078},
  month                = {AUG 24},
  volume               = {6},
  bdsk-url-1           = {https://doi.org/10.3389/fpsyg.2015.01191},
  comment              = {word emotionality recognition},
  doi                  = {10.3389/fpsyg.2015.01191},
  hasabstract          = {N},
  modificationdate     = {2024-05-16T06:02:32},
  orcid-numbers        = {Yao, Bo/0000-0003-1852-2774},
  priority             = {prio3},
  researcherid-numbers = {Sereno, Sara C/A-1808-2010 Yao, Bo/F-9846-2010},
  unique-id            = {WOS:000360040400001},
}

@Article{susino2020mu,
  author               = {Susino, Marco and Schubert, Emery},
  journal              = {PLOS ONE},
  title                = {Musical emotions in the absence of music: A cross-cultural investigation of emotion communication in music by extra-musical cues},
  year                 = {2020},
  issn                 = {1932-6203},
  month                = {NOV 18},
  number               = {11},
  volume               = {15},
  article-number       = {e0241196},
  bdsk-url-1           = {https://doi.org/10.1371/journal.pone.0241196},
  comment              = {lyrics only},
  doi                  = {10.1371/journal.pone.0241196},
  hasabstract          = {N},
  modificationdate     = {2024-05-16T06:03:45},
  orcid-numbers        = {Susino, Marco/0000-0002-1112-6448 Schubert, Emery/0000-0002-6541-8083},
  priority             = {prio3},
  researcherid-numbers = {Susino, Marco/AGS-8890-2022},
  unique-id            = {WOS:000595265900060},
}

@Article{islam2021ee,
  author               = {Islam, Md Rabiul and Islam, Md Milon and Rahman, Md Mustafizur and Mondal, Chayan and Singha, Suvojit Kumar and Ahmad, Mohiuddin and Awal, Abdul and Islam, Md Saiful and Moni, Mohammad Ali},
  journal              = {COMPUTERS IN BIOLOGY AND MEDICINE},
  title                = {EEG Channel Correlation Based Model for Emotion Recognition},
  year                 = {2021},
  issn                 = {0010-4825},
  month                = {SEP},
  volume               = {136},
  article-number       = {104757},
  bdsk-url-1           = {https://doi.org/10.1016/j.compbiomed.2021.104757},
  doi                  = {10.1016/j.compbiomed.2021.104757},
  earlyaccessdate      = {AUG 2021},
  eissn                = {1879-0534},
  hasabstract          = {N},
  orcid-numbers        = {Islam, Md Saiful/0000-0001-9236-380X Islam, Md. Milon/0000-0002-4535-5978 Islam, Md Rabiul/0000-0002-5840-5882 Mondal, Chayan/0000-0002-3871-1065 Moni, Mohammad Ali/0000-0003-0756-1006 Singha, Suvojit Kumar/0000-0002-3747-853X Ahmad, Mohiuddin/0000-0001-9123-0618 Islam, Md Saiful/0000-0001-7181-5328},
  priority             = {prio3},
  researcherid-numbers = {Rahman, Md Mustafizur/HIZ-8077-2022 Islam, Md Saiful/M-5468-2015 Islam, Md. Milon/AAV-4262-2020 Ahmad, Mohiuddin/H-7295-2015 Islam, Md Saiful/I-6354-2013},
  unique-id            = {WOS:000696935800005},
}

@Article{li2022sp,
  author          = {Li, Dongdong and Xie, Li and Chai, Bing and Wang, Zhe and Yang, Hai},
  journal         = {APPLIED SOFT COMPUTING},
  title           = {Spatial-frequency convolutional self-attention network for EEG emotion recognition},
  year            = {2022},
  issn            = {1568-4946},
  month           = {JUN},
  volume          = {122},
  article-number  = {108740},
  bdsk-url-1      = {https://doi.org/10.1016/j.asoc.2022.108740},
  doi             = {10.1016/j.asoc.2022.108740},
  earlyaccessdate = {APR 2022},
  eissn           = {1872-9681},
  hasabstract     = {N},
  orcid-numbers   = {Yang, Hai/0000-0002-1161-4337},
  priority        = {prio3},
  unique-id       = {WOS:000793559700008},
}

@Article{wang2021ac,
  author               = {Wang, Xin and Wei, Yujia and Heng, Lena and McAdams, Stephen},
  journal              = {FRONTIERS IN PSYCHOLOGY},
  title                = {A Cross-Cultural Analysis of the Influence of Timbre on Affect Perception in Western Classical Music and Chinese Music Traditions},
  year                 = {2021},
  issn                 = {1664-1078},
  month                = {SEP 29},
  volume               = {12},
  article-number       = {732865},
  bdsk-url-1           = {https://doi.org/10.3389/fpsyg.2021.732865},
  doi                  = {10.3389/fpsyg.2021.732865},
  hasabstract          = {N},
  modificationdate     = {2024-05-16T06:04:55},
  orcid-numbers        = {McAdams, Stephen/0000-0002-6744-9035 Heng, Lena/0000-0002-4395-2576},
  priority             = {prio1},
  researcherid-numbers = {McAdams, Stephen/GQB-0225-2022 Wei, Yujia/IAQ-8917-2023},
  unique-id            = {WOS:000707264800001},
}

@Article{lin2014fu,
  author               = {Lin, Yuan-Pin and Yang, Yi-Hsuan and Jung, Tzyy-Ping},
  journal              = {FRONTIERS IN NEUROSCIENCE},
  title                = {Fusion of electroencephalographic dynamics and musical contents for estimating emotional responses in music listening},
  year                 = {2014},
  month                = {MAY 1},
  volume               = {8},
  article-number       = {94},
  bdsk-url-1           = {https://doi.org/10.3389/fnins.2014.00094},
  doi                  = {10.3389/fnins.2014.00094},
  eissn                = {1662-453X},
  hasabstract          = {N},
  orcid-numbers        = {Lin, Yuan-Pin/0000-0002-3434-9118 Jung, Tzyy Ping/0000-0002-8377-2166},
  priority             = {prio3},
  researcherid-numbers = {Lin, Yuan-Pin/P-7429-2015 Jung, Tzyy Ping/HPG-7054-2023},
  unique-id            = {WOS:000346475100001},
}

@Article{mustafa2021op,
  author               = {Mustafa, Mahfuzah and Zahari, Zarith Liyana and Abdubrani, Rafiuddin},
  journal              = {JURNAL TEKNOLOGI-SCIENCES \& ENGINEERING},
  title                = {OPTIMAL ACCURACY PERFORMANCE IN MUSIC-BASED EEG SIGNAL USING MATTHEW CORRELATION COEFFICIENT ADVANCED (MCCA)},
  year                 = {2021},
  issn                 = {0127-9696},
  month                = {NOV},
  number               = {6},
  pages                = {53-61},
  volume               = {83},
  bdsk-url-1           = {https://doi.org/10.11113/jurnalteknologi.v83.16750},
  doi                  = {10.11113/jurnalteknologi.v83.16750},
  eissn                = {2180-3722},
  hasabstract          = {N},
  orcid-numbers        = {Mustafa, Mahfuzah/0000-0002-4909-7101 Abdubrani, Rafiuddin/0000-0003-2534-1197},
  priority             = {prio3},
  researcherid-numbers = {Mustafa, Mahfuzah/HQZ-3011-2023},
  unique-id            = {WOS:000747752000005},
}

@Article{chen2021du,
  author               = {Chen, Jing and Li, Haifeng and Ma, Lin and Bo, Hongjian and Soong, Frank and Shi, Yaohui},
  journal              = {FRONTIERS IN NEUROSCIENCE},
  title                = {Dual-Threshold-Based Microstate Analysis on Characterizing Temporal Dynamics of Affective Process and Emotion Recognition From EEG Signals},
  year                 = {2021},
  month                = {JUL 14},
  volume               = {15},
  article-number       = {689791},
  bdsk-url-1           = {https://doi.org/10.3389/fnins.2021.689791},
  doi                  = {10.3389/fnins.2021.689791},
  eissn                = {1662-453X},
  hasabstract          = {N},
  orcid-numbers        = {Bo, Hongjian/0000-0002-2850-6199},
  priority             = {prio3},
  researcherid-numbers = {Bo, Hongjian/HRC-9086-2023},
  unique-id            = {WOS:000678381300001},
}

@Article{chen2023te,
  author          = {Chen, Yu and Zhang, Haopeng and Long, Jun and Xie, Yining},
  journal         = {MULTIMEDIA TOOLS AND APPLICATIONS},
  title           = {Temporal shift residual network for EEG-based emotion recognition: A 3D feature image sequence approach},
  year            = {2023},
  issn            = {1380-7501},
  month           = {2023 OCT 16},
  bdsk-url-1      = {https://doi.org/10.1007/s11042-023-17142-7},
  doi             = {10.1007/s11042-023-17142-7},
  earlyaccessdate = {OCT 2023},
  eissn           = {1573-7721},
  hasabstract     = {N},
  priority        = {prio3},
  unique-id       = {WOS:001142539400025},
}

@Article{sivathasan2023ba,
  author         = {Sivathasan, Shalini and Dahary, Hadas and Burack, Jacob A. and Quintin, Eve-Marie},
  journal        = {PLOS ONE},
  title          = {Basic emotion recognition of children on the autism spectrum is enhanced in music and typical for faces and voices},
  year           = {2023},
  issn           = {1932-6203},
  month          = {JAN 11},
  number         = {1},
  volume         = {18},
  article-number = {e0279002},
  bdsk-url-1     = {https://doi.org/10.1371/journal.pone.0279002},
  doi            = {10.1371/journal.pone.0279002},
  hasabstract    = {N},
  orcid-numbers  = {Sivathasan, Shalini/0000-0002-7324-5039 Quintin, Eve-Marie/0000-0001-8671-7480},
  priority       = {prio3},
  unique-id      = {WOS:001036077600026},
}

@Article{lakhan2019co,
  author               = {Lakhan, Payongkit and Banluesombatkul, Nannapas and Changniam, Vongsagon and Dhithijaiyratn, Ratwade and Leelaarporn, Pitshaporn and Boonchieng, Ekkarat and Hompoonsup, Supanida and Wilaiprasitporn, Theerawit},
  journal              = {IEEE SENSORS JOURNAL},
  title                = {Consumer Grade Brain Sensing for Emotion Recognition},
  year                 = {2019},
  issn                 = {1530-437X},
  month                = {NOV 1},
  number               = {21},
  pages                = {9896-9907},
  volume               = {19},
  bdsk-url-1           = {https://doi.org/10.1109/JSEN.2019.2928781},
  doi                  = {10.1109/JSEN.2019.2928781},
  eissn                = {1558-1748},
  hasabstract          = {N},
  orcid-numbers        = {Boonchieng, Ekkarat/0000-0002-7584-1627 Leelaarporn, Pitshaporn/0000-0001-8755-875X Wilaiprasitporn, Theerawit/0000-0003-4941-4354 Piyayotai, Supanida/0000-0003-0630-7380},
  priority             = {prio3},
  researcherid-numbers = {Boonchieng, Ekkarat/GQQ-6462-2022 Leelaarporn, Pitshaporn/AAR-1349-2021 Wilaiprasitporn, Theerawit/T-5432-2019},
  unique-id            = {WOS:000492361300036},
}

@Article{altenmueller2014pl,
  author               = {Altenmueller, Eckart and Siggel, Susann and Mohammadi, Bahram and Samii, Amir and Muente, Thomas F.},
  journal              = {FRONTIERS IN PSYCHOLOGY},
  title                = {Play it again, Sam: brain correlates of emotional music recognition},
  year                 = {2014},
  issn                 = {1664-1078},
  month                = {FEB 18},
  volume               = {5},
  article-number       = {114},
  bdsk-url-1           = {https://doi.org/10.3389/fpsyg.2014.00114},
  doi                  = {10.3389/fpsyg.2014.00114},
  hasabstract          = {N},
  priority             = {prio3},
  researcherid-numbers = {Altenmuller, Eckart/AAB-9948-2021},
  unique-id            = {WOS:000352414900001},
}

@Article{deng2015dy,
  author           = {Deng, James J. and Leung, Clement H. C.},
  journal          = {IEEE TRANSACTIONS ON AFFECTIVE COMPUTING},
  title            = {Dynamic Time Warping for Music Retrieval Using Time Series Modeling of Musical Emotions},
  year             = {2015},
  issn             = {1949-3045},
  month            = {APR-JUN},
  number           = {2},
  pages            = {137-151},
  volume           = {6},
  bdsk-url-1       = {https://doi.org/10.1109/TAFFC.2015.2404352},
  comment          = {no applicable metrics, discussed},
  doi              = {10.1109/TAFFC.2015.2404352},
  hasabstract      = {N},
  modificationdate = {2024-05-16T11:32:10},
  priority         = {prio3},
  unique-id        = {WOS:000356172600007},
}

@Article{deng2015em,
  author           = {Deng, James J. and Leung, Clement H. C. and Milani, Alfredo and Chen, Li},
  journal          = {ACM TRANSACTIONS ON INTERACTIVE INTELLIGENT SYSTEMS},
  title            = {Emotional States Associated with Music: Classification, Prediction of Changes, and Consideration in Recommendation},
  year             = {2015},
  issn             = {2160-6455},
  month            = {MAR},
  number           = {1},
  volume           = {5},
  bdsk-url-1       = {https://doi.org/10.1145/2723575},
  comment          = {discussed},
  doi              = {10.1145/2723575},
  eissn            = {2160-6463},
  hasabstract      = {N},
  modificationdate = {2024-05-16T11:36:41},
  orcid-numbers    = {Chen, Li/0000-0002-5842-838X MILANI, Alfredo/0000-0003-4534-1805},
  priority         = {prio1},
  unique-id        = {WOS:000360088100004},
}

@Article{grekow2018mu,
  author               = {Grekow, Jacek},
  journal              = {JOURNAL OF INTELLIGENT INFORMATION SYSTEMS},
  title                = {Musical performance analysis in terms of emotions it evokes},
  year                 = {2018},
  issn                 = {0925-9902},
  month                = {OCT},
  number               = {2},
  pages                = {415-437},
  volume               = {51},
  bdsk-url-1           = {https://doi.org/10.1007/s10844-018-0510-y},
  doi                  = {10.1007/s10844-018-0510-y},
  eissn                = {1573-7675},
  hasabstract          = {N},
  orcid-numbers        = {Grekow, Jacek/0000-0003-2094-0107},
  priority             = {prio1},
  researcherid-numbers = {Grekow, Jacek/M-9500-2015},
  unique-id            = {WOS:000444479900009},
}

@Article{saizclar2022pr,
  author               = {Saiz-Clar, Elena and Angel Serrano, Miguel and Manuel Reales, Jose},
  journal              = {PSYCHOLOGY OF MUSIC},
  title                = {Predicting emotions in music using the onset curve},
  year                 = {2022},
  issn                 = {0305-7356},
  month                = {JUL},
  number               = {4},
  pages                = {1107-1120},
  volume               = {50},
  bdsk-url-1           = {https://doi.org/10.1177/03057356211031658},
  doi                  = {10.1177/03057356211031658},
  earlyaccessdate      = {AUG 2021},
  eissn                = {1741-3087},
  hasabstract          = {N},
  modificationdate     = {2024-05-16T06:09:59},
  orcid-numbers        = {Serrano, Miguel-Angel/0000-0002-6574-4532 Serrano, Miguel-Angel/0000-0002-6574-4532 Saiz-Clar, Elena/0000-0002-8714-2305},
  priority             = {prio1},
  researcherid-numbers = {Serrano, Miguel-Angel/ABE-7279-2021 Serrano, Miguel-Angel/AAZ-4393-2020},
  unique-id            = {WOS:000684681100001},
}

@Article{kim2018an,
  author               = {Kim, Seul-Kee and Kang, Hang-Bong},
  journal              = {NEUROCOMPUTING},
  title                = {An analysis of smartphone overuse recognition in terms of emotions using brainwaves and deep learning},
  year                 = {2018},
  issn                 = {0925-2312},
  month                = {JAN 31},
  pages                = {1393-1406},
  volume               = {275},
  bdsk-url-1           = {https://doi.org/10.1016/j.neucom.2017.09.081},
  doi                  = {10.1016/j.neucom.2017.09.081},
  eissn                = {1872-8286},
  hasabstract          = {N},
  orcid-numbers        = {Kang, Hang-Bong/0000-0002-7064-478X},
  priority             = {prio3},
  researcherid-numbers = {Kim, Won Seog/C-9613-2011},
  unique-id            = {WOS:000418370200131},
}

@Article{malheiro2018em,
  author               = {Malheiro, Ricardo and Panda, Renato and Gomes, Paulo and Paiva, Rui Pedro},
  journal              = {IEEE TRANSACTIONS ON AFFECTIVE COMPUTING},
  title                = {Emotionally-Relevant Features for Classification and Regression of Music Lyrics},
  year                 = {2018},
  issn                 = {1949-3045},
  month                = {APR-JUN},
  number               = {2},
  pages                = {240-254},
  volume               = {9},
  bdsk-url-1           = {https://doi.org/10.1109/TAFFC.2016.2598569},
  doi                  = {10.1109/TAFFC.2016.2598569},
  hasabstract          = {N},
  orcid-numbers        = {Malheiro, Ricardo/0000-0002-3010-2732 Panda, Renato/0000-0003-2539-5590 Paiva, Rui Pedro/0000-0003-3215-3960},
  priority             = {prio1},
  researcherid-numbers = {Malheiro, Ricardo/L-9369-2017 Panda, Renato/AAK-7581-2020 Paiva, Rui Pedro/D-9602-2018},
  unique-id            = {WOS:000433351100008},
}

@Article{doma2020ac,
  author               = {Doma, Vikrant and Pirouz, Matin},
  journal              = {JOURNAL OF BIG DATA},
  title                = {A comparative analysis of machine learning methods for emotion recognition using EEG and peripheral physiological signals},
  year                 = {2020},
  month                = {MAR 11},
  number               = {1},
  volume               = {7},
  article-number       = {18},
  bdsk-url-1           = {https://doi.org/10.1186/s40537-020-00289-7},
  doi                  = {10.1186/s40537-020-00289-7},
  eissn                = {2196-1115},
  hasabstract          = {N},
  orcid-numbers        = {Pirouz, Matin/0000-0002-6255-4741},
  priority             = {prio3},
  researcherid-numbers = {Pirouz, Matin/J-3429-2019},
  unique-id            = {WOS:000596137100001},
}

@Article{eyben2015em,
  author               = {Eyben, Florian and Salomao, Glaucia L. and Sundberg, Johan and Scherer, Klaus R. and Schuller, Bjoern W.},
  journal              = {EURASIP JOURNAL ON AUDIO SPEECH AND MUSIC PROCESSING},
  title                = {Emotion in the singing voice-a deeper look at acoustic features in the light of automatic classification},
  year                 = {2015},
  issn                 = {1687-4722},
  month                = {JUN 30},
  article-number       = {19},
  bdsk-url-1           = {https://doi.org/10.1186/s13636-015-0057-6},
  doi                  = {10.1186/s13636-015-0057-6},
  hasabstract          = {N},
  orcid-numbers        = {Schuller, Bj{\"o}rn Wolfgang/0000-0002-6478-8699},
  priority             = {prio1},
  researcherid-numbers = {Schuller, Bj{\"o}rn Wolfgang/D-3241-2011},
  unique-id            = {WOS:000360837600001},
}

@Article{cowen2019ma,
  author               = {Cowen, Alan S. and Elfenbein, Hillary Anger and Laukka, Petri and Keltner, Dacher},
  journal              = {AMERICAN PSYCHOLOGIST},
  title                = {Mapping 24 Emotions Conveyed by Brief Human Vocalization},
  year                 = {2019},
  issn                 = {0003-066X},
  month                = {SEP},
  number               = {6},
  pages                = {698-712},
  volume               = {74},
  bdsk-url-1           = {https://doi.org/10.1037/amp0000399},
  comment              = {vocal emotion},
  doi                  = {10.1037/amp0000399},
  eissn                = {1935-990X},
  hasabstract          = {N},
  modificationdate     = {2024-05-16T06:10:43},
  orcid-numbers        = {Elfenbein, Hillary Anger/0000-0002-3973-1447 Cowen, Alan Samuel/0000-0002-8381-5883 Laukka, Petri/0000-0001-8771-6818},
  priority             = {prio3},
  researcherid-numbers = {Elfenbein, Hillary Anger/KGL-8639-2024 Cowen, Alan Samuel/S-3367-2019 Laukka, Petri/B-5259-2008},
  unique-id            = {WOS:000487093100006},
}

@Article{justel2023di,
  author           = {Justel, Nadia and Diaz Abrahan, Veronika and Moltrasio, Julieta and Rubinstein, Wanda},
  journal          = {COGENT PSYCHOLOGY},
  title            = {Differential effect of music on memory depends on emotional valence: An experimental study about listening to music and music training},
  year             = {2023},
  issn             = {2331-1908},
  month            = {DEC 31},
  number           = {1},
  volume           = {10},
  article-number   = {2234692},
  bdsk-url-1       = {https://doi.org/10.1080/23311908.2023.2234692},
  comment          = {memory},
  doi              = {10.1080/23311908.2023.2234692},
  hasabstract      = {N},
  modificationdate = {2024-05-16T06:11:38},
  orcid-numbers    = {Justel, Nadia/0000-0002-0145-3357 Diaz Abrahan, Veronika/0000-0001-5003-4274 Moltrasio, Julieta/0000-0002-0759-0563 Rubinstein, Wanda/0000-0002-2673-6353},
  priority         = {prio3},
  unique-id        = {WOS:001029601500001},
}

@Article{pandey2022su,
  author               = {Pandey, Pallavi and Seeja, K. R.},
  journal              = {JOURNAL OF KING SAUD UNIVERSITY-COMPUTER AND INFORMATION SCIENCES},
  title                = {Subject independent emotion recognition from EEG using VMD and deep learning},
  year                 = {2022},
  issn                 = {1319-1578},
  month                = {MAY},
  number               = {5},
  pages                = {1730-1738},
  volume               = {34},
  bdsk-url-1           = {https://doi.org/10.1016/j.jksuci.2019.11.003},
  doi                  = {10.1016/j.jksuci.2019.11.003},
  earlyaccessdate      = {APR 2022},
  eissn                = {2213-1248},
  hasabstract          = {N},
  orcid-numbers        = {K.R., Seeja/0000-0001-6618-6758 Pandey, Pallavi/0000-0002-1294-2949},
  priority             = {prio3},
  researcherid-numbers = {K.R., Seeja/AHA-1124-2022},
  unique-id            = {WOS:000796192500009},
}

@Article{li2023ee,
  author        = {Li, Wei and Wang, Mingming and Zhu, Junyi and Song, Aiguo},
  journal       = {IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS},
  title         = {EEG-Based Emotion Recognition Using Trainable Adjacency Relation Driven Graph Convolutional Network},
  year          = {2023},
  issn          = {2379-8920},
  month         = {DEC},
  number        = {4},
  pages         = {1656-1672},
  volume        = {15},
  bdsk-url-1    = {https://doi.org/10.1109/TCDS.2023.3270170},
  doi           = {10.1109/TCDS.2023.3270170},
  eissn         = {2379-8939},
  hasabstract   = {N},
  orcid-numbers = {Song, Aiguo/0000-0002-1982-6780 Li, Wei/0000-0002-9235-9429},
  priority      = {prio3},
  unique-id     = {WOS:001126639000027},
}

@Article{panwar2019ar,
  author               = {Panwar, Sharaj and Rad, Paul and Choo, Kim-Kwang Raymond and Roopaei, Mehdi},
  journal              = {JOURNAL OF SUPERCOMPUTING},
  title                = {Are you emotional or depressed? Learning about your emotional state from your music using machine learning},
  year                 = {2019},
  issn                 = {0920-8542},
  month                = {JUN},
  number               = {6, SI},
  pages                = {2986-3009},
  volume               = {75},
  bdsk-url-1           = {https://doi.org/10.1007/s11227-018-2499-y},
  doi                  = {10.1007/s11227-018-2499-y},
  eissn                = {1573-0484},
  hasabstract          = {N},
  modificationdate     = {2024-05-16T06:12:32},
  orcid-numbers        = {Choo, Kim-Kwang Raymond/0000-0001-9208-5336},
  priority             = {prio1},
  researcherid-numbers = {Choo, Kim-Kwang Raymond/A-3634-2009 najafirad, peyman/ACB-9554-2022},
  unique-id            = {WOS:000468115400005},
}

@Article{xu2023su,
  author               = {Xu, Guixun and Guo, Wenhui and Wang, Yanjiang},
  journal              = {MEDICAL \& BIOLOGICAL ENGINEERING \& COMPUTING},
  title                = {Subject-independent EEG emotion recognition with hybrid spatio-temporal GRU-Conv architecture},
  year                 = {2023},
  issn                 = {0140-0118},
  month                = {JAN},
  number               = {1},
  pages                = {61-73},
  volume               = {61},
  bdsk-url-1           = {https://doi.org/10.1007/s11517-022-02686-x},
  doi                  = {10.1007/s11517-022-02686-x},
  earlyaccessdate      = {NOV 2022},
  eissn                = {1741-0444},
  hasabstract          = {N},
  orcid-numbers        = {Guo, Wenhui/0000-0002-3485-988X},
  priority             = {prio3},
  researcherid-numbers = {Guo, Wenhui/IAP-3000-2023},
  unique-id            = {WOS:000878040400003},
}

@Article{wen2022wh,
  author           = {Wen, Xin and Huang, Zhengxi and Sun, Zaoyi and Xu, Liang},
  journal          = {PSYCH JOURNAL},
  title            = {What a deep song: The role of music features in perceived depth},
  year             = {2022},
  issn             = {2046-0252},
  month            = {OCT},
  number           = {5},
  pages            = {673-683},
  volume           = {11},
  bdsk-url-1       = {https://doi.org/10.1002/pchj.510},
  comment          = {No relevant material,  VA as IV, discussed},
  doi              = {10.1002/pchj.510},
  earlyaccessdate  = {DEC 2021},
  eissn            = {2046-0260},
  hasabstract      = {N},
  modificationdate = {2024-05-16T12:23:15},
  orcid-numbers    = {Xu, Liang/0000-0003-3889-927X Sun, Zaoyi/0000-0002-4551-3606},
  priority         = {prio3},
  unique-id        = {WOS:000729281900001},
}

@Article{kerkova2020pe,
  author               = {Kerkova, Barbora},
  journal              = {PSYCHOLOGY OF MUSIC},
  title                = {Perception and experience of musical emotions in schizophrenia},
  year                 = {2020},
  issn                 = {0305-7356},
  month                = {MAR},
  number               = {2},
  pages                = {199-214},
  volume               = {48},
  bdsk-url-1           = {https://doi.org/10.1177/0305735618792427},
  doi                  = {10.1177/0305735618792427},
  eissn                = {1741-3087},
  hasabstract          = {N},
  orcid-numbers        = {Kerkova, Barbora/0000-0003-1345-8216},
  priority             = {prio3},
  researcherid-numbers = {Kerkova, Barbora/M-6169-2017},
  unique-id            = {WOS:000523904300003},
}

@Article{wang2017ef,
  author           = {Wang, Bo and Ren, Yanju},
  journal          = {QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY},
  title            = {Effect of post-encoding emotion on recollection and familiarity for pictures},
  year             = {2017},
  issn             = {1747-0218},
  number           = {7},
  pages            = {1236-1253},
  volume           = {70},
  bdsk-url-1       = {https://doi.org/10.1080/17470218.2016.1178311},
  comment          = {pictures},
  doi              = {10.1080/17470218.2016.1178311},
  eissn            = {1747-0226},
  hasabstract      = {N},
  modificationdate = {2024-05-16T06:13:46},
  orcid-numbers    = {Ren, Yanju/0000-0002-8776-8711},
  priority         = {prio3},
  unique-id        = {WOS:000396826100011},
}

@Article{lin2017im,
  author               = {Lin, Yuan-Pin and Jung, Tzyy-Ping},
  journal              = {FRONTIERS IN HUMAN NEUROSCIENCE},
  title                = {Improving EEG-Based Emotion Classification Using Conditional Transfer Learning},
  year                 = {2017},
  issn                 = {1662-5161},
  month                = {JUN 27},
  volume               = {11},
  article-number       = {334},
  bdsk-url-1           = {https://doi.org/10.3389/fnhum.2017.00334},
  doi                  = {10.3389/fnhum.2017.00334},
  hasabstract          = {N},
  orcid-numbers        = {Jung, Tzyy Ping/0000-0002-8377-2166 Lin, Yuan-Pin/0000-0002-3434-9118},
  priority             = {prio3},
  researcherid-numbers = {Jung, Tzyy Ping/HPG-7054-2023 Lin, Yuan-Pin/P-7429-2015},
  unique-id            = {WOS:000404482600001},
}

@Article{goshvarpour2020an,
  author               = {Goshvarpour, Ateke and Goshvarpour, Atefeh},
  journal              = {COGNITIVE COMPUTATION},
  title                = {A Novel Approach for EEG Electrode Selection in Automated Emotion Recognition Based on Lagged Poincare's Indices and sLORETA},
  year                 = {2020},
  issn                 = {1866-9956},
  month                = {MAY},
  number               = {3},
  pages                = {602-618},
  volume               = {12},
  bdsk-url-1           = {https://doi.org/10.1007/s12559-019-09699-z},
  doi                  = {10.1007/s12559-019-09699-z},
  earlyaccessdate      = {DEC 2019},
  eissn                = {1866-9964},
  hasabstract          = {N},
  orcid-numbers        = {Goshvarpour, Ateke/0000-0002-5185-5645 Goshvarpour, Ateke/0000-0002-5185-5645 Goshvarpour, Atefeh/0000-0002-0343-4344},
  priority             = {prio3},
  researcherid-numbers = {Goshvarpour, Ateke/IZD-8225-2023 Goshvarpour, Ateke/AAB-5275-2019 Goshvarpour, Atefeh/AAC-3145-2020},
  unique-id            = {WOS:000500461600001},
}

@InCollection{wang2016af,
  author               = {Wang, Ju-Chiang and Yang, Yi-Hsuan and Wang, Hsin-Min},
  booktitle            = {EMOTIONS AND PERSONALITY IN PERSONALIZED SERVICES: MODELS, EVALUATION AND APPLICATIONS},
  title                = {Affective Music Information Retrieval},
  year                 = {2016},
  editor               = {Tkalcic, M and DeCarolis, B and DeGemmis, M and Odic, A and Kosir, A},
  isbn                 = {978-3-319-31413-6; 978-3-319-31411-2},
  pages                = {227-261},
  series               = {Human-Computer Interaction Series},
  bdsk-url-1           = {https://doi.org/10.1007/978-3-319-31413-6%5C_12},
  doi                  = {10.1007/978-3-319-31413-6\_12},
  eissn                = {2524-4477},
  hasabstract          = {N},
  issn                 = {1571-5035},
  orcid-numbers        = {Wang, Hsin-Min/0000-0003-3599-5071},
  priority             = {prio1},
  researcherid-numbers = {Wang, Hsin-Min/ABA-8747-2020},
  unique-id            = {WOS:000389269400013},
}

@Article{trost2015te,
  author               = {Trost, Wiebke and Fruehholz, Sascha and Cochrane, Tom and Cojan, Yann and Vuilleumier, Patrik},
  journal              = {SOCIAL COGNITIVE AND AFFECTIVE NEUROSCIENCE},
  title                = {Temporal dynamics of musical emotions examined through intersubject synchrony of brain activity},
  year                 = {2015},
  issn                 = {1749-5016},
  month                = {DEC},
  number               = {12},
  pages                = {1705-1721},
  volume               = {10},
  bdsk-url-1           = {https://doi.org/10.1093/scan/nsv060},
  doi                  = {10.1093/scan/nsv060},
  eissn                = {1749-5024},
  hasabstract          = {N},
  orcid-numbers        = {Vuilleumier, Patrik/0000-0002-8198-9214 Cochrane, Tom/0000-0001-6246-161X Fruhholz, Sascha/0000-0002-6485-3817},
  priority             = {prio3},
  researcherid-numbers = {Vuilleumier, Patrik/AAJ-1839-2021 cojan, yann/AAK-2383-2020 Fr{\"u}hholz, Sascha/E-9194-2013},
  unique-id            = {WOS:000367192300011},
}

@Article{mangelsdorf2021pe,
  author           = {Mangelsdorf, Heather Harden and Listman, Jason and Maler, Anabel},
  journal          = {MUSIC PERCEPTION},
  title            = {PERCEPTION OF MUSICALITY AND EMOTION IN SIGNED SONGS},
  year             = {2021},
  issn             = {0730-7829},
  month            = {DEC},
  number           = {2},
  pages            = {160-180},
  volume           = {39},
  bdsk-url-1       = {https://doi.org/10.1525/MP.2021.39.2.160},
  comment          = {signed songs},
  doi              = {10.1525/MP.2021.39.2.160},
  hasabstract      = {N},
  modificationdate = {2024-05-16T06:14:27},
  priority         = {prio3},
  unique-id        = {WOS:000723406100004},
}

@Article{platz2015th,
  author               = {Platz, Friedrich and Kopiez, Reinhard and Hasselhorn, Johannes and Wolf, Anna},
  journal              = {MUSICAE SCIENTIAE},
  title                = {The impact of song-specific age and affective qualities of popular songs on music-evoked autobiographical memories (MEAMs)},
  year                 = {2015},
  issn                 = {1029-8649},
  month                = {DEC},
  number               = {4},
  pages                = {327-349},
  volume               = {19},
  bdsk-url-1           = {https://doi.org/10.1177/1029864915597567},
  comment              = {memory},
  doi                  = {10.1177/1029864915597567},
  eissn                = {2045-4147},
  hasabstract          = {N},
  modificationdate     = {2024-05-16T06:15:49},
  orcid-numbers        = {Wolf, Anna/0000-0002-6196-6488 Platz, Friedrich/0000-0002-9758-9434 Hasselhorn, Johannes/0000-0003-3384-224X},
  priority             = {prio3},
  researcherid-numbers = {Wolf, Anna/AAX-5289-2021 Platz, Friedrich/N-9224-2015},
  unique-id            = {WOS:000366096700001},
}

@Article{lee2020fr,
  author               = {Lee, Minji and Shin, Gi-Hwan and Lee, Seong-Whan},
  journal              = {IEEE ACCESS},
  title                = {Frontal EEG Asymmetry of Emotion for the Same Auditory Stimulus},
  year                 = {2020},
  issn                 = {2169-3536},
  pages                = {107200-107213},
  volume               = {8},
  bdsk-url-1           = {https://doi.org/10.1109/ACCESS.2020.3000788},
  doi                  = {10.1109/ACCESS.2020.3000788},
  hasabstract          = {N},
  orcid-numbers        = {Lee, Minji/0000-0003-4261-875X},
  priority             = {prio3},
  researcherid-numbers = {Lee, Minji/ABM-5019-2022 Campailla, Jasmin/AAK-2420-2021},
  unique-id            = {WOS:000544040800058},
}

@Article{drossos2015ev,
  author               = {Drossos, Konstantinos and Floros, Andreas and Kermanidis, Katia-Lida},
  journal              = {JOURNAL OF THE AUDIO ENGINEERING SOCIETY},
  title                = {Evaluating the Impact of Sound Events' Rhythm Characteristics to Listener's Valence},
  year                 = {2015},
  issn                 = {1549-4950},
  month                = {MAR},
  number               = {3},
  pages                = {139-153},
  volume               = {63},
  bdsk-url-1           = {https://doi.org/10.17743/jaes.2015.0010},
  doi                  = {10.17743/jaes.2015.0010},
  hasabstract          = {N},
  orcid-numbers        = {Drosos, Konstantinos/0000-0002-3605-7127},
  priority             = {prio1},
  researcherid-numbers = {Kermanidis, Katia/AAM-2025-2021 Floros, Andreas AF/F-1478-2019 Drossos, Konstantinos/I-8305-2015},
  unique-id            = {WOS:000351323000001},
}

@Article{liu2021mi,
  author         = {Liu, Xiaolin and Shi, Huijuan and Liu, Yong and Yuan, Hong and Zheng, Maoping},
  journal        = {INTERNATIONAL JOURNAL OF ENVIRONMENTAL RESEARCH AND PUBLIC HEALTH},
  title          = {Mindfulness Meditation Improves Musical Aesthetic Emotion Processing in Young Adults},
  year           = {2021},
  month          = {DEC},
  number         = {24},
  volume         = {18},
  article-number = {13045},
  bdsk-url-1     = {https://doi.org/10.3390/ijerph182413045},
  doi            = {10.3390/ijerph182413045},
  eissn          = {1660-4601},
  hasabstract    = {N},
  orcid-numbers  = {Liu, Xiaolin/0000-0002-1753-4653},
  priority       = {prio3},
  unique-id      = {WOS:000777847200001},
}

@Article{liu2021th,
  author               = {Liu, Ying and Wang, Zixuan and Yu, Ge},
  journal              = {FRONTIERS IN PSYCHOLOGY},
  title                = {The Effectiveness of Facial Expression Recognition in Detecting Emotional Responses to Sound Interventions in Older Adults With Dementia},
  year                 = {2021},
  issn                 = {1664-1078},
  month                = {AUG 25},
  volume               = {12},
  article-number       = {707809},
  bdsk-url-1           = {https://doi.org/10.3389/fpsyg.2021.707809},
  doi                  = {10.3389/fpsyg.2021.707809},
  hasabstract          = {N},
  priority             = {prio3},
  researcherid-numbers = {Wang, Zixuan/HZJ-2348-2023},
  unique-id            = {WOS:000729057500001},
}

@Article{narme2016em,
  author      = {Narme, Pauline and Peretz, Isabelle and Strub, Marie-Laure and Ergis, Anne-Marie},
  journal     = {PSYCHOLOGY AND AGING},
  title       = {Emotion Effects on Implicit and Explicit Musical Memory in Normal Aging},
  year        = {2016},
  issn        = {0882-7974},
  month       = {DEC},
  number      = {8},
  pages       = {902-913},
  volume      = {31},
  bdsk-url-1  = {https://doi.org/10.1037/pag0000116},
  doi         = {10.1037/pag0000116},
  eissn       = {1939-1498},
  hasabstract = {N},
  priority    = {prio3},
  unique-id   = {WOS:000390594400007},
}

@Article{pei2024ee,
  author          = {Pei, Guanxiong and Shang, Qian and Hua, Shizhen and Li, Taihao and Jin, Jia},
  journal         = {COMPUTERS IN HUMAN BEHAVIOR},
  title           = {EEG-based affective computing in virtual reality with a balancing of the computational efficiency and recognition accuracy},
  year            = {2024},
  issn            = {0747-5632},
  month           = {MAR},
  volume          = {152},
  article-number  = {108085},
  bdsk-url-1      = {https://doi.org/10.1016/j.chb.2023.108085},
  doi             = {10.1016/j.chb.2023.108085},
  earlyaccessdate = {DEC 2023},
  eissn           = {1873-7692},
  hasabstract     = {N},
  orcid-numbers   = {Jin, Jia/0000-0003-1124-2999},
  priority        = {prio3},
  unique-id       = {WOS:001146607600001},
}

@Article{nardelli2015re,
  author           = {Nardelli, Mimma and Valenza, Gaetano and Greco, Alberto and Lanata, Antonio and Scilingo, Enzo Pasquale},
  journal          = {IEEE TRANSACTIONS ON AFFECTIVE COMPUTING},
  title            = {Recognizing Emotions Induced by Affective Sounds through Heart Rate Variability},
  year             = {2015},
  issn             = {1949-3045},
  month            = {OCT-DEC},
  number           = {4},
  pages            = {385-394},
  volume           = {6},
  bdsk-url-1       = {https://doi.org/10.1109/TAFFC.2015.2432810},
  comment          = {ECG,  heart rate},
  doi              = {10.1109/TAFFC.2015.2432810},
  hasabstract      = {N},
  modificationdate = {2024-05-16T06:16:19},
  orcid-numbers    = {Valenza, Gaetano/0000-0001-6574-1879 Greco, Alberto/0000-0002-4822-5562 Lanata, Antonio/0000-0002-6540-5952 Nardelli, Mimma/0000-0003-0453-7465},
  priority         = {prio3},
  unique-id        = {WOS:000366027900006},
}

@Article{xie2020mu,
  author         = {Xie, Baijun and Kim, Jonathan C. and Park, Chung Hyuk},
  journal        = {APPLIED SCIENCES-BASEL},
  title          = {Musical Emotion Recognition with Spectral Feature Extraction Based on a Sinusoidal Model with Model-Based and Deep-Learning Approaches},
  year           = {2020},
  month          = {FEB},
  number         = {3},
  volume         = {10},
  article-number = {902},
  bdsk-url-1     = {https://doi.org/10.3390/app10030902},
  doi            = {10.3390/app10030902},
  eissn          = {2076-3417},
  hasabstract    = {N},
  orcid-numbers  = {Park, Chung Hyuk/0000-0003-0742-6541 Xie, Baijun/0000-0001-5080-198X},
  priority       = {prio1},
  unique-id      = {WOS:000525305900173},
}

@Article{martins2022en,
  author               = {Martins, Ines and Lima, Cesar F. and Pinheiro, Ana P.},
  journal              = {COGNITIVE AFFECTIVE \& BEHAVIORAL NEUROSCIENCE},
  title                = {Enhanced salience of musical sounds in singers and instrumentalists},
  year                 = {2022},
  issn                 = {1530-7026},
  month                = {OCT},
  number               = {5},
  pages                = {1044-1062},
  volume               = {22},
  bdsk-url-1           = {https://doi.org/10.3758/s13415-022-01007-x},
  comment              = {EEG},
  doi                  = {10.3758/s13415-022-01007-x},
  earlyaccessdate      = {MAY 2022},
  eissn                = {1531-135X},
  hasabstract          = {N},
  modificationdate     = {2024-05-16T06:17:14},
  orcid-numbers        = {Lima, Cesar F./0000-0003-3058-7204},
  priority             = {prio3},
  researcherid-numbers = {Lima, Cesar F./HSF-6972-2023},
  unique-id            = {WOS:000789727200001},
}

@Article{drapeau2017em,
  author      = {Drapeau, Joanie and Gosselin, Nathalie and Peretz, Isabelle and McKerral, Michelle},
  journal     = {BRAIN INJURY},
  title       = {Emotional recognition from dynamic facial, vocal and musical expressions following traumatic brain injury},
  year        = {2017},
  issn        = {0269-9052},
  number      = {2},
  pages       = {221-229},
  volume      = {31},
  bdsk-url-1  = {https://doi.org/10.1080/02699052.2016.1208846},
  doi         = {10.1080/02699052.2016.1208846},
  eissn       = {1362-301X},
  hasabstract = {N},
  priority    = {prio3},
  unique-id   = {WOS:000395163500010},
}

@Article{aljanaki2017de,
  author         = {Aljanaki, Anna and Yang, Yi-Hsuan and Soleymani, Mohammad},
  journal        = {PLOS ONE},
  title          = {Developing a benchmark for emotional analysis of music},
  year           = {2017},
  issn           = {1932-6203},
  month          = {MAR 10},
  number         = {3},
  volume         = {12},
  article-number = {e73392},
  bdsk-url-1     = {https://doi.org/10.1371/journal.pone.0173392},
  doi            = {10.1371/journal.pone.0173392},
  hasabstract    = {N},
  priority       = {prio1},
  unique-id      = {WOS:000396091800035},
}

@Article{sanmillancastillo2022an,
  author               = {San Millan-Castillo, Roberto and Martino, Luca and Morgado, Eduardo and Llorente, Fernando},
  journal              = {IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING},
  title                = {An Exhaustive Variable Selection Study for Linear Models of Soundscape Emotions: Rankings and Gibbs Analysis},
  year                 = {2022},
  issn                 = {2329-9290},
  pages                = {2460-2474},
  volume               = {30},
  bdsk-url-1           = {https://doi.org/10.1109/TASLP.2022.3192664},
  doi                  = {10.1109/TASLP.2022.3192664},
  eissn                = {2329-9304},
  hasabstract          = {N},
  orcid-numbers        = {Llorente Fernandez, Fernando/0000-0003-4436-5709 Morgado, Eduardo/0000-0001-9243-409X Martino, Luca/0000-0002-7611-6558 San Millan-Castillo, Roberto/0000-0001-5821-9767},
  priority             = {prio1},
  researcherid-numbers = {Morgado, Eduardo/G-2185-2016},
  unique-id            = {WOS:000837738700001},
}

@Article{paasschen2014co,
  author               = {van Paasschen, Jorien and Zamboni, Elisa and Bacci, Francesca and Melcher, David},
  journal              = {ART \& PERCEPTION},
  title                = {Consistent Emotions Elicited by Low-Level Visual Features in Abstract Art},
  year                 = {2014},
  issn                 = {2213-4905},
  number               = {1-2},
  pages                = {99-118},
  volume               = {2},
  bdsk-url-1           = {https://doi.org/10.1163/22134913-00002012},
  doi                  = {10.1163/22134913-00002012},
  eissn                = {2213-4913},
  hasabstract          = {N},
  orcid-numbers        = {Melcher, David Paul/0000-0003-0926-585X Zamboni, Elisa/0000-0001-9200-8031},
  priority             = {prio3},
  researcherid-numbers = {Melcher, David Paul/D-1775-2010},
  unique-id            = {WOS:000421322200008},
}

@Article{coutinho2017sh,
  author               = {Coutinho, Eduardo and Schuller, Bjorn},
  journal              = {PLOS ONE},
  title                = {Shared acoustic codes underlie emotional communication in music and speech-Evidence from deep transfer learning},
  year                 = {2017},
  issn                 = {1932-6203},
  month                = {JUN 28},
  number               = {6},
  volume               = {12},
  article-number       = {e0179289},
  bdsk-url-1           = {https://doi.org/10.1371/journal.pone.0179289},
  doi                  = {10.1371/journal.pone.0179289},
  hasabstract          = {N},
  orcid-numbers        = {Coutinho, Eduardo/0000-0001-5234-1497 Schuller, Bj{\"o}rn Wolfgang/0000-0002-6478-8699},
  priority             = {prio1},
  researcherid-numbers = {Coutinho, Eduardo/K-1391-2019 Schuller, Bj{\"o}rn Wolfgang/D-3241-2011},
  unique-id            = {WOS:000404607900019},
}

@Article{diazabrahan2022mu,
  author               = {Diaz Abrahan, Veronika and Bossio, Maximiliano and Benitez, Maria and Justel, Nadia},
  journal              = {PSYCHOLOGY OF MUSIC},
  title                = {Musical strategies to improve children's memory in an educational context},
  year                 = {2022},
  issn                 = {0305-7356},
  month                = {MAY},
  number               = {3},
  pages                = {727-741},
  volume               = {50},
  article-number       = {03057356211024343},
  bdsk-url-1           = {https://doi.org/10.1177/03057356211024343},
  doi                  = {10.1177/03057356211024343},
  earlyaccessdate      = {JUL 2021},
  eissn                = {1741-3087},
  hasabstract          = {N},
  orcid-numbers        = {Ben{\'\i}tez, Mar{\'\i}a Ang{\'e}lica/0000-0001-5231-8430 Diaz Abrahan, Veronika/0000-0001-5003-4274},
  priority             = {prio3},
  researcherid-numbers = {Ben{\'\i}tez, Mar{\'\i}a Ang{\'e}lica/AFR-2370-2022},
  unique-id            = {WOS:000675660600001},
}

@Article{moctezuma2022tw,
  author               = {Moctezuma, Luis Alfredo and Abe, Takashi and Molinas, Marta},
  journal              = {SCIENTIFIC REPORTS},
  title                = {Two-dimensional CNN-based distinction of human emotions from EEG channels selected by multi-objective evolutionary algorithm},
  year                 = {2022},
  issn                 = {2045-2322},
  month                = {MAR 3},
  number               = {1},
  volume               = {12},
  article-number       = {3523},
  bdsk-url-1           = {https://doi.org/10.1038/s41598-022-07517-5},
  doi                  = {10.1038/s41598-022-07517-5},
  hasabstract          = {N},
  orcid-numbers        = {Moctezuma, Luis Alfredo/0000-0002-6632-8784 Molinas, Marta/0000-0002-8791-0917},
  priority             = {prio3},
  researcherid-numbers = {Moctezuma, Luis Alfredo/A-7857-2019 Molinas, Marta/M-2874-2019},
  unique-id            = {WOS:000838718500042},
}

@Article{abadi2015de,
  author               = {Abadi, Mojtaba Khomami and Subramanian, Ramanathan and Kia, Seyed Mostafa and Avesani, Paolo and Patras, Ioannis and Sebe, Nicu},
  journal              = {IEEE TRANSACTIONS ON AFFECTIVE COMPUTING},
  title                = {DECAF: MEG-Based Multimodal Database for Decoding Affective Physiological Responses},
  year                 = {2015},
  issn                 = {1949-3045},
  month                = {JUL-SEP},
  number               = {3},
  pages                = {209-222},
  volume               = {6},
  bdsk-url-1           = {https://doi.org/10.1109/TAFFC.2015.2392932},
  doi                  = {10.1109/TAFFC.2015.2392932},
  hasabstract          = {N},
  orcid-numbers        = {Kia, Seyed Mostafa/0000-0002-7128-814X Avesani, Paolo/0000-0001-8943-8911 Sebe, Niculae/0000-0002-6597-7248 Patras, Ioannis/0000-0003-3913-4738 Subramanian, Ramanathan/0000-0001-9441-7074},
  priority             = {prio3},
  researcherid-numbers = {Kia, Seyed Mostafa/B-6337-2018 Abadi, Mojtaba Khomami/AAA-3552-2021 Avesani, Paolo/AAY-9704-2021 Sebe, Niculae/KEC-2000-2024},
  unique-id            = {WOS:000360797200002},
}

@Article{clerico2018ela,
  author               = {Clerico, Andrea and Tiwari, Abhishek and Gupta, Rishabh and Jayaraman, Srinivasan and Falk, Tiago H.},
  journal              = {FRONTIERS IN COMPUTATIONAL NEUROSCIENCE},
  title                = {Electroencephalography Amplitude Modulation Analysis for Automated Affective Tagging of Music Video Clips},
  year                 = {2018},
  month                = {JAN 10},
  volume               = {11},
  article-number       = {775},
  bdsk-url-1           = {https://doi.org/10.3389/Thcom.2017.00115},
  doi                  = {10.3389/Thcom.2017.00115},
  eissn                = {1662-5188},
  hasabstract          = {N},
  priority             = {prio3},
  researcherid-numbers = {Falk, Tiago/IQW-2566-2023 Tiwari, Abhishek/ABE-1067-2021 Tiwari, Abhishek/HLW-2170-2023},
  unique-id            = {WOS:000419705900001},
}

@Article{hu2017cr,
  author               = {Hu, Xiao and Yang, Yi-Hsuan},
  journal              = {IEEE TRANSACTIONS ON AFFECTIVE COMPUTING},
  title                = {Cross-Dataset and Cross-Cultural Music Mood Prediction: A Case on Western and Chinese Pop Songs},
  year                 = {2017},
  issn                 = {1949-3045},
  month                = {APR-JUN},
  number               = {2},
  pages                = {228-240},
  volume               = {8},
  bdsk-url-1           = {https://doi.org/10.1109/TAFFC.2016.2523503},
  doi                  = {10.1109/TAFFC.2016.2523503},
  hasabstract          = {N},
  modificationdate     = {2024-05-16T06:17:44},
  orcid-numbers        = {Hu, Xiao/0000-0003-3994-0385},
  priority             = {prio1},
  researcherid-numbers = {Hu, Xiao/A-7645-2013 Hu, Xiao/AAD-8405-2020},
  unique-id            = {WOS:000402709900008},
}

@Article{sharma2019au,
  author               = {Sharma, Vivek and Prakash, Neelam R. and Kalra, Parveen},
  journal              = {BIOMEDICAL SIGNAL PROCESSING AND CONTROL},
  title                = {Audio-video emotional response mapping based upon Electrodermal Activity},
  year                 = {2019},
  issn                 = {1746-8094},
  month                = {JAN},
  pages                = {324-333},
  volume               = {47},
  bdsk-url-1           = {https://doi.org/10.1016/j.bspc.2018.08.024},
  doi                  = {10.1016/j.bspc.2018.08.024},
  eissn                = {1746-8108},
  hasabstract          = {N},
  orcid-numbers        = {sharma, Dr .Vivek/0000-0003-1374-9865},
  priority             = {prio3},
  researcherid-numbers = {sharma, Dr .Vivek/R-6884-2017},
  unique-id            = {WOS:000449134500032},
}

@Article{alqazzaz2022an,
  author          = {Al-Qazzaz, Noor Kamal and Sabir, Mohannad K. and Al-Timemy, Ali H. and Grammer, Karl},
  journal         = {MEDICAL \& BIOLOGICAL ENGINEERING \& COMPUTING},
  title           = {An integrated entropy-spatial framework for automatic gender recognition enhancement of emotion-based EEGs},
  year            = {2022},
  issn            = {0140-0118},
  month           = {FEB},
  number          = {2},
  pages           = {531-550},
  volume          = {60},
  bdsk-url-1      = {https://doi.org/10.1007/s11517-021-02452-5},
  doi             = {10.1007/s11517-021-02452-5},
  earlyaccessdate = {JAN 2022},
  eissn           = {1741-0444},
  hasabstract     = {N},
  orcid-numbers   = {H. Al-Timemy, Ali/0000-0003-2738-8896 Al-Lami, mohannad kadhim Sabir/0000-0001-7733-6168 Al-Qazzaz, Noor Kamal/0000-0001-6566-3452},
  priority        = {prio3},
  relevance       = {relevant},
  unique-id       = {WOS:000741884800001},
}

@Article{drossos2016on,
  author               = {Drossos, Konstantinos and Kaliakatsos-Papakostas, Maximos and Floros, Andreas and Virtanen, Tuomas},
  journal              = {JOURNAL OF THE AUDIO ENGINEERING SOCIETY},
  title                = {On the Impact of the Semantic Content of Sound Events in Emotion Elicitation},
  year                 = {2016},
  issn                 = {1549-4950},
  month                = {JUL-AUG},
  note                 = {Audio Mostly (AM) Conference on Sound, Semantics and Social Interaction, Thessaloniki, GREECE, OCT 07-09, 2015},
  number               = {7-8},
  pages                = {525-532},
  volume               = {64},
  bdsk-url-1           = {https://doi.org/10.17743/jaes.2016.0024},
  comment              = {no music},
  doi                  = {10.17743/jaes.2016.0024},
  hasabstract          = {N},
  modificationdate     = {2024-05-16T06:23:25},
  orcid-numbers        = {Kaliakatsos-Papakostas, Maximos/0000-0002-9062-9915 Drosos, Konstantinos/0000-0002-3605-7127 Virtanen, Tuomas/0000-0002-4604-9729},
  priority             = {prio3},
  researcherid-numbers = {Floros, Andreas AF/F-1478-2019 Kaliakatsos-Papakostas, Maximos/AAI-6999-2021 Drossos, Konstantinos/I-8305-2015},
  unique-id            = {WOS:000382183000008},
}

@Article{zhao2018pr,
  author               = {Zhao, Sicheng and Yao, Hongxun and Gao, Yue and Ding, Guiguang and Chua, Tat-Seng},
  journal              = {IEEE TRANSACTIONS ON AFFECTIVE COMPUTING},
  title                = {Predicting Personalized Image Emotion Perceptions in Social Networks},
  year                 = {2018},
  issn                 = {1949-3045},
  month                = {OCT-DEC},
  number               = {4},
  pages                = {526-540},
  volume               = {9},
  bdsk-url-1           = {https://doi.org/10.1109/TAFFC.2016.2628787},
  comment              = {image emotion classification},
  doi                  = {10.1109/TAFFC.2016.2628787},
  hasabstract          = {N},
  modificationdate     = {2024-05-16T06:24:00},
  orcid-numbers        = {Tang, Youbao/0000-0001-8719-3375},
  priority             = {prio3},
  researcherid-numbers = {Gao, Yue/B-3376-2012 Han, Jungong/ABE-6812-2020 Ding, Guiguang/KIL-3528-2024 Xin, Zhao/AFZ-5025-2022},
  unique-id            = {WOS:000451918200010},
}

@Article{thao2021at,
  author               = {Thao, Ha Thi Phuong and Balamurali, B. T. and Roig, Gemma and Herremans, Dorien},
  journal              = {SENSORS},
  title                = {AttendAffectNet-Emotion Prediction of Movie Viewers Using Multimodal Fusion with Self-Attention},
  year                 = {2021},
  month                = {DEC},
  number               = {24},
  volume               = {21},
  article-number       = {8356},
  bdsk-url-1           = {https://doi.org/10.3390/s21248356},
  doi                  = {10.3390/s21248356},
  eissn                = {1424-8220},
  hasabstract          = {N},
  orcid-numbers        = {Herremans, Dorien/0000-0001-8607-1640 Roig, Gemma/0000-0002-6439-8076 B T, Balamurali/0000-0002-8523-6607},
  priority             = {prio3},
  researcherid-numbers = {Herremans, Dorien/G-9599-2018},
  unique-id            = {WOS:000737284600001},
}

@Article{di2015em,
  author      = {Di, Guo-Qing and Wu, Si-Xia},
  journal     = {JOURNAL OF THE ACOUSTICAL SOCIETY OF AMERICA},
  title       = {Emotion recognition from sound stimuli based on back-propagation neural networks and electroencephalograms},
  year        = {2015},
  issn        = {0001-4966},
  month       = {AUG},
  number      = {2},
  pages       = {994-1002},
  volume      = {138},
  bdsk-url-1  = {https://doi.org/10.1121/1.4927693},
  doi         = {10.1121/1.4927693},
  eissn       = {1520-8524},
  hasabstract = {N},
  priority    = {prio3},
  unique-id   = {WOS:000360652900053},
}

@Article{drossos2015in,
  author               = {Drossos, Konstantinos and Floros, Andreas and Giannakoulopoulos, Andreas and Kanellopoulos, Nikolaos},
  journal              = {IEEE TRANSACTIONS ON AFFECTIVE COMPUTING},
  title                = {Investigating the Impact of Sound Angular Position on the Listener Affective State},
  year                 = {2015},
  issn                 = {1949-3045},
  month                = {JAN-MAR},
  number               = {1},
  pages                = {27-42},
  volume               = {6},
  bdsk-url-1           = {https://doi.org/10.1109/TAFFC.2015.2392768},
  comment              = {nonmusical sounds},
  doi                  = {10.1109/TAFFC.2015.2392768},
  hasabstract          = {N},
  modificationdate     = {2024-05-16T06:24:56},
  orcid-numbers        = {Giannakoulopoulos, Andreas/0000-0002-0456-2217 Drosos, Konstantinos/0000-0002-3605-7127},
  priority             = {prio3},
  researcherid-numbers = {Drossos, Konstantinos/I-8305-2015 Floros, Andreas AF/F-1478-2019 Giannakoulopoulos, Andreas/AAM-5795-2021},
  unique-id            = {WOS:000350741300003},
}

@Article{tawdrous2022em,
  author               = {Tawdrous, Marina M. and D'Onofrio, Kristen L. and Gifford, Rene and Picou, Erin M.},
  journal              = {TRENDS IN HEARING},
  title                = {Emotional Responses to Non-Speech Sounds for Hearing-aid and Bimodal Cochlear-Implant Listeners},
  year                 = {2022},
  issn                 = {2331-2165},
  month                = {APR},
  volume               = {26},
  article-number       = {23312165221083091},
  bdsk-url-1           = {https://doi.org/10.1177/23312165221083091},
  doi                  = {10.1177/23312165221083091},
  hasabstract          = {N},
  orcid-numbers        = {Picou, Erin/0000-0003-3083-0809 Tawdrous, Marina/0000-0001-9026-6803},
  priority             = {prio3},
  researcherid-numbers = {Picou, Erin/J-4563-2019},
  unique-id            = {WOS:000783983800001},
}

@Article{kim2018ad,
  author           = {Kim, Byoungjun and Lee, Joonwhoan},
  journal          = {INTERNATIONAL JOURNAL OF FUZZY LOGIC AND INTELLIGENT SYSTEMS},
  title            = {A Deep-Learning Based Model for Emotional Evaluation of Video Clips},
  year             = {2018},
  issn             = {1598-2645},
  month            = {DEC 25},
  number           = {4},
  pages            = {245-253},
  volume           = {18},
  bdsk-url-1       = {https://doi.org/10.5391/IJFIS.2018.18.4.245},
  comment          = {video emotion recognition},
  doi              = {10.5391/IJFIS.2018.18.4.245},
  eissn            = {2093-744X},
  hasabstract      = {N},
  modificationdate = {2024-05-16T06:25:38},
  priority         = {prio3},
  unique-id        = {WOS:000455340300003},
}

@Article{cavallo2021mo,
  author               = {Cavallo, Filippo and Semeraro, Francesco and Mancioppi, Gianmaria and Betti, Stefano and Fiorini, Laura},
  journal              = {JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING},
  title                = {Mood classification through physiological parameters},
  year                 = {2021},
  issn                 = {1868-5137},
  month                = {APR},
  number               = {4},
  pages                = {4471-4484},
  volume               = {12},
  bdsk-url-1           = {https://doi.org/10.1007/s12652-019-01595-6},
  doi                  = {10.1007/s12652-019-01595-6},
  earlyaccessdate      = {DEC 2019},
  eissn                = {1868-5145},
  hasabstract          = {N},
  orcid-numbers        = {Semeraro, Francesco/0000-0002-8812-0968 Fiorini, Laura/0000-0001-5784-3752 Mancioppi, Gianmaria/0000-0001-8109-7956},
  priority             = {prio3},
  researcherid-numbers = {Semeraro, Francesco/GMX-3193-2022},
  unique-id            = {WOS:000574443200001},
}

@Article{zhao2019af,
  author           = {Zhao, Sicheng and Wang, Shangfei and Soleymani, Mohammad and Joshi, Dhiraj and Ji, Qiang},
  journal          = {ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS},
  title            = {Affective Computing for Large-scale Heterogeneous Multimedia Data: A Survey},
  year             = {2019},
  issn             = {1551-6857},
  number           = {3, S, SI},
  volume           = {15},
  article-number   = {93},
  bdsk-url-1       = {https://doi.org/10.1145/3363560},
  comment          = {survey},
  doi              = {10.1145/3363560},
  eissn            = {1551-6865},
  hasabstract      = {N},
  modificationdate = {2024-05-16T06:26:26},
  priority         = {prio3},
  unique-id        = {WOS:000535718800009},
}

@Article{battcock2021in,
  author          = {Battcock, Aimee and Schutz, Michael},
  journal         = {JOURNAL OF NEW MUSIC RESEARCH},
  title           = {Individualized interpretation: Exploring structural and interpretive effects on evaluations of emotional content in Bach's Well Tempered Clavier},
  year            = {2021},
  issn            = {0929-8215},
  month           = {OCT 20},
  number          = {5},
  pages           = {447-468},
  volume          = {50},
  bdsk-url-1      = {https://doi.org/10.1080/09298215.2021.1979050},
  doi             = {10.1080/09298215.2021.1979050},
  earlyaccessdate = {JAN 2022},
  eissn           = {1744-5027},
  hasabstract     = {N},
  priority        = {prio1},
  unique-id       = {WOS:000748818300001},
}

@Article{castro2021mo,
  author           = {Castro, Candela and Diaz Abrahan, Veronika and Justel, Nadia},
  journal          = {INTERDISCIPLINARIA},
  title            = {Modulation of mood by activating musical stimuli. An experimental design with young adults},
  year             = {2021},
  issn             = {1668-7027},
  month            = {JAN-APR},
  number           = {1},
  pages            = {41-51},
  volume           = {38},
  bdsk-url-1       = {https://doi.org/10.16888/interd.2021.38.1.3},
  doi              = {10.16888/interd.2021.38.1.3},
  hasabstract      = {N},
  modificationdate = {2024-05-16T05:40:21},
  orcid-numbers    = {Diaz Abrahan, Veronika/0000-0001-5003-4274 Castro, Candela/0000-0002-5564-1847},
  priority         = {prio3},
  unique-id        = {WOS:000609634200003},
}

@Article{gupta2016re,
  author               = {Gupta, Rishabh and Laghari, Khalil ur Rehman and Falk, Tiago H.},
  journal              = {NEUROCOMPUTING},
  title                = {Relevance vector classifier decision fusion and EEG graph-theoretic features for automatic affective state characterization},
  year                 = {2016},
  issn                 = {0925-2312},
  month                = {JAN 22},
  number               = {B},
  pages                = {875-884},
  volume               = {174},
  bdsk-url-1           = {https://doi.org/10.1016/j.neucom.2015.09.085},
  doi                  = {10.1016/j.neucom.2015.09.085},
  eissn                = {1872-8286},
  hasabstract          = {N},
  orcid-numbers        = {Falk, Tiago/0000-0002-5739-2514},
  priority             = {prio3},
  researcherid-numbers = {Falk, Tiago/IQW-2566-2023},
  unique-id            = {WOS:000367276900032},
}

@Article{xu2018us,
  author               = {Xu, Na and Guo, Gang and Lai, Han and Chen, Hao},
  journal              = {INTERNATIONAL JOURNAL OF HUMAN-COMPUTER INTERACTION},
  title                = {Usability Study of Two In-Vehicle Information Systems Using Finger Tracking and Facial Expression Recognition Technology},
  year                 = {2018},
  issn                 = {1044-7318},
  number               = {11},
  pages                = {1032-1044},
  volume               = {34},
  bdsk-url-1           = {https://doi.org/10.1080/10447318.2017.1411674},
  doi                  = {10.1080/10447318.2017.1411674},
  eissn                = {1532-7590},
  hasabstract          = {N},
  orcid-numbers        = {Xu, Na/0000-0002-7081-3360},
  priority             = {prio3},
  researcherid-numbers = {guo, gang/IAO-4426-2023},
  unique-id            = {WOS:000442685600005},
}

@Article{balan2019fe,
  author               = {Balan, Oana and Moise, Gabriela and Moldoveanu, Alin and Leordeanu, Marius and Moldoveanu, Florica},
  journal              = {SENSORS},
  title                = {Fear Level Classification Based on Emotional Dimensions and Machine Learning Techniques},
  year                 = {2019},
  month                = {APR 1},
  number               = {7},
  volume               = {19},
  article-number       = {1738},
  bdsk-url-1           = {https://doi.org/10.3390/s19071738},
  comment              = {EEG, physiological},
  doi                  = {10.3390/s19071738},
  eissn                = {1424-8220},
  hasabstract          = {N},
  modificationdate     = {2024-05-16T06:27:40},
  orcid-numbers        = {Mitrut, Oana/0000-0002-6822-2684 Moldoveanu, Florica/0000-0002-8357-5840 Alin, Moldoveanu/0000-0002-1368-7249 Moise, Gabriela/0000-0002-3842-9828},
  priority             = {prio3},
  researcherid-numbers = {Mitrut, Oana/O-6319-2019 Moldoveanu, Florica/AAL-4278-2020 Alin, Moldoveanu/P-6958-2018 Moise, Gabriela/B-1609-2010},
  unique-id            = {WOS:000465570700257},
}

@Article{shen2019ch,
  author         = {Shen, Yi-Wei and Lin, Yuan-Pin},
  journal        = {FRONTIERS IN HUMAN NEUROSCIENCE},
  title          = {Challenge for Affective Brain-Computer Interfaces: Non-stationary Spatio-spectral EEG Oscillations of Emotional Responses},
  year           = {2019},
  issn           = {1662-5161},
  month          = {OCT 30},
  volume         = {13},
  article-number = {366},
  bdsk-url-1     = {https://doi.org/10.3389/fnhum.2019.00366},
  doi            = {10.3389/fnhum.2019.00366},
  hasabstract    = {N},
  priority       = {prio3},
  unique-id      = {WOS:000497458200001},
}

@Article{feradov2020ev,
  author               = {Feradov, Firgan and Mporas, Iosif and Ganchev, Todor},
  journal              = {COMPUTERS},
  title                = {Evaluation of Features in Detection of Dislike Responses to Audio-Visual Stimuli from EEG Signals},
  year                 = {2020},
  issn                 = {2073-431X},
  month                = {JUN},
  number               = {2},
  volume               = {9},
  article-number       = {33},
  bdsk-url-1           = {https://doi.org/10.3390/computers9020033},
  doi                  = {10.3390/computers9020033},
  hasabstract          = {N},
  orcid-numbers        = {Mporas, Iosif/0000-0001-6984-0268 Ganchev, Todor/0000-0003-0384-4033},
  priority             = {prio3},
  researcherid-numbers = {Mporas, Iosif/AAK-2346-2020 Ganchev, Todor/A-1915-2017},
  unique-id            = {WOS:000551218000028},
}

@Article{moghimi2016in,
  author               = {Moghimi, Mohammadhossein and Stone, Robert and Rotshtein, Pia and Cooke, Neil},
  journal              = {PRESENCE-VIRTUAL AND AUGMENTED REALITY},
  title                = {Influencing Human Affective Responses to Dynamic Virtual Environments},
  year                 = {2016},
  month                = {SPR},
  number               = {2},
  pages                = {81-107},
  volume               = {25},
  bdsk-url-1           = {https://doi.org/10.1162/PRES%5C_a%5C_00249},
  comment              = {vr},
  doi                  = {10.1162/PRES\_a\_00249},
  eissn                = {1531-3263},
  hasabstract          = {N},
  modificationdate     = {2024-05-16T06:29:26},
  orcid-numbers        = {COOKE, NEIL/0000-0003-2247-0663 Rotshtein, Pia/0000-0002-0301-9511},
  priority             = {prio3},
  researcherid-numbers = {COOKE, NEIL/AAI-1080-2021},
  unique-id            = {WOS:000388476500001},
}

@Article{bahadori2021ac,
  author               = {Bahadori, Mehrdad and Barumerli, Roberto and Geronazzo, Michele and Cesari, Paola},
  journal              = {NEUROPSYCHOLOGIA},
  title                = {Action planning and affective states within the auditory peripersonal space in normal hearing and cochlear-implanted listeners},
  year                 = {2021},
  issn                 = {0028-3932},
  month                = {MAY 14},
  volume               = {155},
  article-number       = {107790},
  bdsk-url-1           = {https://doi.org/10.1016/j.neuropsychologia.2021.107790},
  doi                  = {10.1016/j.neuropsychologia.2021.107790},
  earlyaccessdate      = {MAR 2021},
  eissn                = {1873-3514},
  hasabstract          = {N},
  orcid-numbers        = {Bahadori, Mehrdad/0000-0001-7604-8682 Geronazzo, Michele/0000-0002-0621-2704},
  priority             = {prio3},
  researcherid-numbers = {Geronazzo, Michele/U-8886-2017},
  unique-id            = {WOS:000647671000004},
}

@Article{lopes2019mo,
  author               = {Lopes, Phil and Liapis, Antonios and Yannakakis, Georgios N.},
  journal              = {IEEE TRANSACTIONS ON AFFECTIVE COMPUTING},
  title                = {Modelling Affect for Horror Soundscapes},
  year                 = {2019},
  issn                 = {1949-3045},
  month                = {APR-JUN},
  number               = {2},
  pages                = {209-222},
  volume               = {10},
  bdsk-url-1           = {https://doi.org/10.1109/TAFFC.2017.2695460},
  comment              = {soundscapes},
  doi                  = {10.1109/TAFFC.2017.2695460},
  hasabstract          = {N},
  modificationdate     = {2024-05-16T06:29:53},
  orcid-numbers        = {Lopes, Phil/0000-0002-9567-5806 Yannakakis, Georgios/0000-0003-2728-4026 Yannakakis, Georgios N./0000-0001-7793-1450},
  priority             = {prio3},
  researcherid-numbers = {Lopes, Phil/AAB-3239-2022 Yannakakis, Georgios N./R-9213-2016},
  unique-id            = {WOS:000470020700006},
}

@Article{alqazzaz2021co,
  author         = {Al-Qazzaz, Noor Kamal and Sabir, Mohannad K. and Ali, Sawal Hamid Bin Mohd and Ahmad, Siti Anom and Grammer, Karl},
  journal        = {JOURNAL OF HEALTHCARE ENGINEERING},
  title          = {Complexity and Entropy Analysis to Improve Gender Identification from Emotional-Based EEGs},
  year           = {2021},
  issn           = {2040-2295},
  month          = {SEP 22},
  volume         = {2021},
  article-number = {8537000},
  bdsk-url-1     = {https://doi.org/10.1155/2021/8537000},
  doi            = {10.1155/2021/8537000},
  eissn          = {2040-2309},
  hasabstract    = {N},
  orcid-numbers  = {Al-Qazzaz, Noor Kamal/0000-0001-6566-3452 Al-Lami, mohannad kadhim Sabir/0000-0001-7733-6168},
  priority       = {prio3},
  unique-id      = {WOS:000704323500003},
}

@Article{christensen2014an,
  author               = {Christensen, Julia F. and Nadal, Marcos and Jose Cela-Conde, Camilo and Gomila, Antoni},
  journal              = {PERCEPTION},
  title                = {A norming study and library of 203 dance movements},
  year                 = {2014},
  issn                 = {0301-0066},
  number               = {2-3},
  pages                = {178-206},
  volume               = {43},
  bdsk-url-1           = {https://doi.org/10.1068/p7581},
  doi                  = {10.1068/p7581},
  eissn                = {1468-4233},
  hasabstract          = {N},
  orcid-numbers        = {Christensen, Julia F./0000-0003-0381-5101 Nadal, Marcos/0000-0002-9341-4688 /0000-0001-8350-7728},
  priority             = {prio3},
  researcherid-numbers = {Christensen, Julia F./AAK-4477-2021 Nadal, Marcos/A-5817-2009 Cela-Conde, Camilo J/J-7806-2015 Gomila, Antoni/I-1342-2012},
  unique-id            = {WOS:000336353100007},
}

﻿Scopus
EXPORT DATE: 13 May 2024

@Article{jakubowski2023di,
  author            = {Jakubowski, Kelly and Francini, Emma},
  title             = {Differential effects of familiarity and emotional expression of musical cues on autobiographical memory properties},
  note              = {Cited by: 4; All Open Access, Green Open Access, Hybrid Gold Open Access},
  number            = {9},
  pages             = {2001 – 2016},
  volume            = {76},
  abstract          = {Features of visual cues, such as their familiarity and emotionality, influence the quantity and qualities of the autobiographical memories they evoke. Despite increasing use in autobiographical memory research, comparatively little is known about how such features of musical cues influence memory properties. In a repeated-measures design, we presented 24 musical cues selected to vary on their familiarity (high/low), emotional valence (positive/negative), and emotional arousal (high/low) to 100 young adults, who recorded details of any autobiographical memories that were evoked. Familiarity of the music primarily impacted memory accessibility, with high-familiarity music evoking more memories that were retrieved more quickly. More familiar music also elicited more positive and arousing memories; however, these differences were found to be attributed to greater liking of the high-familiarity music. The emotional expression of the music impacted the emotionality and evaluation of the memories, with negative valence/low-arousal (e.g., “sad”) music evoking the most negative memories, high-arousal and positively valenced music evoking more arousing memories, and low-arousal music evoking memories rated as more important. These results provide important insights for developing effective paradigms for triggering (particular types of) autobiographical memories via music and highlight the need to critically consider potential differences in cue familiarity and emotionality in studies comparing musical with non-musical cues. Future research should extend this approach to other cue types (e.g., visual, olfactory, other auditory cues), to probe how familiarity and emotional qualities of cues conjunctively or interactively constrain autobiographical memory recall across different domains. © Experimental Psychology Society 2022.},
  author_keywords   = {Autobiographical memory; emotion; familiarity; music-evoked autobiographical memory; retrieval cues},
  comment           = {emotion as IV},
  year              = {2023},
  doi               = {10.1177/17470218221129793},
  hasabstract       = {Y},
  journaltitle      = {Quarterly Journal of Experimental Psychology},
  keywords          = {Cues; Emotions; Humans; Memory, Episodic; Mental Recall; Recognition, Psychology; Young Adult; association; emotion; episodic memory; human; physiology; recall; recognition; young adult},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140711815&doi=10.1177%2f17470218221129793&partnerID=40&md5=1b31103422267888c9a6f5ea24d375c6},
}

@Article{ng2023em,
  author            = {Ng, Kok-Why and Lim, Yixen and Haw, Su-Cheng and Yoong, Yih-Jian},
  title             = {Emotion Recognition on Facial Expression and Voice: Analysis and Discussion},
  note              = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
  number            = {5},
  pages             = {1703 – 1709},
  volume            = {13},
  abstract          = {Emotion plays an important role in our daily lives. Emotional individuals can affect the performance of a company, the harmony of a family, the wellness or growth (physical, mental, and spiritual) of a child etc. It renders a wide range of impacts. The existing works on emotion detection from facial expressions differ from the voice. It is deduced that the facial expression is captured on the face externally, whereas the voice is captured from the air passes through the vocal folds internally. Both captured output models may very much deviate from each other. This paper studies and analyses a person's emotion through dual models - facial expression and voice separately. The proposed algorithm uses a Convolutional Neural Network (CNN) with 2-dimensions convolutional layers for facial expression and 1-Dimension convolutional layers for voice. Feature extraction is done via face detection, and Mel-Spectrogram extraction is done via voice. The network layers are fine-tuned to achieve the higher performance of the CNN model. The trained CNN models can recognize emotions from the input videos, which may cover single or multiple emotions from the facial expression and voice perspective. The experimented videos are clean from the background music and environment noise and contain only a person's voice. The proposed algorithm achieved an accuracy of 62.9% through facial expression and 82.3% through voice. © IJASEIT is licensed under a Creative Commons Attribution-Share Alike 4.0 International License.},
  author_keywords   = {convolutional neural network; Emotion recognition; facial expression; Mel-spectrogram; voice},
  comment           = {no music, face recognition},
  year              = {2023},
  doi               = {10.18517/ijaseit.13.5.19023},
  hasabstract       = {Y},
  journaltitle      = {International Journal on Advanced Science, Engineering and Information Technology},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175050710&doi=10.18517%2fijaseit.13.5.19023&partnerID=40&md5=3059978f5adc8f6d4985c501529514cc},
}

@Article{hao2022re,
  author            = {Hao, Wang and Yuanchen, Liu and Meng, Zhao and Jingwen, Qiu},
  title             = {Research on Multi-task Music Emotion Recognition Based on Multi-modal Features; [基于多模态特征的音乐情感多任务识别研究]},
  note              = {Cited by: 1},
  number            = {11},
  pages             = {61 – 75},
  volume            = {42},
  abstract          = {[Purpose/ Significance] Emotion is one of the common ways to organize and retrieve resources on online music platforms. Exploration and research on emotion classification of song lists and songs using feature fusion can optimize management and utilization of music resources, better meeting the demand of internet users for music culture life. [Meth⁃ od/ Process] In this paper, Hevner's music emotion model was introduced to build an emotion lexicon, which was used with names and introduction of song lists to classify emotions of large-grained lists. Multimodal features of lyrics and audios were fused to identify emotions of small-grained songs through pre-trained model's semantic representation and audio signal processing. [Result/ Conclusion] Introduction of emotion lexicon effectively improves the accuracy of song list emotion clas⁃ sification, and manual preprocessing can help algorithms learn features better. Lyrics and audios both contain rich emotion information and the multimodal fusion model performs best in emotion recognition of songs. © 2022 Editorial Board of Journal of Modern Information. All rights reserved.},
  author_keywords   = {emotion classification of song lists; Mel spectro⁃ gram; multimodal fusion; music emotion classification; NetEase cloud music},
  year              = {2022},
  doi               = {10.3969/j.issn.1008-0821.2022.11.006},
  hasabstract       = {Y},
  journaltitle      = {Journal of Modern Information},
  priority          = {prio1},
  publication_stage = {Final},
  ranking           = {rank2},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174213057&doi=10.3969%2fj.issn.1008-0821.2022.11.006&partnerID=40&md5=866512162b3b137dba6e25cf334ecd62},
}

@Article{du2023au,
  author            = {Du, Hanfeng},
  title             = {AUDIOVISUAL EFFECT OF MUSIC AND CULTURAL PROGRAMS IN MASS CULTURAL ACTIVITIES ASSISTED BY INTELLIGENT DEVICES},
  note              = {Cited by: 0},
  number            = {2},
  pages             = {259 – 277},
  volume            = {15},
  abstract          = {Music is the carrier through which human beings express their emotions. It can clean up their hearts and seek emotional resonance. The combination of music and artificial intelligence, when music meets artificial intelligence, the mathematical logic part of data and algorithm replaces the image thinking, resulting in automatic music production. The basic principle of music creation is to use artificial intelligence technology to conduct in-depth training on a large number of songs, and then build a database. Then, within a certain period of time, extract a segment from several songs, and then decompose it to form a new melody. The automatic music production is the collision of music and technology. With the flying of code and notes, it creates a different kind of beauty. Music culture is a new field of music application. With its refreshing audio-visual effect, it has made people have a new understanding of music and has become a hot topic in current mass cultural activities. The sound quality characteristics of music culture programs determine the characteristics of simple, fast and efficient design. However, the current audio and video design still stays in the traditional audio and video form design, with low design efficiency, long cycle, high cost and high requirements for designers. In view of the fact that the traditional audio and video design mode in China's music and cultural programs cannot meet the needs of performance programs at present, this paper proposes a design scheme of audio and visual effects of music and cultural programs with intelligent devices as the core, and develops a complete set of audio and visual effects aided design system on this basis. © (2023), (European Journal for Philosophy of Religion). All Rights Reserved.},
  author_keywords   = {Audiovisual Effect; Feature Recognition; Intelligent Device Assistance; Music Programs},
  comment           = {no emotion},
  year              = {2023},
  doi               = {10.24204/ejpr.2023.4154},
  hasabstract       = {Y},
  journaltitle      = {European Journal for Philosophy of Religion},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188216095&doi=10.24204%2fejpr.2023.4154&partnerID=40&md5=1eba6f069c6af73bea58d27c02c55eb2},
}

@Article{li2022re,
  author            = {Li, Yu and Chen, Yao},
  title             = {Research on Chorus Emotion Recognition and Intelligent Medical Application Based on Health Big Data},
  note              = {Cited by: 1; All Open Access, Gold Open Access, Green Open Access},
  volume            = {2022},
  abstract          = {In chorus activities, the conductor leads chorus members to recreate music works. If you want to interpret music works perfectly with sound, emotion and emotional expression are particularly important. In this paper, a cloud HBD (health big data) integration system based on ensemble learning is designed to realize the high-efficiency and high-precision integration of HBD. An emotional speech database containing three emotions such as pleasure, calmness, and boredom is established, and the corpus problems such as emotional feature analysis and extraction needed for chorus emotion recognition research are solved. It also studies the classification and decision-making in emotional changes, and a DBN (deep belief network) chorus emotion recognition algorithm based on multiple emotional features is proposed. Feature DBN (Deep Belief Network) Chorus Emotion Recognition Algorithm This paper extracts various robust low-level features according to different features' ability to describe emotions and then feeds them into the DBN network to extract high-level feature descriptors. Then, the classification results of ELM (extreme learning machine) are voted and fused with the idea of ensemble learning, and the effectiveness of the algorithm is proved on three public datasets.  © 2022 Yu Li and Yao Chen.},
  comment           = {health data},
  year              = {2022},
  doi               = {10.1155/2022/1363690},
  hasabstract       = {Y},
  journaltitle      = {Journal of Healthcare Engineering},
  keywords          = {Algorithms; Big Data; Emotions; Humans; Music; Speech; Big data; Classification (of information); Decision making; Machine learning; Medical applications; Data integration system; Deep belief networks; Emotion expression; Emotion recognition; Emotional expressions; Ensemble learning; High-precision; Higher efficiency; Precision integration; Recognition algorithm; algorithm; article; big data; boredom; decision making; deep belief network; emotion; extraction; human; human experiment; learning; machine learning; pleasure; speech; algorithm; emotion; music; Speech recognition},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126867615&doi=10.1155%2f2022%2f1363690&partnerID=40&md5=b54742ce069c4c966e83f3d14848e5b3},
}

@Article{deng2024an,
  author            = {Deng, Ya and Lin, Na},
  title             = {Analysis and Expression of Music Emotion Based on CAD and Deep Reinforcement Learning Algorithm},
  note              = {Cited by: 0; All Open Access, Bronze Open Access},
  number            = {S23},
  pages             = {19 – 34},
  volume            = {21},
  abstract          = {This article seeks to introduce a fresh perspective and approach to music emotion analysis by proposing a novel algorithm that integrates Deep Reinforcement Learning (DRL). The algorithm leverages the capabilities of deep neural networks to discern emotional characteristics within music autonomously. By utilizing the reinforcement learning framework, the decision-making process of the model is refined, enabling more precise identification of subtle emotional nuances in music. Experimental findings reveal that this algorithm significantly outperforms traditional methods in music emotion analysis, demonstrating a notable enhancement in detection accuracy. The key advantage of this algorithm lies in its ability to circumvent the intricacies and uncertainties associated with manual feature extraction. Furthermore, it exhibits superior adaptability to intricate music emotion analysis tasks, effectively elevating the accuracy and efficacy of classification. Following extensive training iterations, the model demonstrates a remarkable capacity to swiftly accommodate new data distributions and emotional expression patterns. In conclusion, the DRL-based music emotion analysis algorithm presented in this article contributes innovative research concepts and methodologies to the field and holds substantial theoretical and practical significance. © 2024 U-turn Press LLC.},
  author_keywords   = {CAD; Deep Reinforcement Learning; Music Emotion Analysis},
  year              = {2024},
  doi               = {10.14733/cadaps.2024.S23.19-34},
  hasabstract       = {Y},
  journaltitle      = {Computer-Aided Design and Applications},
  keywords          = {Computer aided design; Decision making; Deep neural networks; Emotion Recognition; Learning algorithms; Music; Decision-making process; Deep reinforcement learning; Detection accuracy; Emotion analysis; Learning frameworks; Music emotion analyse; Music emotions; Novel algorithm; Reinforcement learning algorithms; Reinforcement learnings; Reinforcement learning},
  priority          = {prio1},
  publication_stage = {Final},
  ranking           = {rank2},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188622045&doi=10.14733%2fcadaps.2024.S23.19-34&partnerID=40&md5=8655f1e08bc6af376ed60d18654e2591},
}

@Article{luo2022wa,
  author            = {Luo, Chengping},
  title             = {Waveform Feature Extraction of Intelligent Singing Skills under the Background of Internet of Things},
  note              = {Cited by: 0; All Open Access, Gold Open Access},
  volume            = {2022},
  abstract          = {Aiming at the complex and changeable characteristics of intelligent singing skills in the context of Internet of Things, this paper proposes a feature extraction method suitable for intelligent singing skills in this context. Firstly, focusing on vocal features, the time-domain algorithm based on open-loop and closed-loop gene extraction extracts the genetic features of songs with accompaniment; then, the section and its features are extracted by using the windowed moving matching algorithm, and the segments are divided by using the similarity between adjacent segments to obtain the segment features with emotional factors. The segment features are input into the improved BP emotion recognizer for emotion recognition. Finally, the intelligent singing skills of the whole music are determined. The experimental results show that, with the increase in feature extraction time, the accuracy of the extraction results of the existing methods changes little, which is basically maintained at a low level between 15% and 30%. When the proposed method is for feature extraction of intelligent singing skill information, the accuracy shows a continuous growth trend, and with the growth of time, its accuracy is significantly higher than the existing methods, indicating that the proposed method has significant advantages in the accuracy of feature extraction. Because this waveform feature extraction method is applied to the intelligent singing skills under the background of the Internet of Things, it has the advantages of high extraction efficiency, high accuracy, and reliability.  © 2022 Chengping Luo.},
  year              = {2022},
  doi               = {10.1155/2022/4638801},
  hasabstract       = {Y},
  journaltitle      = {Mobile Information Systems},
  keywords          = {Emotion Recognition; Extraction; Internet of things; Music; Time domain analysis; Closed-loop; Emotion recognition; Emotional factors; Extraction time; Feature extraction methods; Features extraction; Matching algorithm; Open-loop; Time-domain algorithm; Waveform features; Feature extraction},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133974440&doi=10.1155%2f2022%2f4638801&partnerID=40&md5=abf47112d02a52aba439fd91540b6f1e},
}

@Article{shen2024re,
  author            = {Shen, Dan and Zhao, Wenjia},
  title             = {Research on the Construction of Music Animation CAD System Based on Music Form and Emotion Recognition},
  note              = {Cited by: 0; All Open Access, Bronze Open Access},
  number            = {S1},
  pages             = {204 – 217},
  volume            = {21},
  abstract          = {Music, as a unique artistic form of people’s emotional expression, focuses on a rendering of the soul of the music audience and a re-experience of the virtual reality world. This experience is all-round, not just limited to hearing. In the process of music visualization, it is difficult to mechanically transform it into vision by a single rule. How to enhance the musical form and emotional characteristics of visual music works through multi-channel mapping mode is the research purpose of this paper. This article proposes a music based emotion recognition model feature, and a musical animation CAD (Computer Aided Design) system is constructed. The system extracts some basic musical features from MIDI (Musical Instrument Digital Interface) files, and then extracts the musical features of the music. Thus, each music segment is sliced into emotional music visualization programs. And by summarizing the obtained segment features as a whole, design an emotional feature program that can reflect the musical form. Through deep learning algorithms, it visualizes and matches the improved segment nodes, improving the expressive form of music emotional animation. © 2024, CAD Solutions, LLC. All rights reserved.},
  author_keywords   = {Computer Aided Design; Deep Learning; Emotional Recognition; Music Animation; Musical Features; Visualization},
  year              = {2024},
  doi               = {10.14733/cadaps.2024.S1.204-217},
  hasabstract       = {Y},
  journaltitle      = {Computer-Aided Design and Applications},
  keywords          = {Animation; Audio acoustics; Audition; Computer aided design; Computer aided instruction; Deep learning; Emotion Recognition; Learning algorithms; Learning systems; Speech recognition; Virtual reality; Visualization; Computer aided design systems; Computer-aided design; Deep learning; Emotion recognition; Emotional expressions; Emotional recognition; Form recognition; Music animation; Music visualization; Musical features; Music},
  priority          = {prio1},
  publication_stage = {Final},
  ranking           = {rank2},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169049920&doi=10.14733%2fcadaps.2024.S1.204-217&partnerID=40&md5=34fec38348b00e63cecb493ef6e85bad},
}

@Article{song2024de,
  author            = {Song, Li},
  title             = {Design and Implementation of Remote Piano Teaching Based on Attention-Induced Multi-Head Convolutional Neural Network Optimized with Hunter–Prey Optimization},
  note              = {Cited by: 1; All Open Access, Gold Open Access},
  number            = {1},
  volume            = {17},
  abstract          = {The continuous progress of multimedia technology in music educational institutions has led to the recognition of its importance in our country and society. The traditional approach to piano teaching has its limitations, which can be overcome by adopting alternative approaches to the instrument, using advances in science and technology. For pianist, expressing emotions and thoughts through music is crucial, and teachers can now use multimedia tools to exemplify their musical skills to students effectively. This manuscript proposes the Remote Piano Teaching Based on Attention-Induced Multi-Head Convolutional Neural Network Optimized with Hunter–Prey Optimization to improve the piano-teaching quality. At first, input data is taken from Piano Triad Wavset dataset. Afterward, the data are fed to preprocessing stage. The preprocessing stage involve data cleaning or scrubbing that is the process of identifying errors, inconsistencies, and incorrectness in a dataset with the help of adaptive distorted Gaussian matched filter. Then, the preprocessed output is fed to Attention-Induced Multi-Head Convolutional Neural Network (AIMCNN) for effectively predict the piano-teaching quality. The hunter–prey optimization (HPO) algorithm is proposed to optimize the parameters of Attention-Induced Multi-Head Convolutional Neural Network. The performance of the proposed technique is evaluated under performance metrics like accuracy, computational time, learning skill analysis, learning activity analysis, learning behavior analysis; student performance ratio and teaching evaluation analysis are evaluated. The proposed RPT-AIMCNN-HPO attains better prediction accuracy 12.566%, 12.075% and 15.993%, higher learning skill 15.86%, 15.26% and 16.25% compared with existing methods. © 2023, The Author(s).},
  author_keywords   = {Adaptive distorted Gaussian matched filter Attention-Induced Multi-Head Convolutional Neural Network; Hunter–prey optimization; Piano Triads Wavset dataset; Remote piano teaching},
  comment           = {no emotion measurement},
  year              = {2024},
  doi               = {10.1007/s44196-023-00379-3},
  hasabstract       = {Y},
  journaltitle      = {International Journal of Computational Intelligence Systems},
  keywords          = {Adaptive filtering; Adaptive filters; Convolution; Convolutional neural networks; Multimedia systems; Music; Musical instruments; Students; Adaptive distorted gaussian matched filter attention-induced multi-head convolutional neural network; Convolutional neural network; Design and implementations; Gaussians; Hunter–prey optimization; Learning skills; Optimisations; Piano triad wavset dataset; Remote piano teaching; Teaching quality; Matched filters},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181247907&doi=10.1007%2fs44196-023-00379-3&partnerID=40&md5=129becf4522cda405212ccfa35c7c53a},
}

@Article{bai2022op,
  author            = {Bai, Jie},
  title             = {Optimized Piano Music Education Model Based on Multimodal Information Fusion for Emotion Recognition in Multimedia Video Networks},
  note              = {Cited by: 1; All Open Access, Gold Open Access},
  volume            = {2022},
  abstract          = {Emotion is the important information that people transmit in the process of communication, and the change of emotional state affects people's perception and decision-making, which introduces the emotional dimension into human-computer interaction. The modes of emotional expression include facial expressions, speech, posture, physiological signals, text, and so on. Emotion recognition is essentially a multimodal fusion problem. This paper investigates the different teaching modes of the teachers and students of our school, designs the load capacity through the K-means algorithm, builds a multimedia network sharing classroom, and creates a piano music situation to stimulate students' learning interest, using audiovisual and other tools to mobilize students' emotions, using multimedia guidance to extend students' piano music knowledge, and comprehensively improve students' aesthetic ability and autonomous learning ability. Comparing the changes of students after 3 months of teaching, the results of the study found that multimedia sharing classrooms can be up to 50% ahead of traditional teaching methods in enhancing students' interest, and teachers' acceptance of multimedia network sharing classrooms is also high.  © 2022 Jie Bai.},
  year              = {2022},
  doi               = {10.1155/2022/1882739},
  hasabstract       = {Y},
  journaltitle      = {Mobile Information Systems},
  keywords          = {Behavioral research; Decision making; Emotion Recognition; Human computer interaction; K-means clustering; Speech recognition; Emotion recognition; Emotional state; Model-based OPC; Multimedia networks; Multimedia video; Multimodal information fusion; Music education; Network sharing; Piano music; Video networks; Students},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137899411&doi=10.1155%2f2022%2f1882739&partnerID=40&md5=39756c290460dcf527de0696d559b51f},
}

@Article{bagherzadeh2023em,
  author            = {Bagherzadeh, Sara and Maghooli, Keivan and Shalbaf, Ahmad and Maghsoudi, Arash},
  title             = {Emotion Recognition Using Continuous Wavelet Transform and Ensemble of Convolutional Neural Networks through Transfer Learning from Electroencephalogram Signal},
  note              = {Cited by: 2; All Open Access, Gold Open Access},
  number            = {1},
  pages             = {47 – 56},
  volume            = {10},
  abstract          = {Purpose: Emotions are integral brain states that can influence our behavior, decision-making, and functions. Electroencephalogram (EEG) is an appropriate modality for emotion recognition since it has high temporal resolution and is a non-invasive and cheap technique. Materials and Methods: A novel approach based on Ensemble pre-trained Convolutional Neural Networks (ECNNs) is proposed to recognize four emotional classes from EEG channels of individuals watching music video clips. First, scalograms are built from one-dimensional EEG signals by applying the Continuous Wavelet Transform (CWT) method. Then, these images are used to re-train five CNNs: AlexNet, VGG-19, Inception-v1, ResNet-18, and Inception-v3. Then, the majority voting method is applied to make the final decision about emotional classes. The 10-fold cross-validation method is used to evaluate the performance of the proposed method on EEG signals of 32 subjects from the DEAP database. Results: The experiments showed that applying the proposed ensemble approach in combinations of scalograms of frontal and parietal regions improved results. The best accuracy, sensitivity, precision, and F-score to recognize four emotional states achieved 96.90% ± 0.52, 97.30 ± 0.55, 96.97 ± 0.62, and 96.74 ± 0.56, respectively. Conclusion: So, the newly proposed model from EEG signals improves recognition of the four emotional states in the DEAP database. Copyright © 2023 Tehran University of Medical Sciences.},
  author_keywords   = {Continuous Wavelet Transform; Deep Learning; Electroencephalogram; Emotion Recognition; Ensemble Approach; Transfer Learning},
  year              = {2023},
  doi               = {10.18502/fbt.v10i1.11512},
  hasabstract       = {Y},
  journaltitle      = {Frontiers in Biomedical Technologies},
  keywords          = {accuracy; adolescent; adult; anger; arousal; Article; brain nerve cell; clinical article; clinical decision making; convolutional neural network; cumulative scale; electroencephalogram; emotion assessment; excitement; fear; female; Fourier transform; happiness; human; limbic system; male; mental performance; music; neurophysiology; non invasive procedure; occipital cortex; self evaluation; signal processing; support vector machine; transfer of learning; valence (emotion); wavelet transform},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147424502&doi=10.18502%2ffbt.v10i1.11512&partnerID=40&md5=086552d4079133b586f99b9bdcb4a216},
}

@Article{loukas2022mu,
  author            = {Loukas, Serafeim and Lordier, Lara and Meskaldji, Djalel-Eddine and Filippa, Manuela and Sa de Almeida, Joana and Van De Ville, Dimitri and Hüppi, Petra S.},
  title             = {Musical memories in newborns: A resting-state functional connectivity study},
  note              = {Cited by: 8; All Open Access, Green Open Access},
  number            = {2},
  pages             = {647 – 664},
  volume            = {43},
  abstract          = {Music is known to induce emotions and activate associated memories, including musical memories. In adults, it is well known that music activates both working memory and limbic networks. We have recently discovered that as early as during the newborn period, familiar music is processed differently from unfamiliar music. The present study evaluates music listening effects at the brain level in newborns, by exploring the impact of familiar or first-time music listening on the subsequent resting-state functional connectivity in the brain. Using a connectome-based framework, we describe resting-state functional connectivity (RS-FC) modulation after music listening in three groups of newborn infants, in preterm infants exposed to music during their neonatal-intensive-care-unit (NICU) stay, in control preterm, and full-term infants. We observed modulation of the RS-FC between brain regions known to be implicated in music and emotions processing, immediately following music listening in all newborn infants. In the music exposed group, we found increased RS-FC between brain regions known to be implicated in familiar and emotionally arousing music and multisensory processing, and therefore implying memory retrieval and associative memory. We demonstrate a positive correlation between the occurrence of the prior music exposure and increased RS-FC in brain regions implicated in multisensory and emotional processing, indicating strong engagement of musical memories; and a negative correlation with the Default Mode Network, indicating disengagement due to the aforementioned cognitive processing. Our results describe the modulatory effect of music listening on brain RS-FC that can be linked to brain correlates of musical memory engrams in preterm infants. © 2021 The Authors. Human Brain Mapping published by Wiley Periodicals LLC.},
  year              = {2022},
  doi               = {10.1002/hbm.25677},
  hasabstract       = {Y},
  journaltitle      = {Human Brain Mapping},
  keywords          = {Amygdala; Auditory Perception; Cerebral Cortex; Connectome; Default Mode Network; Emotions; Female; Humans; Infant, Newborn; Infant, Premature; Magnetic Resonance Imaging; Male; Music; Recognition, Psychology; Thalamus; algorithm; Article; associative memory; auditory stimulation; BOLD signal; brain region; caudate nucleus; cingulate gyrus; connectome; controlled study; default mode network; emotional network; female; functional connectivity; gestational age; hospitalization; human; human experiment; inferior temporal gyrus; male; middle frontal gyrus; music; neonatal intensive care unit; newborn; newborn period; normal human; nuclear magnetic resonance imaging; parahippocampal gyrus; prematurity; primary motor cortex; resting state network; superior frontal gyrus; supplementary motor area; thalamus; amygdala; brain cortex; connectome; diagnostic imaging; emotion; hearing; physiology},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118480313&doi=10.1002%2fhbm.25677&partnerID=40&md5=89a9b54fc889ade991c5ca8fda6db634},
}

@Article{priya2024th,
  author            = {Priya, Smily Jesu and Paulraj, Victor and Chinchai, Supaporn and Munkhetvit, Peeraya and Sriphetcharawut, Sarinya},
  title             = {The development and content validity of the emotional recognition memory training program (ERMTP) for children with autism spectrum disorder: A trial phase},
  note              = {Cited by: 0; All Open Access, Bronze Open Access},
  number            = {1},
  pages             = {177 – 189},
  volume            = {57},
  abstract          = {Background: Facial expression, tone of voice, body language, and context are unrecognizable to children with autism. Emotional arousal and emotion recognition (required emotion empathy and cognitive processing empathy) induce downstream illnesses in children with ASD. Thus, the proposed study aimed to develop a computer-based Emotional Recognition Memory Training Program (ERMTP) for ASD. Objective: Firstly, to develop and validate the ERMTP for social cognitive abilities in children with ASD and secondly, to conduct pilot-tested it in typically developing children and children with ASD. Materials and methods: This study consisted of 3 phases. The first phase was developing the ERMTP from the literature review. The second phase was analyzed for content validity with five experts about Task 1 (two activities) and Task 2 (nine activities) comprising ERMTP. Computer-based learning of six fundamental facial emotions (happy, sad, angry, fear, disgusted, and surprised) improves social cognition. Finally, the pilot test was analyzed to discover the ERMTP’s challenges for five children with typical development and ASD. Results: The ERMTP’s activity items have good content validity, especially regarding clarity and relevance. All five raters gave the intervention a 1.0 IOC for its distinct components. In the training program, we followed the expert instructions regarding background music or voice and the generalization task. Descriptive analysis indicated that all five normal-developing children followed emotional expressions and instructions (100%). All five parents reported there were changes in focus and memory skills. Emotion regulation, memory abilities, and the social cognition index demonstrated statistically significant (p<0.05) effects before and after ERMTP treatment in ASD. Conclusion: ERMTP seeks to improve the social cognition of children with ASD by the use of feedback from both specialists and the children themselves. However, further research will be necessary to investigate ASD using a randomized control trial. © 2024, Faculty of Associated Medical Sciences, Chiang Mai University. All rights reserved.},
  author_keywords   = {autism; Emotion recognition; facial expression; memory; social cognition},
  year              = {2024},
  doi               = {10.12982/JAMS.2024.020},
  hasabstract       = {Y},
  journaltitle      = {Journal of Associated Medical Sciences},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180675901&doi=10.12982%2fJAMS.2024.020&partnerID=40&md5=9f3b9e113cb56d847c534a4ad17577f6},
}

@Article{mazhar2022mo,
  author            = {Mazhar, Tehseen and Malik, Muhammad Amir and Nadeem, Muhammad Asgher and Mohsan, Syed Agha Hassnain and Haq, Inayatul and Karim, Faten Khalid and Mostafa, Samih M.},
  title             = {Movie Reviews Classification through Facial Image Recognition and Emotion Detection Using Machine Learning Methods},
  note              = {Cited by: 7; All Open Access, Gold Open Access},
  number            = {12},
  volume            = {14},
  abstract          = {The critical component of HCI is face recognition technology. Emotional computing heavily relies on the identification of facial emotions. Applications for emotion-driven face animation and dynamic assessment are numerous (FER). Universities have started to support real-world face expression recognition research as a result. Short video clips are continually uploaded and shared online, building up a library of videos on various topics. The enormous amount of movie data appeals to system engineers and researchers of autonomous emotion mining and sentiment analysis. The main idea is that categorizing things may be done by looking at how individuals feel about specific issues. People might choose to have a basic or complex facial appearance. People worldwide continually express their feelings through their faces, whether they are happy, sad, or uncertain. An online user can visually express themselves through a video’s editing, music, and subtitles. Additionally, before the video data can be used, noise in the data must frequently be eliminated. Automatically figuring out how someone feels in a video is a challenging task that will only get harder over time. Therefore, this paper aims to show how facial recognition video analysis can be used to show how sentiment analysis can help with business growth and essential decision-making. To determine how people are affected by reviewers’ writing, we use a technique for deciding emotions in this analysis. The feelings in movies are assessed using machine learning algorithms to categorize them. A lightweight machine learning algorithm is proposed to help in Aspect-oriented emotion classification for movie reviews. Moreover, to analyze real and published datasets, experimental results are compared with different Machine Learning algorithms, i.e., Naive Bayes, Support Vector Machine, Random Forest, and CNN. The proposed approach obtained 84.72 accuracy and 79.24 sensitivity. Furthermore, the method has a specificity of 90.64 and a precision of 90.2. Thus, the proposed method significantly increases the accuracy and sensitivity of the emotion detection system from facial feature recognition. Our proposed algorithm has shown contribution to detect datasets of different emotions with symmetric characteristics and symmetrically-designed facial image recognition tasks. © 2022 by the authors.},
  author_keywords   = {emotion detection; facial recognition; machine learning; reviews classification},
  year              = {2022},
  doi               = {10.3390/sym14122607},
  hasabstract       = {Y},
  journaltitle      = {Symmetry},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144888881&doi=10.3390%2fsym14122607&partnerID=40&md5=9be6286981d8f11b382e99ea990fe5c5},
}

@Article{pandeya2024gl,
  author            = {Pandeya, Yagya Raj and Lee, Joonwhoan},
  title             = {GlocalEmoNet: An optimized neural network for music emotion classification and segmentation using timbre and chroma features},
  note              = {Cited by: 0},
  abstract          = {Music is a powerful language capable of eliciting a variety of emotions in individuals. Understanding and recognizing these emotions is pivotal for applications ranging from personalized music recommendations and music therapy to automatic music composition and affective computing. Presently, deep learning for music emotion recognition is gaining popularity, primarily relying on timbre features to capture local spatial information. However, there is an untapped potential in incorporating other pertinent audio features and global correlations in the feature space to capture the repetitive temporal information of music for emotion classification. This study introduces GlocalEmoNet as a method to capture both local and global correlations in music, utilizing timbre and Chroma audio features for tasks related to emotion classification and segmentation. The neural network underwent training and testing on approximately six thousand music audio samples, encompassing six music-emotion categories. The utilization of a genetic algorithm is employed for optimizing the hyperparameters of the proposed neural networks, aiming to attain optimal performance, efficiency, and generalization. The best classifier demonstrated superior performance, surpassing previously published results by a significant margin of approximately 14%. The optimal classifier achieved an accuracy score of 81.66%, an f1-score of 0.812, and an area under the curve score of 0.956. The evaluation of classification and segmentation outcomes also involved the use of visual representations. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.},
  author_keywords   = {Classification; Genetic algorithm; GlocalEmoNet; Music emotion; Segmentation},
  year              = {2024},
  doi               = {10.1007/s11042-024-18246-4},
  hasabstract       = {Y},
  journaltitle      = {Multimedia Tools and Applications},
  keywords          = {Audio acoustics; Classification (of information); Computer music; Deep learning; Emotion Recognition; Music; Audio features; Chroma features; Emotion classification; Global correlation; Glocalemonet; Music emotion classifications; Music emotions; Music recommendation; Neural-networks; Segmentation; Genetic algorithms},
  priority          = {prio1},
  publication_stage = {Article in press},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185132967&doi=10.1007%2fs11042-024-18246-4&partnerID=40&md5=96770150ba0f11e7611abf80bffde28a},
}

@Article{yang2022mu,
  author            = {Yang, Chaode and Li, Qingxun},
  title             = {Music Emotion Feature Recognition based on Internet of Things and Computer-Aided Technology},
  note              = {Cited by: 5; All Open Access, Bronze Open Access},
  number            = {S6},
  pages             = {80 – 90},
  volume            = {19},
  abstract          = {As an important part of human life, music can convey emotion and regulate the emotions of listeners. Emotion is one of the essential features of music, and the relationship between music and emotion has become the subject of many academic studies. At present, with the rapid development of information technology and artificial intelligence, music emotion recognition has made rapid progress and become one of the important research directions in the field of digital music. Aiming at the problem of poor classification effect of musical emotion caused by the monotony of Support Vector Machine (SVM) projection space, this paper proposes an optimized SVM model for music feature emotion recognition. The new method can not only improve the accuracy of music emotion classification, but also improve the running speed and interpretability of the model. At the end, the practicality and reliability of the new approach are verified by public classification data sets and real music emotion data sets. This paper proposes an optimized SVM model for music feature emotion recognition. The new method can not only improve the accuracy of music emotion classification, but also improve the running speed and interpretability of the model. Finally, the practicality and reliability of the new approach are verified by both the public classification data sets and real music emotion data sets. © 2022 CAD Solutions, LLC,.},
  author_keywords   = {Computer aided technology; Music feature emotion recognition; Optimization algorithm; SVM},
  year              = {2022},
  doi               = {10.14733/cadaps.2022.S6.80-90},
  hasabstract       = {Y},
  journaltitle      = {Computer-Aided Design and Applications},
  keywords          = {Classification (of information); Internet of things; Music; Speech recognition; Vector spaces; Computer aided technologies; Computer-aided technologies; Data set; Emotion recognition; Music emotion classifications; Music emotions; Music feature emotion recognition; Optimization algorithms; Support vector machine models; Support vectors machine; Support vector machines},
  priority          = {prio1},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122323526&doi=10.14733%2fcadaps.2022.S6.80-90&partnerID=40&md5=d2f6a750386672cd2f4f123548cbd50b},
}

@Article{bakariya2023an,
  author            = {Bakariya, Brijesh and Mohbey, Krishna Kumar and Singh, Arshdeep and Singh, Harmanpreet and Raju, Pankaj and Rajpoot, Rohit},
  title             = {An Efficient Model for Facial Expression Recognition with Music Recommendation},
  note              = {Cited by: 0},
  abstract          = {An AI interactive robot can identify human faces, determine the emotions of the person it is chatting with, and then pick appropriate replies using algorithms that analyze facial expressions and recognize faces. One example of these algorithms is facing recognition and emotion recognition algorithms. Deep learning is currently the most effective method for carrying out tasks. We have developed a real-time system that can recognize human faces, determine human emotions, and even provide users with music recommendations by utilizing deep learning and a few Python modules. The OAHEGA and FER-2013 datasets train the models presented in this article. The accuracy of our suggested model was compared to several baseline approaches, and the results were quite affirmative. Anger, fear, pleasure, neutral, sorrow, and surprise are the six feelings that our CNN model can predict. © 2023, The Author(s), under exclusive licence to The National Academy of Sciences, India.},
  author_keywords   = {Artificial intelligence; CNN; Deep learning; Face expression recognition; Face recognition},
  comment           = {music recommendation},
  year              = {2023},
  doi               = {10.1007/s40009-023-01346-4},
  hasabstract       = {Y},
  journaltitle      = {National Academy Science Letters},
  priority          = {prio3},
  publication_stage = {Article in press},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169582274&doi=10.1007%2fs40009-023-01346-4&partnerID=40&md5=2c42839e489d24ad53815dd076b36c41},
}

@Article{xie2022mu,
  author            = {Xie, Zun and Pan, Jianwei and Li, Songjie and Ren, Jing and Qian, Shao and Ye, Ye and Bao, Wei},
  title             = {Musical Emotions Recognition Using Entropy Features and Channel Optimization Based on EEG},
  note              = {Cited by: 3; All Open Access, Gold Open Access, Green Open Access},
  number            = {12},
  volume            = {24},
  abstract          = {The dynamic of music is an important factor to arouse emotional experience, but current research mainly uses short-term artificial stimulus materials, which cannot effectively awaken complex emotions and reflect their dynamic brain response. In this paper, we used three long-term stimulus materials with many dynamic emotions inside: the “Waltz No. 2” containing pleasure and excitement, the “No. 14 Couplets” containing excitement, briskness, and nervousness, and the first movement of “Symphony No. 5 in C minor” containing passion, relaxation, cheerfulness, and nervousness. Approximate entropy (ApEn) and sample entropy (SampEn) were applied to extract the non-linear features of electroencephalogram (EEG) signals under long-term dynamic stimulation, and the K-Nearest Neighbor (KNN) method was used to recognize emotions. Further, a supervised feature vector dimensionality reduction method was proposed. Firstly, the optimal channel set for each subject was obtained by using a particle swarm optimization (PSO) algorithm, and then the number of times to select each channel in the optimal channel set of all subjects was counted. If the number was greater than or equal to the threshold, it was a common channel suitable for all subjects. The recognition results based on the optimal channel set demonstrated that each accuracy of two categories of emotions based on “Waltz No. 2” and three categories of emotions based on “No. 14 Couplets” was generally above 80%, respectively, and the recognition accuracy of four categories based on the first movement of “Symphony No. 5 in C minor” was about 70%. The recognition accuracy based on the common channel set was about 10% lower than that based on the optimal channel set, but not much different from that based on the whole channel set. This result suggested that the common channel could basically reflect the universal features of the whole subjects while realizing feature dimension reduction. The common channels were mainly distributed in the frontal lobe, central region, parietal lobe, occipital lobe, and temporal lobe. The channel number distributed in the frontal lobe was greater than the ones in other regions, indicating that the frontal lobe was the main emotional response region. Brain region topographic map based on the common channel set showed that there were differences in entropy intensity between different brain regions of the same emotion and the same brain region of different emotions. The number of times to select each channel in the optimal channel set of all 30 subjects showed that the principal component channels representing five brain regions were Fp1/F3 in the frontal lobe, CP5 in the central region, Pz in the parietal lobe, O2 in the occipital lobe, and T8 in the temporal lobe, respectively. © 2022 by the authors.},
  author_keywords   = {channel optimization; EEG signals; emotion recognition; entropy; musical emotions},
  year              = {2022},
  doi               = {10.3390/e24121735},
  hasabstract       = {Y},
  journaltitle      = {Entropy},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144644545&doi=10.3390%2fe24121735&partnerID=40&md5=53c4a545eaab533629baea2dd1d387fb},
}

@Article{bao2023ge,
  author            = {Bao, Chunhui and Sun, Qianru},
  title             = {Generating Music With Emotions},
  note              = {Cited by: 2; All Open Access, Green Open Access},
  pages             = {3602 – 3614},
  volume            = {25},
  abstract          = {We focus on the music generation conditional on human emotions, specifically the positive and negative emotions. There is no existing large-scale music datasets with the annotation of human emotion labels. It is thus not intuitive how to generate music conditioned on emotion labels. In this paper, we propose an annotation-free method to build a new dataset where each sample is a triplet of lyric, melody and emotion label (without requiring any labours). Specifically, we first train the automated emotion recognition model using the BERT (pre-trained on GoEmotions dataset) on Edmonds Dance dataset. We use it to automatically 'label' the music with the emotion labels recognized from the lyrics. We then train the encoder-decoder based model to generate emotional music on that dataset, and call our overall method as Emotional Lyric and Melody Generator (ELMG). The framework of ELMG is consisted of three modules: 1) an encoder-decoder model trained end-to-end to generate lyric and melody; 2) a music emotion classifier trained on labeled data (our proposed dataset); and 3) a modified beam search algorithm that guides the music generation process by incorporating the music emotion classifier. We conduct objective and subjective evaluations on the generated music pieces, and our results show that ELMG is capable of generating tuneful lyric and melody with specified human emotions.  © 1999-2012 IEEE.},
  author_keywords   = {beam search; Conditional music generation; Seq2Seq; transformer},
  year              = {2023},
  doi               = {10.1109/TMM.2022.3163543},
  hasabstract       = {Y},
  journaltitle      = {IEEE Transactions on Multimedia},
  keywords          = {Classification (of information); Decoding; Deep learning; Generative adversarial networks; Large dataset; Music; Signal encoding; Speech recognition; Beam search; Conditional music generation; Deep learning; Emotion recognition; Generator; Human emotion; Natural languages; Seq2seq; Social networking (online); Transformer; Emotion Recognition},
  priority          = {prio1},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127520415&doi=10.1109%2fTMM.2022.3163543&partnerID=40&md5=61bc5f8c7a6ea21475248daeb75a7415},
}

@Article{rufino2024mu,
  author            = {Rufino, Brandon and Khan, Ajmal and Dutta, Tilak and Biddiss, Elaine},
  title             = {Musical instrument classifier for early childhood percussion instruments},
  note              = {Cited by: 0; All Open Access, Gold Open Access},
  number            = {4 April},
  volume            = {19},
  abstract          = {While the musical instrument classification task is well-studied, there remains a gap in identifying non-pitched percussion instruments which have greater overlaps in frequency bands and variation in sound quality and play style than pitched instruments. In this paper, we present a musical instrument classifier for detecting tambourines, maracas and castanets, instruments that are often used in early childhood music education. We generated a dataset with diverse instruments (e.g., brand, materials, construction) played in different locations with varying background noise and play styles. We conducted sensitivity analyses to optimize feature selection, windowing time, and model selection. We deployed and evaluated our best model in a mixed reality music application with 12 families in a home setting. Our dataset was comprised of over 369, 000 samples recorded in-lab and 35, 361 samples recorded with families in a home setting. We observed the Light Gradient Boosting Machine (LGBM) model to perform best using an approximate 93 ms window with only 12 mel-frequency cepstral coefficients (MFCCs) and signal entropy. Our best LGBM model was observed to perform with over 84% accuracy across all three instrument families in-lab and over 73% accuracy when deployed to the home. To our knowledge, the dataset compiled of 369, 000 samples of non-pitched instruments is first of its kind. This work also suggests that a low feature space is sufficient for the recognition of non-pitched instruments. Lastly, real-world deployment and testing of the algorithms created with participants of diverse physical and cognitive abilities was also an important contribution towards more inclusive design practices. This paper lays the technological groundwork for a mixed reality music application that can detect children's use of non-pitched, percussion instruments to support early childhood music education and play. Copyright: © 2024 Rufino et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
  year              = {2024},
  doi               = {10.1371/journal.pone.0299888},
  hasabstract       = {Y},
  journaltitle      = {PLoS ONE},
  keywords          = {Algorithms; Child; Child, Preschool; Cognition; Humans; Music; Percussion; Sound; algorithm; Article; background noise; child; childhood; classifier; clinical article; cognition; education; emotion; entropy; ethnicity; family income; hemiparesis; human; hydrocephalus; machine learning; microgyria; musical instrument; percussion; physical capacity; school child; sensitivity analysis; spinal cord injury; timbre; training; music; preschool child; sound},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189517552&doi=10.1371%2fjournal.pone.0299888&partnerID=40&md5=2321373991cbc08a826f982ca8a35c37},
}

@Article{tiple2022mu,
  author            = {Tiple, Bhavana and Patwardhan, Manasi},
  title             = {Multi-label emotion recognition from Indian classical music using gradient descent SNN model},
  note              = {Cited by: 5},
  number            = {6},
  pages             = {8853 – 8870},
  volume            = {81},
  abstract          = {Music enthusiasts are growing exponentially and based on this, many songs are being introduced to the market and stored in signal music libraries. Due to this development emotion recognition model from music contents has received increasing attention in today’s world. Of these technologies, a novel Music Emotion Recognition (MER) system is introduced to meet the ever-increasing demand for easy and efficient access to music information. Even though this system was well-developed it lacks in maintaining accuracy of the system and finds difficulty in predicting multi-label emotion type. To address these shortcomings, in this research article, a novel MER system is developed by inter-linking the pre-processing, feature extraction and classification steps. Initially, pre-processing step is employed to convert larger audio files into smaller audio frames. Afterwards, music related temporal, spectral and energy features are extracted for those pre-processed frames which are subjected to the proposed gradient descent based Spiking Neural Network (SNN) classifier. While learning SNN, it is important to determine the optimal weight values for reducing the training error so that gradient descent optimization approach is adopted. To prove the effectiveness of proposed research, proposed model is compared with conventional classification algorithms. The proposed methodology was experimentally tested using various evaluation metrics and it achieves 94.55% accuracy. Hence the proposed methodology attains a good accuracy measure and outperforms well than other algorithms. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
  author_keywords   = {Convolutional neural network; Gradient descent; Short Term Fourier Transform; Spectral; Spiking neural network; Temporal},
  year              = {2022},
  doi               = {10.1007/s11042-022-11975-4},
  hasabstract       = {Y},
  journaltitle      = {Multimedia Tools and Applications},
  keywords          = {Audio acoustics; Convolutional neural networks; Music; Optimization; Speech recognition; Convolutional neural network; Emotion recognition; Gradient-descent; Multi-labels; Music emotions; Neural-networks; Short-term Fourier transform; Spectral; Spiking neural network; Temporal; Gradient methods},
  priority          = {prio1},
  publication_stage = {Final},
  ranking           = {rank2},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124341623&doi=10.1007%2fs11042-022-11975-4&partnerID=40&md5=94edd585a680d988f8b434e1fe74cf27},
}

@Article{wan2024de,
  author            = {Wan, Mingxia},
  title             = {Designing an online vocal learning based on ZigBee-enabled wireless platform},
  note              = {Cited by: 0},
  number            = {1},
  pages             = {179 – 192},
  volume            = {30},
  abstract          = {Vocal learning has found its proliferation in recent years due to the advancement in wireless networking. Vocal art encompasses profound cultural values and aesthetic preferences, making wireless platforms, e.g. Wi-Fi and ZigBee, essential for the enhancement of online vocal learning. ZigBee, as a wireless platform, has more recently been used quite frequently for online vocal learning and music. This paper aims to design an efficient wireless-enabled platform by using the lightweight features of ZigBee to support online vocal learning and expressions. The platform enables data transmission, storage, analysis, and playback of vocal learning sounds through the ZigBee network. It is built on a cloud platform, utilizing Docker virtualization technology to deploy a Hadoop distributed cluster, effectively simulating a distributed environment. The platform incorporates a model for recognizing musical sound and noise signals, empowering the online learning control platform to detect various characteristics of vocal music tones. Vocal music learning is enhanced by extracting endpoint fusion spectrum features and employing a tone adjustment and dynamic detection model. The proposed method improves anti-interference capability, online learning control, vocal tone recognition, and emotional expression accuracy. Simulation results demonstrate its effectiveness in vocal music, and tone recognition, leading to more precise vocal music pronunciation and intonation registration. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
  author_keywords   = {Cloud platform; Emotion; Hadoop; Music score; Online learning platform; Spectrum characteristics; Vocal music singing; Wireless network; ZigBee network},
  comment           = {emotion studied indirectly},
  year              = {2024},
  doi               = {10.1007/s11276-023-03443-0},
  hasabstract       = {Y},
  journaltitle      = {Wireless Networks},
  keywords          = {Digital storage; E-learning; Learning algorithms; Learning systems; Music; Spectrum analysis; Wi-Fi; Cloud platforms; Emotion; Hadoop; Learning platform; Music scores; Online learning; Online learning platform; Spectra characteristic; Vocal music; Vocal music singing; ZigBee networks; Zigbee},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168083530&doi=10.1007%2fs11276-023-03443-0&partnerID=40&md5=0b41c512867fe85f2374bb90d0119e10},
}

@Article{cui2023vo,
  author            = {Cui, Yu},
  title             = {Vocal Music Performance Evaluation System Based on Neural Network and Its Application in Piano Teaching},
  note              = {Cited by: 2},
  number            = {Special Issue E55},
  pages             = {451 – 464},
  volume            = {2023},
  abstract          = {Piano note recognition is the process of automatically converting music audio files into digital music files, which plays an important role in piano assisted teaching and automatic music recording. Evaluation of timbre can provide reference to the singer in vocal music performance, so that their pronunciation can be corrected. In the process, their emotions will affect the timbre to some degree. Based on aspect of emotion, the application of evaluation systematic standards applied in evaluation software algorithm is studied. Classifier of music emotion characteristics has been established. SVM algorithm has been used to process and analyze the music signal. After feature selection and test of trail, the related parameters have been selected and then the algorithm is tested. The test shows that the method built can well identify and evaluate the timbre of vocal music and has practical significance. © 2023, Associacao Iberica de Sistemas e Tecnologias de Informacao. All rights reserved.},
  author_keywords   = {Emotional Computing; Evaluation System; Timbre; Vocal Singing},
  comment           = {No relevant material, discussed},
  year              = {2023},
  hasabstract       = {Y},
  journaltitle      = {RISTI - Revista Iberica de Sistemas e Tecnologias de Informacao},
  modificationdate  = {2024-05-16T12:19:00},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162808819&partnerID=40&md5=ad7c5ceb0797157d6e549e2b8f6fab0e},
}

@Article{deshmukh2024em,
  author            = {Deshmukh, Shrikala and Gupta, Preeti},
  title             = {Emotionally Intelligent Music Player for Mood Improvement based on Text Emotion Recognition using Deep Learning Approach},
  note              = {Cited by: 0},
  number            = {2s},
  pages             = {294 – 302},
  volume            = {12},
  abstract          = {This study shows a unique approach of Mood elevating music player based on Text emotion recognition. Emotions play a very vital role in everyday life. In this internet era, textual data is mainly designed for communication. Natural language processing is designed for textual data such as messages, emails, articles, reviews, posts, etc. Sentiment analysis is used in various fields. For emotion recognition from text, Deep learning with machine learning approach is used. CNN(Convolutional Neural Network) with a multiclass support vector machine algorithm is used. One vs Rest approach is used for multiclass SVM classifier. Lexicon database and BBC database are operated. Proposed system is compared with K-nearest Neighbour (KNN), Random Forest (RF), Naïve Bayes (NB) algorithms. Results show the accuracy of around 86.88% using BBC database with and approximately 91.2% using Lexicon database, which is higher than other classifiers. Other classifiers such as Random Forest (RF) shows the accuracy of 68.44% for lexicon and 62.44% for BBC, Naïve Bayes (NB) shows the accuracy of 62.56% for lexicon and 59.28% for BBC, K-nearest neighbour (KNN) shows the accuracy of 74.12% for lexicon and 69.28% for BBC. As a result, CNN with multiclass SVM gives 91.2% accuracy using lexicon database. © 2024, Ismail Saritas. All rights reserved.},
  author_keywords   = {BBC database; Convolutional neural network; Deep learning; Lexicon database; Machine learning; Multiclass Support Vector Machine},
  comment           = {text emotion recognition},
  year              = {2024},
  hasabstract       = {Y},
  journaltitle      = {International Journal of Intelligent Systems and Applications in Engineering},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177882002&partnerID=40&md5=a02d1a89e03cfe3cdb492915c0187abe},
}

@Article{tang2023ap,
  author            = {Tang, Zhangcheng},
  title             = {Application Model Construction of Emotional Expression and Propagation Path of Deep Learning in National Vocal Music},
  note              = {Cited by: 0; All Open Access, Gold Open Access},
  number            = {11},
  pages             = {1055 – 1062},
  volume            = {14},
  abstract          = {Emotional expression is important in Chinese national vocal music art. The emotional expression in national vocal music is based on the art of national vocal music, with distinct characteristics and requirements. The ultimate goal is to spread the expression of various emotions in the national vocal music art. Promoting the spread of national vocal music singing art using modern media is an urgent requirement for the inheritance and development of national vocal music singing art. With the rapid development of science and technology, integrating deep learning and traditional music has become the general trend. It has been gradually applied to melody recognition, intelligent composition, virtual performance, and other aspects of traditional music and has achieved good results, but also hidden behind a series of ideas and technical and ethical issues. In this paper, the application of deep learning has been discussed and prospected. The recognition rate of emotional expression in national vocal music is 92 %. In terms of communication, combined with the deep learning algorithm, this paper analyzes the characteristics and requirements of emotional expression in the art of national vocal music singing and puts forward a new method of promoting the development of the art of national vocal music singing, hoping to attract more attention and enhance the social awareness of the application field, to promote the steady development of Chinese traditional music in the information age. © (2023), (Science and Information Organization). All Rights Reserved.},
  author_keywords   = {Deep learning; dissemination; emotion; innovation; national vocal music},
  year              = {2023},
  doi               = {10.14569/IJACSA.2023.01411107},
  hasabstract       = {Y},
  journaltitle      = {International Journal of Advanced Computer Science and Applications},
  keywords          = {Deep learning; Emotion Recognition; Learning algorithms; Learning systems; Application models; Deep learning; Dissemination; Emotion; Emotional expressions; Innovation; Model construction; National vocal music; Propagation paths; Vocal music; Music},
  priority          = {prio1},
  publication_stage = {Final},
  ranking           = {rank2},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179180244&doi=10.14569%2fIJACSA.2023.01411107&partnerID=40&md5=ff6c976bdcf3b3aeaaf465465cd1f940},
}

@Article{yue2024de,
  author            = {Yue, Yuanyang and Shen, Xiaoyan},
  title             = {Development strategy of early childhood music education industry: An IFS-AHP-SWOT analysis based on dynamic social network},
  note              = {Cited by: 0; All Open Access, Gold Open Access},
  number            = {2 February},
  volume            = {19},
  abstract          = {Early childhood music education has garnered recognition for its unique contribution to cognitive, emotional, and social development in children. Nevertheless, the industry grapples with numerous challenges, including a struggle to adapt traditional educational paradigms to new curriculum reforms, and an excessive emphasis on skill training at the expense of nurturing a love for music and aesthetics in children. To navigate these challenges and explore growth strategies for the early childhood music education industry, we initiated a comprehensive approach that involved distributing surveys to practitioners and parents and engaging experts for insightful discussions. Consequently, we proposed an analytical method based on dynamic social networks in conjunction with Intuitionistic Fuzzy Sets (IFS), Analytic Hierarchy Process (AHP), and Strengths, Weaknesses, Opportunities, and Threats (SWOT) analysis, collectively referred to as IFS-AHP-SWOT. This integrated methodology synergizes the capabilities of dynamic social networks, IFS, AHP, and SWOT analysis to offer a nuanced perspective on industry development strategies. The findings underscore that institutions within the early childhood music education industry need to adopt a development strategy that leverages their strengths and opportunities to foster sustainable growth. Ultimately, this research aims to provide critical decision-making support for industry practitioners, policymakers, and researchers, contributing significantly to the ongoing discourse on strategic development in the early childhood music education industry. Copyright: © 2024 Yue, Shen. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
  year              = {2024},
  doi               = {10.1371/journal.pone.0295419},
  hasabstract       = {Y},
  journaltitle      = {PLoS ONE},
  keywords          = {Analytic Hierarchy Process; Child; Child, Preschool; Emotions; Humans; Industry; Music; Social Networking; algorithm; analytic hierarchy process; Article; artificial intelligence; child; childhood; cognition; early childhood music education industry; education program; emotionality; health education; health promotion; human; intuitionistic fuzzy sets; mathematical analysis; music; psychology; social network; strengths, weaknesses, opportunities, and threats; training; emotion; industry; preschool child; social network},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186306496&doi=10.1371%2fjournal.pone.0295419&partnerID=40&md5=d09ea2ce9877bba6baaee01aff337d87},
}

@Article{fu2023fe,
  author            = {Fu, Ning and Peng, Xia},
  title             = {Feature Recognition Method based on Computer-Aided Technology and its Application in Music Teaching},
  note              = {Cited by: 0; All Open Access, Bronze Open Access},
  number            = {s4},
  pages             = {123 – 132},
  volume            = {20},
  abstract          = {Due to the influence of factors such as strong music specialty, complex music theory knowledge and various changes, it is difficult to identify music features in the process of music teaching. Therefore, a music feature recognition system based on computer aided technology is proposed. The physical sensing layer of the system is equipped with sound sensors in different locations to collect the original music signals, and the digital signal processor is used to analyze and process the music signals. The network transmission layer will process the music signal and transmit it to the music signal database in the system application layer. The music feature analysis module in the application layer adopts dynamic time warping algorithm to obtain the maximum similarity between the test template and the reference template, realize the feature recognition of music signal, and identify music form and music emotion corresponding music feature content according to the recognition results. The experimental results show that the computer-aided music teaching system for music theory knowledge learning, works of music appreciation, music composition activity provides many functions, such as for teachers and students to provide a lot of learning resources, through the network technology to help music learners learn effectively and quickly, rich music knowledge, expand horizons, meet different users' personalized requirements. © 2023 CAD Solutions, LLC.},
  author_keywords   = {computer aided technology; data collection; feature recognition; music teaching; signal processing},
  comment           = {no relevant content, discussed},
  year              = {2023},
  doi               = {10.14733/cadaps.2023.S4.123-132},
  hasabstract       = {Y},
  journaltitle      = {Computer-Aided Design and Applications},
  keywords          = {Computer aided instruction; Digital signal processors; Emotion Recognition; Network layers; Application layers; Computer aided technologies; Computer-aided technologies; Data collection; Features recognition; Music signals; Music teaching; Music theory; Recognition methods; Signal-processing; Signal processing},
  modificationdate  = {2024-05-16T11:48:47},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142630564&doi=10.14733%2fcadaps.2023.S4.123-132&partnerID=40&md5=511ce1d8cda564d537b16032135537a1},
}

@Article{wang2022di,
  author            = {Wang, Xiaoning and Guo, Wei and Tong, Weiwei},
  title             = {Digital Music Feature Recognition Based on Wireless Sensing Technology},
  note              = {Cited by: 0; All Open Access, Gold Open Access},
  volume            = {2022},
  abstract          = {With the rapid development of information technology, digital music is subsequently increasing in large quantities, and how a good integration of vocal input and recognition technology can be transformed into digital music can greatly improve the efficiency of music production while ensuring the quality and effect of music. This paper focuses on the implementation and application of human voice input and recognition technology in digital music creation, enabling users to generate digital music forms by simply humming a melodic fragment of a piece of music into a microphone. The paper begins with an introduction to digital music and speech recognition technology and goes on to describe the respective characteristics of various audio formats, which are selected as data sources for digital music creation based on the advantages of the files in terms of retrieval. Following that, the method of extracting musical information from music is described, and the main melody is successfully extracted from the multitrack file to extract the corresponding musical performance information. The feature extraction of humming input melody is further described in detail. The traditional speech recognition method of using short-time energy and short-time overzero rate features for speech endpoint detection is analyzed. Combining the characteristics of humming music, the method of cutting notes by two-stage cutting mode, i.e., combining energy saliency index, overzero rate, and pitch change, is adopted to cut notes, which leads to a substantial improvement in performance. The algorithm uses the melody extraction algorithm to obtain the melody line, merges the short-time segments of the melody line to reduce the error rate of emotion recognition, uses the melody line to segment the music signal to generate segmented segments, then abstracts the features of the segmented segments through a CNN-based structural model, and inputs the output of the model to the regressor in cascade with the melody contour features of the corresponding segmented segments to finally obtain the emotion V/A value of the segmented segments.  © 2022 Xiaoning Wang et al.},
  year              = {2022},
  doi               = {10.1155/2022/8175834},
  hasabstract       = {Y},
  journaltitle      = {Journal of Sensors},
  keywords          = {Audio acoustics; Data mining; Extraction; Feature extraction; Speech recognition; Data-source; Digital music; Features recognition; Human voice; Music creation; Music production; Musical information; Sensing technology; Speech recognition technology; Wireless sensing; Music},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127506902&doi=10.1155%2f2022%2f8175834&partnerID=40&md5=6c97cd610bea7cbf83b749e3cc1f73ed},
}

@Article{murphy2023th,
  author            = {Murphy, Ellen and North, E. and Nawaz, S. and Omigie, D.},
  title             = {The influence of music liking on episodic memory for rich spatiotemporal contexts},
  note              = {Cited by: 0; All Open Access, Green Open Access, Hybrid Gold Open Access},
  number            = {5},
  pages             = {589 – 604},
  volume            = {31},
  abstract          = {It is thought that the presence of music influences episodic memory encoding. However, no studies have isolated the influence of music liking–the hedonic value listeners attribute to a musical stimulus–from that of the core affect induced by the presence of that music. In an online survey, participants rated musical excerpts in terms of how much they liked them, as well as in terms of felt valence, felt arousal and familiarity. These ratings were then used to inform the stimuli presented in an online episodic memory task which, across different scenarios, involved dragging cued objects to cued locations and then recalling details of what was moved, where they were moved to and the order of movements made. Our results showed an influence of liking and music-reward sensitivity on memory for what was moved, as well as a detrimental effect of arousing musical stimuli on memory for un-cued scenario details. Taken together, our study showcases the importance of episodic memory paradigms that involve rich spatiotemporal contexts and provides insights into how different aspects of episodic memory may be influenced by the presence of music. © 2022 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.},
  author_keywords   = {emotion; episodic memory; liking; Music; what-where-when},
  year              = {2023},
  doi               = {10.1080/09658211.2022.2154367},
  hasabstract       = {Y},
  journaltitle      = {Memory},
  keywords          = {Emotions; Humans; Memory, Episodic; Mental Recall; Music; Recognition, Psychology; emotion; episodic memory; human; music; recall; recognition},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153256318&doi=10.1080%2f09658211.2022.2154367&partnerID=40&md5=af0176624bef7889c8895abf1f5d4e3a},
}

@Article{tian2023mu,
  author            = {Tian, Yuan},
  title             = {Music emotion representation based on non-negative matrix factorization algorithm and user label information},
  note              = {Cited by: 0; All Open Access, Gold Open Access, Green Open Access},
  volume            = {9},
  abstract          = {Music emotion representation learning forms the foundation of user emotion recognition, addressing the challenges posed by the vast volume of digital music data and the scarcity of emotion annotation data. This article introduces a novel music emotion representation model, leveraging the nonnegative matrix factorization algorithm (NMF) to derive emotional embeddings of music by utilizing user-generated listening lists and emotional labels. This approach facilitates emotion recognition by positioning music within the emotional space. Furthermore, a dedicated music emotion recognition algorithm is formulated, alongside the proposal of a user emotion recognition model, which employs similarity-weighted calculations to obtain user emotion representations. Experimental findings demonstrate the method's convergence after a mere 400 iterations, yielding a remarkable 47.62% increase in F1 value across all emotion classes. In practical testing scenarios, the comprehensive accuracy rate of user emotion recognition attains an impressive 52.7%, effectively discerning emotions within seven emotion categories and accurately identifying users' emotional states. © Copyright 2023 Tian},
  author_keywords   = {Depression; Emotional labels; Music; NMF; Social media},
  year              = {2023},
  doi               = {10.7717/peerj-cs.1590},
  hasabstract       = {Y},
  journaltitle      = {PeerJ Computer Science},
  keywords          = {Matrix algebra; Non-negative matrix factorization; Social networking (online); Speech recognition; Depression; Emotion recognition; Emotion representation; Emotional label; Label information; Music emotions; Non-negative matrix factorization algorithms; Social media; User emotions; User labels; Emotion Recognition},
  priority          = {prio1},
  publication_stage = {Final},
  ranking           = {rank2},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173055873&doi=10.7717%2fpeerj-cs.1590&partnerID=40&md5=ab056c686a7baf460d0ef7d1868a6763},
}

@Article{xin2024an,
  author            = {Xin, Lei},
  title             = {Analysis of the effect of music therapy on psychological anxiety relief based on artificial intelligence recognition},
  note              = {Cited by: 0; All Open Access, Gold Open Access},
  number            = {1},
  volume            = {9},
  abstract          = {In order to improve the accuracy and reliability of EEG emotion recognition and avoid the problems of poor decomposition effect and long time consumption caused by manual parameter selection, this paper constructs an EEG emotion recognition model based on optimized variational modal decomposition. Aiming at the modal aliasing problem existing in traditional decomposition methods, the KH algorithm is used to search for the optimal penalty factor and the number of decomposition layers of the VMD, and KH-VMD decomposition is performed on the EEG signals in the DEAP dataset. The time-domain, frequency-domain, and nonlinear features of IMFs under different time windows are extracted, respectively, and the Catboost classifier completes the construction of the EEG emotion recognition model and emotion classification. Considering the two conditions of the complexity of the network structure of the KH-VMD model and the average classification accuracy of different brain regions in different music environments, the WEE features of the target EEG can constitute the optimal classification network by taking the WEE features of the target EEG as the input of the KH-VMD classification model. At this time, the average classification accuracy that can be obtained with differentiated brain regions and differentiated music environments is 0.8314 and 0.8204. After 8 weeks of music therapy, the experimental group's low anxiety scores of pleasure and arousal on the Negative Picture SAM scale were 3.11 and 3.2, which were significantly lower than those of the control group's low-anxiety subjects. The experimental group with high anxiety had anxiety scores and sleep quality scores that were 5.23 and 3.01 points lower than before the intervention. Therefore, music therapy can effectively alleviate psychological anxiety and enhance sleep quality.  © 2023 Lei Xin, published by Sciendo.},
  author_keywords   = {DEAP dataset; Electroencephalographic emotion awareness; IMFs; KH-VMD; Music therapy},
  year              = {2024},
  doi               = {10.2478/amns.2023.2.01517},
  hasabstract       = {Y},
  journaltitle      = {Applied Mathematics and Nonlinear Sciences},
  keywords          = {Biomedical signal processing; Brain; Classification (of information); Emotion Recognition; Frequency domain analysis; Speech recognition; Time domain analysis; Brain regions; Classification accuracy; DEAP dataset; Electroencephalographic emotion awareness; Emotion recognition; Experimental groups; IMF; KH-VMD; Music therapy; Recognition models; Electroencephalography},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183700726&doi=10.2478%2famns.2023.2.01517&partnerID=40&md5=e2f31fee9f1c7d562ff19408d0bd84f2},
}

@Article{zhou2023ee,
  author            = {Zhou, Tie Hua and Liang, Wenlong and Liu, Hangyu and Wang, Ling and Ryu, Keun Ho and Nam, Kwang Woo},
  title             = {EEG Emotion Recognition Applied to the Effect Analysis of Music on Emotion Changes in Psychological Healthcare},
  note              = {Cited by: 0; All Open Access, Gold Open Access, Green Open Access},
  number            = {1},
  volume            = {20},
  abstract          = {Music therapy is increasingly being used to promote physical health. Emotion semantic recognition is more objective and provides direct awareness of the real emotional state based on electroencephalogram (EEG) signals. Therefore, we proposed a music therapy method to carry out emotion semantic matching between the EEG signal and music audio signal, which can improve the reliability of emotional judgments, and, furthermore, deeply mine the potential influence correlations between music and emotions. Our proposed EER model (EEG-based Emotion Recognition Model) could identify 20 types of emotions based on 32 EEG channels, and the average recognition accuracy was above 90% and 80%, respectively. Our proposed music-based emotion classification model (MEC model) could classify eight typical emotion types of music based on nine music feature combinations, and the average classification accuracy was above 90%. In addition, the semantic mapping was analyzed according to the influence of different music types on emotional changes from different perspectives based on the two models, and the results showed that the joy type of music video could improve fear, disgust, mania, and trust emotions into surprise or intimacy emotions, while the sad type of music video could reduce intimacy to the fear emotion. © 2022 by the authors.},
  author_keywords   = {EEG signals; emotion recognition; music therapy; semantic analysis},
  year              = {2023},
  doi               = {10.3390/ijerph20010378},
  hasabstract       = {Y},
  journaltitle      = {International Journal of Environmental Research and Public Health},
  keywords          = {Algorithms; Electroencephalography; Emotions; Humans; Music; Reproducibility of Results; health care; music; psychology; recognition; anger; arousal; Article; classification algorithm; convolutional neural network; disgust; electroencephalogram; emotion; emotion recognition; fear; feature extraction; human; intimacy; mania; mental health; mood change; music therapy; positive valence; recognition; sadness; trust; wavelet transform; algorithm; electroencephalography; emotion; music; procedures; psychology; reproducibility},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145707741&doi=10.3390%2fijerph20010378&partnerID=40&md5=eede633cad4f1689b0430bdf89fe1f08},
}

@Article{na2022mu,
  author            = {Na, Wang and Yong, Fang},
  title             = {Music Recognition and Classification Algorithm considering Audio Emotion},
  note              = {Cited by: 5; All Open Access, Gold Open Access},
  volume            = {2022},
  abstract          = {At present, the existing music classification and recognition algorithms have the problem of low accuracy. Therefore, this paper proposes a music recognition and classification algorithm considering the characteristics of audio emotion. Firstly, the emotional features of music are extracted from the feedforward neural network and parameterized with the mean square deviation. Gradient descent learning algorithm is used to train audio emotion features. The neural network models of input layer, output layer, and hidden layer are established to realize the classification and recognition of music emotion. Experimental results show that the algorithm has good effect on music emotion classification. The data stream driven by the algorithm is higher than 55 MBbs, the anti-attack ability is 91%, the data integrity is 83%, the average accuracy is 85%, and it has good effectiveness and feasibility. © 2022 Wang Na and Fang Yong.},
  year              = {2022},
  doi               = {10.1155/2022/3138851},
  hasabstract       = {Y},
  journaltitle      = {Scientific Programming},
  keywords          = {Audio acoustics; Gradient methods; Multilayer neural networks; Classification algorithm; Classification and recognition; Emotion feature; Gradient descent learning algorithm; Mean-square deviation; Music classification; Music recognition; Neural network model; Parameterized; Recognition algorithm; Music},
  priority          = {prio1},
  publication_stage = {Final},
  ranking           = {rank2},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124103186&doi=10.1155%2f2022%2f3138851&partnerID=40&md5=b3cf6939f5b991ba48c037de5319363b},
}

@Article{akter2024ba,
  author            = {Akter, Mariam and Sultana, Nishat and Noori, Sheak Rashed Haider and Hasan, Md Zahid},
  title             = {Bangla song genre recognition using artificial neural network},
  note              = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
  number            = {2},
  pages             = {2413 – 2422},
  volume            = {13},
  abstract          = {Music has a control over human moods and it can make someone calm or excited. It allows us to feel all emotions we experience. Nowadays, people are often attached with their phones and computers listening to music on Spotify, SoundCloud, or any other internet platform. Music information retrieval plays an important role for music recommendation according to lyrics, pitch, pattern of choices, and genre. In this study, we have tried to recognize the music genre for a better music recommendation system. We have collected an amount of 1820 Bangla songs from six different genres including Adhunik, rock, hip hop, Nazrul, Rabindra, and folk music. We have started with some traditional machine learning algorithms having k-nearest neighbor, logistic regression, random forest, support vector machine, and decision tree but ended up with a deep learning algorithm named artificial neural network with an accuracy of 78% for recognizing music genres from six different genres. All mentioned algorithms are experimented with transformed mel-spectrograms and mean chroma frequency values of that raw amplitude data. But we found that music tempo having beats per minute value with two previous features present better accuracy. © 2024, Institute of Advanced Engineering and Science. All rights reserved.},
  author_keywords   = {Artificial neural network; Bangla song genre recognition; Chroma frequency; coefficients; Deep learning; Mel frequency cepstral; Tempo},
  comment           = {genre recognition},
  year              = {2024},
  doi               = {10.11591/ijai.v13.i2.pp2413-2422},
  hasabstract       = {Y},
  journaltitle      = {IAES International Journal of Artificial Intelligence},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190888686&doi=10.11591%2fijai.v13.i2.pp2413-2422&partnerID=40&md5=dd6d39e1586265d959d099c54b6cb1fa},
}

@Article{baradaran2023cu,
  author            = {Baradaran, Farzad and Farzan, Ali and Danishvar, Sebelan and Sheykhivand, Sobhan},
  title             = {Customized 2D CNN Model for the Automatic Emotion Recognition Based on EEG Signals},
  note              = {Cited by: 8; All Open Access, Gold Open Access, Green Open Access},
  number            = {10},
  volume            = {12},
  abstract          = {Automatic emotion recognition from electroencephalogram (EEG) signals can be considered as the main component of brain–computer interface (BCI) systems. In the previous years, many researchers in this direction have presented various algorithms for the automatic classification of emotions from EEG signals, and they have achieved promising results; however, lack of stability, high error, and low accuracy are still considered as the central gaps in this research. For this purpose, obtaining a model with the precondition of stability, high accuracy, and low error is considered essential for the automatic classification of emotions. In this research, a model based on Deep Convolutional Neural Networks (DCNNs) is presented, which can classify three positive, negative, and neutral emotions from EEG signals based on musical stimuli with high reliability. For this purpose, a comprehensive database of EEG signals has been collected while volunteers were listening to positive and negative music in order to stimulate the emotional state. The architecture of the proposed model consists of a combination of six convolutional layers and two fully connected layers. In this research, different feature learning and hand-crafted feature selection/extraction algorithms were investigated and compared with each other in order to classify emotions. The proposed model for the classification of two classes (positive and negative) and three classes (positive, neutral, and negative) of emotions had 98% and 96% accuracy, respectively, which is very promising compared with the results of previous research. In order to evaluate more fully, the proposed model was also investigated in noisy environments; with a wide range of different SNRs, the classification accuracy was still greater than 90%. Due to the high performance of the proposed model, it can be used in brain–computer user environments. © 2023 by the authors.},
  author_keywords   = {CNN; deep learning; EEG; emotion recognition; music},
  year              = {2023},
  doi               = {10.3390/electronics12102232},
  hasabstract       = {Y},
  journaltitle      = {Electronics (Switzerland)},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160305800&doi=10.3390%2felectronics12102232&partnerID=40&md5=72cc2b2fb8dcd99fbdf510ef4de6680c},
}

@Article{bhangale2023sp,
  author            = {Bhangale, Kishor B. and Kothandaraman, Mohanaprasad},
  title             = {Speech emotion recognition using the novel PEmoNet (Parallel Emotion Network)},
  note              = {Cited by: 1},
  volume            = {212},
  abstract          = {Emotions are very crucial for humans for expressing perception and daily activities such as communication, learning, and decision-making. Human emotion recognition using machines is a very complex task. Recently deep learning techniques have been widely used to automate this task by providing machines with a huge learning capability. However, Speech emotion recognition (SER) is challenging due to language, regional, gender, age, and cultural variations. Most of the previous SER techniques have used only one type of feature representation to train deep learning algorithms, which limits the performance of SER. This paper presents a novel Parallel Emotion Network (PEmoNet) that includes Deep Convolution Neural Network (DCNN) with three parallel arms to address effective SER. The three parallel arms of the proposed PEmoNet accept the Multitaper Mel Frequency Spectrogram (MTMFS), Gammatonegram spectrogram (GS), and Constant Q-Transform Spectrogram (CQTS) as input to improve the feature distinctiveness of the emotion signal. The performance of the proposed SER scheme is evaluated on EMODB and RAVDESS datasets based on accuracy, recall, precision, and F1-score. The proposed technique shows 97.14% and 97.41% accuracy for the EMODB and RAVDESS datasets. It shows that the proposed PEmoNet with different spectral representation inputs helps improve the emotions' distinctiveness and outperforms the existing state of the arts. © 2023 Elsevier Ltd},
  author_keywords   = {Deep convolution neural network; Deep learning; Human-computer interaction; Speech emotion recognition; Speech spectrogram},
  comment           = {no music},
  year              = {2023},
  doi               = {10.1016/j.apacoust.2023.109613},
  hasabstract       = {Y},
  journaltitle      = {Applied Acoustics},
  keywords          = {Behavioral research; Convolution; Deep neural networks; Emotion Recognition; Human computer interaction; Learning algorithms; Learning systems; Music; Spectrographs; Speech recognition; Convolution neural network; Daily activity; Decisions makings; Deep convolution neural network; Deep learning; Perception activity; Performance; Spectrograms; Speech emotion recognition; Speech spectrogram; Decision making},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169028038&doi=10.1016%2fj.apacoust.2023.109613&partnerID=40&md5=0b2273c9072fa9b58e507eeda83a022f},
}

@Article{duan2022co,
  author            = {Duan, Ying},
  title             = {Construction of Vocal Timbre Evaluation System Based on Classification Algorithm},
  note              = {Cited by: 0; All Open Access, Gold Open Access},
  volume            = {2022},
  abstract          = {With the continuous development of communication technology, computer technology, and network technology, a large amount of information such as images, videos, and audios has grown exponentially, and people have started to be exposed to massive multimedia contents, which can easily and quickly access the increasingly rich music resources, so new technologies are urgently needed for their effective management, and automatic classification of audio signals has become the focus of engineering and academic attention. Currently, music retrieval can be achieved by selecting song titles and singer names, but as people's living standards continue to improve, the spiritual realm is also enriched. People want to be able to select music with different types of emotional expressions with their emotions. It mainly includes the basic principles of audio classification, the analysis and extraction of music emotion features, and the selection of the best classifier. Two classification algorithms, hybrid Gaussian model and AdaBoost, are used to classify music emotions, and the two classifiers are combined. In this paper, we propose the Discrete Harmonic Transform (DHT), a sparse transform based on harmonic frequencies. This paper derives and proves the formula of Discrete Harmonic Transform and further analyzes the harmonic structure of musical tone signal and the accuracy of harmonic structure. Since the timbre of musical instruments depends on the harmonic structure, and similar instruments have similar harmonic structures, the discrete harmonic transform coefficients can be defined as objective indicators corresponding to the timbre of musical instruments, and thus the concept of timbre expression spectrum is proposed, and a specific construction algorithm is given in this paper. In the application of musical instrument recognition, the 53-dimensional combined features of LPCC, MFCC, and timbre expression spectrum are selected, and a nonlinear support vector machine is used as the classifier. The classification recognition rate is improved by reducing the number of feature dimensions. © 2022 Ying Duan.},
  year              = {2022},
  doi               = {10.1155/2022/6893128},
  hasabstract       = {Y},
  journaltitle      = {Scientific Programming},
  keywords          = {Adaptive boosting; Audio acoustics; Classification (of information); Emotion Recognition; Harmonic analysis; Multimedia systems; Musical instruments; Support vector machines; Classification algorithm; Communicationtechnology; Computer technology; Continuous development; Discrete harmonic transforms; Harmonic structures; Large amounts; Music emotions; Network technologies; Spectra's; Music},
  priority          = {prio1},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132509543&doi=10.1155%2f2022%2f6893128&partnerID=40&md5=86a4d695f04445072861b069f30d2dab},
}

@Article{kothuri2022ah,
  author            = {Kothuri, Sri Raman and Rajalakshmi, N.R.},
  title             = {A Hybrid Feature Selection Model for Emotion Recognition using Shuffled Frog Leaping Algorithm (SFLA)-Incremental Wrapper-Based Subset Feature Selection (IWSS)},
  note              = {Cited by: 2; All Open Access, Gold Open Access},
  number            = {2},
  pages             = {354 – 364},
  volume            = {13},
  abstract          = {Emotion recognition method is required for therapy to recognize the emotions of patient and helps in treatment. Many computer science based emotion recognition works focused on facial expression, speech, body gesture and multi-modal based machine learning techniques. Existing methods have limitations of poor convergence and easily trap into local optima. In this research, the Shuffled Frog Leaping Algorithm (SFLA)-Incremental Wrapper-based Subset Selection (IWSS) hybrid method is proposed to improve the emotion recognition. The proposed method involves in analysis the emotion of user through video, audio, and text features and recommends the music to the users. The analysis shows that hybrid modality shows the higher performance in emotion recognition. AlexNet model is applied for the feature extraction in video data and Latent Dirichlet Allocation (LDA) is applied for text feature extraction. Multi-Class Support Vector Machine (MC-SVM) model is used for the classification. The proposed SFLA-IWSS method has 97.05 % accuracy and existing gSpan method has 90 % accuracy. © 2022, Engg Journals Publications. All rights reserved.},
  author_keywords   = {AlexNet; Incremental Wrapper-based Subset Selection; Latent Dirichlet Allocation; Multi-Class Support Vector Machine; Shuffled Frog Leaping Algorithm},
  comment           = {no music - discussed},
  year              = {2022},
  doi               = {10.21817/indjcse/2022/v13i2/221302040},
  hasabstract       = {Y},
  journaltitle      = {Indian Journal of Computer Science and Engineering},
  modificationdate  = {2024-05-16T11:02:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129313552&doi=10.21817%2findjcse%2f2022%2fv13i2%2f221302040&partnerID=40&md5=a8848c9abe21c7b9c04fc185389aecce},
}

@Article{feng2024ex,
  author            = {Feng, Qiyue},
  title             = {Exploration of the Integration of Traditional Music Cultural Elements in Music Informatization Teaching},
  note              = {Cited by: 0},
  number            = {1},
  volume            = {9},
  abstract          = {With the development of information technology, music teaching methods are getting more affluent and more prosperous. This paper proposes a model for emotion classification and assessment that integrates traditional music culture elements with information technology in music teaching. The research first combines TextCNN and BiLSTM algorithms to establish the emotion classification model of conventional music. Then it combines PYIN and DTW algorithms to establish the evaluation model of traditional music, which completes the auxiliary efficacy of music informationized teaching. In the emotion classification test of the model, the classification accuracy and F1 value of emotions of different music samples are 82.98% and 75.22%, respectively. The model’s recognition accuracy of the four voices is 86.76%, and the overall effective scoring percentage is 81.98% under different playing abnormalities. This study has had an impact on traditional music evaluation. The model in this paper can be used to classify and evaluate emotions in conventional music, providing more intelligent and high-quality technical services for integrating traditional music into music teaching. © 2023 Qiyue Feng, published by Sciendo.},
  author_keywords   = {BiLSTM; DTW; Music evaluation; PYIN; TextCNN; Traditional music},
  year              = {2024},
  doi               = {10.2478/amns-2024-0973},
  hasabstract       = {Y},
  journaltitle      = {Applied Mathematics and Nonlinear Sciences},
  keywords          = {Quality control; BiLSTM; Classification models; DTW; Emotion classification; Informatization; Music evaluation; PYIN; Teaching methods; TextCNN; Traditional music; Classification (of information)},
  priority          = {prio1},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192244090&doi=10.2478%2famns-2024-0973&partnerID=40&md5=f61a9cbc265402b97c5600f8b5f1d679},
}

@Article{yueksel2023em,
  author            = {Yüksel, Mustafa and Sarlik, Esra and Çiprut, Ayça},
  title             = {Emotions and Psychological Mechanisms of Listening to Music in Cochlear Implant Recipients},
  note              = {Cited by: 0},
  number            = {6},
  pages             = {1451 – 1463},
  volume            = {44},
  abstract          = {Objectives: Music is a multidimensional phenomenon and is classified by its arousal properties, emotional quality, and structural characteristics. Although structural features of music (i.e., pitch, timbre, and tempo) and music emotion recognition in cochlear implant (CI) recipients are popular research topics, music-evoked emotions, and related psychological mechanisms that reflect both the individual and social context of music are largely ignored. Understanding the music-evoked emotions (the "what") and related mechanisms (the "why") can help professionals and CI recipients better comprehend the impact of music on CI recipients' daily lives. Therefore, the purpose of this study is to evaluate these aspects in CI recipients and compare their findings to those of normal hearing (NH) controls. Design: This study included 50 CI recipients with diverse auditory experiences who were prelingually deafened (deafened at or before 6 years of age) - early implanted (N = 21), prelingually deafened - late implanted (implanted at or after 12 years of age - N = 13), and postlingually deafened (N = 16) as well as 50 age-matched NH controls. All participants completed the same survey, which included 28 emotions and 10 mechanisms (Brainstem reflex, Rhythmic entrainment, Evaluative Conditioning, Contagion, Visual imagery, Episodic memory, Musical expectancy, Aesthetic judgment, Cognitive appraisal, and Lyrics). Data were presented in detail for CI groups and compared between CI groups and between CI and NH groups. Results: The principal component analysis showed five emotion factors that are explained by 63.4% of the total variance, including anxiety and anger, happiness and pride, sadness and pain, sympathy and tenderness, and serenity and satisfaction in the CI group. Positive emotions such as happiness, tranquility, love, joy, and trust ranked as most often experienced in all groups, whereas negative and complex emotions such as guilt, fear, anger, and anxiety ranked lowest. The CI group ranked lyrics and rhythmic entrainment highest in the emotion mechanism, and there was a statistically significant group difference in the episodic memory mechanism, in which the prelingually deafened, early implanted group scored the lowest. Conclusion: Our findings indicate that music can evoke similar emotions in CI recipients with diverse auditory experiences as it does in NH individuals. However, prelingually deafened and early implanted individuals lack autobiographical memories associated with music, which affects the feelings evoked by music. In addition, the preference for rhythmic entrainment and lyrics as mechanisms of music-elicited emotions suggests that rehabilitation programs should pay particular attention to these cues.  © 2023 Wolters Kluwer Health, Inc. All rights reserved.},
  author_keywords   = {Cochlear implants; Emotion mechanisms; Music perception; Music-evoked emotions},
  year              = {2023},
  doi               = {10.1097/AUD.0000000000001388},
  hasabstract       = {Y},
  journaltitle      = {Ear and Hearing},
  keywords          = {Adolescent; Auditory Perception; Cochlear Implantation; Cochlear Implants; Emotions; Humans; Music; Recognition, Psychology; adolescent; cochlear implantation; emotion; hearing; human; music; physiology; psychology; recognition},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174641076&doi=10.1097%2fAUD.0000000000001388&partnerID=40&md5=d83ef4653a91206355589e4b8d16dde1},
}

@Article{ansani2023re,
  author            = {Ansani, Alessandro and Marini, Marco and Poggi, Isabella and Mallia, Luca},
  title             = {Recognition memory in movie scenes: the soundtrack induces mood-coherent bias, but not through mood induction},
  note              = {Cited by: 0},
  number            = {1},
  pages             = {59 – 75},
  volume            = {35},
  abstract          = {Several studies have employed music to affect various tasks through mood induction procedures. In this perspective, music’s emotional content coherently affects the listeners’ mood, which, in turn, affects performance. On the contrary, in film music cognition, schema theories suggest that music adds semantic information that interacts with the viewers’ previous knowledge and influences visual information processing. As in this interpretation the viewers’ mood is not deeply considered, it is not clear the extent to which music effects are also due to its power of affecting the viewers’ mood or rather a mere cognitive priming-like influence. An experiment (N = 169) on how music biases the recognition memory of a scene was built comparing semantic and emotional effects of soundtracks differing in valence (happy vs scary) during a recognition task. The results show that 1) music affected the viewers’ mood coherently with its valence, 2) music led to falsely recognise unseen objects as truly present when coherent with the soundtrack valence; and 3) the effect of music on the biased remembering was not mediated by the viewers’ mood, thus suggesting a strong interpretation of the schema theory in film music processing. Finally, a methodological reflection is provided on the issue of the manipulation check in experiments that employ musical stimuli to assess their influence on cognition. © 2022 Informa UK Limited, trading as Taylor & Francis Group.},
  author_keywords   = {audiovisual; false memories; memory; mood induction; schema theory; soundtrack},
  comment           = {movie},
  year              = {2023},
  doi               = {10.1080/20445911.2022.2116448},
  hasabstract       = {Y},
  journaltitle      = {Journal of Cognitive Psychology},
  keywords          = {adult; Article; auditory stimulation; cognition; controlled study; emotion regulation; false memory; fear; female; happiness; human; human experiment; male; memory; memory bias; mood; music; semantic memory; valence (emotion); visual information; visual stimulation; young adult},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136566305&doi=10.1080%2f20445911.2022.2116448&partnerID=40&md5=f006dac2b1fc9242ec00d2b9b8d6eb7f},
}

@Article{khabiri2024mu,
  author            = {Khabiri, Hamid and Talebi, Mohammad Naseh and Kamran, Mehdi Fakhimi and Akbari, Shadi and Zarrin, Farzaneh and Mohandesi, Fatemeh},
  title             = {Music-Induced Emotion Recognition Based on Feature Reduction Using PCA From EEG Signals},
  note              = {Cited by: 0; All Open Access, Gold Open Access},
  number            = {1},
  pages             = {59 – 68},
  volume            = {11},
  abstract          = {Purpose: Listening to music has a great impact on people's emotions and would change brain activity. In other words, music-induced emotions are trackable in electrical brain activities. Therefore, Electroencephalography can be a suitable tool to detect these induced emotions. The present study attempted to use electroencephalography in to recognize four types of emotions (happy, relaxing, stressful, and sad) induced in response to listening to music excerpts, using three classifiers. Materials and Methods: In this empirical study, electroencephalography signals were collected from 20 participants, as they were listening to pieces of selected music. The collected data were then pre-processed, and 28 linear and nonlinear features for recognizing the aforementioned emotions were extracted. Feature-space components were then reduced through a principal components analysis. Finally, the first ten components of feature-space were used as input for three classifiers based on Neural Network (NN), K-Nearest Neighbors (KNN), and Support Vector Machine (SVM) algorithms to identify the induced emotions. Results: The outputs showed that the suggested method was well capable of emotion recognition. Evaluating the music excerpts, on the self-assessment manikin scale, demonstrated that the labeling of the music tracks was accurate. The highest accuracy found among NN, KNN, and SVM algorithms were %84, %84, and %89 for happy emotions, respectively. Conclusion: The findings of this study provide useful insights into emotion classification and brain behavior related to induced emotion extraction. Happiness was the most recognizable emotion and the support vector machine had the highest performance among the classifiers. In the end, the outcomes of the proposed method demonstrate that this system is better than the previous research in EEG-based emotion recognition. © 2024 Tehran University of Medical Sciences.},
  author_keywords   = {Classification; Electroencephalography; Emotion Recognition; Music; Principal Component Analysis},
  year              = {2024},
  doi               = {10.18502/fbt.v11i1.14512},
  hasabstract       = {Y},
  journaltitle      = {Frontiers in Biomedical Technologies},
  keywords          = {adult; Article; brain mapping; calculation; confusion matrix; cross validation; depression; electroencephalogram; electroencephalography; electromyogram; electrooculogram; emotion; empiricism; entropy; false negative result; false positive result; hearing impairment; human; independent component analysis; k nearest neighbor; music; nerve cell network; principal component analysis; recognition; self evaluation; sensitivity and specificity; sleep disorder; support vector machine},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181574150&doi=10.18502%2ffbt.v11i1.14512&partnerID=40&md5=159ea31765f7dbce92a7d3dd9fe41694},
}

@Article{wang2022co,
  author            = {Wang, Xin and Wang, Li and Xie, Lingyun},
  title             = {Comparison and Analysis of Acoustic Features of Western and Chinese Classical Music Emotion Recognition Based on V‐A Model},
  note              = {Cited by: 7; All Open Access, Gold Open Access},
  number            = {12},
  volume            = {12},
  abstract          = {Music emotion recognition is increasingly becoming important in scientific research and practical applications. Due to the differences in musical characteristics between Western and Chinese classical music, it is necessary to investigate the distinctions in music emotional feature sets to improve the accuracy of cross‐cultural emotion recognition models. Therefore, a comparative study on emotion recognition in Chinese and Western classical music was conducted. Using the V‐A model as an emotional perception model, approximately 1000 pieces of Western and Chinese classical excerpts in total were selected, and approximately 20‐dimension feature sets for different emotional dimensions of different datasets were finally extracted. We considered different kinds of al-gorithms at each step of the training process, from pre‐processing to feature selection and regression model selection. The results reveal that the combination of MaxAbsScaler pre‐processing and the wrapper method using the recursive feature elimination algorithm based on extremely randomized trees is the optimal algorithm. The harmonic change detection function is a culturally universal fea-ture, whereas spectral flux is a culturally specific feature for Chinese classical music. It is also found that pitch features are more significant for Western classical music, whereas loudness and rhythm features are more significant for Chinese classical music. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.},
  author_keywords   = {classical music; extreme random tree; feature selection; music emotion recognition; V‐A model},
  year              = {2022},
  doi               = {10.3390/app12125787},
  hasabstract       = {Y},
  journaltitle      = {Applied Sciences (Switzerland)},
  priority          = {prio1},
  publication_stage = {Final},
  ranking           = {rank2},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132193259&doi=10.3390%2fapp12125787&partnerID=40&md5=96f19b725a52bb52a769612e02f0a74c},
}

@Article{jin2024em,
  author            = {Jin, Fenglin},
  title             = {Emotion Appreciation Strategy in College Music Teaching Based on Improved Multimodal RCNN},
  note              = {Cited by: 0; All Open Access, Gold Open Access},
  number            = {1},
  volume            = {9},
  abstract          = {People's judgment of music emotion is highly subjective; how to quantify the music emotion characteristics is the key to solving the music emotion recognition problem. This paper utilizes the Fourier transform method to preprocess the input music sample signal. A digital filter accomplishes the pre-emphasis operation, and the number of frames in the music signal is determined by splitting and windowing through a convolution operation. By utilizing the Mel frequency cepstrum coefficient and cochlear frequency, emotional features of music can be extracted. Improve the multimodal model based on the RCNN algorithm, propose the TWC music emotion framework, and construct a music emotion recognition model that incorporates the improved multimodal RCNN. The proposed model's impact on music emotion appreciation is evaluated through experiments to identify music emotions and an analysis of college music teaching practices that emphasize emotion appreciation. The results show that 1376 songs belonging to the category of "relaxation"are assigned to the category of "healing", which is only 4 songs short of the target, and the labeling of the songs is not homogeneous, and the emotional recognition of the model is consistent with the cognition. The mean value of the empathy ability of college students in music emotion appreciation is 69.13, which is in the middle-upper level, indicating that the model proposed in this paper has a good effect on the cultivation of students' music emotion appreciation. © 2023 Fenglin Jin, published by Sciendo.},
  author_keywords   = {Fourier Transform; Mel Frequency Cepstrum Coefficient; Multimodal RCNN; Music Emotion Characterization; Music Emotion Recognition},
  year              = {2024},
  doi               = {10.2478/amns-2024-0129},
  hasabstract       = {Y},
  journaltitle      = {Applied Mathematics and Nonlinear Sciences},
  keywords          = {Digital filters; Speech recognition; Students; Emotion recognition; Fourier transform method; Mel frequency cepstrum coefficients; Multi-modal; Multimodal RCNN; Music emotion characterization; Music emotion recognition; Music emotions; Preprocess; Emotion Recognition},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185194732&doi=10.2478%2famns-2024-0129&partnerID=40&md5=489a975bb74741ebd5569ca8d4f0abd0},
}

@Article{he2024hu,
  author            = {He, Ruidi and Geng, Miaoping and Guo, Jia},
  title             = {Human-computer Interaction Based Music Emotion Visualization System and User Experience Assessment},
  note              = {Cited by: 0; All Open Access, Bronze Open Access},
  number            = {S7},
  pages             = {133 – 147},
  volume            = {21},
  abstract          = {The perceptual and auditory standard of music is deeply integrated with the emerging multimedia to a higher degree, thus forming the music visualization. It is a process presentation method, which provides a brand-new way of interpretation and deduction for music appreciation. In this article, the application of computer aided design (CAD) in music emotion visualization system is studied, and a mapping model between music characteristics and emotion for digital music emotion recognition is constructed by combining with convolutional neural network (CNN). Combined with CAD technology, the structural music features are extracted and calculated, and the main melody and auxiliary melody of music are obtained. Then, based on the separated main melody and auxiliary melody, comprehensive visualization design is carried out to realize the visualization method of highlighting the main melody. In the experimental part, the performance of music emotion recognition algorithm is tested and the user experience is assessed. The results show that the simulation accuracy and user interaction experience of this system have achieved good results, which can improve the interaction between CAD design and viewing of music emotion visualization. Compared with the recurrent neural network (RNN), support vector machine (SVM) and other emotion recognition models, this model has a higher recognition rate of music emotion, which is of great significance to the research of music emotion visualization system. © 2024 U-turn Press LLC.},
  author_keywords   = {CAD; Emotion Recognition; Human-Computer Interaction; Music Visualization},
  year              = {2024},
  doi               = {10.14733/cadaps.2024.S7.133-147},
  hasabstract       = {Y},
  journaltitle      = {Computer-Aided Design and Applications},
  keywords          = {Emotion Recognition; Human computer interaction; Music; Recurrent neural networks; Speech recognition; Support vector machines; Visualization; Computer-aided design; Convolutional neural network; Design technologies; Digital music; Emotion recognition; Mapping modeling; Music emotions; Music visualization; User experience assessments; Visualization system; Computer aided design},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171627576&doi=10.14733%2fcadaps.2024.S7.133-147&partnerID=40&md5=1b915e8c337698f1c747b31632a37d0d},
}

@Article{liu2023ti,
  author            = {Liu, Dingding and Bu, Su},
  title             = {Timbre Classification Method based on Computer-Aided Technology for Internet of Things},
  note              = {Cited by: 0; All Open Access, Bronze Open Access},
  number            = {S2},
  pages             = {167 – 179},
  volume            = {20},
  abstract          = {The timbre of different melodies may have different personalities, so as to express different feelings and artistic styles. The effective extraction of timbre information is the key to successfully identify musical instruments. In order to solve the problems of low recognition accuracy and high time cost in the current timbre recognition system, an intelligent musical instrument timbre classification system is proposed and designed in combination with the computer-aided technology of the Internet of things. Firstly, a 5-Dimensional emotion space is determined by MDS method. According to the 5-Dimensional emotion space, the emotion evaluation experiment is carried out, and the reliability and validity of the experimental data are tested and the noise is eliminated. Then, the effects of performance content, time domain characteristics and instrument type on the relationship between timbre perception characteristics and emotion are studied. It is found that the time domain characteristics and performance content have little impact on the relationship between timbre perception characteristics and emotion, and the instrument type will have a certain impact on the relationship between timbre perception characteristics and emotion. Finally, five emotion prediction models are established by using multiple linear regression algorithm, and the models have good prediction ability for the five emotions. Simulation and experimental results show that the proposed system can quickly extract the characteristics of harmonic structure of musical signal, and the timbre recognition system based on this can well reflect the timbre characteristics of musical instruments, which provides a new idea for the feature extraction of musical signal. © 2023 CAD Solutions, LLC.},
  author_keywords   = {computer aided design; feature extraction; Internet of things; musical instrument classification; timbre},
  year              = {2023},
  doi               = {10.14733/cadaps.2023.S2.167-179},
  hasabstract       = {Y},
  journaltitle      = {Computer-Aided Design and Applications},
  keywords          = {Behavioral research; Classification (of information); Computer aided design; Emotion Recognition; Extraction; Internet of things; Linear regression; Music; Musical instruments; Time domain analysis; Computer aided technologies; Computer-aided design; Computer-aided technologies; Emotion spaces; Features extraction; Musical instrument classifications; Performance; Recognition systems; Timbre; Timbre classification; Feature extraction},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136208115&doi=10.14733%2fcadaps.2023.S2.167-179&partnerID=40&md5=9ba739616426ea79c4270609e5df0b7c},
}

@Article{wang2022mu,
  author            = {Wang, Chen and Zhao, Yu},
  title             = {Music Emotion Recognition Based on Bilayer Feature Extraction},
  note              = {Cited by: 1; All Open Access, Gold Open Access},
  volume            = {2022},
  abstract          = {Music is a kind of art which is to express the thoughts and emotion and reflect the reality life by organized sounds; every piece of music expresses emotions through lyrics and melodies. Human emotions are rich and colorful, and there are also differences in music. It is unreasonable for a song to correspond to only one emotional feature. According to the results of existing algorithms, some music has obviously wrong category labels. To solve the above problems, we established a two-layer feature extraction model based on a spectrogram from shallow to deep (from primary feature extraction to deep feature extraction), which can not only extract the most basic musical features but also dig out the deep emotional features. And further classify the features with the improved CRNN neural network, and get the final music emotion category. Through a large number of comparative experiments, it is proven that our model is suitable for a music classification task.  © 2022 Chen Wang and Yu Zhao.},
  year              = {2022},
  doi               = {10.1155/2022/7832548},
  hasabstract       = {Y},
  journaltitle      = {Wireless Communications and Mobile Computing},
  keywords          = {Arts computing; Classification (of information); Emotion Recognition; Extraction; Feature extraction; Bi-layer; Emotion recognition; Express emotions; Extraction modeling; Features extraction; Human emotion; Model-based OPC; Music emotions; Spectrograms; Two-layer; Music},
  priority          = {prio1},
  publication_stage = {Final},
  ranking           = {rank2},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134532351&doi=10.1155%2f2022%2f7832548&partnerID=40&md5=52ee0d7f530c4b616baf2965109291f8},
}

@Article{gu2024ex,
  author            = {Gu, Xiaohu and Jiang, Leqi and Chen, Hao and Li, Ming and Liu, Chang},
  title             = {Exploring Brain Dynamics via EEG and Steady-State Activation Map Networks in Music Composition},
  note              = {Cited by: 0; All Open Access, Gold Open Access},
  number            = {3},
  volume            = {14},
  abstract          = {In recent years, the integration of brain–computer interface technology and neural networks in the field of music generation has garnered widespread attention. These studies aimed to extract individual-specific emotional and state information from electroencephalogram (EEG) signals to generate unique musical compositions. While existing research has focused primarily on brain regions associated with emotions, this study extends this research to brain regions related to musical composition. To this end, a novel neural network model incorporating attention mechanisms and steady-state activation mapping (SSAM) was proposed. In this model, the self-attention module enhances task-related information in the current state matrix, while the extended attention module captures the importance of state matrices over different time frames. Additionally, a convolutional neural network layer is used to capture spatial information. Finally, the ECA module integrates the frequency information learned by the model in each of the four frequency bands, mapping these by learning their complementary frequency information into the final attention representation. Evaluations conducted on a dataset specifically constructed for this study revealed that the model surpassed representative models in the emotion recognition field, with recognition rate improvements of 1.47% and 3.83% for two different music states. Analysis of the attention matrix indicates that the left frontal lobe and occipital lobe are the most critical brain regions in distinguishing between ‘recall and creation’ states, while FP1, FPZ, O1, OZ, and O2 are the electrodes most related to this state. In our study of the correlations and significances between these areas and other electrodes, we found that individuals with musical training exhibit more extensive functional connectivity across multiple brain regions. This discovery not only deepens our understanding of how musical training can enhance the brain’s ability to work in coordination but also provides crucial guidance for the advancement of brain–computer music generation technologies, particularly in the selection of key brain areas and electrode configurations. We hope our research can guide the work of EEG-based music generation to create better and more personalized music. © 2024 by the authors.},
  author_keywords   = {EEG state recognition; music creation; steady-state activation map (SSAM)},
  year              = {2024},
  doi               = {10.3390/brainsci14030216},
  hasabstract       = {Y},
  journaltitle      = {Brain Sciences},
  keywords          = {adult; Article; artificial neural network; auditory stimulation; BOLD signal; brain depth stimulation; brain region; convolutional neural network; dynamics; electroencephalogram; electroencephalography; emotion; entropy; female; frontal lobe; functional connectivity; functional magnetic resonance imaging; human; human experiment; image analysis; learning algorithm; machine learning; male; measurement accuracy; music; nerve cell network; neuroimaging; occipital lobe; receiver operating characteristic; skin conductance; steady state; time series analysis; transcranial direct current stimulation; visual field; young adult},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188728231&doi=10.3390%2fbrainsci14030216&partnerID=40&md5=cff276b6c0efb64c099a17554e32391c},
}

@Article{latif2023as,
  author            = {Latif, Siddique and Cuayáhuitl, Heriberto and Pervez, Farrukh and Shamshad, Fahad and Ali, Hafiz Shehbaz and Cambria, Erik},
  title             = {A survey on deep reinforcement learning for audio-based applications},
  note              = {Cited by: 12; All Open Access, Hybrid Gold Open Access},
  number            = {3},
  pages             = {2193 – 2240},
  volume            = {56},
  abstract          = {Deep reinforcement learning (DRL) is poised to revolutionise the field of artificial intelligence (AI) by endowing autonomous systems with high levels of understanding of the real world. Currently, deep learning (DL) is enabling DRL to effectively solve various intractable problems in various fields including computer vision, natural language processing, healthcare, robotics, to name a few. Most importantly, DRL algorithms are also being employed in audio signal processing to learn directly from speech, music and other sound signals in order to create audio-based autonomous systems that have many promising applications in the real world. In this article, we conduct a comprehensive survey on the progress of DRL in the audio domain by bringing together research studies across different but related areas in speech and music. We begin with an introduction to the general field of DL and reinforcement learning (RL), then progress to the main DRL methods and their applications in the audio domain. We conclude by presenting important challenges faced by audio-based DRL agents and by highlighting open areas for future research and investigation. The findings of this paper will guide researchers interested in DRL for the audio domain. © 2022, The Author(s).},
  author_keywords   = {(Embodied) dialogue; Deep learning; Emotion recognition; Reinforcement learning; Speech recognition},
  year              = {2023},
  doi               = {10.1007/s10462-022-10224-2},
  hasabstract       = {Y},
  journaltitle      = {Artificial Intelligence Review},
  keywords          = {Audio acoustics; Audio systems; Deep learning; Emotion Recognition; Learning systems; Music; Reinforcement learning; Surveys; (embodied) dialog; Audio-based; Deep learning; Emotion recognition; Healthcare robotics; Language processing; Natural languages; Real-world; Reinforcement learning algorithms; Reinforcement learnings; Speech recognition},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133247161&doi=10.1007%2fs10462-022-10224-2&partnerID=40&md5=59663fd9ccec2b8bccbbb3623f03249c},
}

@Article{aydin2024gr,
  author            = {Aydın, Serap and Onbaşı, Lara},
  title             = {Graph theoretical brain connectivity measures to investigate neural correlates of music rhythms associated with fear and anger},
  note              = {Cited by: 3},
  number            = {1},
  pages             = {49 – 66},
  volume            = {18},
  abstract          = {The present study tests the hypothesis that emotions of fear and anger are associated with distinct psychophysiological and neural circuitry according to discrete emotion model due to contrasting neurotransmitter activities, despite being included in the same affective group in many studies due to similar arousal-valance scores of them in emotion models. EEG data is downloaded from OpenNeuro platform with access number of ds002721. Brain connectivity estimations are obtained by using both functional and effective connectivity estimators in analysis of short (2 sec) and long (6 sec) EEG segments across the cortex. In tests, discrete emotions and resting-states are identified by frequency band specific brain network measures and then contrasting emotional states are deep classified with 5-fold cross-validated Long Short Term Memory Networks. Logistic regression modeling has also been examined to provide robust performance criteria. Commonly, the best results are obtained by using Partial Directed Coherence in Gamma (31.5-60.5Hz) sub-bands of short EEG segments. In particular, Fear and Anger have been classified with accuracy of 91.79%. Thus, our hypothesis is supported by overall results. In conclusion, Anger is found to be characterized by increased transitivity and decreased local efficiency in addition to lower modularity in Gamma-band in comparison to fear. Local efficiency refers functional brain segregation originated from the ability of the brain to exchange information locally. Transitivity refer the overall probability for the brain having adjacent neural populations interconnected, thus revealing the existence of tightly connected cortical regions. Modularity quantifies how well the brain can be partitioned into functional cortical regions. In conclusion, PDC is proposed to graph theoretical analysis of short EEG epochs in presenting robust emotional indicators sensitive to perception of affective sounds. © The Author(s), under exclusive licence to Springer Nature B.V. 2023.},
  author_keywords   = {Brain connectivity; EEG; Emotion recognition; Graph theory; Music perception},
  year              = {2024},
  doi               = {10.1007/s11571-023-09931-5},
  hasabstract       = {Y},
  journaltitle      = {Cognitive Neurodynamics},
  keywords          = {accuracy; adolescent; adult; aged; anger; Article; correlation coefficient; deep learning; electroencephalogram; emotion; fear; functional connectivity; gamma rhythm; graph theoretical brain connectivity; human; human experiment; logistic regression analysis; music; normal human; psychophysiology; stimulus response},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146813487&doi=10.1007%2fs11571-023-09931-5&partnerID=40&md5=3830a6b2cffb34a4162f2fabd15d541e},
}

@Article{moltrasio2023em,
  author            = {Moltrasio, Julieta and Rubinstein, Wanda},
  title             = {Emotional memory modulation through music in patients with Alzheimer’s Disease; [Modulação da memória emocional por meio da música em pacientes com demência do tipo Alzheimer]; [Modulación de la memoria emocional a través de la música en pacientes con demencia tipo alzhéimer]},
  note              = {Cited by: 0; All Open Access, Gold Open Access},
  number            = {2},
  volume            = {17},
  abstract          = {Emotional stimuli are better remembered than neutral ones. Music generates emotional arousal and can modulate memories in young and older adults. Studies show that in patients with Alzheimer’s Disease (AD) music improves word encoding and retrieval of autobiographical memories. Few studies used music as a post-learning treatment, showing a decrease in false positives in recognition. The aim of this work is to study the modulation of memory through music in patients with AD. 75 patients with AD were assessed. They observed emotional and neutral pictures, and then received a musical or neutral treatment: arousing music, relaxing music or white noise. Then, they recalled the pictures they remembered followed by a recognition task. We repeated this task a week later (delayed recall). The results indicated a decrease in false positives in delayed recognition in the group exposed to arousing music. In conclusion, music is capable of modulating memories in patients with AD. This modulation differs from what happens in other populations, which could be due to anatomical differences. The results support the use of music as a possible treatment for memory consolidation. © 2023 Universidad Catolica del Uruguay. All rights reserved.},
  author_keywords   = {Alzheimer’s disease; emotion; memory; music},
  year              = {2023},
  doi               = {10.22235/cp.v17i2.3270},
  hasabstract       = {Y},
  journaltitle      = {Ciencias Psicologicas},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188462596&doi=10.22235%2fcp.v17i2.3270&partnerID=40&md5=971482db54569d0a6c0d5c3993f864c4},
}

@Article{wang2022mua,
  author            = {Wang, Jingjing and Huang, Ru},
  title             = {Music Emotion Recognition Based on the Broad and Deep Learning Network; [基于宽深学习网络的音乐情感识别]},
  note              = {Cited by: 2},
  number            = {3},
  pages             = {373 – 380},
  volume            = {48},
  abstract          = {With the development of artificial intelligence and digital audio technology, music information retrieval (MIR) has gradually become a research hotspot. Meanwhile, music emotion recognition (MER) is becoming an important research direction, due to its great research value for video soundtracks. Although some researchers combine Mel Frequency Cepstral coefficient (MFCC) and Residual Phase (RP) to extract music emotional features and improve classification accuracy, the training models in traditional deep learning takes longer time. In order to improve the efficiency of feature mining of music emotional features, MFCC and RP are weighted and combined in this work to extract music emotion features so that the mining efficiency of music emotion features can be effectively improved. At the same time, in order to improve the classification accuracy of music emotion and shorten the training time of the model, by integrating the Long Short-Term Memory (LSTM) and the Broad Learning System (BLS), a new wide and deep learning network (LSTM-BLS) is further built to train music emotion recognition and classification by using LSTM as the feature mapping node of BLS. The network structure of this model makes full use of the ability of BLS to quickly process complex data. Its advantages are simple structure and short model training time, thereby improving recognition efficiency, and LSTM has excellent performance in extracting time series features from time series data. The time sequence relationship of music can be extracted so that the emotional characteristics of the music can be preserved to the greatest extent. Finally, the experimental results on the emotion dataset show that the proposed algorithm can achieve higher recognition accuracy than other complex networks and provide new feasible ideas for the music emotion recognition. © 2022 East China University of Science and Technology. All rights reserved.},
  author_keywords   = {broad learning; deep learning; long short-term memory; music emotion recognition; residual phase},
  year              = {2022},
  doi               = {10.14135/j.cnki.1006-3080.20210225007},
  hasabstract       = {Y},
  journaltitle      = {Huadong Ligong Daxue Xuebao/Journal of East China University of Science and Technology},
  priority          = {prio1},
  publication_stage = {Final},
  ranking           = {rank2},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167579262&doi=10.14135%2fj.cnki.1006-3080.20210225007&partnerID=40&md5=dd8ef11135681629c977177fd3d05597},
}

@Article{xia2022st,
  author            = {Xia, Yu and Xu, Fumei},
  title             = {Study on Music Emotion Recognition Based on the Machine Learning Model Clustering Algorithm},
  note              = {Cited by: 6; All Open Access, Gold Open Access},
  volume            = {2022},
  abstract          = {In recent years, the explosive growth of online music resources makes it difficult to retrieve and manage music information. To efficiently retrieve and classify music information has become a hot research topic. Thayer's two-dimensional emotion plane is selected as the basis for establishing the music emotion database. Music is divided into five categories, the concept of continuous emotion perception is introduced, and music emotion is regarded as a point on a two-dimensional emotional plane, together with the two sentiment variables to determine its location. The artificial labeling method is used to determine the position range of the five types of emotions on the emotional plane, and the regression method is used to obtain the relationship between the VA value and the music features so that the music emotion classification problem is transformed into a regression problem. A regression-based music emotion classification system is designed and implemented, which mainly includes a training part and a testing part. In the training part, three algorithms, namely, polynomial regression, support vector regression, and k-plane piecewise regression, are used to obtain the regression model. In the test part, the input music data is regressed and predicted to obtain its VA value and then classified, and the system performance is considered by classification accuracy. Results show that the combined method of support vector regression and k-plane piecewise regression improves the accuracy by 3 to 4 percentage points compared to using one algorithm alone; compared with the traditional classification method based on a support vector machine, the accuracy improves by 6 percentage points. Music emotion is classified by algorithms such as support vector machine classification, K-neighborhood classification, fuzzy neural network classification, fuzzy K-neighborhood classification, Bayesian classification, and Fisher linear discrimination, among which the support vector machine, fuzzy K-neighborhood, and the accuracy rate of music emotion classification realized by Fisher linear discriminant algorithm are more than 80%; a new algorithm "mixed classifier"is proposed, and the music emotion recognition rate based on this algorithm reaches 84.9%. © 2022 Yu Xia and Fumei Xu.},
  year              = {2022},
  doi               = {10.1155/2022/9256586},
  hasabstract       = {Y},
  journaltitle      = {Mathematical Problems in Engineering},
  keywords          = {Behavioral research; Classification (of information); Clustering algorithms; Fuzzy neural networks; Information management; Regression analysis; Speech recognition; Support vector machines; Vectors; Classifieds; Emotion recognition; K neighborhoods; Music emotion classifications; Music emotions; Music information; Percentage points; Piecewise regression; Support vector regressions; Two-dimensional; Emotion Recognition},
  priority          = {prio1},
  publication_stage = {Final},
  ranking           = {rank2},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140847927&doi=10.1155%2f2022%2f9256586&partnerID=40&md5=95604b41d31354a5954af89be5f47193},
}

@Article{asadzadeh2023ac,
  author            = {Asadzadeh, Shiva and Rezaii, Tohid Yousefi and Beheshti, Soosan and Meshgini, Saeed},
  title             = {Accurate Emotion Recognition Utilizing Extracted EEG Sources as Graph Neural Network Nodes},
  note              = {Cited by: 6},
  number            = {1},
  pages             = {176 – 189},
  volume            = {15},
  abstract          = {Automated analysis and recognition of human emotion play an important role in the development of a human–computer interface. High temporal resolution of EEG signals enables us to noninvasively study the emotional brain activities. However, one major obstacle in this procedure is extracting the essential information in presence of the low spatial resolution of EEG recordings. The pattern of each emotion is clearly defined by mapping from scalp sensors to brain sources using the standardized low-resolution electromagnetic tomography (sLORETA) method. A graph neural network (GNN) is then used for EEG-based emotion recognition in which sLORETA sources are considered as the nodes of the underlying graph. In the proposed method, the inter-source relations in EEG source signals are encoded in the adjacency matrix of GNN. Finally, the labels of the unseen emotions are recognized using a GNN classifier. The experiments on the recorded EEG dataset by inducing excitement through music (recorded in brain-computer interface research lab, University of Tabriz) indicate that the brain source activity modeling by ESB-G3N significantly improves the accuracy of emotion recognition. Experimental results show classification accuracy of 98.35% for two-class classification of positive and negative emotions. In this paper, we concentrate on extracting active emotional cortical sources using EEG source imaging (ESI) techniques. Auditory stimuli are used to rapidly and efficiently induce emotions in participants (visual stimuli in terms of video/image are either slow or inefficient in inducing emotions). We propose the use of active EEG sources as graph nodes by EEG source-based GNN node (ESB-G3N) algorithm. The results show an absolute improvement of 1–2% over subject-dependent and subject-independent scenarions compared to the existing approaches. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
  author_keywords   = {EEG source localization; Emotion recognition; Graph neural network; sLORETA; The inverse problem},
  year              = {2023},
  doi               = {10.1007/s12559-022-10077-5},
  hasabstract       = {Y},
  journaltitle      = {Cognitive Computation},
  keywords          = {Brain; Brain mapping; Emotion Recognition; Graph theory; Speech recognition; AS graph; Automated analysis; EEG source localization; Electromagnetic tomography; Emotion recognition; Graph neural networks; Lower resolution; Network node; Standardized low-resolution electromagnetic tomography; The inverse problem; Inverse problems},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143664829&doi=10.1007%2fs12559-022-10077-5&partnerID=40&md5=296e6f35e6af88aadc7291a590282f9b},
}

@Article{li2024im,
  author            = {Li, Jiajia and Soradi-Zeid, Samaneh and Yousefpour, Amin and Pan, Daohua},
  title             = {Improved differential evolution algorithm based convolutional neural network for emotional analysis of music data},
  note              = {Cited by: 1},
  volume            = {153},
  abstract          = {Evolutionary computation is derived from the simulation of natural selection and genetic processes in biological evolution. This approach provides a method for optimizing the structure and parameters of neural networks. When combined with neural networks, forming what's termed as evolutionary computation based neural networks, it offers a systematic approach to optimize neural network models in diverse applications. In this study, we introduce a method that employs differential evolution algorithms to optimize parameters of convolutional neural network (CNN) for music emotion recognition tasks. This method optimizes the initial weights of the CNN, aiming to achieve near-global optimal solutions and expedite network convergence. Comparative experiments indicate that the proposed approach effectively identifies optimal parameters and structures for CNN, suggesting potential advancements in automated music emotion recognition. © 2024 Elsevier B.V.},
  author_keywords   = {CNN; Differential evolution; Emotional analysis; Evolutionary computation; Music data},
  year              = {2024},
  doi               = {10.1016/j.asoc.2024.111262},
  hasabstract       = {Y},
  journaltitle      = {Applied Soft Computing},
  keywords          = {Bioinformatics; Convolution; Convolutional neural networks; Emotion Recognition; Speech recognition; Structural optimization; Convolutional neural network; Differential Evolution; Differential evolution algorithms; Emotion recognition; Emotional analysis; Improved differential evolutions; Music data; Music emotions; Natural selection process; Neural-networks; Evolutionary algorithms},
  priority          = {prio1},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183833934&doi=10.1016%2fj.asoc.2024.111262&partnerID=40&md5=0c127bfaac9efc8e0dd2135c3ef8af5d},
}

@Article{zhang2024ap,
  author            = {Zhang, Yao and Cai, Delin and Zhang, Dongmei},
  title             = {Application and algorithm optimization of music emotion recognition in piano performance evaluation},
  note              = {Cited by: 0; All Open Access, Gold Open Access},
  number            = {4},
  volume            = {9},
  abstract          = {In the current research, we integrate distinct learning modalities—Curriculum Learning (CL) and Reinforcement Learning (RL)—in an attempt to develop and optimize Music Emotion Recognition (MER) in piano performance. Classical approaches have never been successful when applied in the field of determining the degree of emotion in the music of the piano, owing to the substantial complexity required. Addressing this particular issue is the primary motivation for the present endeavour. In an approach that’s comparable to how human beings acquire information, it trains the RL agent CL in phases; such an approach improves the student’s learning model in understanding the diverse emotions expressed by musical compositions. A higher rating of performance can be achieved after learning the model to recognize emotions more effectively and precisely. A set of piano melodies with emotional content notes has been included in the EMOPIA repository for use when conducting the process of evaluation. In order to benchmark the proposed approach with different models, parameters including R2, Mean Absolute Error (MAE), and Root Mean Square Error (RMSE) were deployed. Studies indicate that the recommended approach accurately recognizes the emotions expressed by piano-playing music. In challenging tasks like MER, the significance of implementing the CL paradigm with the RL has been emphasized using the outcomes mentioned earlier. © 2024 by author(s).},
  author_keywords   = {Curriculum Learning; Machine Learning; MBE; Music Emotion Recognition; piano music; Reinforcement Learning; RMSE},
  year              = {2024},
  doi               = {10.54517/esp.v9i4.2344},
  hasabstract       = {Y},
  journaltitle      = {Environment and Social Psychology},
  priority          = {prio1},
  publication_stage = {Final},
  ranking           = {rank2},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184876855&doi=10.54517%2fesp.v9i4.2344&partnerID=40&md5=f2f5f142bc51490d199e335a63b73647},
}

@Article{wang2024em,
  author            = {Wang, Lu},
  title             = {Embedded Systems for Analyzing Digital Art Aesthetics in Piano Performances using Emotional Recognition},
  note              = {Cited by: 0; All Open Access, Bronze Open Access},
  number            = {s8},
  pages             = {44 – 55},
  volume            = {21},
  abstract          = {Aesthetics is an innate ability. It is a meaningful study to make computers perceive "beauty", discover "beauty" and generate "beauty". With the deepening of intelligent optimization algorithm research, artificial intelligence technology "aesthetic" has penetrated into photos, paintings, web pages, ICONS, men and other aspects. However, there are very few studies on the evaluation of piano performance aesthetics. The study of piano performance aesthetics has certain research significance. First of all, limited by personal time and energy, people cannot select high-quality piano repertoire quickly. Secondly, limited by personal aesthetic consciousness and aesthetic ability, people cannot improve the aesthetic quality of piano music just like professional piano players. In the face of such problems, the aesthetic quality evaluation and improvement technology with artificial intelligence as the core provides economically feasible solutions for people to obtain high-quality tracks. Meanwhile, this technology promotes the development of simulated human aesthetic and thinking technology in the field of artificial intelligence. Since the key to aesthetics lies in the perception and classification of piano music score, timbre, audio and emotion, the emotion recognition of piano performance is crucial for the research of artificial intelligence "aesthetics". Piano performance emotion recognition is realized by using the computer to analyze performance characteristics and according to the mapping relationship between performance characteristics and emotion. The study of automatic emotion recognition of piano performance is of great significance to improve the human-computer emotional interaction ability of computer. Based on the above analysis, the main work and innovations of this paper are as follows:This paper first with MIDI music file as a research sample, follow the research method of classical music theory, combined with music psychology, cognitive psychology, music aesthetics and other related research results, the characteristics of the piano performance of a comprehensive and detailed description, and established a set of suitable for computer understanding and expression of the piano performance characteristics system. In the process of feature extraction of piano performance features, high-level features such as rhythm, speed and melody are mathematically defined. In this paper, we realize the computer recognition of the piano playing emotion by using the BP neural network. Finally, the research in this paper can realize the emotional classification of piano performance from the perspective of artificial intelligence, which can use the above research content to quickly and automatically select high-quality piano performance tracks, saving a lot of time for manual screening. © 2024 U-turn Press LLC.},
  author_keywords   = {Aesthetic Research; BP neural network; Digital Art; Embedded Systems; Emotional Recognition; Piano Playing},
  year              = {2024},
  doi               = {10.14733/cadaps.2024.S8.44-55},
  hasabstract       = {Y},
  journaltitle      = {Computer-Aided Design and Applications},
  keywords          = {Behavioral research; Embedded systems; Emotion Recognition; Human computer interaction; Musical instruments; Neural networks; Quality control; Speech recognition; Websites; Aesthetic qualities; BP neural networks; Digital art; Embedded-system; Emotional recognition; Esthetic research; High quality; Performance; Performance characteristics; Piano playing; Music},
  priority          = {prio1},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171447451&doi=10.14733%2fcadaps.2024.S8.44-55&partnerID=40&md5=a5abac1875bbab01840e5162b2f2ad88},
}

@Article{deng2022br,
  author            = {Deng, Jin and Chen, Yuewei and Zeng, Weiming and Luo, Xiaoqi and Li, Ying},
  title             = {Brain Response of Major Depressive Disorder Patients to Emotionally Positive and Negative Music},
  note              = {Cited by: 2},
  number            = {10},
  pages             = {2094 – 2105},
  volume            = {72},
  abstract          = {Depression is characterized by poor emotion regulation that makes it difficult to escape the effects of emotional pain, but the neuromodulation behind these symptoms is still unclear. This study investigated the neural mechanism of emotional state-related responses during music stimuli in participants with major depressive disorder (MDD) compared to never-depressed (ND) controls. A novel two-level feature selection method, integrating recursive feature elimination based on support vector machine (SVM-RFE) and random forest algorithm (RF), was proposed to screen emotional recognition brain regions (ERBRs). On this basis, the differences of functional connectivity (FC) were systematically analyzed by two-sample t-test. The results demonstrate that ND participants show eight pairs of FCs with a significant difference between positive emotional music stimuli (pEMS) versus negative emotional music stimuli (nEMS) in 15 ERBRs of MDD, but the participants with MDD show one pair of significant difference in FC. The decreased number reflects the fuzzy response to positive and negative emotions in MDD, which appears to arise from obstacle to emotional cognition and regulation. Furthermore, there was no significant difference in FC between MDDs and NDs under pEMS, but a significant difference was detected between the two groups under nEMS (p < 0.01), revealing a ‘bias’ against the negative state in MDD. The current study may help to better comprehend the abnormal evolution from normal to depression and inform the utilization of pEMS in formal treatment for depression. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
  author_keywords   = {Depression; Emotion; fMRI; Functional connectivity},
  year              = {2022},
  doi               = {10.1007/s12031-022-02061-3},
  hasabstract       = {Y},
  journaltitle      = {Journal of Molecular Neuroscience},
  keywords          = {Brain; Depressive Disorder, Major; Emotions; Humans; Magnetic Resonance Imaging; Music; adult; amygdala; Article; back propagation neural network; brain region; classification algorithm; clinical article; cognition; controlled study; diagnostic accuracy; emotion; emotion regulation; feature selection; female; functional connectivity; functional magnetic resonance imaging; human; k nearest neighbor; linear regression analysis; major depression; male; music; random forest; recognition; recursive feature elimination; support vector machine; time series analysis; brain; emotion; major depression; nuclear magnetic resonance imaging; physiology; procedures},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137063103&doi=10.1007%2fs12031-022-02061-3&partnerID=40&md5=ef7091a7f43d3575fdabc166b0ab74d6},
}

@Article{bhuvanakumar2023em,
  author            = {Bhuvana Kumar, V. and Kathiravan, M.},
  title             = {Emotion recognition from MIDI musical file using Enhanced Residual Gated Recurrent Unit architecture},
  note              = {Cited by: 0; All Open Access, Gold Open Access},
  volume            = {5},
  abstract          = {The complex synthesis of emotions seen in music is meticulously composed using a wide range of aural components. Given the expanding soundscape and abundance of online music resources, creating a music recommendation system is significant. The area of music file emotion recognition is particularly fascinating. The RGRU (Enhanced Residual Gated Recurrent Unit), a complex architecture, is used in our study to look at MIDI (Musical Instrument and Digital Interface) compositions for detecting emotions. This involves extracting diverse features from the MIDI dataset, encompassing harmony, rhythm, dynamics, and statistical attributes. These extracted features subsequently serve as input to our emotion recognition model for emotion detection. We use an improved RGRU version to identify emotions and the Adaptive Red Fox Algorithm (ARFA) to optimize the RGRU hyperparameters. Our suggested model offers a sophisticated classification framework that effectively divides emotional content into four separate quadrants: positive-high, positive-low, negative-high, and negative-low. The Python programming environment is used to implement our suggested approach. We use the EMOPIA dataset to compare its performance to the traditional approach and assess its effectiveness experimentally. The trial results show better performance compared to traditional methods, with higher accuracy, recall, F-measure, and precision. Copyright © 2023 Kumar and Kathiravan.},
  author_keywords   = {adaptive Red Fox algorithm; EMOPIA; emotion recognition; Enhanced Residual Gated Recurrent Unit; Musical Instrument Digital Interface},
  year              = {2023},
  doi               = {10.3389/fcomp.2023.1305413},
  hasabstract       = {Y},
  journaltitle      = {Frontiers in Computer Science},
  priority          = {prio1},
  publication_stage = {Final},
  ranking           = {rank2},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181443074&doi=10.3389%2ffcomp.2023.1305413&partnerID=40&md5=977f4cd7e7b4a9a110b29a8f48b0e632},
}

@Article{elrefaiy2024ee,
  author            = {Elrefaiy, Ahmed and Tawfik, Nahed and Zayed, Nourhan and Elhenawy, Ibrahim},
  title             = {EEG emotion recognition framework based on invariant wavelet scattering convolution network},
  note              = {Cited by: 0},
  number            = {4},
  pages             = {2181 – 2199},
  volume            = {15},
  abstract          = {EEG signals for real-time emotion identification are crucial for affective computing and human-computer interaction. The current emotion recognition models, which rely on a small number of emotion classes and stimuli like music and images in controlled lab conditions, have poor ecological validity. Furthermore, identifying relevant EEG signal features is crucial for efficient emotion identification. According to the complexity, non-stationarity, and variation nature of EEG signals, which make it challenging to identify relevant features to categorize and identify emotions, a novel approach for feature extraction and classification concerning EEG signals is suggested based on invariant wavelet scattering transform (WST) and support vector machine algorithm (SVM). The WST is a new time-frequency domain equivalent to a deep convolutional network. It produces scattering feature matrix representations that are stable against time-warping deformations, noise-resistant, and time-shift invariant existing in EEG signals. So, small, difficult-to-measure variations in the amplitude and duration of EEG signals can be captured. As a result, it addresses the limitations of the previous feature extraction approaches, which are unstable and sensitive to time-shift variations. In this paper, the zero, first, and second order features from DEAP datasets are obtained by performing the WST with two deep layers. Then, the PCA method is used for dimensionality reduction. Finally, the extracted features are fed as inputs for different classifiers. In the classification step, the SVM classifier is utilized with different classification algorithms such as k-nearest neighbours (KNN), random forest (RF), and AdaBoost classifier. This research employs a principal component analysis (PCA) approach to reduce the high dimensionality of scattering characteristics and increase the computational efficiency of our classifiers. The proposed method is performed across four different emotional classification models based on valence, arousal, dominance, and liking dimensions on the DEAP dataset. It achieves over 98% for two emotional classes and over 97% for three, four, and eight emotional classes. The results unequivocally demonstrate the efficacy of the proposed WST, PCA, and SVM-based emotion recognition approach for EEG signal emotion recognition. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.},
  author_keywords   = {EEG; Emotion recognition; k-Nearest neighbor; Principal component analysis; Support Vector Machine; Wavelet scattering transform},
  year              = {2024},
  doi               = {10.1007/s12652-023-04746-y},
  hasabstract       = {Y},
  journaltitle      = {Journal of Ambient Intelligence and Humanized Computing},
  keywords          = {Adaptive boosting; Biomedical signal processing; Classification (of information); Computational efficiency; Convolution; Emotion Recognition; Extraction; Fast Fourier transforms; Feature extraction; Frequency domain analysis; Human computer interaction; Nearest neighbor search; Principal component analysis; Speech recognition; Wavelet transforms; EEG signals; Emotion identifications; Emotion recognition; Principal-component analysis; Real- time; Scattering transforms; Support vector machines algorithms; Support vectors machine; Time shifts; Wavelet scattering transform; Support vector machines},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182982292&doi=10.1007%2fs12652-023-04746-y&partnerID=40&md5=80c3e764eb119cbd575b042c12b480cc},
}

@Article{li2024re,
  author            = {Li, Lin},
  title             = {Research on the Comparative Development of Modern Popular Music and Traditional Music Culture in Colleges and Universities in the Age of Artificial Intelligence},
  note              = {Cited by: 0; All Open Access, Gold Open Access},
  number            = {1},
  volume            = {9},
  abstract          = {In this paper, the forward neural network multi-feature fusion algorithm is used to extract the emotional features of music culture on artificial intelligence technology, considering the diversity and intermittency of the emotional features of the study, which needs to be parameterized. In the forward neural network architecture, the activation value obtained by using the nonlinear activation function is used, and the results obtained are passed to the next layer of data to realize layer-by-layer forward computation, which leads to the back-propagation activation function. The music culture emotion classification model is constructed based on the propagation mode of the forward neural network to determine the emotion recognition process. The research object is selected, the research process is determined, and in order to ensure the true validity of the research, it is necessary to test the reliability and validity of the research design scheme and to develop an empirical analysis of the comparison between popular music and traditional music culture. The results show that on the model, especially in the recognition of sacred, sad, passionate emotion type of music classification accuracy reached more than 88.2%. This paper’s model can improve the classification accuracy of music emotion to a certain extent. In the ontological knowledge analysis of popular music and traditional music culture, all three editions of textbooks show that general knowledge of music is predominant and has a large proportion, appreciation knowledge and extended knowledge are also considerable, and music knowledge is the least and has a small proportion. This study demonstrates the synergistic development of traditional culture and modern popular music, which is of great significance to the development of music education in colleges and universities. © 2023 Lin Li, published by Sciendo.},
  author_keywords   = {Activation function; Forward neural network; Multi-feature fusion algorithm; Music culture; Sentiment classification model},
  year              = {2024},
  doi               = {10.2478/amns.2023.2.01359},
  hasabstract       = {Y},
  journaltitle      = {Applied Mathematics and Nonlinear Sciences},
  keywords          = {Backpropagation; Chemical activation; Classification (of information); Emotion Recognition; Multilayer neural networks; Music; Reliability analysis; Activation functions; Classification models; Forward neural network; Fusion algorithms; Multi-feature fusion; Multi-feature fusion algorithm; Music culture; Neural-networks; Sentiment classification; Sentiment classification model; Network architecture},
  priority          = {prio1},
  publication_stage = {Final},
  ranking           = {rank2},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180349364&doi=10.2478%2famns.2023.2.01359&partnerID=40&md5=04f5a74d5769a6a8d15be347ef78bee2},
}

@Article{yan2023pe,
  author            = {Yan, Xiuli},
  title             = {Personalized Music Recommendation Based on Interest and Emotion: A Comparison of Multiple Algorithms},
  note              = {Cited by: 0; All Open Access, Gold Open Access},
  number            = {4},
  pages             = {230 – 235},
  volume            = {14},
  abstract          = {Recommendation algorithms can greatly improve the efficiency of information retrieval for users. This article briefly introduced recommendation algorithms based on association rules and algorithms based on interest and emotion analysis. After crawling music and comment data from the NetEase Cloud platform, a simulation experiment was conducted. Firstly, the performance of the Back-Propagation Neural Network (BPNN) in the interest and emotion-based algorithm for recommending music was tested, and then the impact of the proportion of emotion weight between comments and music on the emotion analysis-based algorithm was tested. Finally, the three recommendation algorithms based on association rules, user ratings, and interest and emotion analysis were compared. The results showed that when the BPNN used the dominant interest and emotion and secondary interest and emotion as judgment criteria, the accuracy of interest and emotion recognition for music and comments was higher. When the proportion of interest and emotion weight between comments and music was 6:4, the interest and emotion analysis-based recommendation algorithm had the highest accuracy. The interest and emotion-based recommendation algorithm had higher recommendation accuracy than the association rule-based and user rating-based algorithms, and could provide users with more personalized and emotional music recommendations. © 2023, International Journal of Advanced Computer Science and Applications. All Rights Reserved.},
  author_keywords   = {Interest and emotion; music; personalization; recommendation algorithm},
  comment           = {Quality issues, no relevant task, discussed},
  year              = {2023},
  doi               = {10.14569/IJACSA.2023.0140426},
  hasabstract       = {Y},
  journaltitle      = {International Journal of Advanced Computer Science and Applications},
  keywords          = {Association rules; Emotion Recognition; Neural networks; Back-propagation neural networks; Cloud platforms; Emotion analysis; Interest and emotion; Multiple algorithms; Music recommendation; Performance; Personalizations; Recommendation algorithms; User rating; Simulation platform},
  modificationdate  = {2024-05-16T11:59:50},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158085107&doi=10.14569%2fIJACSA.2023.0140426&partnerID=40&md5=16fad6be333ec3e3dd27bbdcd10db725},
}

@Article{patel2023co,
  author            = {Patel, Jigna and Padaria, Ali Asgar and Mehta, Aryan and Chokshi, Aaryan and Patel, Jitali and Kapdi, Rupal},
  title             = {CONCOLLA - A SMART EMOTION-BASED MUSIC RECOMMENDATION SYSTEM FOR DRIVERS},
  note              = {Cited by: 0; All Open Access, Bronze Open Access},
  number            = {4},
  pages             = {919 – 939},
  volume            = {24},
  abstract          = {Music recommender system is an area of information retrieval system that suggests customized music recommendations to users based on their previous preferences and experiences with music. While existing systems often overlook the emotional state of the driver, we propose a hybrid music recommendation system - ConCollA to provide a personalized experience based on user emotions. By incorporating facial expression recognition, ConCollA accurately identifies the driver’s emotions using convolution neural network(CNN) model and suggests music tailored to their emotional state. ConCollA combines collaborative filtering, a novel content-based recommendation system named Mood Adjusted Average Similarity (MAAS), and apriori algorithm to generate personalized music recommendations. The performance of ConCollA is assessed using various evaluation parameters. The results show that proposed emotion-aware model outperforms a collaborative-based recommender system. © 2023 SCPE. All Rights Reserved.},
  author_keywords   = {apriori algorithm; associative rule mining; deep learning; Emotion; matrix factorization collaborative filtering; mood; music; personalized contentbased recommendation; recommendation system},
  comment           = {facial recognition},
  year              = {2023},
  doi               = {10.12694/scpe.v24i4.2467},
  hasabstract       = {Y},
  journaltitle      = {Scalable Computing},
  keywords          = {Collaborative filtering; Deep learning; Emotion Recognition; Factorization; Learning algorithms; Music; Search engines; Apriori algorithms; Associative rule minings; Content-based recommendation; Deep learning; Emotion; Matrix factorization collaborative filtering; Matrix factorizations; Mood; Music Recommendation System; Personalized contentbased recommendation; Recommender systems},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178478319&doi=10.12694%2fscpe.v24i4.2467&partnerID=40&md5=83823eb69a57c98c0d16e5ba19bc05a2},
}

@Article{alvarez2023ri,
  author            = {Álvarez, P. and de Quirós, J. García and Baldassarri, S.},
  title             = {RIADA: A Machine-Learning Based Infrastructure for Recognising the Emotions of Spotify Songs},
  note              = {Cited by: 4; All Open Access, Gold Open Access, Green Open Access},
  number            = {2},
  pages             = {168 – 181},
  volume            = {8},
  abstract          = {The music emotions can help to improve the personalization of services and contents offered by music streaming providers. Many research works based on the use of machine learning techniques have addressed the problem of recognising the music emotions during the last years. Nevertheless, the results obtained are only applied on small-size music repositories and do not consider what the users feel when they listen to the songs. These issues prevent the existing proposals to be integrated into the personalization mechanisms of the online music providers. In this paper, we present the RIADA infrastructure which is composed by a set of systems able to annotate emotionally the catalog of songs offered by Spotify based on the users’ perception. RIADA works with the Spotify playlist miner and data services to build emotion recognition models that can solve the open challenges previously mentioned. Machine learning algorithms, music information retrieval techniques, architectures for parallelization of applications and cloud computing have been combined to develop a complex result of engineering able to integrate the music emotions into the Spotify-based applications. © 2023, Universidad Internacional de la Rioja. All rights reserved.},
  author_keywords   = {Affective Annotation; Cloud Computing; Emotion Recognition; Machine Learning; Music; Spotify},
  year              = {2023},
  doi               = {10.9781/ijimai.2022.04.002},
  hasabstract       = {Y},
  journaltitle      = {International Journal of Interactive Multimedia and Artificial Intelligence},
  priority          = {prio1},
  publication_stage = {Final},
  ranking           = {rank2},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147947463&doi=10.9781%2fijimai.2022.04.002&partnerID=40&md5=8f61f829c998052047f5da67adcf48c9},
}

@Article{huang2023th,
  author            = {Huang, Linna},
  title             = {The application and research of double-layer music emotion classification model based on random forest algorithm in digital music},
  note              = {Cited by: 0},
  number            = {2-4},
  pages             = {445 – 460},
  volume            = {28},
  abstract          = {It is urgent to solve the problem of music emotion classification. The stochastic forest algorithm is easy to operate and performs better than other single-layer classification models. Aiming at the problems of feature extraction and classification in conventional music emotion classification methods, music features are divided into long-term features and short-term features, and a two-layer music emotion classification model integrating a random forest (RF) algorithm is designed. The experimental results showed that the SVM model using the Gaussian radial basis kernel function had the highest classification accuracy of 90.78% in training the SVM model. The overall classification accuracy of the two-layer music emotion classification model was 98.92%, the recall rate was 97.63%, and its indicators in different emotion categories were the highest, with an average F1 value of 0.919. To sum up, the two-layer music emotion classification model based on the RF algorithm proposed in the research has excellent recognition and classification capabilities. Copyright © 2023 Inderscience Enterprises Ltd.},
  author_keywords   = {double layer model; emotional classification; music characteristics; random forest; RF; SVM},
  year              = {2023},
  doi               = {10.1504/IJNVO.2023.133878},
  hasabstract       = {Y},
  journaltitle      = {International Journal of Networking and Virtual Organisations},
  keywords          = {Classification (of information); Emotion Recognition; Stochastic systems; Support vector machines; Classification models; Double layer models; Emotional classification; Music characteristic; Music emotion classifications; Random forest algorithm; Random forests; SVM; Two-layer; Stochastic models},
  priority          = {prio1},
  publication_stage = {Final},
  ranking           = {rank2},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174308407&doi=10.1504%2fIJNVO.2023.133878&partnerID=40&md5=9a511d481ab7c25caee2871f387596ba},
}

@Article{hu2023ad,
  author            = {Hu, Jia-Xin and Song, Yu and Zhang, Yi-Yao},
  title             = {Adoption of Gesture Interactive Robot in Music Perception Education with Deep Learning Approach},
  note              = {Cited by: 0},
  number            = {1},
  pages             = {19 – 37},
  volume            = {39},
  abstract          = {This work intends to help students perceive music, study music, create music, and realize the “human-computer interaction” music teaching mode. A distributed design pattern is adopted to design a gesture interactive robot suitable for music education. First, the client is designed. The client gesture acquisition module employs a dual-channel convolutional neural network (DCCNN) for gesture recognition. The convolutional layer of the constructed DCCNN contains convolution kernels with two sizes, which operate on the image. Second, the server is designed, which recognizes the collected gesture instruction data through two-stream convolutional neural network (CNN). This network cuts the gesture instruction data into K segments, and sparsely samples each segment into a short sequence. The optical flow algorithm is employed to extract the optical flow features of each short sequence. Finally, the performance of the robot is tested. The results show that the combination of convolution kernels with sizes of 5×5 and 7×7 has a recognition accuracy of 98%, suggesting that DCCNN can effectively collect gesture command data. After training, DCCNN's gesture recognition accuracy rate reaches 90%, which is higher than mainstream dynamic gesture recognition algorithms under the same conditions. In addition, the recognition accuracy of the gesture interactive robot is above 90%, suggesting that this robot can meet normal requirements and has good reliability and stability. It is also recommended to be utilized in music perception teaching to provide a basis for establishing a multi-sensory music teaching model. © 2023 Institute of Information Science. All rights reserved.},
  author_keywords   = {DCCNN; deep learning; gesture recognition; robot; two-stream convolutional neural networks},
  comment           = {no emotion},
  year              = {2023},
  doi               = {10.6688/JISE.202301_39(1).0002},
  hasabstract       = {Y},
  journaltitle      = {Journal of Information Science and Engineering},
  keywords          = {Audio acoustics; Convolution; Convolutional neural networks; Deep learning; Emotion Recognition; Gesture recognition; Human computer interaction; Human robot interaction; Machine design; Optical flows; Convolutional neural network; Deep learning; Dual channel; Dual-channel convolutional neural network; Gestures recognition; Interactive robot; Recognition accuracy; Two-stream; Two-stream convolutional neural network; Music},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164953976&doi=10.6688%2fJISE.202301_39%281%29.0002&partnerID=40&md5=2756243a7845b937254436b3c448ad11},
}

@Conference{sana2022fa,
  author            = {Sana, S.K. and Sruthi, G. and Suresh, D. and Rajesh, G. and Subba Reddy, G.V.},
  title             = {Facial emotion recognition based music system using convolutional neural networks},
  note              = {Cited by: 9},
  pages             = {4699 – 4706},
  volume            = {62},
  abstract          = {Our face is amongst the most significant body organs. It is critical in determining a person's emotions and feelings. With the use of certain traits discernible on the face, the emotion of an individual can be approximated to a certain degree of precision. With new technology advances, recognizable features of the face can be retrieved as inputs utilizing a webcam. The gathered data helps in determining the mood and songs are played from a customized playlist. This eliminates that time-consuming procedure of physically selecting music or modifying playlists and allowed for the creation of an appropriate playlist dependent on the person's emotional level or mood. We will look at a variety of algorithms to come up with an automatic playlist generating methodology that uses emotion recognition to suggest songs. The facial expression-driven music player is set up in a way that allows you to listen to music based on your facial expression. In this work FER-2013, dataset and CNN algorithm are used for emotion recognition. © 2022},
  author_keywords   = {CNN algorithm; Emotion Detection; Face Detection; Music Recommendation},
  comment           = {face recognition},
  year              = {2022},
  doi               = {10.1016/j.matpr.2022.03.131},
  hasabstract       = {Y},
  journaltitle      = {Materials Today: Proceedings},
  keywords          = {Convolutional neural networks; Face recognition; Speech recognition; CNN algorithm; Convolutional neural network; Degree of precision; Emotion detection; Emotion recognition; Faces detection; Facial emotions; Facial Expressions; Music recommendation; Technology advances; Emotion Recognition},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127586340&doi=10.1016%2fj.matpr.2022.03.131&partnerID=40&md5=4417e06d71a5aac27e4eef5166880157},
}

@Article{singh2023as,
  author            = {Singh, Krishna Kumar and Dembla, Payal},
  title             = {A Study on Emotion Analysis and Music Recommendation Using Transfer Learning},
  note              = {Cited by: 1; All Open Access, Hybrid Gold Open Access},
  number            = {6},
  pages             = {707 – 726},
  volume            = {19},
  abstract          = {As more and more people access and consume music through streaming platforms and digital services, music recommendation has grown in importance within the music industry. Given the abundance of music at our disposal, music recommendation algorithms are essential for guiding users toward new music and for creating individualized listening experiences. People frequently seek out music that fits their current emotional state or desired emotional state, which means that emotions can have a big impact on music recommendations. Emotions can be taken into account by music recommendation algorithms when deciding which songs or playlists to recommend to listeners. Face expressions are frequently used to gauge a person's mood. By using a webcam or any other external device, recognizable facial traits can now be extracted as inputs thanks to modern technology. Transfer learning is a method that is increasingly in demand for enhancing emotion recognition and music recommendation systems in the modern world. Transfer learning has evolved into a potent method for utilizing prior knowledge to enhance model performance and lessen the requirement for massive volumes of labeled data as a result of the data explosion and the availability of big pre-trained models. Hence, the objective of this study is to understand how transfer learning impacts the accuracy of detecting emotions from facial expressions and how the music recommendations can be personalized based on the detected emotions. This study aims at recommending songs by detecting the facial expressions of users using the FER2013 dataset for emotion recognition which is further extended by adding own images to the categories in the dataset from Google. A basic CNN, finetuned pre-trained ResNet50V2, finetuned pre-trained VGG16, and finetuned pre-trained EfficientNet50 B0 are trained on the dataset for emotion detection and compared. The music recommendation system is developed using the Spotify songs dataset extracted using Spotify web API. It uses k-means clustering for grouping tracks based on emotions and getting song recommendations based on the emotion predictions using finetuned ResNet50-V2 model with the highest training accuracy of 77.16% and validation accuracy of 69.04%. The findings reveal that using a transfer learning approach may effectively identify emotions from facial expressions and can have a potential impact on recommending music. It improves duties related to music recommendations and might be a useful method for assisting users in finding new music that fits the intended emotional state. © 2023 Krishna Kumar Singh and Payal Dembla. This open-access article is distributed under a Creative Commons Attribution (CC-BY) 4.0 license.},
  author_keywords   = {Emotion Prediction; Music Recommendation System; Transfer Learning},
  comment           = {face recognition},
  year              = {2023},
  doi               = {10.3844/jcssp.2023.707.726},
  hasabstract       = {Y},
  journaltitle      = {Journal of Computer Science},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162152487&doi=10.3844%2fjcssp.2023.707.726&partnerID=40&md5=7bcda4922bc4a0b04809ee7cbff8a472},
}

@Article{liu2023an,
  author            = {Liu, Jianwen},
  title             = {An Automatic Classification Method for Multiple Music Genres by Integrating Emotions and Intelligent Algorithms},
  note              = {Cited by: 3; All Open Access, Gold Open Access},
  number            = {1},
  volume            = {37},
  abstract          = {In this paper, we present an approach to improve the effectiveness of automatic classification of music genres by integrating emotion and intelligent algorithms. We propose an automatic recognition and classification algorithm for music spectra, which takes into account emotional cues that can be extracted from music to improve classification accuracy. To achieve this goal, we set different weight coefficients, which are continuously adjusted based on the convergence process of the previous iteration. The size of each weighting coefficient is adaptively controlled to reduce the number of iterations of the reconstruction process, thereby reducing the algorithm’s computational complexity and speeding up its convergence. We conducted several experiments to evaluate the effectiveness of our proposed method. The experimental results demonstrate that the automatic classification method of music genres, which integrates emotion and intelligent algorithms, can significantly improve the accuracy of automatic music genre classification. Moreover, our approach reduces the algorithm’s computational complexity, resulting in a faster convergence speed. Our proposed approach provides a promising solution for automatic music genre classification that takes into account emotional cues. The integration of emotion and intelligent algorithms can help achieve higher accuracy and reduce computational complexity, making the proposed method applicable in various scenarios. © 2023 The Author(s). Published with license by Taylor & Francis Group, LLC.},
  comment           = {no emotion, discussed},
  year              = {2023},
  doi               = {10.1080/08839514.2023.2211458},
  hasabstract       = {Y},
  journaltitle      = {Applied Artificial Intelligence},
  keywords          = {Emotion Recognition; Iterative methods; Automatic classification; Automatic recognition; Classification accuracy; Classification algorithm; Classification methods; Intelligent Algorithms; Music genre; Music genre classification; MUSIC spectrum; Recognition algorithm; Computational complexity},
  modificationdate  = {2024-05-16T11:09:39},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159193958&doi=10.1080%2f08839514.2023.2211458&partnerID=40&md5=043accf42b33035a4d0afc6801969664},
}

@Article{zaman2023as,
  author            = {Zaman, Khalid and Sah, Melike and Direkoglu, Cem and Unoki, Masashi},
  title             = {A Survey of Audio Classification Using Deep Learning},
  note              = {Cited by: 4},
  pages             = {106620 – 106649},
  volume            = {11},
  abstract          = {Deep learning can be used for audio signal classification in a variety of ways. It can be used to detect and classify various types of audio signals such as speech, music, and environmental sounds. Deep learning models are able to learn complex patterns of audio signals and can be trained on large datasets to achieve high accuracy. To employ deep learning for audio signal classification, the audio signal must first be represented in a suitable form. This can be done using signal representation techniques such as using spectrograms, Mel-frequency Cepstral coefficients, linear predictive coding, and wavelet decomposition. Once the audio signal is represented in a suitable form, it can then be fed into a deep learning model. Various deep learning models can be utilized for audio classification. We provide an extensive survey of current deep learning models used for a variety of audio classification tasks. In particular, we focus on works published under five different deep neural network architectures, namely Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Autoencoders, Transformers and Hybrid Models (hybrid deep learning models and hybrid deep learning models with traditional classifiers). CNNs can be used to classify audio signals into different categories such as speech, and environmental sounds. They can also be used for speech recognition, speaker identification, and emotion recognition. RNNs are widely used for audio classification and audio segmentation. RNN models can capture temporal patterns of audio signals and be used to classify audio segments into different categories. Another approach is to use autoencoders for learning the features of audio signals and then classifying the signals into different categories. Transformers are also well-suited for audio classification. In particular, temporal and frequency features can be extracted to identify the characteristics of the audio signals. Finally, hybrid models for audio classification either combine various deep learning architectures (i.e. CNN-RNN) or combine deep learning models with traditional machine learning techniques (i.e. CNN-Support Vector Machine). These hybrid models take advantage of the strengths of different architectures while avoiding their weaknesses. Existing literature under different categories of deep learning are summarized and compared in detail. © 2013 IEEE.},
  author_keywords   = {Audio; autoencoders; classification; CNNs; deep learning; emotion; hybrid models; music; noise; recognition; RNNs; speech; transformers},
  comment           = {survey},
  year              = {2023},
  doi               = {10.1109/ACCESS.2023.3318015},
  hasabstract       = {Y},
  journaltitle      = {IEEE Access},
  keywords          = {Audio acoustics; Classification (of information); Deep neural networks; Emotion Recognition; Feature extraction; Music; Network architecture; Recurrent neural networks; Wavelet decomposition; Audio; Auto encoders; Classification algorithm; Convolutional neural network; Deep learning; Emotion; Emotion recognition; Features extraction; Hidden-Markov models; Hybrid model; Noise; Recognition; Task analysis; Transformer; Speech recognition},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173032328&doi=10.1109%2fACCESS.2023.3318015&partnerID=40&md5=03f7347b49aef324707d5714d974b460},
}

@Article{feng2022am,
  author            = {Feng, Huanghao and Mahoor, Mohammad H. and Dino, Francesca},
  title             = {A Music-Therapy Robotic Platform for Children With Autism: A Pilot Study},
  note              = {Cited by: 9; All Open Access, Gold Open Access},
  volume            = {9},
  abstract          = {Children with Autism Spectrum Disorder (ASD) experience deficits in verbal and nonverbal communication skills including motor control, turn-taking, and emotion recognition. Innovative technology, such as socially assistive robots, has shown to be a viable method for Autism therapy. This paper presents a novel robot-based music-therapy platform for modeling and improving the social responses and behaviors of children with ASD. Our autonomous social interactive system consists of three modules. Module one provides an autonomous initiative positioning system for the robot, NAO, to properly localize and play the instrument (Xylophone) using the robot’s arms. Module two allows NAO to play customized songs composed by individuals. Module three provides a real-life music therapy experience to the users. We adopted Short-time Fourier Transform and Levenshtein distance to fulfill the design requirements: 1) “music detection” and 2) “smart scoring and feedback”, which allows NAO to understand music and provide additional practice and oral feedback to the users as applicable. We designed and implemented six Human-Robot-Interaction (HRI) sessions including four intervention sessions. Nine children with ASD and seven Typically Developing participated in a total of fifty HRI experimental sessions. Using our platform, we collected and analyzed data on social behavioral changes and emotion recognition using Electrodermal Activity (EDA) signals. The results of our experiments demonstrate most of the participants were able to complete motor control tasks with 70% accuracy. Six out of the nine ASD participants showed stable turn-taking behavior when playing music. The results of automated emotion classification using Support Vector Machines illustrates that emotional arousal in the ASD group can be detected and well recognized via EDA bio-signals. In summary, the results of our data analyses, including emotion classification using EDA signals, indicate that the proposed robot-music based therapy platform is an attractive and promising assistive tool to facilitate the improvement of fine motor control and turn-taking skills in children with ASD. Copyright © 2022 Feng, Mahoor and Dino.},
  author_keywords   = {autism; emotion classification; motor control; music therapy; social robotics; turn-taking},
  year              = {2022},
  doi               = {10.3389/frobt.2022.855819},
  hasabstract       = {Y},
  journaltitle      = {Frontiers in Robotics and AI},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131892650&doi=10.3389%2ffrobt.2022.855819&partnerID=40&md5=b26040075b7802c53c890c5039e3b4ee},
}

@Article{dong2022op,
  author            = {Dong, Liusha},
  title             = {Optimization Simulation of Dance Technical Movements and Music Matching Based on Multifeature Fusion},
  note              = {Cited by: 0; All Open Access, Gold Open Access, Green Open Access},
  volume            = {2022},
  abstract          = {Music and dance videos have been popular among researchers in recent years. Music is one of the most important forms of human communication; it carries a wealth of emotional information, and it is studied using computer tools. In the feature engineering process, most present machine learning approaches suffer from information loss or insufficient extracted features despite the relevance of computer interface and multimedia technologies in sound and music matching tasks. Multifeature fusion is widely utilized in education, aerospace, intelligent transportation, biomedicine, and other fields, and it plays a critical part in how humans get information. In this research, we offer an effective simulation method for matching dance technique movements with music based on multifeature fusion. The initial step is to use music beat extraction theory to segment the synchronized dance movements and music data, then locate mutation points in the music, and dynamically update the pheromones based on the merits of the dance motions. The audio feature sequence is obtained by extracting audio features from the dancing video's accompanying music. Then, we combine the two sequences to create an entropy value sequence based on audio variations. By comparing the consistency of several approaches for optimizing dance movement simulation trials, the optimized simulation method described in this research has an average consistency of 87%, indicating a high consistency. As a result, even though the background and the subject are readily confused, the algorithm in this research can keep a consistent recognition rate for more complicated dance background music, and the approach in this study can still guarantee a certain accuracy rate. © 2022 Liusha Dong.},
  year              = {2022},
  doi               = {10.1155/2022/8679748},
  hasabstract       = {Y},
  journaltitle      = {Computational Intelligence and Neuroscience},
  keywords          = {Computer Simulation; Emotions; Humans; Movement; Music; Recognition, Psychology; Multimedia systems; Music; Audio features; Computer tools; Dance movement; Emotional information; Engineering process; Feature engineerings; Human communications; Multi-feature fusion; Music matching; Optimization-simulation; computer simulation; emotion; human; movement (physiology); music; psychology; Audio acoustics},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132245035&doi=10.1155%2f2022%2f8679748&partnerID=40&md5=c768ccd8579719eac06c0e4d0362f867},
}

@Article{long2024ex,
  author            = {Long, Tao},
  title             = {Exploring the diversified teaching mode of vocal singing in colleges and universities by integrating audio-visual and multi-sensory senses},
  note              = {Cited by: 0; All Open Access, Gold Open Access},
  number            = {1},
  volume            = {9},
  abstract          = {In this paper, starting from the fusion of audio-visual multisensory vocal music feature representation, the RAE algorithm is used to represent the music lyrics text in vocal singing teaching. Due to the heterogeneity of the feature space of the audio modality and the lyrics modality, which makes it exceptionally tricky to directly mine the correlation between these two modalities, it is necessary to optimize the research through the fusion of the audio and the textual modality for the representation of the implicit space of the music. According to the SVM's optimal classification, hyperplane is not limited by the data dimension. Combined with the kernel function parameters and emotional LFSM fusion, a visual multisensory vocal singing teaching emotion model based on SVM is constructed. The parameter environment and dataset are selected, and the comparison method and evaluation criteria are determined to analyze emotional research on vocal singing teaching in colleges and universities. The results show that in terms of model performance, the SVM model in this paper is 8.5% higher than model 6, reaching the highest 0.873, with stronger emotion extraction and recognition ability, greatly improving the emotion classification results of the model. The multimedia type of audio singing material is the least helpful for expression and physical performance in vocal singing teaching, with a total value of 67. This study provides a more comprehensive understanding of the emotional changes of students in teaching activities so as to identify problems, improve and optimize them, and give more students guidance and support in performing vocal singing.  © 2023 Tao Long, published by Sciendo.},
  author_keywords   = {Audiovisual multisensory; Kernel function; LFSM fusion; Optimal classification hyperplane; SVM; Vocal singing},
  year              = {2024},
  doi               = {10.2478/amns.2023.2.01468},
  hasabstract       = {Y},
  journaltitle      = {Applied Mathematics and Nonlinear Sciences},
  keywords          = {Audio acoustics; Emotion Recognition; Geometry; Students; Support vector machines; Audio-visual; Audiovisual multisensory; Colleges and universities; Kernel function; LFSM fusion; Multisensory; Optimal classification; Optimal classification hyperplane; SVM; Vocal singing; Music},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183766557&doi=10.2478%2famns.2023.2.01468&partnerID=40&md5=44012995547565ef39c0830f41ebab1a},
}

@Article{wang2022pr,
  author            = {Wang, Mingxun and Luo, Gang and Chen, Hao},
  title             = {Practice of Music Therapy for Autistic Children Based on Music Data Mining},
  note              = {Cited by: 0; All Open Access, Gold Open Access},
  volume            = {2022},
  abstract          = {For children with autism, music therapy has aroused great concern with its novelty and better influence. Music therapy, as one of the effective treatment methods, has an important influence on the social interaction, behavior, and emotion of autistic children. This study attempts to explore a form of applying highly specialized impromptu music therapy to the personal treatment of autistic children in schools for the disabled, as well as the design method of specific music activities. Based on music data mining, the machine learning method is introduced to model music emotion features, and various algorithms are compared to find a model with higher recognition rate, and, at the same time, the antinoise ability and generalization ability of the model are further improved. Finally, a music emotion cognitive model with better performance is established. The results show that the model can effectively promote the overall development of autistic children's cognitive movement, social communication, language communication, and cognition. © 2022 Mingxun Wang et al.},
  year              = {2022},
  doi               = {10.1155/2022/4576211},
  hasabstract       = {Y},
  journaltitle      = {Mathematical Problems in Engineering},
  keywords          = {Machine learning; Music; Autistic children; Children with autisms; Interaction behavior; Music data; Music emotions; Music therapy; Social behaviour; Social emotions; Social interactions; Treatment methods; Data mining},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128692602&doi=10.1155%2f2022%2f4576211&partnerID=40&md5=b4377eaefa6bb3e04d9b428c27eb664d},
}

@Article{cheng2022co,
  author            = {Cheng, Chaozhi and Xiao, Yujun},
  title             = {Construction of AI Environmental Music Education Application Model Based on Deep Learning},
  note              = {Cited by: 1; All Open Access, Gold Open Access, Green Open Access},
  volume            = {2022},
  abstract          = {The art of music, which is a necessary component of daily life and an ideology older than language, reflects the emotions of human reality. Many new elements have been introduced into music as a result of the quick development of technology, gradually altering how people create, perform, and enjoy music. It is incredible to see how actively AI has been used in music applications and music education over the past few years and how significantly it has advanced. AI technology can efficiently pull in the course, stratify complex large-scale music or sections, simplify teaching, improve student understanding of music, solve challenging student problems in class, and simplify the tasks of teachers. The traditional music education model has been modified, and the music education model's audacious innovation has been made possible by reducing the distance between the teacher and the student. A classification algorithm based on spectrogram and NNS is proposed in light of the advantages in image processing. The abstract features on the spectrogram are automatically extracted using the NNS, which completes the end-to-end learning and avoids the tediousness and inaccuracy of manual feature extraction. This study, which uses experimental analysis to support its findings, demonstrates that different music teaching genres can be accurately classified at a rate of over 90%, which has a positive impact on recognition.  © 2022 Chaozhi Cheng and Yujun Xiao.},
  year              = {2022},
  doi               = {10.1155/2022/6440464},
  hasabstract       = {Y},
  journaltitle      = {Journal of Environmental and Public Health},
  keywords          = {Deep Learning; Humans; Image Processing, Computer-Assisted; Models, Theoretical; Music; Students; article; classification algorithm; deep learning; education; educational model; feature extraction; human; human experiment; image processing; learning; music; teacher; teaching; psychology; student; theoretical model},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137055640&doi=10.1155%2f2022%2f6440464&partnerID=40&md5=64bb98297d03ed96e87847f21e549e38},
}

@Article{he2022al,
  author            = {He, Jiao},
  title             = {Algorithm Composition and Emotion Recognition Based on Machine Learning},
  note              = {Cited by: 5; All Open Access, Gold Open Access, Green Open Access},
  volume            = {2022},
  abstract          = {This paper proposes a new algorithm composition network from the perspective of machine learning, based on an in-depth study of related literature. At the same time, this paper examines the characteristics of music and develops a model for recognising musical emotions. Using the model's information entropy of pitch and intensity to extract the main melody track, note features are extracted from bar features. Finally, the cosine of the vector included angle is used to judge the similarity between feature vectors of several adjacent sections, allowing the music to be divided into several independent segments. The emotional model of music is used to analyze each segment's emotion. By quantifying music features, this paper classifies and quantifies music emotion based on the mapping relationship between music features and emotion. Music emotion can be accurately identified by the model. The model's emotion recognition accuracy is up to 93.78 percent, and the algorithm's recall rate is up to 96.3 percent, according to simulation results. The recognition method used in this paper has a higher recognition ability than other methods, and the emotion recognition result is more reliable. This paper can not only meet the composer's auxiliary creative needs, but it can also help intelligent music services. © 2022 Jiao He.},
  year              = {2022},
  doi               = {10.1155/2022/1092383},
  hasabstract       = {Y},
  journaltitle      = {Computational Intelligence and Neuroscience},
  keywords          = {Algorithms; Emotions; Machine Learning; Music; Recognition, Psychology; Learning algorithms; Machine learning; Music; Speech recognition; Emotion recognition; Emotional models; Features vector; In-depth study; Information entropy; Machine-learning; Model informations; Music emotions; Musical emotion; On-machines; algorithm; emotion; machine learning; music; psychology; Emotion Recognition},
  priority          = {prio1},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132146675&doi=10.1155%2f2022%2f1092383&partnerID=40&md5=c8b96f3849ce73b57928a3d94771a96c},
}

@Article{ospitiamedina2023en,
  author            = {Ospitia-Medina, Yesid and Beltrán, José Ramón and Baldassarri, Sandra},
  title             = {ENSA dataset: a dataset of songs by non-superstar artists tested with an emotional analysis based on time-series},
  note              = {Cited by: 0; All Open Access, Green Open Access},
  number            = {5},
  pages             = {1909 – 1925},
  volume            = {27},
  abstract          = {This paper presents a novel dataset of songs by non-superstar artists in which a set of musical data is collected, identifying for each song its musical structure, and the emotional perception of the artist through a categorical emotional labeling process. The generation of this preliminary dataset is motivated by the existence of biases that have been detected in the analysis of the most used datasets in the field of emotion-based music recommendation. This new dataset contains 234 min of audio and 60 complete and labeled songs. In addition, an emotional analysis is carried out based on the representation of dynamic emotional perception through a time-series approach, in which the similarity values generated by the dynamic time warping (DTW) algorithm are analyzed and then used to implement a clustering process with the K-means algorithm. In the same way, clustering is also implemented with a Uniform Manifold Approximation and Projection (UMAP) technique, which is a manifold learning and dimension reduction algorithm. The algorithm HDBSCAN is applied for determining the optimal number of clusters. The results obtained from the different clustering strategies are compared and, in a preliminary analysis, a significant consistency is found between them. With the findings and experimental results obtained, a discussion is presented highlighting the importance of working with complete songs, preferably with a well-defined musical structure, considering the emotional variation that characterizes a song during the listening experience, in which the intensity of the emotion usually changes between verse, bridge, and chorus. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.},
  author_keywords   = {MER (Music Emotion Recognition); MRS (Music Recommender Systems); Musical datasets; Non-superstar artists; Popularity bias; Time-series approach},
  comment           = {no comparable metrics, discussed},
  year              = {2023},
  doi               = {10.1007/s00779-023-01721-4},
  hasabstract       = {Y},
  journaltitle      = {Personal and Ubiquitous Computing},
  keywords          = {Approximation algorithms; Behavioral research; Cluster analysis; Emotion Recognition; K-means clustering; Music; Time series analysis; Emotion recognition; Music emotion recognition; Music emotions; Music recommende system; Music recommender systems; Musical dataset; Non-superstar artist; Popularity bias; Time-series approach; Times series; Time series},
  modificationdate  = {2024-05-16T11:45:30},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160831741&doi=10.1007%2fs00779-023-01721-4&partnerID=40&md5=74752ffaeafb746f7e2b41d905d15792},
}

@Article{tian2023mua,
  author            = {Tian, Rui and Yin, Ruheng and Gan, Feng},
  title             = {Music sentiment classification based on an optimized CNN-RF-QPSO model},
  note              = {Cited by: 1},
  number            = {5},
  pages             = {719 – 733},
  volume            = {57},
  abstract          = {Purpose: Music sentiment analysis helps to promote the diversification of music information retrieval methods. Traditional music emotion classification tasks suffer from high manual workload and low classification accuracy caused by difficulty in feature extraction and inaccurate manual determination of hyperparameter. In this paper, the authors propose an optimized convolution neural network-random forest (CNN-RF) model for music sentiment classification which is capable of optimizing the manually selected hyperparameters to improve the accuracy of music sentiment classification and reduce labor costs and human classification errors. Design/methodology/approach: A CNN-RF music sentiment classification model is designed based on quantum particle swarm optimization (QPSO). First, the audio data are transformed into a Mel spectrogram, and feature extraction is conducted by a CNN. Second, the music features extracted are processed by RF algorithm to complete a preliminary emotion classification. Finally, to select the suitable hyperparameters for a CNN, the QPSO algorithm is adopted to extract the best hyperparameters and obtain the final classification results. Findings: The model has gone through experimental validations and achieved a classification accuracy of 97 per cent for different sentiment categories with shortened training time. The proposed method with QPSO achieved 1.2 and 1.6 per cent higher accuracy than that with particle swarm optimization and genetic algorithm, respectively. The proposed model had great potential for music sentiment classification. Originality/value: The dual contribution of this work comprises the proposed model which integrated two deep learning models and the introduction of a QPSO into model optimization. With these two innovations, the efficiency and accuracy of music emotion recognition and classification have been significantly improved. © 2023, Emerald Publishing Limited.},
  author_keywords   = {Classification; CNN; Music; QPSO; RF},
  year              = {2023},
  doi               = {10.1108/DTA-07-2022-0267},
  hasabstract       = {Y},
  journaltitle      = {Data Technologies and Applications},
  keywords          = {Classification (of information); Deep learning; Emotion Recognition; Extraction; Feature extraction; Music; Particle swarm optimization (PSO); Wages; Classification accuracy; Convolution neural network; Emotion classification; Features extraction; Hyper-parameter; Particle swarm optimization models; Quanta particle swarm optimizations; Random forests; RF; Sentiment classification; Genetic algorithms},
  priority          = {prio1},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150013677&doi=10.1108%2fDTA-07-2022-0267&partnerID=40&md5=b0a35894329ea82150a07e0b87d4e6c6},
}

@Article{kumargs2022wa,
  author            = {Kumar G S, Shashi and Sampathila, Niranjana and Tanmay, Tanishq},
  title             = {Wavelet based machine learning models for classification of human emotions using EEG signal},
  note              = {Cited by: 11; All Open Access, Gold Open Access},
  volume            = {24},
  abstract          = {Humans have the ability to portray different expressions contrary to the emotional state of mind. Therefore, it is difficult to judge the human's real emotional state simply by judging the physical appearance. Although researchers are working on facial expressions analysis, voice recognition, gesture recognition accuracy levels of such analysis are much less and the results are not reliable. Classifying the human emotions with machine learning models and extracting discrete wavelet features of Electroencephalogram (EEG) is proposed. The EEG data from Database for Emotion Analysis using Physiological signal (DEAP) online datasets is used for analysis and consists of peripheral biological signals as well as EEG recordings. EEG signal is collected from 32 subjects while watching 40 1-min-long music videos. Each video clip is rated by the participants in terms of the level of Valence, Arousal, Dominance. In the proposed work we have considered a significant band of EEG with a reduced frontal electrode (Fp1, F3, F4, Fp2) to get a comparable good result. The accuracy obtained from K- nearest neighbour (KNN), Fine KNN and Support Vector Machine (SVM) are 92.5%, 90% and 90% respectively for Valence, Arousal and Dominance. © 2022 The Authors},
  author_keywords   = {Convolutional neural network; Discrete wavelet transform; Electroencephalogram; Machine learning; Support vector machine},
  year              = {2022},
  doi               = {10.1016/j.measen.2022.100554},
  hasabstract       = {Y},
  journaltitle      = {Measurement: Sensors},
  keywords          = {Biomedical signal processing; Convolutional neural networks; Electroencephalography; Emotion Recognition; Learning systems; Nearest neighbor search; Physiological models; Signal reconstruction; Support vector machines; Convolutional neural network; Discrete-wavelet-transform; Electroencephalogram signals; Emotional state; Facial expressions analysis; Human emotion; Machine learning models; Machine-learning; Nearest-neighbour; Support vectors machine; Discrete wavelet transforms},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141768529&doi=10.1016%2fj.measen.2022.100554&partnerID=40&md5=33dd74d8b5c32e5ac978c7854bb46ade},
}

@Article{nag2022on,
  author            = {Nag, Sayan and Basu, Medha and Sanyal, Shankha and Banerjee, Archi and Ghosh, Dipak},
  title             = {On the application of deep learning and multifractal techniques to classify emotions and instruments using Indian Classical Music},
  note              = {Cited by: 19},
  volume            = {597},
  abstract          = {Music is often considered as the language of emotions. The way it stimulates the emotional appraisal across people from different communities, culture and demographics has long been known and hence categorizing on the basis of emotions is indeed an intriguing basic research area. Indian Classical Music (ICM) is famous for its ambiguous nature, i.e. its ability to evoke a number of mixed emotions through only a single musical narration, and hence classifying evoked emotions from ICM becomes a more challenging task. With the rapid advancements in the field of Deep Learning, this Music Emotion Recognition (MER) task is becoming more and more relevant and robust, hence can be applied to one of the most challenging test case i.e. classifying emotions elicited from ICM. In this paper we present a new dataset called JUMusEmoDB which presently has 1600 audio clips (approximately 30 s each) where 400 clips each correspond to happy, sad, calm and anxiety emotional scales. The initial annotations and emotional classification of the database was done based on an emotional rating test (5-point Likert scale) performed by 100 participants. The clips have been taken from different conventional ‘raga’ renditions played in two Indian stringed instruments – sitar and sarod by eminent maestros of ICM and digitized in 44.1 kHz frequency. The ragas, which are unique to ICM, are described as musical structures capable of inducing different moods or emotions. For supervised classification purposes, we have used Convolutional Neural Network (CNN) based architectures (resnet50, mobilenet v2.0, squeezenet v1.0 and a proposed ODE-Net) on corresponding music spectrograms of the 6400 sub-clips (where every clip was segmented into 4 sub-clips) which contain both time as well as frequency domain information. Along with emotion classification, instrument classification based response was also attempted on the same dataset using the CNN based architectures. In this context, a nonlinear technique, Multifractal Detrended Fluctuation Analysis (MFDFA) was also applied on the musical clips to classify them on the basis of complexity values extracted from the method. The initial classification accuracy obtained from the applied methods are quite inspiring and have been corroborated with ANOVA results to determine the statistical significance. This type of CNN based classification algorithm using a rich corpus of Indian Classical Music is unique even in the global perspective and can be replicated in other modalities of music also. The link to this newly developed dataset has been provided in the dataset description section of the paper. This dataset is still under development and we plan to include more data containing other emotional as well as instrumental entities into consideration. © 2022 Elsevier B.V.},
  author_keywords   = {Classification; CNN; Emotions; Indian Classical Music; Instruments; MFDFA},
  year              = {2022},
  doi               = {10.1016/j.physa.2022.127261},
  hasabstract       = {Y},
  journaltitle      = {Physica A: Statistical Mechanics and its Applications},
  keywords          = {Classification (of information); Convolutional neural networks; Deep learning; Frequency domain analysis; Music; Network architecture; Convolutional neural network; Emotion; Emotion recognition; Indian classical music; Learning techniques; Multifractal detrended fluctuation analysis; Multifractal technique; Music emotions; Network-based architectures; Research areas; Fractals},
  priority          = {prio1},
  publication_stage = {Final},
  ranking           = {rank2},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127732290&doi=10.1016%2fj.physa.2022.127261&partnerID=40&md5=634cc77ee93e9ac834ca0dad8b815119},
}

@Article{li2022di,
  author            = {Li, Yi},
  title             = {Digital Development for Music Appreciation of Information Resources Using Big Data Environment},
  note              = {Cited by: 1; All Open Access, Gold Open Access},
  volume            = {2022},
  abstract          = {With the continuous development of information technology and the arrival of the era of big data, music appreciation has also entered the digital development. Big data essence is highlighted by comparison with traditional data management and processing technologies. Under different requirements, the required time processing range is different. Music appreciation is an essential and important part of music lessons, which can enrich people's emotional experience, improve aesthetic ability, and cultivate noble sentiments. Data processing of music information resources will greatly facilitate the management, dissemination, and big data analysis and processing of music resources and improve the ability of music lovers to appreciate music. This paper aims to study the digital development of music in the environment of big data, making music appreciation more convenient and intelligent. This paper proposes an intelligent music recognition and appreciation model based on deep neural network (DNN) model. The use of DNN allows this study to have significant improvement over the traditional algorithm. This paper proposes an intelligent music recognition and appreciation model based on the DNN model and improves the traditional algorithm. The improved method in this paper refers to the Dropout method on the traditional DNN model. The DNN is trained on the database and tested on the data. The results show that, in the same database, the traditional DNN model is 114 and the RNN model is 120. The PPL of the improved DNN model in this paper is 98, i.e., the lowest value. The convergence speed is faster, which indicates that the model has stronger music recognition ability and it is more conducive to the digital development of music appreciation.  © 2022 Yi Li.},
  comment           = {no emotion},
  year              = {2022},
  doi               = {10.1155/2022/7873636},
  hasabstract       = {Y},
  journaltitle      = {Mobile Information Systems},
  keywords          = {Big data; Data handling; Emotion Recognition; Information management; Music; Neural network models; Continuous development; Data environment; Information resource; Management technologies; Model-based OPC; Music recognition; Neural network model; Processing technologies; Required time; Time processing; Deep neural networks},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138358215&doi=10.1155%2f2022%2f7873636&partnerID=40&md5=511e220b1fe520e2028f131f19bc400a},
}

@Article{ouyang2022de,
  author            = {Ouyang, Wensi},
  title             = {Design of Semantic Matching Model of Folk Music in Occupational Therapy Based on Audio Emotion Analysis},
  note              = {Cited by: 0; All Open Access, Gold Open Access, Green Open Access},
  volume            = {2022},
  abstract          = {The main semantic symbol systems for people to express their emotions include natural language and music. The analysis and establishment of semantic association between language and music is helpful to provide more accurate retrieval and recommendation services for text and music. Existing researches mainly focus on the surface symbolic features and association of natural language and music, which limits the performance and interpretability of applications based on semantic association of natural language and music. Emotion is the main meaning of music expression, and the semantic range of text expression includes emotion. In this paper, the semantic features of music are extracted from audio features, and the semantic matching model of audio emotion analysis is constructed to analyze ethnic music audio emotion through feature extraction ability of deep structure. The model is based on the framework of emotional semantic matching technology and realizes the emotional semantic matching of music fragments and words through semantic emotional recognition algorithm. Multiple experiments show that when W=0.65, the recognition rate of multichannel fusion model is 88.42%, and the model can reasonably realize audio emotion analysis. When the spatial dimension of music data changes, the classification accuracy reaches the highest when the spatial dimension is 25. Analysing the semantic association of audio promotes the application of folk music in occupational therapy. © 2022 Wensi Ouyang.},
  comment           = {not mer task, discussed},
  year              = {2022},
  doi               = {10.1155/2022/6841445},
  hasabstract       = {Y},
  journaltitle      = {Occupational Therapy International},
  keywords          = {Algorithms; Emotions; Humans; Music; Occupational Therapy; Semantics; algorithm; article; emotion; feature extraction; human; human experiment; molecular recognition; music; occupational therapy; algorithm; emotion; psychology; semantics},
  modificationdate  = {2024-05-16T11:18:14},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133137200&doi=10.1155%2f2022%2f6841445&partnerID=40&md5=645a2a866a5fe1131adcf6e2277fa02f},
}

@Article{priscillajoy2023mu,
  author            = {Priscilla Joy, R. and Roshni Thanka, M. and Sangeetha, Dr. and Malar Dhas, Julia Punitha and Edwin, E. Bijolin and Ebenezer},
  title             = {Music Mood Based Recognition System Based on Machine Learning and Deep Learning},
  note              = {Cited by: 1},
  number            = {2},
  pages             = {904 – 911},
  volume            = {11},
  abstract          = {There are extensive studies about music’s impact on human’s emotional state. Humans detect a wide range of emotions from various genres of music, and music plays an integral role in personality development and the treatment of ailments. Music has tremendous effects on human moods and thoughts. Consequently, it impacts cognitive and biological health, and the concept of well-being through music is acquiring traction. In the treatment of depression, music therapy gets witnessed as an addendum to psychoanalysis. Music can enhance intellectual and physical work, study, sports, relaxation, relieve fatigue, and music therapy, among other things. People often get confused while searching for music according to their interests and mood. Individuals usually listen to a particular genre or performer when they are in a certain mood. Music has the ability to control mood, specifically to boost energy, and reduce anxiety. Listening to the correct song at the opportune timing, may help with mental health. As a result, human mood changes and music have an interdependent affinity. In this paper, we aim to develop an application that can understand facial features (Mood and Emotions) and recommend music accordingly using Machine Learning and Deep Learning as tools and algorithms. © 2023, Ismail Saritas. All rights reserved.},
  author_keywords   = {Deep Learning; Facial Recognition; Machine Learning; Mood; Music; OpenCV},
  year              = {2023},
  hasabstract       = {Y},
  journaltitle      = {International Journal of Intelligent Systems and Applications in Engineering},
  priority          = {prio1},
  publication_stage = {Final},
  ranking           = {rank2},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165961875&partnerID=40&md5=a64b5b04b8094d3750a5f90a23c6df02},
}

@Article{xiang2022co,
  author            = {Xiang, Yuehua},
  title             = {Computer Analysis and Automatic Recognition Technology of Music Emotion},
  note              = {Cited by: 7; All Open Access, Gold Open Access},
  volume            = {2022},
  abstract          = {With the rapid development of the related computer industry, the use of computer-related technologies has become more and more frequent. The music industry is no exception. The research and analysis of music emotions has been a problem since ancient times. Due to the diversification of music emotions, people with different music in the same piece of music will have different feelings. The research topic of this article is to make a comprehensive analysis of the computer's automatic identification technology, combined with the powerful subcapacity of the computer, so that the research on music emotion can be developed rapidly. The article analyzes the technical research of the automatic recognition and analysis of music emotion in the computer, and conducts a comprehensive analysis of the music emotion through the research of the computer-related automatic recognition technology. This paper focuses on the computer automatic recognition model of music emotion, and successfully realizes the design and simulation of the automatic recognition system based on the MATLAB platform. An automatic identification model using BP neural network algorithm is proposed. By comparing it with the statistical classification algorithm, the experimental results verify the effectiveness of the designed BP network in music emotion recognition. © 2022 Yuehua Xiang.},
  year              = {2022},
  doi               = {10.1155/2022/3145785},
  hasabstract       = {Y},
  journaltitle      = {Mathematical Problems in Engineering},
  keywords          = {Automation; Music; Neural networks; Simulation platform; Automatic identification; Automatic recognition; Comprehensive analysis; Computer analysis; Computer industry; Identification technology; Music emotions; Music industry; Research and analysis; Research topics; MATLAB},
  priority          = {prio1},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128135662&doi=10.1155%2f2022%2f3145785&partnerID=40&md5=3ee19512d1d7e1d34ae7e823a78310b3},
}

@Article{rezaee2022fu,
  author            = {Rezaee, Khosro and Yang, Xuan and Khosravi, Mohammad R. and Zhang, Ruowei and Lin, Wenmin and Jeon, Gwanggil},
  title             = {Fusion-based learning for stress recognition in smart home: An IoMT framework},
  note              = {Cited by: 6},
  volume            = {216},
  abstract          = {Today, in order to prevent chronic stress from causing irreparable damage, it is imperative to diagnose and treat it in its early stages. Using the Internet of Things (IoT) and automated learning methods in homes, creating an intelligent environment can help identify stress-related emotions. An approach to stress detection based on metaheuristic fuzzy inference system-based learning (fMFiS-L) and emotion recognition is presented in this paper. Accordingly, our study focuses on the use of fusion learning to diagnose stress using the healthcare system and the Internet of Medical Things (IoMT) for smart homes. Music videos were shown to participants in the first stage to arouse emotional states such as anger, anxiety, and depression. Volunteers were divided into two groups, with one group practicing Reiki meditation for two weeks. The EEG signals were recorded before and after meditation, stress levels were assessed using the Likert scale, and emotions were classified using the modified fusion fuzzy inference system. In addition, a method is presented for determining the optimal parameters in the fMFiS-L structure by optimizing the innovative gunner algorithm (AIG). We conclude that Reiki meditation can significantly reduce negative emotions and stress levels in the IoMT environment of smart homes. Furthermore, the fMFiS-L architecture was evaluated for generalization of emotion recognition based on unseen EEG data. Generally, the classification of emotions produced satisfactory results, with a 92% accuracy rate. © 2022},
  author_keywords   = {Fusion learning; Human emotion recognition; IoMT environment; Smart building; Smart homes; Stress treatment},
  year              = {2022},
  doi               = {10.1016/j.buildenv.2022.108988},
  hasabstract       = {Y},
  journaltitle      = {Building and Environment},
  keywords          = {Fuzzy inference; Fuzzy systems; Intelligent buildings; Internet of things; Learning systems; Optimization; Speech recognition; Chronic stress; Emotion recognition; Fusion learning; Fuzzy inference systems; Human emotion recognition; Internet of medical thing environment; Smart homes; Stress levels; Stress recognition; Stress treatment; algorithm; detection method; learning; mental disorder; mental health; technological development; Automation},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127358728&doi=10.1016%2fj.buildenv.2022.108988&partnerID=40&md5=391decdebfedd42c1a936bddc3e9940a},
}

@Article{jafari2017po,
  author            = {Jafari, Zahra and Esmaili, Mahdiye and Delbari, Ahmad and Mehrpour, Masoud and Mohajerani, Majid H.},
  title             = {Post-stroke acquired amusia: A comparison between right- A nd left-brain hemispheric damages},
  note              = {Cited by: 12},
  number            = {2},
  pages             = {233 – 241},
  volume            = {40},
  abstract          = {BACKGROUND: Although extensive research has been published about the emotional consequences of stroke, most studies have focused on emotional words, speech prosody, voices, or facial expressions. The emotional processing of musical excerpts following stroke has been relatively unexplored. OBJECTIVE: The present study was conducted to investigate the effects of chronic stroke on the recognition of basic emotions in music. METHODS: Seventy persons, including 25 normal controls (NC), 25 persons with right brain damage (RBD) from stroke, and 20 persons with left brain damage (LBD) from stroke between the ages of 31-71 years were studied. The Musical Emotional Bursts (MEB) test, which consists of a set of short musical pieces expressing basic emotional states (happiness, sadness, and fear) and neutrality, was used to test musical emotional perception. RESULTS: Both stroke groups were significantly poorer than normal controls for the MEB total score and its subtests (p<0.001). The RBD group was significantly less able than the LBD group to recognize sadness (p = 0.047) and neutrality (p = 0.015). Negative correlations were found between age and MEB scores for all groups, particularly the NC and RBD groups. CONCLUSION: Our findings indicated that stroke affecting the auditory cerebrum can cause acquired amusia with greater severity in RBD than LBD. These results supported the "valence hypothesis" of right hemisphere dominance in processing negative emotions. © 2017 IOS Press and the authors. All rights reserved.},
  author_keywords   = {acquired amusia; aging; emotion recognition; Musical Emotional Bursts; Stroke},
  year              = {2017},
  doi               = {10.3233/NRE-161408},
  hasabstract       = {Y},
  journaltitle      = {NeuroRehabilitation},
  keywords          = {Adult; Aged; Cerebrum; Emotions; Facial Expression; Female; Humans; Male; Middle Aged; Music; Perceptual Disorders; Stroke; adult; aged; Article; brain injury; cerebrovascular accident; controlled study; emotion; fear; female; happiness; human; major clinical study; male; middle aged; music; perception; sadness; stroke patient; brain; comparative study; diagnostic imaging; emotion; facial expression; music; Perceptual Disorders; psychology; Stroke},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016433073&doi=10.3233%2fNRE-161408&partnerID=40&md5=46de1228b92c3f60b4bc1aa5e2c30a81},
}

@Article{he2020di,
  author            = {He, Jing-Xian and Zhou, Li and Liu, Zhen-Tao and Hu, Xin-Yue},
  title             = {Digital empirical research of influencing factors of musical emotion classification based on pleasure-arousal musical emotion fuzzy model},
  note              = {Cited by: 4; All Open Access, Gold Open Access},
  number            = {7},
  pages             = {872 – 881},
  volume            = {24},
  abstract          = {In recent years, with the further breakthrough of artificial intelligence theory and technology, as well as the further expansion of the Internet scale, the recognition of human emotions and the necessity for satisfying human psychological needs in future artificial intelligence technology development tendencies have been highlighted, in addition to physical task accomplishment. Musical emotion classification is an important research topic in artificial intelligence. The key premise of realizing music emotion classification is to construct a musical emotion model that conforms to the characteristics of music emotion recognition. Currently, three types of music emotion classification models are available: discrete category, continuous dimensional, and music emotion-specific models. The pleasure-arousal music emotion fuzzy model, which includes a wide range of emotions compared with other models, is selected as the emotional classification system in this study to investigate the influencing factor for musical emotion classification. Two representative emotional attributes, i.e., speed and strength, are used as variables. Based on test experiments involving music and non-music majors combined with questionnaire results, the relationship between music properties and emotional changes under the pleasure-arousal model is revealed quantitatively. © 2020 Fuji Technology Press. All rights reserved.},
  author_keywords   = {Affective computing; Classification; Fuzzy model; Music emotion; Pleasure-arousal emotion space},
  year              = {2020},
  doi               = {10.20965/JACIII.2020.P0872},
  hasabstract       = {Y},
  journaltitle      = {Journal of Advanced Computational Intelligence and Intelligent Informatics},
  keywords          = {Classification (of information); Artificial intelligence technologies; Emotional change; Emotional classification; Empirical research; Music emotion classifications; Psychological needs; Research topics; Task accomplishment; Artificial intelligence},
  priority          = {prio1},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098774308&doi=10.20965%2fJACIII.2020.P0872&partnerID=40&md5=a20634ec00d9b4bcc6e9980a4b708007},
}

@Article{paquette2018mu,
  author            = {Paquette, S. and Ahmed, G.D. and Goffi-Gomez, M.V. and Hoshino, A.C.H. and Peretz, I. and Lehmann, A.},
  title             = {Musical and vocal emotion perception for cochlear implants users},
  note              = {Cited by: 38},
  pages             = {272 – 282},
  volume            = {370},
  abstract          = {Cochlear implants can successfully restore hearing in profoundly deaf individuals and enable speech comprehension. However, the acoustic signal provided is severely degraded and, as a result, many important acoustic cues for perceiving emotion in voices and music are unavailable. The deficit of cochlear implant users in auditory emotion processing has been clearly established. Yet, the extent to which this deficit and the specific cues that remain available to cochlear implant users are unknown due to several confounding factors. Here we assessed the recognition of the most basic forms of auditory emotion and aimed to identify which acoustic cues are most relevant to recognize emotions through cochlear implants. To do so, we used stimuli that allowed vocal and musical auditory emotions to be comparatively assessed while controlling for confounding factors. These stimuli were used to evaluate emotion perception in cochlear implant users (Experiment 1) and to investigate emotion perception in natural versus cochlear implant hearing in the same participants with a validated cochlear implant simulation approach (Experiment 2). Our results showed that vocal and musical fear was not accurately recognized by cochlear implant users. Interestingly, both experiments found that timbral acoustic cues (energy and roughness) correlate with participant ratings for both vocal and musical emotion bursts in the cochlear implant simulation condition. This suggests that specific attention should be given to these cues in the design of cochlear implant processors and rehabilitation protocols (especially energy, and roughness). For instance, music-based interventions focused on timbre could improve emotion perception and regulation, and thus improve social functioning, in children with cochlear implants during development. © 2018 Elsevier B.V.},
  author_keywords   = {Cochlear implants; Cross-domain comparison; Emotional acoustic cues; Music; Timbre; Voice},
  year              = {2018},
  doi               = {10.1016/j.heares.2018.08.009},
  hasabstract       = {Y},
  journaltitle      = {Hearing Research},
  keywords          = {Acoustic Stimulation; Adult; Auditory Perception; Cochlear Implantation; Cochlear Implants; Cues; Electric Stimulation; Emotions; Female; Humans; Judgment; Male; Middle Aged; Music; Persons With Hearing Impairments; Voice Quality; Young Adult; acoustic cue; adult; arousal; Article; association; auditory discrimination; auditory emotion; auditory stimulation; clinical article; controlled study; emotion; fear; female; happiness; human; male; measurement accuracy; middle aged; music; musical emotion perception; perception; priority journal; regulatory mechanism; sadness; simulation; social interaction; vocal emotion perception; young adult; association; cochlea prosthesis; cochlear implantation; decision making; devices; electrostimulation; emotion; hearing; hearing impaired person; music; psychology; rehabilitation; voice},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052838987&doi=10.1016%2fj.heares.2018.08.009&partnerID=40&md5=c9dc902a65c24daa691f803e29963706},
}

@Article{hou2019di,
  author            = {Hou, Yimin and Chen, Shuaiqi},
  title             = {Distinguishing Different Emotions Evoked by Music via Electroencephalographic Signals},
  note              = {Cited by: 23; All Open Access, Bronze Open Access, Green Open Access},
  volume            = {2019},
  abstract          = {Music can evoke a variety of emotions, which may be manifested by distinct signals on the electroencephalogram (EEG). Many previous studies have examined the associations between specific aspects of music, including the subjective emotions aroused, and EEG signal features. However, no study has comprehensively examined music-related EEG features and selected those with the strongest potential for discriminating emotions. So, this paper conducted a series of experiments to identify the most influential EEG features induced by music evoking different emotions (calm, joy, sad, and angry). We extracted 27-dimensional features from each of 12 electrode positions then used correlation-based feature selection method to identify the feature set most strongly related to the original features but with lowest redundancy. Several classifiers, including Support Vector Machine (SVM), C4.5, LDA, and BPNN, were then used to test the recognition accuracy of the original and selected feature sets. Finally, results are analyzed in detail and the relationships between selected feature set and human emotions are shown clearly. Through the classification results of 10 random examinations, it could be concluded that the selected feature sets of Pz are more effective than other features when using as the key feature set to classify human emotion statues. © 2019 Yimin Hou and Shuaiqi Chen.},
  year              = {2019},
  doi               = {10.1155/2019/3191903},
  hasabstract       = {Y},
  journaltitle      = {Computational Intelligence and Neuroscience},
  keywords          = {Algorithms; Arousal; Auditory Perception; Brain; Electroencephalography; Emotions; Evoked Potentials; Humans; Music; Support Vector Machine; Electroencephalography; Man machine systems; Support vector machines; Classification results; Electro-encephalogram (EEG); Electroencephalographic signals; Feature selection methods; Feature sets; Human emotion; Key feature; Recognition accuracy; algorithm; arousal; brain; electroencephalography; emotion; evoked response; hearing; human; music; physiology; procedures; support vector machine; Biomedical signal processing},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063229122&doi=10.1155%2f2019%2f3191903&partnerID=40&md5=3f2f93a7ad6cfac7a64f379cd607b8cd},
}

@Article{alnafjan2020li,
  author            = {Al-Nafjan, Abeer and Alharthi, Khulud and Kurdi, Heba},
  title             = {Lightweight building of an electroencephalogram-based emotion detection system},
  note              = {Cited by: 16; All Open Access, Gold Open Access, Green Open Access},
  number            = {11},
  pages             = {1 – 17},
  volume            = {10},
  abstract          = {Brain–computer interface (BCI) technology provides a direct interface between the brain and an external device. BCIs have facilitated the monitoring of conscious brain electrical activity via electroencephalogram (EEG) signals and the detection of human emotion. Recently, great progress has been made in the development of novel paradigms for EEG-based emotion detection. These studies have also attempted to apply BCI research findings in varied contexts. Interestingly, advances in BCI technologies have increased the interest of scientists because such technologies’ practical applications in human–machine relationships seem promising. This emphasizes the need for a building process for an EEG-based emotion detection system that is lightweight, in terms of a smaller EEG dataset size and no involvement of feature extraction methods. In this study, we investigated the feasibility of using a spiking neural network to build an emotion detection system from a smaller version of the DEAP dataset with no involvement of feature extraction methods while maintaining decent accuracy. The results showed that by using a NeuCube-based spiking neural network, we could detect the valence emotion level using only 60 EEG samples with 84.62% accuracy, which is a comparable accuracy to that of previous studies. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.},
  author_keywords   = {Brain–computer interface (BCI); EEG-based emotion detection; Electroencephalogram (EEG); NeuCube; Spiking neural network},
  year              = {2020},
  doi               = {10.3390/brainsci10110781},
  hasabstract       = {Y},
  journaltitle      = {Brain Sciences},
  keywords          = {action potential; Article; artificial neural network; autism; Bayesian network; behavior; classification algorithm; cognition; comprehension; consciousness disorder; controlled study; convolutional neural network; data analysis; deep learning; electric activity; electroencephalogram; electroencephalography; emotion; emotion detection system; feasibility study; feature extraction; feature extraction algorithm; fuzzy system; human; human computer interaction; human experiment; k nearest neighbor; measurement accuracy; model; motor dysfunction; multilayer perceptron; music; neucube model; overall survival; Parkinson disease; pattern recognition; postsynaptic density; power spectrum; prediction; psychology; sample size; schizophrenia; signal noise ratio; software; spatiotemporal analysis; spiking neural network; support vector machine},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094198226&doi=10.3390%2fbrainsci10110781&partnerID=40&md5=18f3d3a214aaf01c8abb182eb9205e01},
}

@Article{goshvarpour2017fu,
  author            = {Goshvarpour, Atefeh and Abbasi, Ataollah and Goshvarpour, Ateke},
  title             = {Fusion of heart rate variability and pulse rate variability for emotion recognition using lagged poincare plots},
  note              = {Cited by: 37},
  number            = {3},
  pages             = {617 – 629},
  volume            = {40},
  abstract          = {Designing an efficient automatic emotion recognition system based on physiological signals has attracted great interests within the research of human–machine interactions. This study was aimed to classify emotional responses by means of a simple dynamic signal processing technique and fusion frameworks. The electrocardiogram and finger pulse activity of 35 participants were recorded during rest condition and when subjects were listening to music intended to stimulate certain emotions. Four emotion categories, including happiness, sadness, peacefulness, and fear were chosen. Estimating heart rate variability (HRV) and pulse rate variability (PRV), 4 Poincare indices in 10 lags were extracted. The support vector machine classifier was used for emotion classification. Both feature level (FL) and decision level (DL) fusion schemes were examined. Significant differences have been observed between lag 1 Poincare plot indices and the other lagged measures. The mean accuracies of 84.1, 82.9, 79.68, and 76.05% were obtained for PRV, DL, FL, and HRV measures, respectively. However, DL outperformed others in discriminating sadness and peacefulness, using SD1 and total features, correspondingly. In both cases, the classification rates improved up to 92% (with the sensitivity of 95% and specificity of 83.33%). Totally, DL resulted in better performances compared to FL. In addition, the impact of the fusion rules on the classification performances has been confirmed. © 2017, Australasian College of Physical Scientists and Engineers in Medicine.},
  author_keywords   = {Classification; Emotion; Fusion; Lagged Poincare plot},
  comment           = {physiological},
  year              = {2017},
  doi               = {10.1007/s13246-017-0571-1},
  hasabstract       = {Y},
  journaltitle      = {Australasian Physical and Engineering Sciences in Medicine},
  keywords          = {Algorithms; Emotions; Female; Heart Rate; Humans; Male; Pulse; Young Adult; Classification (of information); Fusion reactions; Heart; Speech recognition; Automatic emotion recognition; Classification performance; Dynamic signal processing; Emotion; Heart rate variability; Poincare plots; Pulse rate variability; Support vector machine classifiers; adult; Article; controlled study; electrocardiography; emotionality; female; heart rate variability; human; human experiment; male; music; normal human; pulse rate; pulse rate variability; sadness; sensitivity and specificity; support vector machine; algorithm; emotion; heart rate; physiology; young adult; Biomedical signal processing},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024505036&doi=10.1007%2fs13246-017-0571-1&partnerID=40&md5=595af2db595c4c27f2336e4ba3850fe0},
}

@Article{giannakopoulos2015py,
  author            = {Giannakopoulos, Theodoros},
  title             = {PyAudioAnalysis: An open-source python library for audio signal analysis},
  note              = {Cited by: 326; All Open Access, Gold Open Access, Green Open Access},
  number            = {12},
  volume            = {10},
  abstract          = {Audio information plays a rather important role in the increasing digital content that is available today, resulting in a need for methodologies that automatically analyze such content: audio event recognition for home automations and surveillance systems, speech recognition, music information retrieval, multimodal analysis (e.g. audio-visual analysis of online videos for content-based recommendation), etc. This paper presents pyAudioAnalysis, an open-source Python library that provides a wide range of audio analysis procedures including: feature extraction, classification of audio signals, supervised and unsupervised segmentation and content visualization. pyAudioAnalysis is licensed under the Apache License and is available at GitHub (https://github.com/tyiannak/pyAudioAnalysis/). Here we present the theoretical background behind the wide range of the implemented methodologies, along with evaluation metrics for some of the methods. pyAudioAnalysis has been already used in several audio analysis research applications: smart-home functionalities through audio event detection, speech emotion recognition, depression classification based on audiovisual features, music segmentation, multimodal content-based movie recommendation and health applications (e.g. monitoring eating habits). The feedback provided from all these particular audio applications has led to practical enhancement of the library. © 2015 Theodoros Giannakopoulos.},
  comment           = {No relevant task, discussed},
  year              = {2015},
  doi               = {10.1371/journal.pone.0144610},
  hasabstract       = {Y},
  journaltitle      = {PLoS ONE},
  keywords          = {Acoustic Stimulation; Animals; Humans; Information Storage and Retrieval; Libraries, Digital; Music; Pattern Recognition, Automated; Software; Sound; Speech; algorithm; Article; automatic speech recognition; computer analysis; computer graphics; computer interface; computer language; computer program; data extraction; information retrieval; multimedia; online analytical processing; signal detection; animal; auditory stimulation; automated pattern recognition; human; library; music; physiology; software; sound; speech},
  modificationdate  = {2024-05-16T12:01:15},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961354674&doi=10.1371%2fjournal.pone.0144610&partnerID=40&md5=5b51e0800fac922c60ba6c7598037f07},
}

@Article{gomezcanon2021mu,
  author            = {Gomez-Canon, Juan Sebastia and Cano, Estefania and Eerola, Tuomas and Herrera, Perfecto and Hu, Xiao and Yang, Yi-Hsuan and Gomez, Emilia},
  title             = {Music Emotion Recognition: Toward new, robust standards in personalized and context-sensitive applications},
  note              = {Cited by: 40; All Open Access, Green Open Access},
  number            = {6},
  pages             = {106 – 114},
  volume            = {38},
  abstract          = {Emotion is one of the main reasons why people engage and interact with music [1]. Songs can express our inner feelings, produce goosebumps, bring us to tears, share an emotional state with a composer or performer, or trigger specific memories. Interest in a deeper understanding of the relationship between music and emotion has motivated researchers from various areas of knowledge for decades [2], including computational researchers. Imagine an algorithm capable of predicting the emotions that a listener perceives in a musical piece, or one that dynamically generates music that adapts to the mood of a conversation in a film - a particularly fascinating and provocative idea. These algorithms typify music emotion recognition (MER), a computational task that attempts to automatically recognize either the emotional content in music or the emotions induced by music to the listener [3]. To do so, emotionally relevant features are extracted from music. The features are processed, evaluated, and then associated with certain emotions. MER is one of the most challenging high-level music description problems in music information retrieval (MIR), an interdisciplinary research field that focuses on the development of computational systems to help humans better understand music collections. MIR integrates concepts and methodologies from several disciplines, including music theory, music psychology, neuroscience, signal processing, and machine learning.  © 1991-2012 IEEE.},
  year              = {2021},
  doi               = {10.1109/MSP.2021.3106232},
  hasabstract       = {Y},
  journaltitle      = {IEEE Signal Processing Magazine},
  keywords          = {Behavioral research; Computation theory; Search engines; Signal processing; Speech recognition; Computational task; Context-sensitive applications; Emotion recognition; Emotional state; Inner feelings; Music and emotions; Music emotions; Music information retrieval; Musical pieces; Relevant features; Music},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118593358&doi=10.1109%2fMSP.2021.3106232&partnerID=40&md5=65ab889060f2c2b20783e6691066b4d1},
}

@Article{hizlisoy2021mu,
  author            = {Hizlisoy, Serhat and Yildirim, Serdar and Tufekci, Zekeriya},
  title             = {Music emotion recognition using convolutional long short term memory deep neural networks},
  note              = {Cited by: 85; All Open Access, Gold Open Access},
  number            = {3},
  pages             = {760 – 767},
  volume            = {24},
  abstract          = {In this paper, we propose an approach for music emotion recognition based on convolutional long short term memory deep neural network (CLDNN) architecture. In addition, we construct a new Turkish emotional music database composed of 124 Turkish traditional music excerpts with a duration of 30 s each and the performance of the proposed approach is evaluated on the constructed database. We utilize features obtained by feeding convolutional neural network (CNN) layers with log-mel filterbank energies and mel frequency cepstral coefficients (MFCCs) in addition to standard acoustic features. Classification results show that the best performance is obtained when the new feature set is combined with the standard features using the long short term memory (LSTM) + deep neural network (DNN) classi fier. The overall accuracy of 99.19% is obtained using the proposed system with 10 fold cross-validation. Specifically, 6.45 points improvement is achieved. Additionally, the results also show that the LSTM + DNN classifier yields 1.61, 1.61 and 3.23 points improvements in music emotion recognition accuracies compared to k-nearest neighbor (k-NN), support vector machine (SVM), and Random Forest classifiers, respectively. © 2020 Karabuk University},
  author_keywords   = {Convolutional long short term memory deep neural networks; Music emotion recognition; Turkish emotional music database},
  year              = {2021},
  doi               = {10.1016/j.jestch.2020.10.009},
  hasabstract       = {Y},
  journaltitle      = {Engineering Science and Technology, an International Journal},
  priority          = {prio1},
  publication_stage = {Final},
  ranking           = {rank2},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096857185&doi=10.1016%2fj.jestch.2020.10.009&partnerID=40&md5=a6d9948932df1065662495509f77bfe5},
}

@Article{kopylov2020de,
  author            = {Kopylov, Andrei and Seredin, Oleg and Filin, Andrei and Tyshkevich, Boris},
  title             = {Detection of interactive voice response (IVR) in phone call records},
  note              = {Cited by: 2},
  number            = {4},
  pages             = {907 – 915},
  volume            = {23},
  abstract          = {Separation of pre-recorded messages (Interactive Voice Response, IVR) from live speech fragments in real-time plays a significant role in speech emotion recognition (SER) systems, unwanted calls filtering, automatic detection of answering machine responses, reduction of stored record sizes, voice mail spam filtration, etc. The problem complexity is that, unlike with silent, music, and noise fragments studied by the conventional voice activity recognition (VAD), IVR usually contains speech. Three classifiers for live speech fragments detection in phone call records are considered: based on the support vector machine (SVM), gradient boosting (XGBoost) and convolutional neural network (CNN). The Geneva Minimalistic Acoustic Parameter Set for XGBoost and SVM, and log-spectrograms and gammatonegrams for CNN were used for feature representation of audio fragments. Experiments with a dataset of phone calls demonstrate comparable quality (around 0.96 according to the F1-averaged measure) of the considered algorithms with CNN having a advantage (0.98). © 2020, Springer Science+Business Media, LLC, part of Springer Nature.},
  author_keywords   = {CNN; Gammatonegram; GeMAPS; Gradient boosting; IVR; Log-spectrogram; Speech analysis; SVM},
  year              = {2020},
  doi               = {10.1007/s10772-020-09754-3},
  hasabstract       = {Y},
  journaltitle      = {International Journal of Speech Technology},
  keywords          = {Audio acoustics; Convolutional neural networks; Personal communication systems; Real time systems; Support vector machines; Telephone sets; Acoustic parameters; Answering machines; Automatic Detection; Feature representation; Gradient boosting; Interactive voice response; Problem complexity; Speech emotion recognition; Speech recognition},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096118230&doi=10.1007%2fs10772-020-09754-3&partnerID=40&md5=d0c9fd33fc018619fa7a8e4958da7fc3},
}

@Article{zangenehsoroush2018an,
  author            = {Zangeneh Soroush, Morteza and Maghooli, Keivan and Setarehdan, Seyed Kamaledin and Nasrabadi, Ali Motie},
  title             = {A novel approach to emotion recognition using local subset feature selection and modified Dempster-Shafer theory},
  note              = {Cited by: 26; All Open Access, Gold Open Access, Green Open Access},
  number            = {1},
  volume            = {14},
  abstract          = {Background: Emotion recognition is an increasingly important field of research in brain computer interactions. Introduction: With the advance of technology, automatic emotion recognition systems no longer seem far-fetched. Be that as it may, detecting neural correlates of emotion has remained a substantial bottleneck. Settling this issue will be a breakthrough of significance in the literature. Methods: The current study aims to identify the correlations between different emotions and brain regions with the help of suitable electrodes. Initially, independent component analysis algorithm is employed to remove artifacts and extract the independent components. The informative channels are then selected based on the thresholded average activity value for obtained components. Afterwards, effective features are extracted from selected channels common between all emotion classes. Features are reduced using the local subset feature selection method and then fed to a new classification model using modified Dempster-Shafer theory of evidence. Results: The presented method is employed to DEAP dataset and the results are compared to those of previous studies, which highlights the significant ability of this method to recognize emotions through electroencephalography, by the accuracy of about 91%. Finally, the obtained results are discussed and new aspects are introduced. Conclusions: The present study addresses the long-standing challenge of finding neural correlates between human emotions and the activated brain regions. Also, we managed to solve uncertainty problem in emotion classification which is one of the most challenging issues in this field. The proposed method could be employed in other practical applications in future. © 2018 The Author(s).},
  author_keywords   = {Brain computer interactions; Dempster Shafer theory; Emotion identification; Independent component analysis; Local subset feature selection; Machine learning methods},
  comment           = {brain study},
  year              = {2018},
  doi               = {10.1186/s12993-018-0149-4},
  hasabstract       = {Y},
  journaltitle      = {Behavioral and Brain Functions},
  keywords          = {Adult; Brain; Databases, Factual; Electroencephalography; Emotions; Female; Humans; Machine Learning; Male; Music; Recognition (Psychology); Video Recording; Young Adult; accuracy; algorithm; Article; artifact; brain region; Dempster Shafer theory; electroencephalography; emotion; human; priority journal; psychological theory; adult; brain; emotion; factual database; female; machine learning; male; music; physiology; procedures; psychology; recognition; videorecording; young adult},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055842429&doi=10.1186%2fs12993-018-0149-4&partnerID=40&md5=7090559db899acaeff0494ebb3d42e13},
}

@Article{zhang2021an,
  author            = {Zhang, Jin and Xu, Ziming and Zhou, Yueying and Wang, Pengpai and Fu, Ping and Xu, Xijia and Zhang, Daoqiang},
  title             = {An Empirical Comparative Study on the Two Methods of Eliciting Singers’ Emotions in Singing: Self-Imagination and VR Training},
  note              = {Cited by: 3; All Open Access, Gold Open Access, Green Open Access},
  volume            = {15},
  abstract          = {Emotional singing can affect vocal performance and the audience’s engagement. Chinese universities use traditional training techniques for teaching theoretical and applied knowledge. Self-imagination is the predominant training method for emotional singing. Recently, virtual reality (VR) technologies have been applied in several fields for training purposes. In this empirical comparative study, a VR training task was implemented to elicit emotions from singers and further assist them with improving their emotional singing performance. The VR training method was compared against the traditional self-imagination method. By conducting a two-stage experiment, the two methods were compared in terms of emotions’ elicitation and emotional singing performance. In the first stage, electroencephalographic (EEG) data were collected from the subjects. In the second stage, self-rating reports and third-party teachers’ evaluations were collected. The EEG data were analyzed by adopting the max-relevance and min-redundancy algorithm for feature selection and the support vector machine (SVM) for emotion recognition. Based on the results of EEG emotion classification and subjective scale, VR can better elicit the positive, neutral, and negative emotional states from the singers than not using this technology (i.e., self-imagination). Furthermore, due to the improvement of emotional activation, VR brings the improvement of singing performance. The VR hence appears to be an effective approach that may improve and complement the available vocal music teaching methods. © Copyright © 2021 Zhang, Xu, Zhou, Wang, Fu, Xu and Zhang.},
  author_keywords   = {electroencephalogram; emotion classification; self-imagination; singing emotion; virtual reality; vocal music teaching},
  year              = {2021},
  doi               = {10.3389/fnins.2021.693468},
  hasabstract       = {Y},
  journaltitle      = {Frontiers in Neuroscience},
  keywords          = {adult; article; comparative study; controlled study; electroencephalogram; emotion; feature selection; female; human; human experiment; imagination; male; music; singing; support vector machine; teacher; teaching; theoretical study; virtual reality},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113442071&doi=10.3389%2ffnins.2021.693468&partnerID=40&md5=66bc319e8122006c5daa8f2feb92cc00},
}

@Article{mohamed2021mu,
  author            = {Mohamed, Marwa Hussien and Khafagy, Mohamed Helmy and Ibrahim, Mohamed Hasan and Elmenshawy, Khaled and Fadlallah, Haitham Rizk},
  title             = {Music recommendation system used emotions to track and change negative users' mood},
  note              = {Cited by: 1},
  number            = {17},
  pages             = {4358 – 4375},
  volume            = {99},
  abstract          = {Recently, the Recommender system is the most important research area with the advent of e-commerce and e-business on the web. Emotion-based music recovery will have extraordinary potential in catering nowadays, digital music archives quickly extending in the developing smartphones and ubiquitous environments. Many types of research are conducted to improve the music recommendation to users based on their emotions. Human emotions have much difficulty due to the subjective perception of emotions and accuracy challenges. In this paper, we need to solve the problem of recommending songs to the user based on his selection if it was bad, sad, or angry mood by using our system we will recommend to the user songs from pleasant mood to try changing him to the good mood and track if user listen to this song or scaped it. Our new algorithm, "Hybrid emotion-based music recommendation system," will recommend music to the next level, generating playlist which suits and matches your mood of listening to music. The user can try three choices to get the emotion by using face recognition, choosing three colors, and using the arousal map to select the emotion will appear to users then recommended songs according to his status we merge the output of the system to detect the right mood. Our new system has good novelty and diversity of songs recommended to users and changes the user's mood to the pleasure. At our experimental results We are using precision, recall and f-measure accuracy equations to calculate the effective of our system. To gain high results we apply different experiments detect users' emotions like using face only, colors, arousal map then let users select to types of emotion like face and colors or colors and arousal and finally apply hybrid emotions system. Every time we measure the accuracy of the results. Based on the experiments results using our new hybrid emotions model is best accuracy in surprised, anger, natural and relaxed. While user's emotion sadness using face. arousal map has high accuracy with happy emotions. © 2021 Little Lion Scientific. All rights reserved.},
  author_keywords   = {Collaborative filtering; Content-based filtering; Emotions; Face recognition; Recommender system},
  comment           = {Quality issues, no relevant task, discussed},
  year              = {2021},
  hasabstract       = {Y},
  journaltitle      = {Journal of Theoretical and Applied Information Technology},
  modificationdate  = {2024-05-16T11:57:31},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115224058&partnerID=40&md5=b6a6ab9b6fadfa2baf8f9e46ed5b8e4e},
}

@Article{hsiao2017me,
  author            = {Hsiao, Shih-Wen and Chen, Shih-Kai and Lee, Chu-Hsuan},
  title             = {Methodology for stage lighting control based on music emotions},
  note              = {Cited by: 21},
  pages             = {14 – 35},
  volume            = {412-413},
  abstract          = {Traditionally, stage lighting regulations have required that professionally trained technicians operate the lighting equipment; however, contemporary demands for higher-quality performances require more preparation before a performance. Thus, technicians or club DJs now spend double to triple the time previously required before a show on matching the lighting control sequence musical instrument digital interface (MIDI) with the music, which is very time consuming. Thus, a methodology for automatic stage-lighting regulation would be very useful. Recently, the development of music emotion recognition (MER) and neural network algorithms has progressed significantly. Feelings related to music can be recognized and are even quantifiable using a supervised machine learning approach. In this study, a variety of music signal features from 2, 087 song clips were captured, and then, a cross-validation test based on the support vector machine's (SVM) accuracy of classifying them into Thayer's emotion plane was applied to the main features related to music emotions, in order to produce linear quantitative values for describing music emotions. Music emotions and color preferences for stage lighting were subsequently studied. Using the experimental results, a support vector regression (SVR) model was trained to construct simulations. To increase the realism of the simulations, we developed an automatic music segment detection methodology based on music signal intensity to capture the different music strengths and feelings in each segment. Furthermore, music genres were studied as a factor for developing a comprehensive automatic stage lighting system based on feelings, genre, and the intensity of each segment of music. © 2017 Elsevier Inc.},
  author_keywords   = {Automatic music segment detection; Automatic stage-lighting regulation; Lighting color regulation based on music emotions and genre; Music emotion recognition; Support vector regression (SVR)},
  year              = {2017},
  doi               = {10.1016/j.ins.2017.05.026},
  hasabstract       = {Y},
  journaltitle      = {Information Sciences},
  keywords          = {Electronic musical instruments; Learning systems; Speech recognition; Supervised learning; Support vector machines; Cross-validation tests; Music emotions; Music segments; Musical instrument digital interfaces; Neural network algorithm; Stage lighting; Supervised machine learning; Support vector regression (SVR); Lighting},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019625713&doi=10.1016%2fj.ins.2017.05.026&partnerID=40&md5=273e15e75f4401f136d5826a8b70821e},
}

@Article{kim2021mo,
  author            = {Kim, Tae-Yeun and Ko, Hoon and Kim, Sung-Hwan and Kim, Ho-Da},
  title             = {Modeling of recommendation system based on emotional information and collaborative filtering},
  note              = {Cited by: 29; All Open Access, Gold Open Access, Green Open Access},
  number            = {6},
  pages             = {1 – 25},
  volume            = {21},
  abstract          = {Emotion information represents a user’s current emotional state and can be used in a variety of applications, such as cultural content services that recommend music according to user emotional states and user emotion monitoring. To increase user satisfaction, recommendation methods must understand and reflect user characteristics and circumstances, such as individual preferences and emotions. However, most recommendation methods do not reflect such characteristics accurately and are unable to increase user satisfaction. In this paper, six human emotions (neutral, happy, sad, angry, surprised, and bored) are broadly defined to consider user speech emotion information and recommend matching content. The “genetic algorithms as a feature selection method” (GAFS) algorithm was used to classify normalized speech according to speech emotion information. We used a support vector machine (SVM) algorithm and selected an optimal kernel function for recognizing the six target emotions. Performance evaluation results for each kernel function revealed that the radial basis function (RBF) kernel function yielded the highest emotion recognition accuracy of 86.98%. Additionally, content data (images and music) were classified based on emotion information using factor analysis, correspondence analysis, and Euclidean distance. Finally, speech information that was classified based on emotions and emotion information that was recognized through a collaborative filtering technique were used to predict user emotional preferences and recommend content that matched user emotions in a mobile application. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
  author_keywords   = {Collaborative filtering; Emotion recognition; Speech emotion information; Support vector machine algorithm},
  comment           = {no relevant content, discussed},
  year              = {2021},
  doi               = {10.3390/s21061997},
  hasabstract       = {Y},
  journaltitle      = {Sensors},
  keywords          = {Algorithms; Emotions; Humans; Music; Speech; Support Vector Machine; Classification (of information); Collaborative filtering; Genetic algorithms; Speech recognition; Support vector machines; Collaborative filtering techniques; Correspondence analysis; Emotional information; Feature selection methods; Individual preference; Radial Basis Function(RBF); Recommendation methods; Support vector machine algorithm; algorithm; emotion; human; music; speech; support vector machine; Information use},
  modificationdate  = {2024-05-16T11:52:44},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102176699&doi=10.3390%2fs21061997&partnerID=40&md5=163e963e5ef610e4435953e01685a1f5},
}

@Article{teo2019de,
  author            = {Teo, Jason and Chia, Jia Tian and Lee, Jie Yu},
  title             = {Deep learning for emotion recognition in affective virtual reality and music applications},
  note              = {Cited by: 1; All Open Access, Bronze Open Access},
  number            = {2 Special Issue 2},
  pages             = {162 – 170},
  volume            = {8},
  abstract          = {This paper presents a deep learning approach to emotion recognition as applied to virtual reality and music predictive analytics. Firstly, it investigates the deep parameter tuning of the multi-hidden layer neural networks, which are also commonly referred to simply as deep networks that are used to conduct emotion detection in virtual reality (VR)-electroencephalography (EEG) predictive analytics. Deep networks have been studied extensively over the last decade and have shown to be among the most accurate methods for predictive analytics in image recognition and speech processing domains. However, most predictive analytics deep network studies focus on the shallow parameter tuning when attempting to boost prediction accuracies, which includes deep network tuning parameters such as number of hidden layers, number of hidden nodes per hidden layer and the types of activation functions used in the hidden nodes. Much less effort has been put into investigating the tuning of deep parameters such as input dropout ratios, L1 (lasso) regularization and L2 (ridge regularization) parameters of the deep networks. As such, the goal of this study is to perform a parameter tuning investigation on these deep parameters of the deep networks for predicting emotions in a virtual reality environment using electroencephalography (EEG) signal obtained when the user is exposed to immersive content. The results show that deep tuning of deep networks in VR-EEG can improve the accuracies of predicting emotions. The best emotion prediction accuracy was improved to over 96% after deep tuning was conducted on the deep network parameters of input dropout ratio, L1 and L2 regularization parameters. Secondly, it investigates a similar possible approach when applied to 4-quadrant music emotion recognition. Recent studies have been characterizing music based on music genres and various classification techniques have been used to achieve the best accuracy rate. Several researches on deep learning have shown outstanding results in relation to dimensional music emotion recognition. Yet, there is no concrete and concise description to express music. In regards to this research gap, a research using more detailed metadata on two-dimensional emotion annotations based on the Russell’s model is conducted. Rather than applying music genres or lyrics into machine learning algorithm to MER, higher representation of music information, acoustic features are used. In conjunction with the four classes classification problem, an available dataset named AMG1608 is feed into a training model built from deep neural network. The dataset is first preprocessed to get full access of variables before any machine learning is done. The classification rate is then collected by running the scripts in R environment. The preliminary result showed a classification rate of 46.0%. Experiments on architecture and hyper-parameter tuning as well as instance reduction were designed and conducted. The tuned parameters that increased the accuracy for deep learners were hidden layer architecture, number of epochs, instance reduction, input dropout ratio and ℓ1 and ℓ2 regularization. The final best prediction accuracy obtained was 61.7%, giving an overall improvement of more than 15% for music emotion recognition which are based purely on the music’s acoustical features. © BEIESP.},
  author_keywords   = {Acoustic features; Deep learning; Electroencephalography; Emotion classification; Music emotion recognition; Neuroinformatics; Virtual reality},
  comment           = {EEG},
  year              = {2019},
  doi               = {10.35940/ijrte.B1030.0782S219},
  hasabstract       = {Y},
  journaltitle      = {International Journal of Recent Technology and Engineering},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071251566&doi=10.35940%2fijrte.B1030.0782S219&partnerID=40&md5=55349bfff59ab4f8778279d327f66e5a},
}

@Article{wang2021mu,
  author            = {Wang, Yu},
  title             = {Music Composition and Emotion Recognition Using Big Data Technology and Neural Network Algorithm},
  note              = {Cited by: 3; All Open Access, Gold Open Access, Green Open Access},
  volume            = {2021},
  abstract          = {To implement a mature music composition model for Chinese users, this paper analyzes the music composition and emotion recognition of composition content through big data technology and Neural Network (NN) algorithm. First, through a brief analysis of the current music composition style, a new Music Composition Neural Network (MCNN) structure is proposed, which adjusts the probability distribution of the Long Short-Term Memory (LSTM) generation network by constructing a reasonable Reward function. Meanwhile, the rules of music theory are used to restrict the generation of music style and realize the intelligent generation of specific style music. Afterward, the generated music composition signal is analyzed from the time-frequency domain, frequency domain, nonlinearity, and time domain. Finally, the emotion feature recognition and extraction of music composition content are realized. Experiments show that: When the iteration times of the function increase, the number of weight parameter adjustments and learning ability will increase, and thus the accuracy of the model for music composition can be greatly improved. Meanwhile, when the iteration times increases, the loss function will decrease slowly. Moreover, the music composition generated through the proposed model includes the following four aspects: Sadness, joy, loneliness, and relaxation. The research results can promote music composition intellectualization and impacts traditional music composition mode. © 2021 Yu Wang.},
  year              = {2021},
  doi               = {10.1155/2021/5398922},
  hasabstract       = {Y},
  journaltitle      = {Computational Intelligence and Neuroscience},
  keywords          = {Algorithms; Big Data; Emotions; Music; Neural Networks, Computer; Technology; Big data; Frequency domain analysis; Iterative methods; Long short-term memory; Music; Probability distributions; Time domain analysis; Composition content; Composition modeling; Data technologies; Emotion recognition; Iteration time; Music composition; Music emotions; Neural networks algorithms; Paper analysis; Technology network; algorithm; emotion; music; technology; Speech recognition},
  priority          = {prio1},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122382786&doi=10.1155%2f2021%2f5398922&partnerID=40&md5=8526c913cb2f0b1c8962a1b7da8afb30},
}

@Article{he2020st,
  author            = {He, Hong and Tan, Yonghong and Ying, Jun and Zhang, Wuxiong},
  title             = {Strengthen EEG-based emotion recognition using firefly integrated optimization algorithm},
  note              = {Cited by: 48},
  volume            = {94},
  abstract          = {Emotion recognition is helpful for human to enhance self-awareness and respond appropriately towards the happenings around them. Due to the complexity and diversity of emotions, EEG-based emotion recognition is still a challenging task in pattern recognition. In order to recognize diverse emotions, we propose a novel firefly integrated optimization algorithm (FIOA) in this paper. It can simultaneously accomplish multiple tasks, i.e. the optimal feature selection, parameter setting and classifier selection according to different EEG-based emotion datasets. The FIOA utilizes a ranking probability objection function to guarantee the high accuracy recognition with less features. Moreover, the hybrid encoding expression and the dual updating strategy are developed in the FIOA so as to realize the optimal selection of feature subset and classifier without stagnating in the local optimum. In addition to the public DEAP datasets, we also conducted an EEG-based music emotion experiment involving 20 subjects for the validation of the proposed FIOA. After filtering and segmentation, three categories of features were extracted from every EEG signal. Then FIOA was applied to every subject dataset for two pattern recognition of emotions. The results show that the FIOA can automatically find the optimal features, parameter and classifier for different emotion datasets, which greatly reduces the artificial selection workload. Furthermore, comparing with the binary particle swarm optimization (PSObinary) and the binary firefly (FAbinary), the FIOA can achieve the higher accuracy with less features in the emotion recognition. © 2020 Elsevier B.V.},
  author_keywords   = {Classification; EEG; Emotion recognition; Feature selection; Firefly algorithm},
  year              = {2020},
  doi               = {10.1016/j.asoc.2020.106426},
  hasabstract       = {Y},
  journaltitle      = {Applied Soft Computing Journal},
  keywords          = {Bioluminescence; Biomedical signal processing; Feature extraction; Particle swarm optimization (PSO); Speech recognition; Artificial selection; Binary particle swarm optimization; Classifier selection; Emotion recognition; Integrated optimization; Objection functions; Optimal feature selections; Recognition of emotion; Classification (of information)},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085937163&doi=10.1016%2fj.asoc.2020.106426&partnerID=40&md5=536ef66e1584b040e63724f7083e152d},
}

@Article{vidas2020de,
  author            = {Vidas, Dianna and Calligeros, Renee and Nelson, Nicole L. and Dingle, Genevieve A.},
  title             = {Development of emotion recognition in popular music and vocal bursts},
  note              = {Cited by: 5},
  number            = {5},
  pages             = {906 – 919},
  volume            = {34},
  abstract          = {Previous research on the development of emotion recognition in music has focused on classical, rather than popular music. Such research does not consider the impact of lyrics on judgements of emotion in music, impact that may differ throughout development. We had 172 children, adolescents, and adults (7- to 20-year-olds) judge emotions in popular music. In song excerpts, the melody of the music and the lyrics had either congruent valence (e.g. happy lyrics and melody), or incongruent valence (e.g. scared lyrics, happy melody). We also examined participants’ judgements of vocal bursts, and whether emotion identification was linked to emotion lexicon. Recognition of emotions in congruent music increased with age. For incongruent music, age was positively associated with judging the emotion in music by the melody. For incongruent music with happy or sad lyrics, younger participants were more likely to answer with the emotion of the lyrics. For scared incongruent music, older adolescents were more likely to answer with the lyrics than older and younger participants. Age groups did not differ on their emotion lexicons, nor recognition of emotion in vocal bursts. Whether children use lyrics or melody to determine the emotion of popular music may depend on the emotion conveyed. © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group.},
  author_keywords   = {development; Emotion recognition; lyrics; music; vocal bursts},
  comment           = {aging, no feature modeling, discussed},
  year              = {2020},
  doi               = {10.1080/02699931.2019.1700482},
  hasabstract       = {Y},
  journaltitle      = {Cognition and Emotion},
  keywords          = {Adolescent; Adult; Age Factors; Auditory Perception; Child; Cues; Emotions; Female; Humans; Judgment; Male; Music; Recognition, Psychology; Singing; Young Adult; adolescent; article; child; decision making; female; groups by age; human; human experiment; major clinical study; male; music; adult; age; association; emotion; hearing; music; physiology; psychology; singing; young adult},
  modificationdate  = {2024-05-16T11:19:36},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076441712&doi=10.1080%2f02699931.2019.1700482&partnerID=40&md5=932297b7b8bb8abcf0a1c2e75e1e8209},
}

@Article{nguyen2017an,
  author            = {Nguyen, Van Loi and Kim, Donglim and Ho, Van Phi and Lim, Younghwan},
  title             = {A new recognition method for visualizing music emotion},
  note              = {Cited by: 14},
  number            = {3},
  pages             = {1246 – 1254},
  volume            = {7},
  abstract          = {This paper proposes an emotion detection method using a combination of dimensional approach and categorical approach. Thayer's model is divided into discrete emotion sections based on the level of arousal and valence. The main objective of the method is to increase the number of detected emotions which is used for emotion visualization. To evaluate the suggested method, we conducted various experiments with supervised learning and feature selection strategies. We collected 300 music clips with emotions annotated by music experts. Two feature sets are employed to create two training models for arousal and valence dimensions of Thayer's model. Finally, 36 music emotions are detected by proposed method. The results showed that the suggested algorithm achieved the highest accuracy when using RandomForest classifier with 70% and 57.3% for arousal and valence, respectively. These rates are better than previous studies. © 2017 Institute of Advanced Engineering and Science.},
  author_keywords   = {Feature extraction; Music emotion recognition algorithm; Music information retrieval; Music mood detection},
  year              = {2017},
  doi               = {10.11591/ijece.v7i3.pp1246-1254},
  hasabstract       = {Y},
  journaltitle      = {International Journal of Electrical and Computer Engineering},
  priority          = {prio1},
  publication_stage = {Final},
  ranking           = {rank2},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021156960&doi=10.11591%2fijece.v7i3.pp1246-1254&partnerID=40&md5=c3ac2cfa0f3a859b3232ebd8875ba59b},
}

@Article{zhao2016ad,
  author            = {Zhao, Guozhen and Song, Jinjing and Ge, Yan and Liu, Yongjin and Yao, Lin and Wen, Tao},
  title             = {Advances in emotion recognition based on physiological big data},
  note              = {Cited by: 26},
  number            = {1},
  pages             = {80 – 92},
  volume            = {53},
  abstract          = {Affective computing (AC) is a new field of emotion research along with the development of computing technology and human-machine interaction technology. Emotion recognition is a crucial part of the AC research framework. Emotion recognition based on physiological signals provides richer information without deception than other techniques such as facial expression, tone of voice, and gestures. Many studies of emotion recognition have been conducted, but the classification accuracy is diverse due to variability in stimuli, emotion categories, devices, feature extraction and machine learning algorithms. This paper reviews all works that cited DEAP dataset (a public available dataset which uses music video to induce emotion and record EEG and peripheral physiological signals) and introduces detailed methods and algorithms on feature extraction, normalization, dimension reduction, emotion classification, and cross validation. Eventually, this work presents the application of AC on game development, multimedia production, interactive experience, and social network as well as the current limitations and the direction of future investigation. © 2016, Science Press. All right reserved.},
  author_keywords   = {Electroencephalograph (EEG); Emotion recognition; Feature extraction; Machine learning; Peripheral physiological signal},
  year              = {2016},
  doi               = {10.7544/issn1000-1239.2016.20150636},
  hasabstract       = {Y},
  journaltitle      = {Jisuanji Yanjiu yu Fazhan/Computer Research and Development},
  keywords          = {Artificial intelligence; Big data; Classification (of information); Computer music; Electroencephalography; Extraction; Face recognition; Feature extraction; Human computer interaction; Learning algorithms; Learning systems; Multimedia services; Physiology; Speech recognition; Classification accuracy; Computing technology; Electroencephalograph (EEG); Emotion classification; Emotion recognition; Human machine interaction; Multimedia productions; Physiological signals; Biomedical signal processing},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957011355&doi=10.7544%2fissn1000-1239.2016.20150636&partnerID=40&md5=504e9ac209dbaaea95b4655e1ce679dd},
}

@Article{hakanpaeae2019em,
  author            = {Hakanpää, Tua and Waaramaa, T. and Laukkanen, Anne-Maria},
  title             = {Emotion Recognition From Singing Voices Using Contemporary Commercial Music and Classical Styles},
  note              = {Cited by: 11; All Open Access, Green Open Access},
  number            = {4},
  pages             = {501 – 509},
  volume            = {33},
  abstract          = {Objectives: This study examines the recognition of emotion in contemporary commercial music (CCM) and classical styles of singing. This information may be useful in improving the training of interpretation in singing. Study design: This is an experimental comparative study. Methods: Thirteen singers (11 female, 2 male) with a minimum of 3 years' professional-level singing studies (in CCM or classical technique or both) participated. They sang at three pitches (females: a, e1, a1, males: one octave lower) expressing anger, sadness, joy, tenderness, and a neutral state. Twenty-nine listeners listened to 312 short (0.63- to 4.8-second) voice samples, 135 of which were sung using a classical singing technique and 165 of which were sung in a CCM style. The listeners were asked which emotion they heard. Activity and valence were derived from the chosen emotions. Results: The percentage of correct recognitions out of all the answers in the listening test (N = 9048) was 30.2%. The recognition percentage for the CCM-style singing technique was higher (34.5%) than for the classical-style technique (24.5%). Valence and activation were better perceived than the emotions themselves, and activity was better recognized than valence. A higher pitch was more likely to be perceived as joy or anger, and a lower pitch as sorrow. Both valence and activation were better recognized in the female CCM samples than in the other samples. Conclusions: There are statistically significant differences in the recognition of emotions between classical and CCM styles of singing. Furthermore, in the singing voice, pitch affects the perception of emotions, and valence and activity are more easily recognized than emotions. © 2018 The Voice Foundation},
  author_keywords   = {Emotion expression; Perception; Singing style; Song genre; Voice quality},
  year              = {2019},
  doi               = {10.1016/j.jvoice.2018.01.012},
  hasabstract       = {Y},
  journaltitle      = {Journal of Voice},
  keywords          = {Adult; Auditory Perception; Cues; Emotions; Female; Humans; Male; Pitch Perception; Recognition, Psychology; Singing; Voice Quality; Young Adult; adult; anger; article; comparative study; female; human; major clinical study; male; music; perception; pitch; sadness; singing; sorrow; study design; voice; association; emotion; hearing; pitch perception; young adult},
  priority          = {prio1},
  publication_stage = {Final},
  ranking           = {rank2},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042298581&doi=10.1016%2fj.jvoice.2018.01.012&partnerID=40&md5=2d3215e2fbb4734dceec5b1f5cdbdf16},
}

@Article{ntalampiras2017at,
  author            = {Ntalampiras, Stavros},
  title             = {A transfer learning framework for predicting the emotional content of generalized sound events},
  note              = {Cited by: 23; All Open Access, Green Open Access},
  number            = {3},
  pages             = {1694 – 1701},
  volume            = {141},
  abstract          = {Predicting the emotions evoked by generalized sound events is a relatively recent research domain which still needs attention. In this work a framework aiming to reveal potential similarities existing during the perception of emotions evoked by sound events and songs is presented. To this end the following are proposed: (a) the usage of temporal modulation features, (b) a transfer learning module based on an echo state network, and (c) a k-medoids clustering algorithm predicting valence and arousal measurements associated with generalized sound events. The effectiveness of the proposed solution is demonstrated after a thoroughly designed experimental phase employing both sound and music data. The results demonstrate the importance of transfer learning in the specific field and encourage further research on approaches which manage the problem in a synergistic way. © 2017 Acoustical Society of America.},
  comment           = {discussed},
  year              = {2017},
  doi               = {10.1121/1.4977749},
  hasabstract       = {Y},
  journaltitle      = {Journal of the Acoustical Society of America},
  keywords          = {Acoustic Stimulation; Adult; Algorithms; Auditory Perception; Cues; Emotions; Female; Humans; Male; Models, Theoretical; Music; Pattern Recognition, Physiological; Sound; Time Factors; Transfer (Psychology); Young Adult; Behavioral research; Forecasting; Echo state networks; K-medoids clustering; Music data; Recent researches; Sound events; Temporal modulations; Transfer learning; adult; algorithm; association; auditory stimulation; emotion; female; hearing; human; male; music; pattern recognition; sound; theoretical model; time factor; transfer of learning; young adult; Clustering algorithms},
  modificationdate  = {2024-05-16T11:15:16},
  priority          = {prio1},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015211289&doi=10.1121%2f1.4977749&partnerID=40&md5=744823c3bb1062a093db56fb31511f25},
}

@Article{zhang2017fe,
  author            = {Zhang, Jiang Long and Huang, Xiang Lin and Yang, Li Fang and Xu, Ye and Sun, Shu Tao},
  title             = {Feature selection and feature learning in arousal dimension of music emotion by using shrinkage methods},
  note              = {Cited by: 10},
  number            = {2},
  pages             = {251 – 264},
  volume            = {23},
  abstract          = {Music emotion recognition is an important topic in music information retrieval area. A lot of acoustic features are used to train a music classification or regression emotion model. However, these existing features may not be efficient for classification or regression task. Furthermore, most works do not explain why these features do work for classification. In our work, eight features are extracted to represent the arousal dimension of music emotion, and various commonly used statistical learning methods such as Logistic Regression, and tree-based methods are applied to interpret important features. Then the shrinkage methods are applied to feature selection and classification in music emotion recognition for the first time. Our tests show that the proposed approaches are efficient for feature selection just as entropy-based filter methods, and better than wrapper methods. The shrinkage methods can produce more continuous and low variance model than wrapper methods. Then, we discover that the most useful features are low specific loudness sensation coefficients (low-SONE), root mean square and loudness-flux. Moreover, the shrinkage methods apply in logistic regression perform better for classification than most of other methods. We get an average accuracy rate of 83.8 %. © 2015, Springer-Verlag Berlin Heidelberg.},
  author_keywords   = {Features learning; Features selection; Music arousal dimension classification; Shrinkage method; Statistical learning},
  year              = {2017},
  doi               = {10.1007/s00530-015-0489-y},
  hasabstract       = {Y},
  journaltitle      = {Multimedia Systems},
  keywords          = {Feature extraction; Regression analysis; Shrinkage; Speech recognition; Feature selection and classification; Features learning; Features selection; Music classification; Music information retrieval; Shrinkage methods; Statistical learning; Statistical learning methods; Classification (of information)},
  priority          = {prio1},
  publication_stage = {Final},
  ranking           = {rank2},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944937222&doi=10.1007%2fs00530-015-0489-y&partnerID=40&md5=f12764992ee29a4976644aacf6bbb43d},
}

@Article{redondopedregal2021au,
  author            = {Redondo Pedregal, Celia and Heaton, Pamela},
  title             = {Autism, music and Alexithymia: A musical intervention to enhance emotion recognition in adolescents with ASD},
  note              = {Cited by: 4},
  volume            = {116},
  abstract          = {Background: Difficulties identifying and describing emotions in Autism Spectrum Disorder (ASD) have been linked with an increased prevalence of Type 2 Alexithymia. Alexithymia is associated with difficulties in interpreting and verbally labelling physiological arousal. Children and adults with ASD show typical patterns of physiological arousal to music and can attribute verbal labels to musical emotions. Aim: This pilot study aimed to develop a music-based intervention to improve facial and vocal emotion recognition (ER) and Alexithymia in adolescents with ASD. Methods and procedures: Adolescents with ASD completed 5 music sessions and pre and post-tests of Alexithymia, ER and language. Each intervention began with a researcher-led group analysis of the emotions expressed in a series of musical excerpts, followed by a group-led discussion of the participants’ experiences of these emotions and the ways they may be communicated. Finally, the likely causes and outward expression of these emotions were discussed. Outcome and results: Results showed that at pre-test, chronological age (CA) and receptive vocabulary were significantly associated with recognition of facial and verbal emotions and Not hiding emotions. At post-test, older children showed a greater increase in recognition of voices and in emotional bodily awareness. Correlations suggested a trend towards increased ER in voices and faces in children with lower language scores. Conclusions and implications: Music-based interventions may enhance ER in adolescents with ASD and Alexithymia. Limitations and recommendations for future investigations are discussed. © 2021 Elsevier Ltd},
  author_keywords   = {Alexithymia; Autism Spectrum Disorder; Emotion recognition; Language; Music},
  year              = {2021},
  doi               = {10.1016/j.ridd.2021.104040},
  hasabstract       = {Y},
  journaltitle      = {Research in Developmental Disabilities},
  keywords          = {Adolescent; Adult; Affective Symptoms; Autism Spectrum Disorder; Autistic Disorder; Child; Emotions; Humans; Music; Pilot Projects; adolescent; age; alexithymia; Article; autism; awareness; clinical article; emotion; facial recognition; female; human; language; male; music; music therapy; pilot study; self report; voice recognition; adult; autism; child; emotional disorder; music},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111198375&doi=10.1016%2fj.ridd.2021.104040&partnerID=40&md5=a9aa92878c6d380db678e3ff0383d164},
}

@Article{wang2021re,
  author            = {Wang, Daliang and Guo, Xiaowen},
  title             = {Research on Intelligent Recognition and Classification Algorithm of Music Emotion in Complex System of Music Performance},
  note              = {Cited by: 7; All Open Access, Gold Open Access},
  volume            = {2021},
  abstract          = {In the complex system of music performance, there are differences in the expression of music emotions by listeners, so it is of great significance to study the classification of different emotions under different audio signals. In this paper, the research of human emotional intelligence recognition and classification algorithm in the complex system of music performance is proposed. Through the recognition of SVM, KNN, ANN, and ID3 classifiers, the accuracy of a single classifier is compared, and then the four classifiers are combined to compare the classification accuracy of audio signals before and after preprocessing. The results show that the accuracy of SVM and ANN fusion is the highest. Finally, recall and F1 are comprehensively compared in the fusion algorithm, and the fusion classification effect of SVM and ANN is better than that of the algorithm model.  © 2021 Daliang Wang and Xiaowen Guo.},
  year              = {2021},
  doi               = {10.1155/2021/4251827},
  hasabstract       = {Y},
  journaltitle      = {Complexity},
  keywords          = {Audio acoustics; Emotional intelligence; Support vector machines; Algorithm model; Classification accuracy; Classification algorithm; Fusion algorithms; Fusion classification; Intelligent recognition; Music emotions; Music performance; Computer music},
  priority          = {prio1},
  publication_stage = {Final},
  ranking           = {rank2},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109435136&doi=10.1155%2f2021%2f4251827&partnerID=40&md5=b1866e5ba2f0b3b1ca5047cf2e22c97c},
}

@Article{bai2017mu,
  author            = {Bai, Junjie and Luo, Kan and Peng, Jun and Shi, Jinliang and Wu, Ying and Feng, Lixiao and Li, Jianqing and Wang, Yingxu},
  title             = {Music emotions recognition by machine learning with cognitive classification methodologies},
  note              = {Cited by: 6},
  number            = {4},
  pages             = {80 – 92},
  volume            = {11},
  abstract          = {Music emotions recognition (MER) is a challenging field of studies addressed in multiple disciplines such as musicology, cognitive science, physiology, psychology, arts and affective computing. In this article, music emotions are classified into four types known as those of pleasing, angry, sad and relaxing. MER is formulated as a classification problem in cognitive computing where 548 dimensions of music features are extracted and modeled. A set of classifications and machine learning algorithms are explored and comparatively studied for MER, which includes Support Vector Machine (SVM), k-Nearest Neighbors (KNN), Neuro-Fuzzy Networks Classification (NFNC), Fuzzy KNN (FKNN), Bayes classifier and Linear Discriminant Analysis (LDA). Experimental results show that the SVM, FKNN and LDA algorithms are the most effective methodologies that obtain more than 80% accuracy for MER. © 2017 IGI Global.},
  author_keywords   = {Emotion Classification; Feature Extraction; Machine Learning; Music Emotion Recognition; Pattern Recognition},
  year              = {2017},
  doi               = {10.4018/IJCINI.2017100105},
  hasabstract       = {Y},
  journaltitle      = {International Journal of Cognitive Informatics and Natural Intelligence},
  keywords          = {Artificial intelligence; Arts computing; Barium compounds; Discriminant analysis; Feature extraction; Fuzzy neural networks; Image retrieval; Learning systems; Nearest neighbor search; Pattern recognition; Psychophysiology; Support vector machines; Affective Computing; Classification methodologies; Emotion classification; K nearest neighbor (KNN); Linear discriminant analysis; Multiple disciplines; Music emotions; Neuro-fuzzy network; Learning algorithms},
  priority          = {prio1},
  publication_stage = {Final},
  ranking           = {rank2},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038434665&doi=10.4018%2fIJCINI.2017100105&partnerID=40&md5=a2e1f1d69ab6ceb6f0cee7f2eb26985a},
}

@Article{brown2017th,
  author            = {Brown, Laura S.},
  title             = {The influence of music on facial emotion recognition in children with autism spectrum disorder and neurotypical children},
  note              = {Cited by: 18},
  number            = {1},
  pages             = {55 – 79},
  volume            = {54},
  abstract          = {Background: Children with autism spectrum disorder (ASD) often struggle with social skills, including the ability to perceive emotions based on facial expressions. Research evidence suggests that many individuals with ASD can perceive emotion in music. Examining whether music can be used to enhance recognition of facial emotion by children with ASD would inform development of music therapy interventions. Objective: The purpose of this study was to investigate the influence of music with a strong emotional valance (happy; sad) on children with ASD's ability to label emotions depicted in facial photographs, and their response time. Methods: Thirty neurotypical children and 20 children with high-functioning ASD rated expressions of happy, neutral, and sad in 30 photographs under two music listening conditions (sad music; happy music). During each music listening condition, participants rated the 30 images using a 7-point scale that ranged from very sad to very happy. Response time data were also collected across both conditions. Results: A significant two-way interaction revealed that participants' ratings of happy and neutral faces were unaffected by music conditions, but sad faces were perceived to be sadder with sad music than with happy music. Across both conditions, neurotypical children rated the happy faces as happier and the sad faces as sadder than did participants with ASD. Response times of the neurotypical children were consistently shorter than response times of the children with ASD; both groups took longer to rate sad faces than happy faces. Response times of neurotypical children were generally unaffected by the valence of the music condition; however, children with ASD took longer to respond when listening to sad music. Conclusions: Music appears to affect perceptions of emotion in children with ASD, and perceptions of sad facial expressions seem to be more affected by emotionally congruent background music than are perceptions of happy or neutral faces. © 2016 The American Music Therapy Association. All rights reserved.},
  year              = {2017},
  doi               = {10.1093/jmt/thw017},
  hasabstract       = {Y},
  journaltitle      = {Journal of Music Therapy},
  keywords          = {Acoustic Stimulation; Adolescent; Autism Spectrum Disorder; Child; Face; Facial Expression; Female; Happiness; Humans; Male; Music; Music Therapy; Pattern Recognition, Visual; Photic Stimulation; Photography; Reaction Time; adolescent; anatomy and histology; auditory stimulation; autism; child; face; facial expression; female; happiness; human; male; music; music therapy; pattern recognition; photography; photostimulation; psychology; reaction time},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019942610&doi=10.1093%2fjmt%2fthw017&partnerID=40&md5=ec57292815967398a1f718798cba0333},
}

@Article{arnaugonzalez2017fu,
  author            = {Arnau-González, Pablo and Arevalillo-Herráez, Miguel and Ramzan, Naeem},
  title             = {Fusing highly dimensional energy and connectivity features to identify affective states from EEG signals},
  note              = {Cited by: 71; All Open Access, Green Open Access},
  pages             = {81 – 89},
  volume            = {244},
  abstract          = {In this paper, a novel method for affect detection is presented. The method combines both connectivity-based and channel-based features with a selection method that considerably reduces the dimensionality of the data and allows for an efficient classification. In particular, the Relative Energy (RE) and its logarithm in the spacial domain, and the Spectral Power (SP) in the frequency domain are computed for the four typical frequency bands (α, β, γ and θ), and complemented with the Mutual Information measured over all channel pairs. The resulting features are then reduced by using a hybrid method that combines supervised and unsupervised feature selection. First, Welch's t-test is used to select the features that best separate the classes, and discard the ones that are less useful for classification. To this end, all features where the t-test yields a p-value above a threshold are eliminated. The remaining ones are further reduced by using Principal Component Analysis. Detection results are compared to state-of-the-art methods on DEAP, a database for emotion analysis composed of labeled recordings from 32 subjects while watching 40 music videos. The effect of using different classifiers is also evaluated, and a significant improvement is observed in all cases. © 2017 Elsevier B.V.},
  author_keywords   = {Connectivity features; EEG; Emotion recognition; Energy features; Feature extraction; Feature reduction},
  year              = {2017},
  doi               = {10.1016/j.neucom.2017.03.027},
  hasabstract       = {Y},
  journaltitle      = {Neurocomputing},
  keywords          = {Electroencephalography; Frequency domain analysis; Connectivity features; Emotion recognition; Energy feature; Feature reduction; Frequency domains; Mutual informations; State-of-the-art methods; Unsupervised feature selection; adult; affect; Article; Bayes theorem; classification algorithm; connectome; controlled study; electroencephalogram; entropy; female; frequency modulation; human; human experiment; logarithmic relative energy algorithm; male; measurement accuracy; normal human; principal component analysis; radial based function; relative energy dimensionality algorithm; signal processing; support vector machine; systematic error; validation process; visual stimulation; Feature extraction},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016559824&doi=10.1016%2fj.neucom.2017.03.027&partnerID=40&md5=1756593f46eb5d2bc2ff7c692969cdfb},
}

@Article{vignolo2016fe,
  author            = {Vignolo, Leandro D. and Prasanna, S.R. Mahadeva and Dandapat, Samarendra and Rufiner, H. Leonardo and Milone, Diego H.},
  title             = {Feature optimisation for stress recognition in speech},
  note              = {Cited by: 10},
  pages             = {1 – 7},
  volume            = {84},
  abstract          = {Mel-frequency cepstral coefficients introduced biologically-inspired features into speech technology, becoming the most commonly used representation for speech, speaker and emotion recognition, and even for applications in music. While this representation is quite popular, it is ambitious to assume that it would provide the best results for every application, as it is not designed for each specific objective. This work proposes a methodology to learn a speech representation from data by optimising a filter bank, in order to improve results in the classification of stressed speech. Since population-based metaheuristics have proved successful in related applications, an evolutionary algorithm is designed to search for a filter bank that maximises the classification accuracy. For the codification, spline functions are used to shape the filter banks, which allows reducing the number of parameters to optimise. The filter banks obtained with the proposed methodology improve the results in stressed and emotional speech classification. © 2016 Elsevier B.V.},
  author_keywords   = {Cepstral coefficients; Emotional speech; Evolutionary algorithms; Speech processing; Stressed speech},
  comment           = {no music},
  year              = {2016},
  doi               = {10.1016/j.patrec.2016.07.017},
  hasabstract       = {Y},
  journaltitle      = {Pattern Recognition Letters},
  keywords          = {Evolutionary algorithms; Filter banks; Optimization; Speech; Speech analysis; Speech processing; Biologically inspired; Cepstral coefficients; Classification accuracy; Emotion recognition; Emotional speech; Mel frequency cepstral co-efficient; Stress recognition; Stressed speech; Speech recognition},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982684046&doi=10.1016%2fj.patrec.2016.07.017&partnerID=40&md5=6cd92f5324609ba595410d55ee59eb3a},
}

@Article{bravo2017se,
  author            = {Bravo, Fernando and Cross, Ian and Stamatakis, Emmanuel Andreas and Rohrmeier, Martin},
  title             = {Sensory cortical response to uncertainty and low salience during recognition of affective cues in musical intervals},
  note              = {Cited by: 2; All Open Access, Gold Open Access, Green Open Access},
  number            = {4},
  volume            = {12},
  abstract          = {Previous neuroimaging studies have shown an increased sensory cortical response (i.e., heightened weight on sensory evidence) under higher levels of predictive uncertainty. The signal enhancement theory proposes that attention improves the quality of the stimulus representation, and therefore reduces uncertainty by increasing the gain of the sensory signal. The present study employed functional magnetic resonance imaging (fMRI) to investigate the neural correlates for ambiguous valence inferences signaled by auditory information within an emotion recognition paradigm. Participants categorized sound stimuli of three distinct levels of consonance/dissonance controlled by interval content. Separate behavioural and neuroscientific experiments were conducted. Behavioural results revealed that, compared with the consonance condition (perfect fourths, fifths and octaves) and the strong dissonance condition (minor/major seconds and tritones), the intermediate dissonance condition (minor thirds) was the most ambiguous, least salient and more cognitively demanding category (slowest reaction times). The neuroscientific findings were consistent with a heightened weight on sensory evidence whilst participants were evaluating intermediate dissonances, which was reflected in an increased neural response of the right Heschl's gyrus. The results support previous studies that have observed enhanced precision of sensory evidence whilst participants attempted to represent and respond to higher degrees of uncertainty, and converge with evidence showing preferential processing of complex spectral information in the right primary auditory cortex. These findings are discussed with respect to music-theoretical concepts and recent Bayesian models of perception, which have proposed that attention may heighten the weight of information coming from sensory channels to stimulate learning about unknown predictive relationships. © 2017 Bravo et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
  year              = {2017},
  doi               = {10.1371/journal.pone.0175991},
  hasabstract       = {Y},
  journaltitle      = {PLoS ONE},
  keywords          = {Acoustic Stimulation; Adult; Attention; Auditory Cortex; Auditory Perception; Bayes Theorem; Brain Mapping; Cues; Female; Frustration; Humans; Learning; Magnetic Resonance Imaging; Male; Music; Pleasure; Reaction Time; Uncertainty; attention; controlled study; functional magnetic resonance imaging; human; human experiment; learning; music; nerve potential; participant observation; perception; primary auditory cortex; reaction time; sound; stimulus; theoretical model; uncertainty; adult; anatomy and histology; association; auditory cortex; auditory stimulation; Bayes theorem; brain mapping; female; frustration; hearing; male; nuclear magnetic resonance imaging; physiology; pleasure; psychology; uncertainty},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018517487&doi=10.1371%2fjournal.pone.0175991&partnerID=40&md5=bc4de1649aa92d585f991d81da38a629},
}

@Article{hong2017an,
  author            = {Hong, Yu and Chau, Chuck-Jee and Horner, Andrew},
  title             = {An analysis of low-arousal piano music ratings to uncover what makes calm and sad music so difficult to distinguish in music emotion recognition},
  note              = {Cited by: 14; All Open Access, Bronze Open Access},
  number            = {4},
  pages             = {304 – 320},
  volume            = {65},
  abstract          = {Music emotion recognition and recommendation systems often use a simplified 4-quadrant model with categories such as Happy, Sad, Angry, and Calm. Previous research has shown that both listeners and automated systems often have difficulty distinguishing low-arousal categories such as Calm and Sad. This paper seeks to explore what makes the categories Calm and Sad so difficult to distinguish. We used 300 low-arousal excerpts from the classical piano repertoire to determine the coverage of the categories Calm and Sad in the low-arousal space, their overlap, and their balance to one another. Our results show that Calm was 40% bigger in terms of coverage than Sad, but that on average Sad excerpts were significantly more negative in mood than Calm excerpts were positive. Calm and Sad overlapped in nearly 20% of the excerpts, meaning 20% of the excerpts were about equally Calm and Sad. Calm and Sad covered about 92% of the low-arousal space, where 8% of the space were holes that were not-at-all Calm or Sad. The largest holes were for excerpts considered Mysterious and Doubtful, but there were smaller holes among positive excerpts as well. Due to the holes in the coverage, the overlaps, and imbalances the Calm-Sad model adds about 6% more errors when compared to asking users directly whether the mood of the music is positive or negative. Nevertheless, the Calm-Sad model is still useful and appropriate for applications in music emotion recognition and recommendation such as when a simple and intuitive interface is preferred or when categorization is more important than precise differentiation.},
  year              = {2017},
  doi               = {10.17743/jaes.2017.0001},
  hasabstract       = {Y},
  journaltitle      = {AES: Journal of the Audio Engineering Society},
  keywords          = {Automation; Automated systems; Intuitive interfaces; Music emotions; Piano music; Speech recognition},
  priority          = {prio1},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019993531&doi=10.17743%2fjaes.2017.0001&partnerID=40&md5=e66e2e892e26fd861e70e06d45bf06b9},
}

@Article{huang2021re,
  author            = {Huang, Chun and Shen, Diao},
  title             = {Research on Music Emotion Intelligent Recognition and Classification Algorithm in Music Performance System},
  note              = {Cited by: 3; All Open Access, Gold Open Access},
  volume            = {2021},
  abstract          = {The music performance system works by identifying the emotional elements of music to control the lighting changes. However, if there is a recognition error, a good stage effect will not be able to create. Therefore, this paper proposes an intelligent music emotion recognition and classification algorithm in the music performance system. The first part of the algorithm is to analyze the emotional features of music, including acoustic features, melody features, and audio features. Then, the three kinds of features are combined together to form a feature vector set. In the latter part of the algorithm, it divides the feature vector set into training samples and test samples. The training samples are trained by using recognition and classification model based on the neural network. And then, the testing samples are input into the trained model, which is aiming to realize the intelligent recognition and classification of music emotion. The result shows that the kappa coefficient k values calculated by the proposed algorithm are greater than 0.75, which indicates that the recognition and classification results are consistent with the actual results, and the accuracy of recognition and classification is high. So, the research purpose is achieved.  © 2021 Chun Huang and Diao Shen.},
  year              = {2021},
  doi               = {10.1155/2021/7886570},
  hasabstract       = {Y},
  journaltitle      = {Scientific Programming},
  keywords          = {Audio acoustics; Sampling; Speech recognition; Classification algorithm; Features vector; Intelligent classification; Intelligent recognition; Music emotions; Music performance; Performance system; Recognition algorithm; Recognition error; Training sample; Music},
  priority          = {prio1},
  publication_stage = {Final},
  ranking           = {rank2},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119913107&doi=10.1155%2f2021%2f7886570&partnerID=40&md5=04d9fb372c6e843a9daf134c1e0885f8},
}

@Article{panda2020no,
  author            = {Panda, Renato and Malheiro, Ricardo and Paiva, Rui Pedro},
  title             = {Novel Audio Features for Music Emotion Recognition},
  note              = {Cited by: 83; All Open Access, Green Open Access},
  number            = {4},
  pages             = {614 – 626},
  volume            = {11},
  abstract          = {This work advances the music emotion recognition state-of-the-art by proposing novel emotionally-relevant audio features. We reviewed the existing audio features implemented in well-known frameworks and their relationships with the eight commonly defined musical concepts. This knowledge helped uncover musical concepts lacking computational extractors, to which we propose algorithms - namely related with musical texture and expressive techniques. To evaluate our work, we created a public dataset of 900 audio clips, with subjective annotations following Russell's emotion quadrants. The existent audio features (baseline) and the proposed features (novel) were tested using 20 repetitions of 10-fold cross-validation. Adding the proposed features improved the F1-score to 76.4 percent (by 9 percent), when compared to a similar number of baseline-only features. Moreover, analysing the features relevance and results uncovered interesting relations, namely the weight of specific features and musical concepts to each emotion quadrant, and warrant promising new directions for future research in the field of music emotion recognition, interactive media, and novel music interfaces. © 2010-2012 IEEE.},
  author_keywords   = {Affective computing; audio databases; emotion recognition; feature extraction; music information retrieval},
  year              = {2020},
  doi               = {10.1109/TAFFC.2018.2820691},
  hasabstract       = {Y},
  journaltitle      = {IEEE Transactions on Affective Computing},
  keywords          = {Feature extraction; Speech recognition; Textures; 10-fold cross-validation; Affective Computing; Audio database; Emotion recognition; Interactive media; Music information retrieval; Music interfaces; Musical concepts; Audio acoustics},
  priority          = {prio1},
  publication_stage = {Final},
  ranking           = {rank2},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044777756&doi=10.1109%2fTAFFC.2018.2820691&partnerID=40&md5=6ff59839d539ee19db6d2a75a843d7d6},
}

@Article{xiao2018ol,
  author            = {Xiao, Naiqi G. and Quinn, Paul C. and Liu, Shaoying and Ge, Liezhong and Pascalis, Olivier and Lee, Kang},
  title             = {Older but not younger infants associate own-race faces with happy music and other-race faces with sad music},
  note              = {Cited by: 65},
  number            = {2},
  volume            = {21},
  abstract          = {We used a novel intermodal association task to examine whether infants associate own- and other-race faces with music of different emotional valences. Three- to 9-month-olds saw a series of neutral own- or other-race faces paired with happy or sad musical excerpts. Three- to 6-month-olds did not show any specific association between face race and music. At 9 months, however, infants looked longer at own-race faces paired with happy music than at own-race faces paired with sad music. Nine-month-olds also looked longer at other-race faces paired with sad music than at other-race faces paired with happy music. These results indicate that infants with nearly exclusive own-race face experience develop associations between face race and music emotional valence in the first year of life. The potential implications of such associations for developing racial biases in early childhood are discussed. © 2017 John Wiley & Sons Ltd},
  year              = {2018},
  doi               = {10.1111/desc.12537},
  hasabstract       = {Y},
  journaltitle      = {Developmental Science},
  keywords          = {Age Factors; Child; Emotions; Facial Recognition; Female; Happiness; Humans; Infant; Male; Music; Racism; age; child; emotion; facial recognition; female; happiness; human; infant; male; music; psychology; racism},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020860676&doi=10.1111%2fdesc.12537&partnerID=40&md5=4ff39f8409f3a6f90d1ccd9525362bb3},
}

@Article{han2021he,
  author            = {Han, Yongming and Zhang, Mingxing and Geng, Zhiqiang},
  title             = {Heart rate variability features for emotion dimensional prediction by using a principal component analysis-support vector regression (PCA-SVR) model; [基于心率变异性特征和PCA-SVR的PAD维度情感预测分析]},
  note              = {Cited by: 0},
  number            = {5},
  pages             = {102 – 110},
  volume            = {48},
  abstract          = {In order to solve the numerical prediction problem in PAD (pleasure, arousal and dominance) dimensional emotion prediction, a PAD dimensional emotion prediction model integrating heart rate variability (HRV) based on principal component analysis (PCA) and support vector regression (SVR) is proposed in this paper. The heart rate and heart interval data of 12 volunteers in two emotion states with relaxation and anxiety induced by music and video were collected by flexible iontronic sensing, and labeled on a PAD emotion scale. The time-domain, frequency-domain and nonlinear features of HRV were then extracted by different statistical methods, namely mean and variance, Welch power spectrum and Poincaré scatter diagram, respectively. Moreover, the PCA model was used to reduce the dimension of HRV features. The HRV features after dimensionality reduction were used as the input features of the SVR model for training and prediction. The experimental results show that the PCA-SVR model combined with HRV features had good prediction effects for the three dimensions of PAD, and its average consistency correlation coefficient (CCC) reached 0.51. The three prediction methods of the SVR, extreme learning machine (ELM) and the ELM based the PCA were compared, and the results showed that the proposed method resulted in improvements in CCC of 0.14, 0.10, and 0.04, respectively. Furthermore, the proposed method can divide emotions in detail, and has a certain complementary role in emotion recognition and analysis. Thus using the method in combination with wearable devices, it is possible to identify and predict emotions in daily life. © 2021, Editorial Board of Journal of Beijing University of Chemical Technology (Natural Science Edition). All right reserved.},
  author_keywords   = {Heart rate variability; PAD dimensional emotion; Principal component analysis; Support vector regression},
  comment           = {physiological},
  year              = {2021},
  doi               = {10.13543/j.bhxbzr.2021.05.013},
  hasabstract       = {Y},
  journaltitle      = {Beijing Huagong Daxue Xuebao (Ziran Kexueban)/Journal of Beijing University of Chemical Technology (Natural Science Edition)},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117879673&doi=10.13543%2fj.bhxbzr.2021.05.013&partnerID=40&md5=dc0712cdfb4b191fcf90be3ac2f857b2},
}

@Article{vrysis20201d,
  author            = {Vrysis, Lazaros and Tsipas, Nikolaos and Thoidis, Iordanis and Dimoulas, Charalampos},
  title             = {1D/2D Deep CNNs vs. Temporal Feature Integration for General Audio Classification},
  note              = {Cited by: 34},
  number            = {1-2},
  pages             = {66 – 77},
  volume            = {68},
  abstract          = {Semantic audio analysis has become a fundamental task in modern audio applications, making the improvement and optimization of classification algorithms a necessity. Standard frame-based audio classification methods have been optimized and modern approaches introduce engineering methodologies that capture the temporal dependency between successive feature observations, following the process of temporal feature integration. Moreover, the deployment of the convolutional neural networks defined a new era on semantic audio analysis. The current paper attempts a thorough comparison between standard feature-based classification strategies, state-of-the-art temporal feature integration tactics and 1D/2D deep convolutional neural network setups, on typical audio classification tasks. Experiments focus on optimizing a lightweight configuration for convolutional network topologies on a Speech/Music/Other classification scheme that can be deployed on various audio information retrieval tasks, such as voice activity detection, speaker diarization, or speech emotion recognition. The outmost target of this work is to establish an optimized protocol for constructing deep convolutional topologies on general audio detection classification schemes, minimizing complexity and computational needs. © 2020 Audio Engineering Society. All rights reserved.},
  comment           = {no emotion - discussed},
  year              = {2020},
  doi               = {10.17743/JAES.2019.0058},
  hasabstract       = {Y},
  journaltitle      = {AES: Journal of the Audio Engineering Society},
  keywords          = {Audio acoustics; Audio systems; Convolution; Convolutional neural networks; Deep neural networks; Integration; Semantics; Speech recognition; Topology; Audio information retrievals; Classification algorithm; Convolutional networks; Engineering methodology; Feature-based classification; Speech emotion recognition; Temporal feature integrations; Voice activity detection; Classification (of information)},
  modificationdate  = {2024-05-16T10:58:07},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084651899&doi=10.17743%2fJAES.2019.0058&partnerID=40&md5=b861aa6d7390598e18fbf677e859398b},
}

@Article{agarwal2021an,
  author            = {Agarwal, Gaurav and Om, Hari},
  title             = {An efficient supervised framework for music mood recognition using autoencoder-based optimised support vector regression model},
  note              = {Cited by: 23},
  number            = {2},
  pages             = {98 – 121},
  volume            = {15},
  abstract          = {Music is the art of ‘language of emotions’. Recently, music mood recognition is an emerging task. An efficient supervised framework for music mood recognition using autoencoder-based optimised support vector regression (SVR) model is developed for the music emotion recognition. Our main intention is to increase the accuracy of emotion classification of music by considering text-dependent and non-text-dependent features. For the high level feature representation, stacked autoencoder is used with two hidden layers. Modified K-Medoid-based brain storm optimisation-based support vector regression (SVR_KMBSO) model is utilised for the emotion classification. Using the K-Medoid-based brain storm algorithm, the optimal parameters of the SVR are selected. The proposed framework utilises ISMIR2012 dataset and NJU_V1 dataset for English and for Hindi; online songs are also gathered and used for the music mood recognition. All the three datasets include songs based on four emotions like happy, angry, relax and sad. The experimental results are evaluated and compared with the existing classifiers including SVR, deep belief network (DBN) and Recurrent neural network (RNN). The proposed method SVR_KMBSO achieved high accuracy using three different datasets. © 2021 The Authors. IET Signal Processing published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.},
  year              = {2021},
  doi               = {10.1049/sil2.12015},
  hasabstract       = {Y},
  journaltitle      = {IET Signal Processing},
  keywords          = {Character recognition; Emotion Recognition; Learning systems; Recurrent neural networks; Regression analysis; Storms; Auto encoders; Emotion classification; Emotion recognition; Feature representation; Hidden layers; High-level features; K-medoid; Music emotions; Support vector regression models; Support vector regressions; Music},
  priority          = {prio1},
  publication_stage = {Final},
  ranking           = {rank2},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116929889&doi=10.1049%2fsil2.12015&partnerID=40&md5=4e078ee8b1c740d21d4b7d8dbf7f6cd8},
}

@Article{ferreri2017mu,
  author            = {Ferreri, Laura and Rodriguez-Fornells, Antoni},
  title             = {Music-related reward responses predict episodic memory performance},
  note              = {Cited by: 28},
  number            = {12},
  pages             = {3721 – 3731},
  volume            = {235},
  abstract          = {Music represents a special type of reward involving the recruitment of the mesolimbic dopaminergic system. According to recent theories on episodic memory formation, as dopamine strengthens the synaptic potentiation produced by learning, stimuli triggering dopamine release could result in long-term memory improvements. Here, we behaviourally test whether music-related reward responses could modulate episodic memory performance. Thirty participants rated (in terms of arousal, familiarity, emotional valence, and reward) and encoded unfamiliar classical music excerpts. Twenty-four hours later, their episodic memory was tested (old/new recognition and remember/know paradigm). Results revealed an influence of music-related reward responses on memory: excerpts rated as more rewarding were significantly better recognized and remembered. Furthermore, inter-individual differences in the ability to experience musical reward, measured through the Barcelona Music Reward Questionnaire, positively predicted memory performance. Taken together, these findings shed new light on the relationship between music, reward and memory, showing for the first time that music-driven reward responses are directly implicated in higher cognitive functions and can account for individual differences in memory performance. © 2017, Springer-Verlag GmbH Germany.},
  author_keywords   = {Episodic memory; Music; Musical hedonia; Reward},
  comment           = {applied study, no emotion prediction},
  year              = {2017},
  doi               = {10.1007/s00221-017-5095-0},
  hasabstract       = {Y},
  journaltitle      = {Experimental Brain Research},
  keywords          = {Acoustic Stimulation; Adolescent; Adult; Analysis of Variance; Arousal; Emotions; Female; Humans; Male; Memory, Episodic; Mental Recall; Music; Recognition (Psychology); Reward; Young Adult; adult; arousal; Article; barcelona music reward questionnaire; cognition; emotion; episodic memory; experience; female; human; human experiment; male; mental task; music; neuromodulation; normal human; priority journal; questionnaire; recognition; reward; university student; adolescent; analysis of variance; auditory stimulation; music; physiology; psychology; recall; young adult},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029758928&doi=10.1007%2fs00221-017-5095-0&partnerID=40&md5=1792cc6f0a0d64add5f64e4718cb99ab},
}

@Article{fernandez2021in,
  author            = {Fernandez, Natalia B. and Vuilleumier, Patrik and Gosselin, Nathalie and Peretz, Isabelle},
  title             = {Influence of Background Musical Emotions on Attention in Congenital Amusia},
  note              = {Cited by: 3; All Open Access, Gold Open Access, Green Open Access},
  volume            = {14},
  abstract          = {Congenital amusia in its most common form is a disorder characterized by a musical pitch processing deficit. Although pitch is involved in conveying emotion in music, the implications for pitch deficits on musical emotion judgements is still under debate. Relatedly, both limited and spared musical emotion recognition was reported in amusia in conditions where emotion cues were not determined by musical mode or dissonance. Additionally, assumed links between musical abilities and visuo-spatial attention processes need further investigation in congenital amusics. Hence, we here test to what extent musical emotions can influence attentional performance. Fifteen congenital amusic adults and fifteen healthy controls matched for age and education were assessed in three attentional conditions: executive control (distractor inhibition), alerting, and orienting (spatial shift) while music expressing either joy, tenderness, sadness, or tension was presented. Visual target detection was in the normal range for both accuracy and response times in the amusic relative to the control participants. Moreover, in both groups, music exposure produced facilitating effects on selective attention that appeared to be driven by the arousal dimension of musical emotional content, with faster correct target detection during joyful compared to sad music. These findings corroborate the idea that pitch processing deficits related to congenital amusia do not impede other cognitive domains, particularly visual attention. Furthermore, our study uncovers an intact influence of music and its emotional content on the attentional abilities of amusic individuals. The results highlight the domain-selectivity of the pitch disorder in congenital amusia, which largely spares the development of visual attention and affective systems. © Copyright © 2021 Fernandez, Vuilleumier, Gosselin and Peretz.},
  author_keywords   = {congenital amusia; emotion; executive control; music exposure; selective attention},
  year              = {2021},
  doi               = {10.3389/fnhum.2020.566841},
  hasabstract       = {Y},
  journaltitle      = {Frontiers in Human Neuroscience},
  keywords          = {adult; arousal; article; controlled study; education; emotion; executive function; female; human; human experiment; male; music; pitch; reaction time; sadness; selective attention; tension; visual attention},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100679612&doi=10.3389%2ffnhum.2020.566841&partnerID=40&md5=08cef00f3012a04e714cdc4da213fe16},
}

@Article{belfi2021th,
  author            = {Belfi, Amy M. and Kacirek, Kaelyn},
  title             = {The famous melodies stimulus set},
  note              = {Cited by: 8; All Open Access, Bronze Open Access},
  number            = {1},
  pages             = {34 – 48},
  volume            = {53},
  abstract          = {Famous musical melodies, such as “Row, Row, Row Your Boat” and “Hot Cross Buns, ” are frequently used in psychological research. Such melodies have been used to assess the degree of cognitive impairments in various neurological disorders, and to investigate differences between “naming” vs. “knowing.” Despite their utility as an experimental stimulus, there is currently no standardized, openly available set of famous musical melodies based on a United States population, as prior work on the topic has primarily relied on creating stimuli in an ad hoc manner. Therefore, the goal of the present work was to create a set of famous musical melodies. Here, we describe the development of the Famous Melodies Stimulus Set, a set of 107 melodies. We provide normative data for the melodies on five dimensions: familiarity, age of acquisition, emotional valence, emotional arousal, and naming ability. Participants (N = 397) rated the melodies on these five variables, validating that most melodies were highly familiar and reliably named. While familiarity ratings were skewed, all other rating scales covered a relatively broad range, allowing for researchers to select melodies for future work based on particular attributes. © 2020, The Psychonomic Society, Inc.},
  author_keywords   = {Arousal; Familiarity; Music; Naming; Valence},
  year              = {2021},
  doi               = {10.3758/s13428-020-01411-6},
  hasabstract       = {Y},
  journaltitle      = {Behavior Research Methods},
  keywords          = {Arousal; Emotions; Humans; Music; Recognition, Psychology; adult; arousal; article; human; human experiment; major clinical study; music; rating scale; arousal; emotion},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086733915&doi=10.3758%2fs13428-020-01411-6&partnerID=40&md5=1f82768fc612de27263c38beb00876dc},
}

@Article{sorussa2020em,
  author            = {Sorussa, Kanawat and Choksuriwong, Anant and Karnjanadecha, Montri},
  title             = {Emotion classi cation system for digital music with a cascaded technique},
  note              = {Cited by: 4; All Open Access, Gold Open Access},
  number            = {1},
  pages             = {53 – 66},
  volume            = {14},
  abstract          = {Music selection is diffcult without effcient organization based on metadata or tags, and one effective tag scheme is based on the emotion expressed by the music. However, manual annotation is labor intensive and unstable because the perception of music emotion varies from person to person. This paper presents an emotion classi cation system for digital music with a resolution of eight emotional classes. Russell’s emotion model was adopted as common ground for emotional annotation. The music information retrieval (MIR) toolbox was employed to extract acoustic features from audio les. The classi cation system utilized a supervised machine learning technique to recognize acoustic features and create predictive models. Four predictive models were proposed and compared. The models were composed by crossmatching two types of neural networks, the Levenberg-Marquardt (LM) and resilient backpropagation (Rprop), with two types of structures: a traditional multiclass model and the cascaded structure of a binary-class model. The performance of each model was evaluated via the MediaEval Database for Emotional Analysis (DEAM) benchmark. The best result was achieved by the model trained with the cascaded Rprop neural network (accuracy of 89.5%). In addition, correlation coeffcient analysis showed that timbre features were the most impactful for prediction. Our work offers an opportunity for a competitive advantage in music classi cation because only a few music providers currently tag music with emotional terms. © 2020, ECTI Association Sirindhon International Institute of Technology. All rights reserved.},
  author_keywords   = {Articial neural networks; Classi cation algorithms; Emotion recognition; Music information retrieval},
  year              = {2020},
  doi               = {10.37936/ecti-cit.2020141.205317},
  hasabstract       = {Y},
  journaltitle      = {ECTI Transactions on Computer and Information Technology},
  priority          = {prio1},
  publication_stage = {Final},
  ranking           = {rank2},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084006024&doi=10.37936%2fecti-cit.2020141.205317&partnerID=40&md5=508dd81e9a9120b0c6e87d44e90a2d5a},
}

@Article{clerico2018el,
  author            = {Clerico, Andrea and Tiwari, Abhishek and Gupta, Rishabh and Jayaraman, Srinivasan and Falk, Tiago H.},
  title             = {Electroencephalography amplitude modulation analysis for automated affective tagging of music video clips},
  note              = {Cited by: 20; All Open Access, Gold Open Access, Green Open Access},
  volume            = {11},
  abstract          = {The quantity of music content is rapidly increasing and automated affective tagging of music video clips can enable the development of intelligent retrieval, music recommendation, automatic playlist generators, and music browsing interfaces tuned to the users’ current desires, preferences, or affective states. To achieve this goal, the field of affective computing has emerged, in particular the development of so-called affective brain-computer interfaces, which measure the user’s affective state directly from measured brain waves using non-invasive tools, such as electroencephalography (EEG). Typically, conventional features extracted from the EEG signal have been used, such as frequency subband powers and/or inter-hemispheric power asymmetry indices. More recently, the coupling between EEG and peripheral physiological signals, such as the galvanic skin response (GSR), have also been proposed. Here, we show the importance of EEG amplitude modulations and propose several new features that measure the amplitude-amplitude cross-frequency coupling per EEG electrode, as well as linear and non-linear connections between multiple electrode pairs. When tested on a publicly available dataset of music video clips tagged with subjective affective ratings, support vector classifiers trained on the proposed features were shown to outperform those trained on conventional benchmark EEG features by as much as 6, 20, 8, and 7% for arousal, valence, dominance and liking, respectively. Moreover, fusion of the proposed features with EEG-GSR coupling features showed to be particularly useful for arousal (feature-level fusion) and liking (decision-level fusion) prediction. Together, these findings show the importance of the proposed features to characterize human affective states during music clip watching. © 2018 Clerico, Tiwari, Gupta, Jayaraman and Falk.},
  author_keywords   = {Affective computing; Electroencephalography; Emotion classification; Multimedia content; Pattern classification; Physiological signals; Signal processing},
  year              = {2018},
  doi               = {10.3389/fncom.2017.00115},
  hasabstract       = {Y},
  journaltitle      = {Frontiers in Computational Neuroscience},
  keywords          = {Amplitude modulation; Brain computer interface; Classification (of information); Electrodes; Electrophysiology; Human computer interaction; Interface states; Interfaces (computer); Modulation; Pattern recognition; Physiological models; Physiology; Signal processing; User interfaces; Video cameras; Affective Computing; Crossfrequency couplings (CFC); Emotion classification; Galvanic skin response; Intelligent retrieval; Multimedia contents; Physiological signals; Support vector classifiers; adult; affect; amplitude modulation; amplitude modulation coherence; amplitude modulation energy; amplitude modulation interaction; arousal; Article; brain computer interface; brain region; breathing; computer prediction; electrodermal response; electroencephalogram; electroencephalography; female; frontal cortex; human; information processing; male; music; neurophysiological monitoring; non invasive measurement; pulse rate; skin temperature; stimulus response; support vector machine; videorecording; Electroencephalography},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041100606&doi=10.3389%2ffncom.2017.00115&partnerID=40&md5=c92d8ecc61c786b6b72105c9b47639ad},
}

@Article{li2016ap,
  author            = {Li, Xin and Tian, Yanxiu and Hou, Yongjie and Qi, Xiaoying and Sun, Xiaoqi and Fan, MEngdi and Cai, Erjuan},
  title             = {Applications of Wavelet Transform Combining Empirical Mode Decomposition in EEG Analysis with Music Intervention},
  note              = {Cited by: 1},
  number            = {4},
  pages             = {762 – 769},
  volume            = {33},
  abstract          = {In the present paper, wavelet transform and empirical mode decomposition(EMD)are combined to extracted the features of electroencephalogram(EEG)signal with music intervention, and to achieve a better classification accuracy rate and reliability in emotional assessment in order to provide a support for music therapy.The data were from Database for Emotion Analysis using Physiological Signals(DEAP).Based on wavelet transformα, βandθrhythms were extracted at frontal(F3, F4), temporal(T7, T8)and central regions(C3, C4).Based on the EMD, the intrinsic mode function(IMF)was analyzed and extracted.Furthermore, average energy and amplitude difference of IMF were analyzed and obtained.The support vector machine was used to assess the state of emotion in order to support music therapy.According to this algorithm, the classification accuracy rate could reach 100% between no emotions, positive emotions and negative emotions, which made a 10%improvement between positive and negative emotion recognition.Effective evaluation result between positive and negative emotions was achieved.The states of emotion would influence the effect of music therapy, undoubtedly, the classification accuracy rate increasing of emotional assessment will further help improve the effect of music therapy and provide a better support to the therapy.},
  year              = {2016},
  hasabstract       = {Y},
  journaltitle      = {Sheng wu yi xue gong cheng xue za zhi = Journal of biomedical engineering = Shengwu yixue gongchengxue zazhi},
  keywords          = {Algorithms; Electroencephalography; Emotions; Humans; Music Therapy; Reproducibility of Results; Signal Processing, Computer-Assisted; Support Vector Machine; Wavelet Analysis; algorithm; electroencephalography; emotion; human; music therapy; reproducibility; signal processing; support vector machine; wavelet analysis},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049117812&partnerID=40&md5=9c6fbda5a1a79391f2f6a5ede2fba382},
}

@Article{czepiel2021sy,
  author            = {Czepiel, Anna and Fink, Lauren K. and Fink, Lea T. and Wald-Fuhrmann, Melanie and Tröndle, Martin and Merrill, Julia},
  title             = {Synchrony in the periphery: inter-subject correlation of physiological responses during live music concerts},
  note              = {Cited by: 21; All Open Access, Gold Open Access, Green Open Access},
  number            = {1},
  volume            = {11},
  abstract          = {While there is an increasing shift in cognitive science to study perception of naturalistic stimuli, this study extends this goal to naturalistic contexts by assessing physiological synchrony across audience members in a concert setting. Cardiorespiratory, skin conductance, and facial muscle responses were measured from participants attending live string quintet performances of full-length works from Viennese Classical, Contemporary, and Romantic styles. The concert was repeated on three consecutive days with different audiences. Using inter-subject correlation (ISC) to identify reliable responses to music, we found that highly correlated responses depicted typical signatures of physiological arousal. By relating physiological ISC to quantitative values of music features, logistic regressions revealed that high physiological synchrony was consistently predicted by faster tempi (which had higher ratings of arousing emotions and engagement), but only in Classical and Romantic styles (rated as familiar) and not the Contemporary style (rated as unfamiliar). Additionally, highly synchronised responses across all three concert audiences occurred during important structural moments in the music—identified using music theoretical analysis—namely at transitional passages, boundaries, and phrase repetitions. Overall, our results show that specific music features induce similar physiological responses across audience members in a concert context, which are linked to arousal, engagement, and familiarity. © 2021, The Author(s).},
  year              = {2021},
  doi               = {10.1038/s41598-021-00492-3},
  hasabstract       = {Y},
  journaltitle      = {Scientific Reports},
  keywords          = {Acoustic Stimulation; Adolescent; Adult; Aged; Aged, 80 and over; Arousal; Auditory Perception; Emotions; Facial Muscles; Female; Heart Rate; Humans; Logistic Models; Male; Middle Aged; Music; Recognition, Psychology; Respiratory Rate; Young Adult; adult; arousal; article; case report; clinical article; emotion; face muscle; female; human; human experiment; male; music; perception; physiological coregulation; psychology; quantitative analysis; skin conductance; adolescent; aged; auditory stimulation; breathing rate; hearing; heart rate; middle aged; music; physiology; procedures; psychology; statistical model; very elderly; young adult},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119294610&doi=10.1038%2fs41598-021-00492-3&partnerID=40&md5=b6bf4b482127d12f8785e58a63a25445},
}

@Article{yang2021an,
  author            = {Yang, Jing},
  title             = {A Novel Music Emotion Recognition Model Using Neural Network Technology},
  note              = {Cited by: 14; All Open Access, Gold Open Access, Green Open Access},
  volume            = {12},
  abstract          = {Music plays an extremely important role in people’s production and life. The amount of music is growing rapidly. At the same time, the demand for music organization, classification, and retrieval is also increasing. Paying more attention to the emotional expression of creators and the psychological characteristics of music are also indispensable personalized needs of users. The existing music emotion recognition (MER) methods have the following two challenges. First, the emotional color conveyed by the first music is constantly changing with the playback of the music, and it is difficult to accurately express the ups and downs of music emotion based on the analysis of the entire music. Second, it is difficult to analyze music emotions based on the pitch, length, and intensity of the notes, which can hardly reflect the soul and connotation of music. In this paper, an improved back propagation (BP) algorithm neural network is used to analyze music data. Because the traditional BP network tends to fall into local solutions, the selection of initial weights and thresholds directly affects the training effect. This paper introduces artificial bee colony (ABC) algorithm to improve the structure of BP neural network. The output value of the ABC algorithm is used as the weight and threshold of the BP neural network. The ABC algorithm is responsible for adjusting the weights and thresholds, and feeds back the optimal weights and thresholds to the BP neural network system. BP neural network with ABC algorithm can improve the global search ability of the BP network, while reducing the probability of the BP network falling into the local optimal solution, and the convergence speed is faster. Through experiments on public music data sets, the experimental results show that compared with other comparative models, the MER method used in this paper has better recognition effect and faster recognition speed. © Copyright © 2021 Yang.},
  author_keywords   = {ABC algorithm; BP neural network; emotion recognition; MediaEval Emotion in Music data set; music},
  year              = {2021},
  doi               = {10.3389/fpsyg.2021.760060},
  hasabstract       = {Y},
  journaltitle      = {Frontiers in Psychology},
  priority          = {prio1},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116855247&doi=10.3389%2ffpsyg.2021.760060&partnerID=40&md5=d8babd4e6050f14375d2b4632dfdb587},
}

@Article{ziegler2021no,
  author            = {Ziegler, Chris},
  title             = {No Body lives here (ODO). 2020: An Interactive Theater Performance for five people + one AI characterMuffatwerk, Munich, Germany/Centre for Art and Media ZKM, Karlsruhe},
  note              = {Cited by: 0},
  number            = {1-2},
  pages             = {50 – 60},
  volume            = {7},
  abstract          = {ODO is a journey through worlds of imagination inspired by Antoine de Saint-Exupéry’s Little Prince and Stanley Kubrick’s “HAL9000” in 2001: A Space Odyssey. An AI character lives on stage as in Plato’s Cave. ODO can’t leave the stage; ODO is offline … Every visitor to the installation means the world to him/her/it. It collects stories and narratives to understand how our world works. ODO is theatre, opera and choreographic architecture. The stage is the orchestra pit of an ancient Greek tragedy where a chorus of three to five audience members interrogates the main character. ODO is a world builder who creates imaginary worlds on stage using a robotic light matrix, moving LEDs like pixels in space. ODO continues through the storyline of the piece with verbal and physical dialogues. ODO uses AI algorithms–Natural Language Processing (NLP)–to conduct natural conversations with the audience and Deep Learning to create Haiku poems and music. ODO has sensors to hear and see the audience. ODO uses Face Recognition Algorithms and Crowd Cluster Tools to understand emotions and physical behaviour. With all means possible, ODO tries to get “in touch” with us!. © 2021 Informa UK Limited, trading as Taylor & Francis Group.},
  year              = {2021},
  doi               = {10.1080/23322551.2021.1940682},
  hasabstract       = {Y},
  journaltitle      = {Theatre and Performance Design},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113744629&doi=10.1080%2f23322551.2021.1940682&partnerID=40&md5=fa53f9dba3b9a6e54d8ee86a3dd1d96f},
}

@Article{zhang2016rea,
  author            = {Zhang, Yang and Liu, Heng},
  title             = {Research on music production based on computer analysis and recognition technology},
  note              = {Cited by: 0},
  number            = {E13},
  pages             = {321 – 331},
  volume            = {2016},
  abstract          = {Based on the researches and improvement of the main melody recognition technology of music, this paper constructs the music feature space model, and the characteristic parameters of the model feature space are marked, furthermore on this basis, the music emotion computer automatic recognition model is studied, and design and simulation of automatic recognition system are achieved based on the MATLAB platform. This paper also proposes an automatic recognition model by using BP neural network algorithm. Experimental results show that the proposed BP neural network is effective in music emotion recognition after comparing it with the statistical classification algorithm. © AISTI 2016.},
  author_keywords   = {Computer analysis; Computer recognition technology; Emotion analysis; Music production},
  year              = {2016},
  hasabstract       = {Y},
  journaltitle      = {RISTI - Revista Iberica de Sistemas e Tecnologias de Informacao},
  priority          = {prio1},
  publication_stage = {Final},
  ranking           = {rank2},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016046810&partnerID=40&md5=7c6f1412441ed987b75e88238a9e53ce},
}

@Article{hausmann2016mu,
  author            = {Hausmann, Markus and Hodgetts, Sophie and Eerola, Tuomas},
  title             = {Music-induced changes in functional cerebral asymmetries},
  note              = {Cited by: 17; All Open Access, Green Open Access},
  pages             = {58 – 71},
  volume            = {104},
  abstract          = {After decades of research, it remains unclear whether emotion lateralization occurs because one hemisphere is dominant for processing the emotional content of the stimuli, or whether emotional stimuli activate lateralised networks associated with the subjective emotional experience. By using emotion-induction procedures, we investigated the effect of listening to happy and sad music on three well-established lateralization tasks. In a prestudy, Mozart's piano sonata (K. 448) and Beethoven's Moonlight Sonata were rated as the most happy and sad excerpts, respectively. Participants listened to either one emotional excerpt, or sat in silence before completing an emotional chimeric faces task (Experiment 1), visual line bisection task (Experiment 2) and a dichotic listening task (Experiment 3 and 4). Listening to happy music resulted in a reduced right hemispheric bias in facial emotion recognition (Experiment 1) and visuospatial attention (Experiment 2) and increased left hemispheric bias in language lateralization (Experiments 3 and 4). Although Experiments 1-3 revealed an increased positive emotional state after listening to happy music, mediation analyses revealed that the effect on hemispheric asymmetries was not mediated by music-induced emotional changes. The direct effect of music listening on lateralization was investigated in Experiment 4 in which tempo of the happy excerpt was manipulated by controlling for other acoustic features. However, the results of Experiment 4 made it rather unlikely that tempo is the critical cue accounting for the effects. We conclude that listening to music can affect functional cerebral asymmetries in well-established emotional and cognitive laterality tasks, independent of music-induced changes in the emotion state. © 2016 Elsevier Inc.},
  author_keywords   = {Brain asymmetries; Emotion induction; Emotional valence; Lateralization; Music},
  year              = {2016},
  doi               = {10.1016/j.bandc.2016.03.001},
  hasabstract       = {Y},
  journaltitle      = {Brain and Cognition},
  keywords          = {Adolescent; Attention; Auditory Perception; Brain; Dichotic Listening Tests; Emotions; Facial Recognition; Female; Functional Laterality; Happiness; Humans; Male; Music; Task Performance and Analysis; Young Adult; adult; Article; brain asymmetry; controlled study; emotionality; facial expression; female; hearing; hemispheric dominance; human; human experiment; male; mental task; music; priority journal; task performance; adolescent; attention; brain; dichotic listening; emotion; facial recognition; happiness; physiology; psychology; young adult},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960347235&doi=10.1016%2fj.bandc.2016.03.001&partnerID=40&md5=01c1c5770924cba000201aa62c170760},
}

@Article{juremi2017in,
  author            = {Juremi, Nor Rashidah Md and Zulkifley, Mohd Asyraf and Hussain, Aini and Zaki, Wan Mimi Diyana Wan},
  title             = {Inter-rater reliability of actual tagged emotion categories validation using Cohen’s Kappa coefficient},
  note              = {Cited by: 8},
  number            = {2},
  pages             = {259 – 264},
  volume            = {95},
  abstract          = {It is necessary to find the human inter-rater agreement in emotion recognition research especially when handling with publicly available database. This paper discusses the Cohen’s Kappa coefficient technique to verify the actual tagged emotion categories for hybrid emotion model using music video as stimulus. This method has been done by finding the degree of inter-rater reliability between the five selected raters. As the results, the values of Cohen’s Kappa coefficients are over 0.87 for four actual tagged emotion categories which are happy, relaxed, sad and angry. These values demonstrate that the degree of inter-rater agreement are excellent. The actual tagged emotion categories are selected based on the division of average value of arousal-valence rating. © 2005 - 2017 JATIT & LLS. All rights reserved.},
  author_keywords   = {Cohen’s Kappa coefficient; Emotion recognition},
  year              = {2017},
  hasabstract       = {Y},
  journaltitle      = {Journal of Theoretical and Applied Information Technology},
  priority          = {prio1},
  publication_stage = {Final},
  ranking           = {rank2},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011659640&partnerID=40&md5=b3c6804a58c78ab147e87dc60d3f986a},
}

@Article{fawcett2021tw,
  author            = {Fawcett, Christine and Kreutz, Gunter},
  title             = {Twelve-month-old infants’ physiological responses to music are affected by others’ positive and negative reactions},
  note              = {Cited by: 2; All Open Access, Green Open Access},
  number            = {6},
  pages             = {784 – 797},
  volume            = {26},
  abstract          = {Infants show remarkable skills for processing music in the first year of life. Such skills are believed to foster social and communicative development, yet little is known about how infants’ own preferences for music develop and whether social information plays a role. Here, we investigate whether the reactions of another person influence infants’ responses to music. Specifically, 12-month-olds (N = 33) saw an actor react positively or negatively after listening to clips of instrumental music. Arousal (measured via pupil dilation) and attention (measured via looking time) were assessed when infants later heard the clips without the actor visible. Results showed greater pupil dilation when listening to music clips that had previously been reacted to negatively than those that had been reacted to positively (Exp. 1). This effect was not replicated when a similar, rather than identical, clip from the piece of music was used in the test phase (Exp. 2, N = 35 12-month-olds). There were no effects of the actor's positive or negative reaction on looking time. Together, our findings suggest that infants are sensitive to others’ positive and negative reactions not only for concrete objects, such as food or toys, but also for more abstract stimuli including music. © 2021 The Authors. Infancy published by Wiley Periodicals LLC on behalf of International Congress of Infant Studies.},
  year              = {2021},
  doi               = {10.1111/infa.12415},
  hasabstract       = {Y},
  journaltitle      = {Infancy},
  keywords          = {Attention; Auditory Perception; Humans; Infant; Music; noradrenalin; prolactin; aphasia; Article; circadian rhythm; clinical article; electroencephalography; emotion; executive function; facial recognition; female; human; infant; male; physiological coregulation; predictive value; pupillometry; skin conductance; task performance; visual attention; visual field; waveform; attention; hearing; music},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107752072&doi=10.1111%2finfa.12415&partnerID=40&md5=f21a126dfe30ac256ad6d0b79b929ee8},
}

@Article{sheykhivand2020re,
  author            = {Sheykhivand, Sobhan and Mousavi, Zohreh and Rezaii, Tohid Yousefi and Farzamnia, Ali},
  title             = {Recognizing Emotions Evoked by Music Using CNN-LSTM Networks on EEG Signals},
  note              = {Cited by: 74; All Open Access, Gold Open Access},
  pages             = {139332 – 139345},
  volume            = {8},
  abstract          = {Emotion is considered to be critical for the actual interpretation of actions and relationships. Recognizing emotions from EEG signals is also becoming an important computer-aided method for diagnosing emotional disorders in neurology and psychiatry. Another advantage of this approach is recognizing emotions without clinical and medical examination, which plays a major role in completing the Brain-Computer Interface (BCI) structure. Emotions recognition ability, without traditional utilization strategies such as self-assessment tests, is of paramount importance. EEG signals are considered the most reliable technique for emotions recognition because of the non-invasive nature. Manual analysis of EEG signals is impossible for emotions recognition, so an automatic method of EEG signals should be provided for emotions recognition. One problem with automatic emotions recognition is the extraction and selection of discriminative features that generally lead to high computational complexity. This paper was design to prepare a new approach to automatic two-stage classification (negative and positive) and three-stage classification (negative, positive, and neutral) of emotions from EEG signals. In the proposed method, directly apply the raw EEG signal to the convolutional neural network and long short-term memory network (CNN-LSTM), without involving feature extraction/selection. In prior literature, this is a challenging method. The suggested deep neural network architecture includes 10-convolutional layers with 3-LSTM layers followed by 2-fully connected layers. The LSTM network in a fusion of the CNN network has been used to increase stability and reduce oscillation. In the present research, we also recorded the EEG signals of 14 subjects with music stimulation for the process. The simulation results of the proposed algorithm for two-stage classification (negative and positive) and three-stage classification (negative, neutral and positive) of emotion for 12 active channels showed 97.42% and 96.78% accuracy and Kappa coefficient of 0.94 and 0.93 respectively. We also compared our proposed LSTM-CNN network (end-to-end) with other hand-crafted methods based on MLP and DBM classifiers and achieved promising results in comparison with similar approaches. According to the high accuracy of the proposed method, it can be used to develop the human-computer interface system.  © 2013 IEEE.},
  author_keywords   = {CNN; EEG; Emotions Recognition; LSTM},
  year              = {2020},
  doi               = {10.1109/ACCESS.2020.3011882},
  hasabstract       = {Y},
  journaltitle      = {IEEE Access},
  keywords          = {Biomedical signal processing; Brain computer interface; Convolutional neural networks; Deep neural networks; Diagnosis; Extraction; Feature extraction; Lead compounds; Multilayer neural networks; Network architecture; Signal analysis; Computer aided methods; Discriminative features; Emotions recognition; Human computer interfaces; Kappa coefficient; Recognizing emotions; Short term memory; Utilization strategy; Long short-term memory},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089604796&doi=10.1109%2fACCESS.2020.3011882&partnerID=40&md5=0a89bc479ae6925f3ca89f0a18bed53d},
}

@Article{fabio2019ad,
  author            = {Fabio, Rosa Angela and Iannizzotto, Giancarlo and Nucita, Andrea and Caprì, Tindara},
  title             = {Adult listening behaviour, music preferences and emotions in the mobile context. Does mobile context affect elicited emotions?},
  note              = {Cited by: 7; All Open Access, Gold Open Access, Green Open Access},
  number            = {1},
  volume            = {6},
  abstract          = {After the introduction of mobile computing devices, the way people listen to music has changed considerably. Although there is a broad scientific consensus on the fact that people show music preferences and make music choices based on their feelings and emotions, the sources of such preferences and choices are still debated. The main aim of this study is to understand whether listening in ecological (mobile) contexts differs from listening in non-mobile contexts in terms of the elicited emotive response. A total of 328 participants listen to 100 classical music tracks, available through an ad-hoc mobile application for mobile devices. The participants were asked to report their self-evaluation of each of the tracks, according to the Pleasure-Arousal-Dominance model and filled out a questionnaire about their listening behaviour. Our findings show that the same factors that affect music listening in non-mobile contexts also affect it in a mobile context. © 2019, © 2019 The Author(s). This open access article is distributed under a Creative Commons Attribution (CC-BY) 4.0 license.},
  author_keywords   = {emotion recognition; mobile context; music; music listening in ecological contexts},
  year              = {2019},
  doi               = {10.1080/23311916.2019.1597666},
  hasabstract       = {Y},
  journaltitle      = {Cogent Engineering},
  priority          = {prio1},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064259930&doi=10.1080%2f23311916.2019.1597666&partnerID=40&md5=66927d75f900041fc51c968baa4ac66c},
}

@Article{ren2019em,
  author            = {Ren, Fuji and Dong, Yindong and Wang, Wei},
  title             = {Emotion recognition based on physiological signals using brain asymmetry index and echo state network},
  note              = {Cited by: 35},
  number            = {9},
  pages             = {4491 – 4501},
  volume            = {31},
  abstract          = {This paper proposes a method to evaluate the degree of emotion being motivated in continuous music videos based on asymmetry index (AsI). By collecting two groups of electroencephalogram (EEG) signals from 6 channels (Fp1, Fp2, Fz and AF3, AF4, Fz) in the left and right hemispheres, multidimensional directed information is used to measure the mutual information shared between two frontal lobes, and then, we get AsI to estimate the degree of emotional induction. In order to evaluate the effect of AsI processing on physiological emotion recognition, 32-channel EEG signals, 2-channel EEG signals and 2-channel EMG signals are selected for each subject from the DEAP dataset, and different sub-bands are extracted using wavelet packet transform. k-means algorithm is used to cluster the wavelet packet coefficients of each sub-band, and the probability distribution of the coefficients under each cluster is calculated. Finally, the probability distribution value of each sample is sent as the original features into echo state network for unsupervised intrinsic plasticity training; the reservoir state nodes are selected as the final feature vector and fed into the support vector machine. The experimental results show that the proposed algorithm can achieve an average recognition rate of 70.5% when the subjects are independent. Compared with the case without AsI, the recognition rate is increased by 8.73%. On the other hand, the ESN is adopted for the original physiological feature refinement which can significantly reduce feature dimensions and be more beneficial to the emotion classification. Therefore, this study can effectively improve the performance of human–machine interface systems based on emotion recognition. © 2018, The Natural Computing Applications Forum.},
  author_keywords   = {Brain asymmetry index; Echo state network; Emotion recognition; Physiological signals},
  year              = {2019},
  doi               = {10.1007/s00521-018-3664-1},
  hasabstract       = {Y},
  journaltitle      = {Neural Computing and Applications},
  keywords          = {Electroencephalography; Image retrieval; Physiology; Probability distributions; Speech recognition; Wavelet analysis; Echo state networks; Electroencephalogram signals; Emotion recognition; Multidimensional directed informations; Physiological features; Physiological signals; Wavelet packet coefficient; Wavelet packet transforms; Biomedical signal processing},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051487502&doi=10.1007%2fs00521-018-3664-1&partnerID=40&md5=f4f813981d0b98faf66f5b573a6ecc4e},
}

@Article{goshvarpour2016fu,
  author            = {Goshvarpour, Atefeh and Abbasi, Ataollah and Goshvarpour, Ateke and Daneshvar, Sabalan},
  title             = {Fusion framework for emotional electrocardiogram and galvanic skin response recognition: Applying wavelet transform},
  note              = {Cited by: 7},
  number            = {3},
  pages             = {163 – 173},
  volume            = {13},
  abstract          = {Introduction To extract and combine information from different modalities, fusion techniques are commonly applied to promote system performance. In this study, we aimed to examine the effectiveness of fusion techniques in emotion recognition. Materials and Methods Electrocardiogram (ECG) and galvanic skin responses (GSR) of 11 healthy female students (mean age: 22.73±1.68 years) were collected while the subjects were listening to emotional music clips. For multi-resolution analysis of signals, wavelet transform (Coiflets 5 at level 14) was used. Moreover, a novel feature-level fusion method was employed, in which low-frequency sub-band coefficients of GSR signals and high-frequency sub-band coefficients of ECG signals were fused to reconstruct a new feature. To reduce the dimensionality of the feature vector, the absolute value of some statistical indices was calculated and considered as input of PNN classifier. To describe emotions, two-dimensional models (four quadrants of valence and arousal dimensions), valence-based emotional states, and emotional arousal were applied. Results The highest recognition rates were obtained from sigma=0.01. Mean classification rate of 100% was achieved through applying the proposed fusion methodology. However, the accuracy rates of 97.90% and 97.20% were attained for GSR and ECG signals, respectively. Conclusion Compared to the previously published articles in the field of emotion recognition using musical stimuli, promising results were obtained through application of the proposed methodology.},
  author_keywords   = {Electrocardiogram; Emotion; Galvanic skin responses; Neural networks; Wavelet analyses},
  year              = {2016},
  doi               = {10.22038/ijmp.2016.7960},
  hasabstract       = {Y},
  journaltitle      = {Iranian Journal of Medical Physics},
  keywords          = {Electrocardiography; Electrophysiology; Face recognition; Neural networks; Speech recognition; Wavelet analysis; Wavelet transforms; Emotion; Emotion recognition; Feature level fusion; Fusion methodology; Galvanic skin response; Mean classification; Statistical indices; Two dimensional model; Biomedical signal processing},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010918197&doi=10.22038%2fijmp.2016.7960&partnerID=40&md5=a84db768b9832db789d7daa80302d122},
}

@Article{hendry2019pr,
  author            = {Hendry and Chen, Rung-Ching},
  title             = {Predicting business category with multi-label classification from user-item review and business data based on K-means},
  note              = {Cited by: 2},
  number            = {3},
  pages             = {255 – 262},
  volume            = {13},
  abstract          = {Currently, many recommendation systems propose the breakthrough of traditional single recommendation. Many items usually belong to more than one label at a time, for example, genres of music, categories of the products and emotions. One data point could be labeled more than one tag which is a problem for many classification algorithms. Clustering analysis is a primary task of data mining, which works by dividing the dataset into the partitions based on the distance of data points. Clustering is an unsupervised learning model, which is suitable to learn multi-label classification problem. The technique is commonly used in machine learning, pattern recognition, and many others. K-means is one of the simple and widely used clustering algorithms. In this paper, we propose the collaboration between business and user-item reviews to predict the multi-label classification. We implement the combination of k-means between business and user-items review. We found that the value of k equal to three will have the best multi-label classification results for business categories and business rating. © 2019, ICIC International. All rights reserved.},
  author_keywords   = {K-means; Multi-label classification; User-item reviews},
  year              = {2019},
  doi               = {10.24507/icicel.13.03.255},
  hasabstract       = {Y},
  journaltitle      = {ICIC Express Letters},
  priority          = {prio1},
  publication_stage = {Final},
  ranking           = {rank2},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062332918&doi=10.24507%2ficicel.13.03.255&partnerID=40&md5=6cc587c4901a00bacb697bc0f8ad4151},
}

@Article{pralus2020re,
  author            = {Pralus, Agathe and Belfi, Amy and Hirel, Catherine and Lévêque, Yohana and Fornoni, Lesly and Bigand, Emmanuel and Jung, Julien and Tranel, Daniel and Nighoghossian, Norbert and Tillmann, Barbara and Caclin, Anne},
  title             = {Recognition of musical emotions and their perceived intensity after unilateral brain damage},
  note              = {Cited by: 5; All Open Access, Hybrid Gold Open Access},
  pages             = {78 – 93},
  volume            = {130},
  abstract          = {For the hemispheric laterality of emotion processing in the brain, two competing hypotheses are currently still debated. The first hypothesis suggests a greater involvement of the right hemisphere in emotion perception whereas the second hypothesis suggests different involvements of each hemisphere as a function of the valence of the emotion. These hypotheses are based on findings for facial and prosodic emotion perception. Investigating emotion perception for other stimuli, such as music, should provide further insight and potentially help to disentangle between these two hypotheses. The present study investigated musical emotion perception in patients with unilateral right brain damage (RBD, n = 16) or left brain damage (LBD, n = 16), as well as in matched healthy comparison participants (n = 28). The experimental task required explicit recognition of musical emotions as well as ratings on the perceived intensity of the emotion. Compared to matched comparison participants, musical emotion recognition was impaired only in LBD participants, suggesting a potential specificity of the left hemisphere for explicit emotion recognition in musical material. In contrast, intensity ratings of musical emotions revealed that RBD patients underestimated the intensity of negative emotions compared to positive emotions, while LBD patients and comparisons did not show this pattern. To control for a potential generalized emotion deficit for other types of stimuli, we also tested facial emotion recognition in the same patients and their matched healthy comparisons. This revealed that emotion recognition after brain damage might depend on the stimulus category or modality used. These results are in line with the hypothesis of a deficit of emotion perception depending on lesion laterality and valence in brain-damaged participants. The present findings provide critical information to disentangle the currently debated competing hypotheses and thus allow for a better characterization of the involvement of each hemisphere for explicit emotion recognition and their perceived intensity. © 2020 Elsevier Ltd},
  author_keywords   = {Brain lesion; Emotion perception; Music; Right hemisphere hypothesis; Valence hypothesis},
  year              = {2020},
  doi               = {10.1016/j.cortex.2020.05.015},
  hasabstract       = {Y},
  journaltitle      = {Cortex},
  keywords          = {adult; Article; audiometry; brain damage; clinical article; clinical assessment; comparative study; controlled study; emotion; female; functional dissociation; hemispheric dominance; human; left hemisphere; male; middle aged; music; neuropsychological test; outcome assessment; perception; recognition; right hemisphere; task performance},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087421145&doi=10.1016%2fj.cortex.2020.05.015&partnerID=40&md5=c1629b258d735cfa9a743ac6b05f7d3c},
}

@Article{raheel2019as,
  author            = {Raheel, Aasim and Majid, Muhammad and Anwar, Syed Muhammad},
  title             = {A study on the effects of traditional and olfaction enhanced multimedia on pleasantness classification based on brain activity analysis},
  note              = {Cited by: 18},
  volume            = {114},
  abstract          = {Human emotions are recognized in response to content engaging one (audio music) or two human senses (videos). An enhanced sensation with a more realistic feel could be achievable by engaging more than two human senses. In this study, olfaction enhanced multimedia content is generated by synchronizing traditional multimedia content with an olfaction dispenser for engaging olfactory sense in addition to vision and auditory senses. Brain activity of 20 participants (10 males and 10 females) is recorded with a commercially available EEG headband, while engaging with traditional and olfaction enhanced multimedia content. The human brain activity is used to analyze and differentiate the content engaging two (traditional multimedia content) or more than two (olfaction enhanced multimedia content) human senses. For brain activity analysis, we apply a t-test on the power spectra of five frequency sub-bands (delta, theta, alpha, beta, and gamma) of the acquired EEG data in response to traditional and olfaction enhanced multimedia. We observe that alpha, and delta bands are significant in discriminating the response to traditional and olfaction enhanced multimedia content. High brain activity is observed in alpha, and delta bands of frontal channels, while experiencing the olfaction enhanced multimedia content. A user-independent pleasantness classification based on human brain activity is also presented, where classification performance is measured using 10-fold cross validation. We extract features in frequency domain i.e., rational asymmetry (RASM) and differential asymmetry (DASM) from five EEG bands to classify two pleasantness states based on their valence scores using support vector machine (SVM) classifier. Features are further selected based on EEG electrode pair positions and sub-bands. We observed that RASM and DASM features selected from delta band (olfaction enhanced content), and alpha or gamma bands (traditional multimedia content) gives best classification accuracy. We achieved an accuracy of 75%, sensitivity of 77.7%, and specificity of 72.7% in response to olfaction enhanced multimedia content and an accuracy of 68.7%, sensitivity of 71.4%, and specificity of 69.2% in response to traditional multimedia content in classifying pleasant and unpleasant states using SVM. We observed that classification of pleasant state was comparatively better with olfaction enhanced multimedia content than traditional multimedia content. © 2019 Elsevier Ltd},
  author_keywords   = {Brain activity; Classification; Electroencephalography; Emotion recognition; Olfaction enhanced multimedia},
  year              = {2019},
  doi               = {10.1016/j.compbiomed.2019.103469},
  hasabstract       = {Y},
  journaltitle      = {Computers in Biology and Medicine},
  keywords          = {Adolescent; Adult; Brain; Electroencephalography; Emotions; Female; Humans; Male; Multimedia; Sensitivity and Specificity; Smell; Support Vector Machine; Young Adult; Audio acoustics; Classification (of information); Electroencephalography; Electrophysiology; Frequency domain analysis; Information analysis; Neurophysiology; Support vector machines; 10-fold cross-validation; Brain activity; Brain activity analysis; Classification accuracy; Classification performance; Emotion recognition; Multimedia contents; Olfaction enhanced multimedia; adult; alpha rhythm; Article; beta rhythm; classifier; delta rhythm; electroencephalogram; emotion; feature selection; female; gamma rhythm; human; human experiment; male; normal human; priority journal; smelling; support vector machine; theta rhythm; adolescent; brain; electroencephalography; multimedia; odor; physiology; sensitivity and specificity; young adult; Brain},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072708828&doi=10.1016%2fj.compbiomed.2019.103469&partnerID=40&md5=da3e9b55256bb2a1e08ae7c3c3050394},
}

@Article{markov2014mu,
  author            = {Markov, Konstantin and Matsui, Tomoko},
  title             = {Music genre and emotion recognition using Gaussian processes},
  note              = {Cited by: 73; All Open Access, Gold Open Access},
  pages             = {688 – 697},
  volume            = {2},
  abstract          = {Gaussian Processes (GPs) are Bayesian nonparametric models that are becoming more and more popular for their superior capabilities to capture highly nonlinear data relationships in various tasks, such as dimensionality reduction, time series analysis, novelty detection, as well as classical regression and classification tasks. In this paper, we investigate the feasibility and applicability of GP models for music genre classification and music emotion estimation. These are two of the main tasks in the music information retrieval (MIR) field. So far, the support vector machine (SVM) has been the dominant model used in MIR systems. Like SVM, GP models are based on kernel functions and Gram matrices; but, in contrast, they produce truly probabilistic outputs with an explicit degree of prediction uncertainty. In addition, there exist algorithms for GP hyperparameter learning - something the SVM framework lacks. In this paper, we built two systems, one for music genre classification and another for music emotion estimation using both SVM and GP models, and compared their performances on two databases of similar size. In all cases, the music audio signal was processed in the same way, and the effects of different feature extraction methods and their various combinations were also investigated. The evaluation experiments clearly showed that in both music genre classification and music emotion estimation tasks the GP performed consistently better than the SVM. The GP achieved a 13.6% relative genre classification error reduction and up to an 11% absolute increase of the coefficient of determination in the emotion estimation task. © 2014 IEEE.},
  author_keywords   = {Gaussian processes; Music emotion estimation; Music genre classification},
  year              = {2014},
  doi               = {10.1109/ACCESS.2014.2333095},
  hasabstract       = {Y},
  journaltitle      = {IEEE Access},
  keywords          = {Classification (of information); Feature extraction; Gaussian distribution; Gaussian noise (electronic); Image retrieval; Support vector machines; Time series analysis; Coefficient of determination; Dimensionality reduction; Feature extraction methods; Gaussian Processes; Music emotions; Music genre classification; Music information retrieval; Prediction uncertainty; Audio acoustics},
  priority          = {prio1},
  publication_stage = {Final},
  ranking           = {rank2},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923317754&doi=10.1109%2fACCESS.2014.2333095&partnerID=40&md5=91033029d14599965aafb73fa7175273},
}

@Article{parncutt2014th,
  author            = {Parncutt, Richard},
  title             = {The emotional connotations of major versus minor tonality: One or more origins?},
  note              = {Cited by: 46},
  number            = {3},
  pages             = {324 – 353},
  volume            = {18},
  abstract          = {The association between major/minor tonality and positive/negative emotional valence is psychologically robust, but without a single accepted explanation. I compare six partially related theories. Dissonance: On average, passages in minor keys are more dissonant because, on average, the minor triad is more dissonant (rougher, less harmonic) or because tonal structure is more complex. Alterity and markedness: Major triads and scales are more common than minor, and positive valence is more common than negative. Major and positive valence are the norm; minor and negative are marked Others. Uncertainty: The minor triad has a more ambiguous (less salient) root than the major, and the minor scale has more variable form and a more ambiguous (less stable) tonic; uncertainty is associated with anger, sadness, distress, and grief. Speech: By comparison to major triads and scales, minor contain pitch(es) that are lower than expected – just as sad speech is lower than expected. Salience: In diatonic chord progressions, flattened diatonic scale degrees are more salient than sharpened because their harmonics better match the prevailing scale. Scale degrees 3 and 6 are more likely to destabilize tonality in minor than major tonalities. Familiarity: Arbitrary emotional differences between major and minor were reinforced in a historical process of cultural differentiation. For each theory, there are credible arguments and evidence for and against. All theories are broadly consistent with Terhardt’s pattern-recognition model of pitch perception (non-musical perceptual familiarity with the harmonic series), Schenker’s concept of prolongation (specifically, tonal voice leading as a prolongation of the tonic triad), evolutionary explanations of the emotional connotations of alterity, and a psychohistory of tonality in which melody, polyphony, leading tones, and the major–minor system emerged at different times, explicable by different psychological principles. © The Author(s) 2014.},
  author_keywords   = {emotion; leading tone; major; minor; music and evolution; pitch; prolongation; salience; Schenker; speech; uncertainty},
  year              = {2014},
  doi               = {10.1177/1029864914542842},
  hasabstract       = {Y},
  journaltitle      = {Musicae Scientiae},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910098161&doi=10.1177%2f1029864914542842&partnerID=40&md5=b02c97a8a9bc1bfcfb13706023e565ff},
}

@Article{gebauer2014in,
  author            = {Gebauer, Line and Skewes, Joshua and Westphael, Gitte and Heaton, Pamela and Vuust, Peter},
  title             = {Intact brain processing of musical emotions in autism spectrum disorder, but more cognitive load and arousal in happy vs. sad music},
  note              = {Cited by: 60; All Open Access, Gold Open Access, Green Open Access},
  number            = {8 JUL},
  abstract          = {Music is a potent source for eliciting emotions, but not everybody experience emotions in the same way. Individuals with autism spectrum disorder (ASD) show difficulties with social and emotional cognition. Impairments in emotion recognition are widely studied in ASD, and have been associated with atypical brain activation in response to emotional expressions in faces and speech. Whether these impairments and atypical brain responses generalize to other domains, such as emotional processing of music, is less clear. Using functional magnetic resonance imaging, we investigated neural correlates of emotion recognition in music in high-functioning adults with ASD and neurotypical adults. Both groups engaged similar neural networks during processing of emotional music, and individuals with ASD rated emotional music comparable to the group of neurotypical individuals. However, in the ASD group, increased activity in response to happy compared to sad music was observed in dorsolateral prefrontal regions and in the rolandic operculum/insula, and we propose that this reflects increased cognitive processing and physiological arousal in response to emotional musical stimuli in this group. © 2014 Gebauer, Skewes, Westphael, Heaton and Vuust.},
  author_keywords   = {Autism spectrum disorder; Emotion; FMRI; Music},
  year              = {2014},
  doi               = {10.3389/fnins.2014.00192},
  hasabstract       = {Y},
  journaltitle      = {Frontiers in Neuroscience},
  keywords          = {adult; anterior cingulate; arousal; article; autism; brain function; cingulate gyrus; clinical article; cognition; Cognitive load; controlled study; corpus striatum; emotion; emotionality; female; functional magnetic resonance imaging; human; insula; male; mesencephalon; middle frontal gyrus; music; nerve cell network; operculum (brain); orbital cortex; parahippocampal gyrus; postcentral gyrus; prefrontal cortex; primary motor cortex; stimulus response; superior frontal gyrus},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905925105&doi=10.3389%2ffnins.2014.00192&partnerID=40&md5=e37ef39814e060d74736eba867d87352},
}

@Article{kim2014re,
  author            = {Kim, Ga-Hyung and Kim, Byung-Joo},
  title             = {Reducing the student's stress from studying by personalized brain music training},
  note              = {Cited by: 7},
  number            = {5},
  pages             = {83 – 91},
  volume            = {9},
  abstract          = {In this research we implement an EEG based music therapy. Music therapy can help the student deal with the stress, anxiety and depression problems. To do so we will develop EEG-based human emotion recognition algorithm. Proposed training program works as a therapist. The music choice and duration of the music is adjusted based on the student's current emotion recognized automatically from EEG. If the happy emotion is not induced by the current music, the system would automatically switch to another one until he or she feel happy. Proposed system is personalized brain music treatment that is making a brain training application running on smart phone or pad. That overcomes the critical problems of time and space constraints of existing brain training program. By using this brain training program, student can manage the stress easily without the help of expert. © 2014 SERSC.},
  author_keywords   = {Electroencephlagraphy; Personalized Music Training},
  year              = {2014},
  doi               = {10.14257/ijmue.2014.9.5.08},
  hasabstract       = {Y},
  journaltitle      = {International Journal of Multimedia and Ubiquitous Engineering},
  keywords          = {Occupational therapy; Critical problems; Electroencephlagraphy; Human emotion recognition; Music therapy; Space constraints; Training applications; Training program; Students},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901941545&doi=10.14257%2fijmue.2014.9.5.08&partnerID=40&md5=d77a346c913347af9de9adee24016ce6},
}

@Article{borella2014ar,
  author            = {Borella, Erika and Carretti, Barbara and Grassi, Massimo and Nucci, Massimo and Sciore, Roberta},
  title             = {Are age-related differences between young and older adults in an affective working memory test sensitive to the music effects?},
  note              = {Cited by: 9; All Open Access, Gold Open Access, Green Open Access},
  number            = {OCT},
  volume            = {6},
  abstract          = {There are evidences showing that music can affect cognitive performance by improving our emotional state. The aim of the current study was to analyze whether age-related differences between young and older adults in a Working Memory (WM) Span test in which the stimuli to be recalled have a different valence (i.e., neutral, positive, or negative words), are sensitive to exposure to music. Because some previous studies showed that emotional words can sustain older adults' performance in WM, we examined whether listening to music could enhance the benefit of emotional material, with respect to neutral words, on WM performance decreasing the age-related difference between younger and older adults. In particular, the effect of two types of music (Mozart vs. Albinoni), which differ in tempo, arousal and mood induction, on age-related differences in an affective version of the Operation WM Span task were analyzed. Results showed no effect of music on the WM test regardless of the emotional content of the music (Mozart vs. Albinoni). However, as in previous studies, a valence effect for the words in the WM task was found with a higher number of negative words recalled with respect to positive and neutral ones in both younger and older adults. When individual differences, in terms of accuracy in the processing phase of the Operation Span task, were considered, only younger low-performing participants were affected by the type music, with the Albinoni condition that lowered their performance with respect to the Mozart condition. Such a result suggests that individual differences in WM performance, at least when young adults are considered, could be affected by the type of music. Altogether, these findings suggest that complex span tasks, such as WM tasks, along with age-related differences are less sensitive to music effects. © 2014 Borella, Carretti, Grassi, Nucci and Sciore.},
  author_keywords   = {Aging; Emotions; Individual differences; Music; Working memory},
  year              = {2014},
  doi               = {10.3389/fnagi.2014.00298},
  hasabstract       = {Y},
  journaltitle      = {Frontiers in Aging Neuroscience},
  keywords          = {accuracy; adult; affective working memory test; aged; aging; arousal; Article; controlled study; groups by age; human; human experiment; learning and memory test; mood; music; normal human; performance; randomized controlled trial; word recognition; working memory; young adult},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949125827&doi=10.3389%2ffnagi.2014.00298&partnerID=40&md5=3894646f04579cdbad496ff64ddaaa9d},
}

@Article{gingras2014be,
  author            = {Gingras, Bruno and Marin, Manuela M. and Fitch, W. Tecumseh},
  title             = {Beyond intensity: Spectral features effectively predict music-induced subjective arousal},
  note              = {Cited by: 32},
  number            = {7},
  pages             = {1428 – 1446},
  volume            = {67},
  abstract          = {Emotions in music are conveyed by a variety of acoustic cues. Notably, the positive association between sound intensity and arousal has particular biological relevance. However, although amplitude normalization is a common procedure used to control for intensity in music psychology research, direct comparisons between emotional ratings of original and amplitude-normalized musical excerpts are lacking.In this study, 30 nonmusicians retrospectively rated the subjective arousal and pleasantness induced by 84 six-second classical music excerpts, and an additional 30 nonmusicians rated the same excerpts normalized for amplitude. Following the cue-redundancy and Brunswik lens models of acoustic communication, we hypothesized that arousal and pleasantness ratings would be similar for both versions of the excerpts, and that arousal could be predicted effectively by other acoustic cues besides intensity.Although the difference in mean arousal and pleasantness ratings between original and amplitude-normalized excerpts correlated significantly with the amplitude adjustment, ratings for both sets of excerpts were highly correlated and shared a similar range of values, thus validating the use of amplitude normalization in music emotion research. Two acoustic parameters, spectral flux and spectral entropy, accounted for 65% of the variance in arousal ratings for both sets, indicating that spectral features can effectively predict arousal. Additionally, we confirmed that amplitude-normalized excerpts were adequately matched for loudness. Overall, the results corroborate our hypotheses and support the cue-redundancy and Brunswik lens models. © 2013 The Experimental Psychology Society.},
  author_keywords   = {Arousal; Brunswik lens model; Emotion; Intensity; Music},
  year              = {2014},
  doi               = {10.1080/17470218.2013.863954},
  hasabstract       = {Y},
  journaltitle      = {Quarterly Journal of Experimental Psychology},
  keywords          = {Acoustic Stimulation; Acoustics; Adult; Arousal; Emotions; Female; Humans; Linear Models; Loudness Perception; Male; Music; Predictive Value of Tests; Questionnaires; Recognition (Psychology); Young Adult; acoustics; adult; arousal; auditory stimulation; emotion; female; hearing; human; male; music; physiology; predictive value; psychology; questionnaire; recognition; statistical model; young adult},
  priority          = {prio1},
  publication_stage = {Final},
  ranking           = {rank2},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902376903&doi=10.1080%2f17470218.2013.863954&partnerID=40&md5=021d634d0f679aaff17d6770894daad5},
}

@Article{akdemirakar2015no,
  author            = {Akdemir Akar, Saime and Kara, Sadik and Agambayev, Sümeyra and Bilgiç, Vedat},
  title             = {Nonlinear analysis of EEGs of patients with major depression during different emotional states},
  note              = {Cited by: 91},
  pages             = {49 – 60},
  volume            = {67},
  abstract          = {Background: Although patients with major depressive disorder (MDD) have dysfunctions in cognitive behaviors and the regulation of emotions, the underlying brain dynamics of the pathophysiology are unclear. Therefore, nonlinear techniques can be used to understand the dynamic behavior of the EEG signals of MDD patients. Methods: To investigate and clarify the dynamics of MDD patients' brains during different emotional states, EEG recordings were analyzed using nonlinear techniques. The purpose of the present study was to assess whether there are different EEG complexities that discriminate between MDD patients and healthy controls during emotional processing. Therefore, nonlinear parameters, such as Katz fractal dimension (KFD), Higuchi fractal dimension (HFD), Shannon entropy (ShEn), Lempel-Ziv complexity (LZC) and Kolmogorov complexity (KC), were computed from the EEG signals of two groups under different experimental states: noise (negative emotional content) and music (positive emotional content) periods. Results: First, higher complexity values were generated by MDD patients relative to controls. Significant differences were obtained in the frontal and parietal scalp locations using KFD (p<0.001), HFD (p<0.05), and LZC (p=0.05). Second, lower complexities were observed only in the controls when they were subjected to music compared to the resting baseline state in the frontal (p<0.05) and parietal (p=0.005) regions. In contrast, the LZC and KFD values of patients increased in the music period compared to the resting state in the frontal region (p<0.05). Third, the patients' brains had higher complexities when they were exposed to noise stimulus than did the controls' brains. Moreover, MDD patients' negative emotional bias was demonstrated by their higher brain complexities during the noise period than the music stimulus. Additionally, we found that the KFD, HFD and LZC values were more sensitive in discriminating between patients and controls than the ShEn and KC measures, according to the results of ANOVA and ROC calculations. Conclusion: It can be concluded that the nonlinear analysis may be a useful and discriminative tool in investigating the neuro-dynamic properties of the brain in patients with MDD during emotional stimulation. © 2015 Elsevier Ltd.},
  author_keywords   = {Complexity; EEG; Emotion; Entropy; Fractal dimension; Major depressive disorder; Music; Noise},
  year              = {2015},
  doi               = {10.1016/j.compbiomed.2015.09.019},
  hasabstract       = {Y},
  journaltitle      = {Computers in Biology and Medicine},
  keywords          = {Adult; Algorithms; Computer Simulation; Depressive Disorder, Major; Diagnosis, Computer-Assisted; Electroencephalography; Emotions; Female; Humans; Male; Models, Neurological; Nonlinear Dynamics; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Electroencephalography; Entropy; Fractal dimension; Nonlinear analysis; Complexity; Emotion; Major depressive disorders; Music; Noise; adult; Article; brain depth stimulation; clinical article; controlled study; electroencephalogram; emotion; emotional bias; entropy; female; frontal cortex; Higuchi fractal dimension; human; Katz fractal dimension; Kolmogorov complexity; Lempel Ziv complexity; major depression; male; music; noise; nonlinear system; parietal cortex; priority journal; Shannon entropy; algorithm; automated pattern recognition; biological model; clinical trial; computer assisted diagnosis; computer simulation; Depressive Disorder, Major; electroencephalography; pathophysiology; procedures; psychology; reproducibility; sensitivity and specificity; Biomedical signal processing},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944883862&doi=10.1016%2fj.compbiomed.2015.09.019&partnerID=40&md5=a2a1f3133c737f8e1624d858c17bd197},
}

@Article{chin2014mu,
  author            = {Chin, Yu-Hao and Lin, Chang-Hong and Siahaan, Ernestasia and Wang, Jia-Ching},
  title             = {Music emotion detection using hierarchical sparse kernel machines},
  note              = {Cited by: 4; All Open Access, Gold Open Access, Green Open Access},
  volume            = {2014},
  abstract          = {For music emotion detection, this paper presents a music emotion verification system based on hierarchical sparse kernel machines. With the proposed system, we intend to verify if a music clip possesses happiness emotion or not. There are two levels in the hierarchical sparse kernel machines. In the first level, a set of acoustical features are extracted, and principle component analysis (PCA) is implemented to reduce the dimension. The acoustical features are utilized to generate the first-level decision vector, which is a vector with each element being a significant value of an emotion. The significant values of eight main emotional classes are utilized in this paper. To calculate the significant value of an emotion, we construct its 2-class SVM with calm emotion as the global (non-target) side of the SVM. The probability distributions of the adopted acoustical features are calculated and the probability product kernel is applied in the first-level SVMs to obtain first-level decision vector feature. In the second level of the hierarchical system, we merely construct a 2-class relevance vector machine (RVM) with happiness as the target side and other emotions as the background side of the RVM. The first-level decision vector is used as the feature with conventional radial basis function kernel. The happiness verification threshold is built on the probability value. In the experimental results, the detection error tradeoff (DET) curve shows that the proposed system has a good performance on verifying if a music clip reveals happiness emotion. © 2014 Yu-Hao Chin et al.},
  year              = {2014},
  doi               = {10.1155/2014/270378},
  hasabstract       = {Y},
  journaltitle      = {The Scientific World Journal},
  keywords          = {Algorithms; Auditory Perception; Biomimetics; Emotions; Humans; Music; Pattern Recognition, Automated; Sound Spectrography; Support Vector Machines; algorithm; article; emotion; happiness; kernel method; music; principal component analysis; probability; relevance vector machine; support vector machine; algorithm; automated pattern recognition; biomimetics; emotion; hearing; human; physiology; procedures; sound detection; support vector machine},
  priority          = {prio1},
  publication_stage = {Final},
  ranking           = {rank2},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896904854&doi=10.1155%2f2014%2f270378&partnerID=40&md5=48fe84315e318b35b7b402c797cb9a66},
}

@Article{xing2015em,
  author            = {Xing, Baixi and Zhang, Kejun and Sun, Shouqian and Zhang, Lekai and Gao, Zenggui and Wang, Jiaxi and Chen, Shi},
  title             = {Emotion-driven Chinese folk music-image retrieval based on DE-SVM},
  note              = {Cited by: 28},
  pages             = {619 – 627},
  volume            = {148},
  abstract          = {In this study, we attempt to explore cross-media retrieval between music and image data based on the emotional correlation. Emotion feature analytic could be the bridge of cross-media retrieval, since emotion represents the user's perspective and effectively meets the user's retrieval need. Currently, there is little research about the emotion correlation of different multimedia data (e.g. image or music). We propose a promising model based on Differential Evolutionary-Support Vector Machine (DE-SVM) to build up the emotion-driven cross-media retrieval system between Chinese folk image and Chinese folk music. In this work, we first build up the Chinese Folk Music Library and Chinese Folk Image Library.Second, we compare Back Propagation(BP), Linear Regression(LR) and Differential Evolutionary-Support Vector Machine (DE-SVM), and find that DE-SVM has the best performance. Then we conduct DE-SVM to build the optimal model for music/image emotion recognition. Finally, an Emotion-driven Chinese Folk Music-Image Exploring System based on DE-SVM is developed and experiment results show our method is effective in terms of retrieval performance. © 2014 Elsevier B.V.},
  author_keywords   = {Back propagation; Cross-media information retrieval; Differential Evolutionary algorithm; Image emotion recognition; Music emotion recognition; Support vector machine},
  comment           = {discussed, classification, unique},
  year              = {2015},
  doi               = {10.1016/j.neucom.2014.08.007},
  hasabstract       = {Y},
  journaltitle      = {Neurocomputing},
  keywords          = {Cross-media information retrieval; Differential evolutionary algorithm; Emotion recognition; Music emotions; accuracy; Article; back propagation; calculation; Chinese; classification algorithm; cultural period; Differential Evolutionary Support Vector Machine algorithm; emotion; image retrieval; library; linear system; mathematical analysis; mathematical model; music; nonlinear system; support vector machine; Support vector machines},
  modificationdate  = {2024-05-16T11:41:56},
  priority          = {prio1},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908032200&doi=10.1016%2fj.neucom.2014.08.007&partnerID=40&md5=62cf1f46b87868a59e2d0da9852809db},
}

@Article{niu2014em,
  author            = {Niu, Xiaowei and Chen, Liwan and Xie, Hui and Chen, Qiang and Li, Hongbing},
  title             = {Emotion pattern recognition using physiological signals},
  note              = {Cited by: 10},
  number            = {6},
  pages             = {147 – 156},
  volume            = {172},
  abstract          = {In this paper, we first regard emotion recognition as a pattern recognition problem, a novel feature selection method was presented to recognize human emotional state from four physiological signals. Electrocardiogram (ECG), electromyogram (EMG), skin conductance (SC) and respiration (RSP). The raw training data was collected from four sensors, ECG, EMG, SC, RSP, when a single subject intentionally expressed four different affective states, joy, anger, sadness, pleasure. The total 193 features were extracted for the recognition. A music induction method was used to elicit natural emotional reactions from the subject, after calculating a sufficient amount of features from the raw signals, the genetic algorithm and the K-neighbor methods were tested to extract a new feature set consisting of the most significant features which represents exactly the relevant emotional state for improving classification performance. The numerical results demonstrate that there is significant information in physiological signals for recognizing the affective state. It also turned out that it was much easier to separate emotions along the arousal axis than along the valence axis. © 2014 by IFSA Publishing, S. L.},
  author_keywords   = {Affective state; Feature selection; Pattern recognition; Physiological sensor; Physiological signals},
  year              = {2014},
  hasabstract       = {Y},
  journaltitle      = {Sensors and Transducers},
  keywords          = {Computer music; Electrocardiography; Feature extraction; Genetic algorithms; Pattern recognition; Physiology; Affective state; Classification performance; Emotion recognition; Emotional reactions; Feature selection methods; Pattern recognition problems; Physiological sensors; Physiological signals; Signal processing},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924376515&partnerID=40&md5=8c6eb06b70e87d57c4568d424b16d809},
}

@Article{yeh2014po,
  author            = {Yeh, Chia-Hung and Tseng, Wen-Yu and Chen, Chia-Yen and Lin, Yu-Dun and Tsai, Yi-Ren and Bi, Hsuan-I and Lin, Yu-Ching and Lin, Ho-Yi},
  title             = {Popular music representation: chorus detection & emotion recognition},
  note              = {Cited by: 11},
  number            = {3},
  pages             = {2103 – 2128},
  volume            = {73},
  abstract          = {This paper proposes a popular music representation strategy based on the song’s emotion. First, a piece of popular music is decomposed into chorus and verse segments through the proposed chorus detection algorithm. Three descriptive features: intensity, frequency band and rhythm regularity are extracted from the structured segments for emotion detection. A hierarchical Adaboost classifier is employed to recognize the emotion of a piece of popular music. The general emotion of the music is classified according to Thayer’s model into four emotions: happy, angry, depressed and relaxed. Experiments conducted on a 350-popular-music database show the average recall and precision of our proposed chorus detection are approximately 95 % and 84 %, respectively; and the average precision rate of emotion detection is 92 %. Additional tests are performed on songs with cover versions in different lyrics and languages, and the resultant precision rate is 90 %. The proposes approaches have been tested and proven by the professional online music company, KKBOX Inc. and show promising performance for effectively and efficiently identifying the emotions of a variety of popular music. © 2013, Springer Science+Business Media New York.},
  author_keywords   = {Adaboost; Chorus; Emotion; MFCCs; Popular music; Rhythm; Verse},
  year              = {2014},
  doi               = {10.1007/s11042-013-1687-2},
  hasabstract       = {Y},
  journaltitle      = {Multimedia Tools and Applications},
  keywords          = {Multimedia systems; Chorus; Emotion; MFCCs; Popular music; Rhythm; Verse; Adaptive boosting},
  priority          = {prio1},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84912027522&doi=10.1007%2fs11042-013-1687-2&partnerID=40&md5=6ce0fe7a0b38a6e2037403e140bd0a60},
}

@Article{kragel2014mu,
  author            = {Kragel, Philip A. and LaBar, Kevin S.},
  title             = {Multivariate neural biomarkers of emotional states are categorically distinct},
  note              = {Cited by: 128; All Open Access, Green Open Access},
  number            = {11},
  pages             = {1437 – 1448},
  volume            = {10},
  abstract          = {Understanding how emotions are represented neurally is a central aim of affective neuroscience. Despite decades of neuroimaging efforts addressing this question, it remains unclear whether emotions are represented as distinct entities, as predicted by categorical theories, or are constructed from a smaller set of underlying factors, as predicted by dimensional accounts. Here, we capitalize on multivariate statistical approaches and computational modeling to directly evaluate these theoretical perspectives. We elicited discrete emotional states using music and films during functional magnetic resonance imaging scanning. Distinct patterns of neural activation predicted the emotion category of stimuli and tracked subjective experience. Bayesian model comparison revealed that combining dimensional and categorical models of emotion best characterized the information content of activation patterns. Surprisingly, categorical and dimensional aspects of emotion experience captured unique and opposing sources of neural information. These results indicate that diverse emotional states are poorly differentiated by simple models of valence and arousal, and that activity within separable neural systems can be mapped to unique emotion categories. © The Author (2015). Published by Oxford University Press.},
  author_keywords   = {Affect; Bayesian model comparison; Emotion; Functional magnetic resonance imaging; Multi-voxel pattern analysis; Pattern classification},
  year              = {2014},
  doi               = {10.1093/scan/nsv032},
  hasabstract       = {Y},
  journaltitle      = {Social Cognitive and Affective Neuroscience},
  keywords          = {Adult; Bayes Theorem; Biomarkers; Brain; Brain Mapping; Emotions; Female; Humans; Magnetic Resonance Imaging; Male; Motion Pictures; Music; Pattern Recognition, Automated; Young Adult; biological marker; adult; automated pattern recognition; Bayes theorem; brain; brain mapping; emotion; female; human; male; movie; music; nuclear magnetic resonance imaging; physiology; procedures; young adult},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947706348&doi=10.1093%2fscan%2fnsv032&partnerID=40&md5=cce5ddb245e9f926d92aa921e9580a00},
}

@Article{justel2015mo,
  author            = {Justel, Nadia and O'Conor, Jaime and Rubinstein, Wanda},
  title             = {Modulation of emotional memory through music in older adults: A preliminary study; [Modulación de la memoria emocional a través de la música en adultos mayores: Un estudio preliminar]},
  note              = {Cited by: 13},
  number            = {2},
  pages             = {247 – 259},
  volume            = {32},
  abstract          = {In the last decades, different neuroscientific investigations have shown that emotions can be determinant in memory storage and consolidation. Events with emotional content are remembered more easily than neutral events. There are several factors that could affect memory consolidation for emotional events, strengthening or deteriorating them. Stress is one of them, since investigations indicate that moderate levels of stress improve the memory of emotional events, while high or low levels have the opposite effect (Justel, Psyrdellis, & Ruetti, 2013, 2014). Multiple studies showed that the exposition to different musical pieces could modulate memory. Activating music, both positive and negative valence, increases de levels of arousal and strengthen memory consolidation (Judde & Rickard, 2010), while relaxing music has the opposite effect and deteriorate the capacity of emotional memory (Rickard, Wing Wong, & Velik, 2012). However, there are not studies with older adults. The goal of this study was to evaluate how different musical pieces, arousing and relaxing ones, modulate memory consolidation in older adults. 27 participants were included, divided in three groups. 12 slides with emotional content and 12 neutral were presented in a computer, selected from the International Affective Picture System (Lang, Bradley, & Cuthbert, 1995). The older adults watched the emotional and neutral images, and evaluated the arousal /emotionality degree of the images as they filled in a table of five choices, from not exciting to very exciting. According to the assignment group, they were exposed to different musical stimuli: activating or relaxing music for the experimental groups or white noise in the control group. The auditive stimulies were selected according to the previous literature. For the activating musical stimuli, Symphony No. 70 in D major of Joseph Haydn was chosen (Kreutz, Ott, Teichmann, Osawa, & Vaitl, 2008); for the relaxing musical stimuli, Pachebel's canon in D major was chosen (Knight & Rickard, 2001); for the control stimuli, white noise was chosen (Rickard et al., 2012). Then free recall and recognition test were performed, immediately and deferred (one week later). In free recall, each subject briefly listed the images he remembered, mentioning them with a word or a short sentence. For the recognition test, the 24 images were mixed up with 24 new images, and the participants had to indicate if they had seen each image or not, as they filled in a table. Regarding to the evaluation of the arousal / emotionality degree of the images, the results indicate that older adults, no matter the group, punctuated the emotional images as more activating tan neutral images, consolidating, as other studies, the validity and reliability of IAPS. The participants exposed to relaxing music had a worse performance in free recall and recognition, compared with the other two groups. On the other hand, it was expected that participants exposed to activating music have a better performance in free recall and recognition test. In free recall, both immediate and deferred, there is a tendency for the activating group to perform better than the control group, but no significant statistical data was found. Regarding to the recognition test, no significant differences were found between the activating and the control groups. This may be because the sample was formed by a reduced number of participants, but if the sample is extended, the results may change. These results allow to conclude that music modulate the consolidation of visual emotional memory in older adults, being music a useful tool in memory stimulation and a possible therapeutic resource for patients with memory dysfunctions.},
  author_keywords   = {Emotion; Memory; Modulation; Music; Older adults},
  year              = {2015},
  hasabstract       = {Y},
  journaltitle      = {Interdisciplinaria},
  modificationdate  = {2024-05-16T06:35:35},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971513893&partnerID=40&md5=4423b15fe24ccd7cec68d4b87ea5de6c},
}

@Article{winters2014so,
  author            = {Winters, R. Michael and Wanderley, Marcelo M.},
  title             = {Sonification of Emotion: Strategies and results from the intersection with music},
  note              = {Cited by: 12},
  number            = {1},
  pages             = {60 – 69},
  volume            = {19},
  abstract          = {Emotion is a word not often heard in sonification, though advances in affective computing make the data type imminent. At times the relationship between emotion and sonification has been contentious due to an implied overlap with music. This paper clarifies the relationship, demonstrating how it can be mutually beneficial. After identifying contexts favourable to auditory display of emotion, and the utility of its development to research in musical emotion, the current state of the field is addressed, reiterating the necessary conditions for sound to qualify as a sonification of emotion. With this framework, strategies for display are presented that use acoustic and structural cues designed to target select auditory-cognitive mechanisms of musical emotion. Two sonifications are then described using these strategies to convey arousal and valence though differing in design methodology: one designed ecologically, the other computationally. Each model is sampled at 15-second intervals at 49 evenly distributed points on the AV space, and evaluated using a publically available tool for computational music emotion recognition. The computational design performed 65 times better in this test, but the ecological design is argued to be more useful for emotional communication. Conscious of these limitations, computational design and evaluation is supported for future development. © 2014 Cambridge University Press.},
  comment           = {No relevant task, discussed},
  year              = {2014},
  doi               = {10.1017/S1355771813000411},
  hasabstract       = {Y},
  journaltitle      = {Organised Sound},
  modificationdate  = {2024-05-16T12:04:55},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896367343&doi=10.1017%2fS1355771813000411&partnerID=40&md5=61581876533d8231c214659c80642b60},
}

@Article{keelawat2019su,
  author           = {Keelawat, Panayu and Thammasan, Nattapong and Kijsirikul, Boonserm and Numao, Masayuki},
  title            = {Subject-Independent Emotion Recognition During Music Listening Based on EEG Using Deep Convolutional Neural Networks},
  year             = {2019},
  month            = mar,
  abstract         = {Emotion recognition during music listening using electroencephalogram (EEG) has gained more attention from researchers, recently.Many studies focused on accuracy on one subject while subject-independent performance evaluation was still unclear.In this paper, the objective is to create an emotion recognition model that can be applied to multiple subjects.By adopting convolutional neural networks (CNNs), advantage could be gained from utilizing information from electrodes and time steps.Using CNNs also does not need feature extraction which might leave out other related but unobserved features.CNNs with three to seven convolutional layers were deployed in this research.We measured their performance with a binary classification task for compositions of emotions including arousal and valence.The results showed that our method captured EEG signal patterns from numerous subjects by 10-fold cross validation with 81.54{\%} and 86.87{\%} accuracy from arousal and valence respectively.The method also showed a higher capability of generalization to unseen subjects than the previous method as can be observed from the results of leave-one-subject-out validation.},
  bdsk-url-1       = {https://doi.org/10.1109/cspa.2019.8696054},
  c1               = {Department of Computer Engineering Chulalongkorn University Bangkok, Thailand; Nattapong Thammasan Human Media Interaction University of Twente Enschede, Netherlands; The Institute of Scientific and Industrial Research Osaka University Osaka, Japan; Human Media Interaction, University of Twente, Enschede, Netherlands.; Department of Computer Engineering Chulalongkorn University Bangkok, Thailand; Nattapong Thammasan Human Media Interaction University of Twente Enschede, Netherlands; The Institute of Scientific and Industrial Research Osaka University Osaka, Japan; Department of Computer Engineering Chulalongkorn University Bangkok, Thailand; Nattapong Thammasan Human Media Interaction University of Twente Enschede, Netherlands; The Institute of Scientific and Industrial Research Osaka University Osaka, Japan},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1109/cspa.2019.8696054},
  hasabstract      = {Y},
  keywords         = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Epilepsy Detection},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.1109/cspa.2019.8696054},
}

@Article{mollahosseini2019af,
  author        = {Mollahosseini, Ali and Hasani, Behzad and Mahoor, Mohammad},
  journal       = {IEEE transactions on affective computing},
  title         = {AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild},
  year          = {2019},
  month         = jan,
  number        = {1},
  pages         = {18--31},
  volume        = {10},
  abstract      = {Automated affective computing in the wild setting is a challenging problem in computer vision. Existing annotated databases of facial expressions in the wild are small and mostly cover discrete emotions (aka the categorical model). There are very limited annotated facial databases for affective computing in the continuous dimensional model (e.g., valence and arousal). To meet this need, we collected, annotated, and prepared for public distribution a new database of facial emotions in the wild (called AffectNet). AffectNet contains more than 1, 000, 000 facial images from the Internet by querying three major search engines using 1250 emotion related keywords in six different languages. About half of the retrieved images were manually annotated for the presence of seven discrete facial expressions and the intensity of valence and arousal. AffectNet is by far the largest database of facial expression, valence, and arousal in the wild enabling research in automated facial expression recognition in two different emotion models. Two baseline deep neural networks are used to classify images in the categorical model and predict the intensity of valence and arousal. Various evaluation metrics show that our deep neural network baselines can perform better than conventional machine learning methods and off-the-shelf facial expression recognition systems.},
  bdsk-url-1    = {https://doi.org/10.1109/taffc.2017.2740923},
  c1            = {Department of Electrical and Computer Engineering, University of Denver, Denver, CO; Department of Electrical and Computer Engineering, University of Denver, Denver, CO; Department of Electrical and Computer Engineering, University of Denver, Denver, CO},
  comment       = {facial recognition},
  date-added    = {2024-05-13 15:05:25 +0100},
  date-modified = {2024-05-13 15:05:25 +0100},
  doi           = {10.1109/taffc.2017.2740923},
  hasabstract   = {Y},
  isbn          = {1949-3045},
  keywords      = {Affective Computing; Emotion Recognition; Emotional Expressions; Facial Expression; Face Perception},
  la            = {en},
  priority      = {prio3},
  publisher     = {Institute of Electrical and Electronics Engineers},
  url           = {https://doi.org/10.1109/taffc.2017.2740923},
}

@Article{livingstone2018th,
  author        = {Livingstone, Steven and Russo, Frank},
  journal       = {PloS one},
  title         = {The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English},
  year          = {2018},
  month         = may,
  number        = {5},
  pages         = {e0196391--e0196391},
  volume        = {13},
  abstract      = {The RAVDESS is a validated multimodal database of emotional speech and song. The database is gender balanced consisting of 24 professional actors, vocalizing lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, and fearful emotions. Each expression is produced at two levels of emotional intensity, with an additional neutral expression. All conditions are available in face-and-voice, face-only, and voice-only formats. The set of 7356 recordings were each rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity and test-retest intrarater reliability were reported. Corrected accuracy and composite "goodness" measures are presented to assist researchers in the selection of stimuli. All recordings are made freely available under a Creative Commons license and can be downloaded at https://doi.org/10.5281/zenodo.1188976.},
  bdsk-url-1    = {https://doi.org/10.1371/journal.pone.0196391},
  c1            = {Department of Computer Science and Information Systems, University of Wisconsin-River Falls, Wisconsin, WI, United States of America; Department of Psychology, Ryerson University, Toronto, Canada; Department of Psychology, Ryerson University, Toronto, Canada},
  date-added    = {2024-05-13 15:05:25 +0100},
  date-modified = {2024-05-13 15:05:25 +0100},
  doi           = {10.1371/journal.pone.0196391},
  hasabstract   = {Y},
  isbn          = {1932-6203},
  keywords      = {Speech Emotion; Emotion Recognition; Facial Expression; Affective Computing; Speech Perception},
  la            = {en},
  priority      = {prio1},
  publisher     = {Public Library of Science},
  ranking       = {rank2},
  url           = {https://doi.org/10.1371/journal.pone.0196391},
}

@Article{alakus2020em,
  author        = {Alaku{\c s}, Talha and T{\"u}rko{\u g}lu, İ̇brahim},
  journal       = {Electronics letters},
  title         = {Emotion recognition with deep learning using GAMEEMO data set},
  year          = {2020},
  month         = oct,
  number        = {25},
  pages         = {1364--1367},
  volume        = {56},
  abstract      = {Electronics LettersVolume 56, Issue 25 p. 1364-1367 Special Issue: Current Trends in Cognitive Science and Brain Computing Research and ApplicationsFree Access Emotion recognition with deep learning using GAMEEMO data set T. B. Alakus, Corresponding Author T. B. Alakus talhaburakalakus{\char64}klu.edu.tr orcid.org/0000-0003-3136-3341 Kirklareli University, Faculty of Engineering, Department of Software Engineering, Kirklareli, TurkeySearch for more papers by this authorI. Turkoglu, I. Turkoglu Firat University, Faculty of Technology, Elazig, TurkeySearch for more papers by this author T. B. Alakus, TurkeySearch for more papers by this author First published: 22 October 2020 https://doi.org/10.1049/el.2020.2460Citations: 10AboutSectionsPDF ToolsRequest permissionExport citationAdd to favoritesTrack citation ShareShare Give accessShare full text accessShare full-text accessPlease review our Terms and Conditions of Use and check box below to share full-text version of article.I have read and accept the Wiley Online Library Terms and Conditions of UseShareable LinkUse the link below to share a full-text version of this article with your friends and colleagues. Learn more.Copy URL Share a linkShare onFacebookTwitterLinkedInRedditWechat Abstract Emotion recognition is actively used in brain--computer interface, health care, security, e-commerce, education and entertainment applications to increase and control human--machine interaction. Therefore, emotions affect people's lives and decision-making mechanisms throughout their lives. However, the fact that emotions vary from person to person, being an abstract concept and being dependent on internal and external factors makes the studies in this field difficult. In recent years, studies based on electroencephalography (EEG) signals, which perform emotion analysis in a more robust and reliable way, have gained momentum. In this article, emotion analysis based on EEG signals was performed to predict positive and negative emotions. The study consists of four parts. In the first part, EEG signals were obtained from the GAMEEMO data set. In the second stage, the spectral entropy values of the EEG signals of all channels were calculated and these values were classified by the bidirectional long-short term memory architecture in the third stage. In the last stage, the performance of the deep-learning architecture was evaluated with accuracy, sensitivity, specificity and receiver operating characteristic (ROC) curve. With the proposed method, an accuracy of 76.91{\%} and a ROC value of 90{\%} were obtained. Introduction Emotion can be defined as the voluntary or involuntary reaction of people against an external stimulus while performing actions such as talking, thinking, communicating, learning, making decisions etc. Since all these and similar actions are carried out through emotions, emotions have a great impact on daily life. While negative emotions affect people both physically and psychologically, positive emotions make people more successful in society and bring better living conditions {$[$}1, 2 {$]$}. There are many different emotion analysis studies in order to comprehend the nature and behaviour of emotions. However, the fact that the concept of emotion is abstract and does not have an objective result makes it difficult to analyse emotions {$[$}3, 4 {$]$}. In addition, a large number of methods to collect and process emotion data makes the analysis process more difficult and time-consuming. For these reasons, a computer-based system is needed {$[$}5 {$]$}. Emotions can be obtained through physical and non-physical methods. Examples of these include voice signals, body language, facial expressions and physical activities. Since these methods are easy to apply, data can be obtained quickly and easily. However, during the data collection phase, emotions can be manipulated intentionally or unintentionally by the subjects. While the voice signals are collected, subjects can imitate their voices and similarly hide their facial expressions {$[$}6 {$]$}. Therefore, the fact that the data obtained by these methods are both incomplete and untrustworthy caused the need for a more reliable system and increased the importance of physiological signals such as electroencephalography (EEG) {$[$}7 {$]$}. EEG signals are the most widely used method in this area because of their ease of use, low cost and portable models {$[$}8 {$]$}. There are two types of emotional patterns in the literature, discrete and dimensional. There are eight basic emotions (anger, joy, trust, fear, surprise, sadness, disgust and anticipation) in the discrete emotion model {$[$}9 {$]$}. In the dimensional model, emotions are expressed not by their names but according to their positions in the arousal--valence plane {$[$}10 {$]$}. In this plane, emotions are divided into four main areas. Valence axis refers to the x -axis, and this axis indicates whether the emotion is negative or positive. Y -axis expresses arousal and emotions are ordered from low to high according to the degree of activity. The arousal--valence plane is given in Fig. 1. The plane is divided into four different zones, as can be seen in Fig. 1. While there are high arousal positive valence emotions in the first zone, there are high valence negative emotions in the second zone. In the third and fourth zones, there are emotions of negative valence-low arousal and positive valence-low arousal, respectively. In this model, emotions are named according to their location in the coordinate plane rather than their names. For example, the emotion of happiness is expressed as high arousal positive valence. Similar inferences can be made for other types of emotions. In this study, dimensional emotion model was used and emotions were evaluated as positive-valence and negative-valence. Fig. 1Open in figure viewerPowerPoint Example of arousal--valence dimensional emotion model The study consists of four stages. In the first step, EEG signals were collected from the GAMEEMO data set. In the second step, the spectral entropy values of each EEG signal were calculated. Then these values were classified with the bidirectional long-short term memory (BiLSTM) deep-learning model and the prediction process was carried out. In the last stage, the performance of the BiLSTM model was measured with different evaluation metrics. The main contributions of the study can be summarised as follows: To the best of our knowledge, the GAMEEMO data set was analysed for the first time in this study with the BiLSTM deep-learning model. With this study, it was observed that EEG signals obtained from a portable device can also be used for emotion analysis. The rest of the work is organized as follows: studies conducted with EEG signals are mentioned in the related works section. In the data and methods section, general information about the data set, the spectral entropy and BiLSTM model used in this study are given. In the application results section, the performance of the BiLSTM model was examined and the results were discussed. In the conclusion section, the study was examined and explanations were made based on possible future applications. Related works In this section, emotion analysis studies performed with EEG signals are examined. The authors in {$[$}11 {$]$} used the LSTM model for emotion prediction and classified the EEG signals for it. The DEAP data set was used in the study and a classification process was performed for valence, arousal and liking classes. The signals have not been preprocessed and classified directly in the LSTM architecture. The study was validated with four-fold cross-validation and the performance of the LSTM model was evaluated with the accuracy metric only. At the end of the study, the accuracy of 0.8565 for arousal, 0.8545 for valence and 0.8799 for liking was obtained. Authors in {$[$}12 {$]$} carried out emotion analysis using deep learning. The DEAP data set was used in the study and the feature extraction was carried out before classification. In the feature extraction phase, the signals were transformed with empirical model decomposition and variational model decomposition and the power spectral density, and the first difference values of intrinsic mode functions were collected from these converted signals. Then the signals were classified with both support vector machines (SVMs) and deep neural network. After the classification process, accuracies of 0.6125 for arousal and 0.6250 for valence were reached. The authors in {$[$}13 {$]$} performed emotion analysis with principal component analysis and deep learning. As in other studies, the DEAP data set was used in this study. The signals were first transformed into five different bands with fast Fourier transform and power spectral values were obtained from each band. Then the values were normalised and classified with deep-learning network. Valence emotions were predicted with 0.5342 and arousal emotions with 0.5205 accuracies with the proposed method. Data and methods In this study, EEG signals belonging to the GAMEEMO {$[$}14 {$]$} data set are used. The data set contains EEG signals of 28 people. Unlike conventional EEG collecting devices, the data were obtained with a portable EEG device (Emotiv EPOC + 14-Channel Wireless EEG Headset). The EEG device used has 14 channels in total as AF3, AF4, F3, F4, F7, F8, FC5, FC6, O1, O2, P7, P8, T7 and T8. The sampling rate of the obtained signals is 128 Hz. The data set contains raw and preprocessed signals. Since noise-free data were used in this study, pre-processed data were considered. In order to obtain emotions, the subjects played four computer games and each subject played games for 5 min. There are a total of 1568 (4 ×14 ×28) EEG data in the data set. The number 4 refers to the stimuli used. This value is 4 because 4 games were played in the data set. The number 14 indicates the number of EEG channels, while number 28 refers to the subjects. Sample length of each EEG data is 38, 252. More technical and detailed information about the data set can be obtained from {$[$}14 {$]$}. Researchers who want to use GAMEEMO data can access the data from the link provided (https://data.mendeley.com/datasets/b3pn4kwpmn/3 ). In the study, feature extraction was carried out and spectral entropy values of EEG signals were calculated. Spectral entropy measures how sharp the spectrum of a signal is {$[$}15 {$]$}. A signal with a sharp spectrum, such as the sum of sinusoids, has low spectral entropy. In contrast, a flat spectrum signal such as white noise has high spectral entropy. Spectral entropy treats the normalised power distribution in the frequency domain of the signal as a probability distribution and calculates the Shannon entropy. Shannon entropy in this context is the spectral entropy of the signal. Spectral entropy is effectively used in fault detection and diagnosis {$[$}16, 17 {$]$}, speech recognition {$[$}18 {$]$} and biomedical signal processing {$[$}19 {$]$}. In this study, the spectral entropy value of each EEG signal was calculated and these values were used to classify with BiLSTM. In this study, recurrent neural network was used instead of traditional CNN architectures because of their success in time series applications {$[$}20-22 {$]$}. For this reason, a recurrent neural network model --bidirectional LSTM, was used in the proposed study. Bidirectional LSTMs are an extension of the LSTM model and have been proposed to improve model performance in classification problems. In the BiLSTM architecture, input values train two LSTMs instead of one. Therefore, information flows both from the past to the future and from the future to the past. In traditional LSTM architectures, information from the future is evaluated and preserved, while in BLSTM architecture, information from both the past and the future is preserved and valued. Owing to this advantage, BiLSTM is more successful than LSTM {$[$}23 {$]$}. Thus, BiLSTM was considered in the study. The graphical abstract of the study is given in Fig. 2. Fig. 2Open in figure viewerPowerPoint Flow chart of the study Application results In this study, the EEG signals of the GAMEEMO data set were classified and positive--negative emotions were predicted. BiLSTM was used for the classification process and the performance of the deep-learning model was measured with accuracy, specificity and receiver operating characteristic (ROC) values. The parameters of the developed BiLSTM model can be summarised as follows: EEG data, whose spectral entropy values were calculated, were used in the input layer. Then the 128-unit BiLSTM layer was designed. ReLU function was used as an activation function. Then, the data were transformed into a one-dimensional vector by the flattening process. Later, the batch normalisation was performed and the data were normalised. Dropout was used to prevent overfitting problem and its degree was set to 0.25. Finally, a fully connected layer was designed and the number of neurons was determined as 512. In the classification layer, the sigmoid function has been used and the binary classification process has been made. Stochastic gradient descent was applied as an optimiser with default values. The loss of the model was calculated by binary cross-entropy. The epoch value was chosen to be 250. To validate the model, the train-test split approach was used and 80{\%} of the data was used for training and 20{\%} for testing. All of these parameters were determined by trial and error approach and the parameters giving the best result were used in the study. Table 1 shows the classification results of the BiLSTM model. Table 1. Classification results of positive and negative emotions Accuracy, {\%} Sensitivity, {\%} Specificity, {\%} ROC 76.91 76.93 76.89 0.90 As seen in Table 1, positive and negative emotions were classified with an accuracy rate of 0.7691 with the proposed BiLSTM model. In addition, the sensitivity value was 0.7693 and the specificity value was 0.7689. ROC value was measured as 0.90 for both classes. The graph of the ROC is given in Fig. 3. The area under the curve (AUC) score is used effectively in biomedical studies and is expressed as a better analysis {$[$}24 {$]$}. In order for the classification process to be considered good, the AUC score must be > 0.8 {$[$}24 {$]$}. Furthermore, the AUC score between 0.9 and 1.0 indicates that the classification is excellent {$[$}24 {$]$}. In this study, the AUC score was calculated as 0.9, indicating that the proposed method is effective and successful. Fig. 3Open in figure viewerPowerPoint ROC curve of positive and negative emotions (class 0 refers to negative emotions, class 1 refers to positive emotions) These results were also compared with the machine-learning algorithm results used in the original article. The comparison results are given in Table 2. Since there is only one study in the literature with this data set, only the results in the original article could be examined. According to the results given in Table 2, it is seen that the proposed deep-learning method was better than the machine-learning algorithms used in the original study. While 73 and 66{\%} accuracy values were achieved for SVM and KNN, respectively, this rate increased to ∼77{\%} with the BiLSTM model. According to these results, it can be inferred that the deep-learning method is at least as successful and even better as existing machine-learning methods. Table 2. Comparison of classification results Reference SVM (accuracy) KNN (accuracy) BiLSTM (accuracy) {$[$}14 {$]$} 73{\%} 66{\%} ---the proposed method ------76.93{\%} Conclusion In this study, positive and negative emotions were analysed using the EEG data of the GAMEEMO data set. In the first part of the study, pre-processed data were obtained from the data set. Then, spectral entropy values were collected from the data of each EEG channel and these values were used in the BiLSTM model. In the final phase, the classification process was made with BiLSTM and the performance of the deep-learning model was measured with accuracy, specificity and ROC values. With the proposed method, 76.91{\%} accuracy, 76.93{\%} sensitivity, 76.89{\%} specificity and 90{\%} ROC values were achieved. In addition, the proposed method was compared with the machine-learning algorithms used in the original article and it was observed that the proposed method was at least as successful as them. In the future, this data set will be examined in more detail and comparisons will be made using different deep-learning algorithms and signal processing methods. Emotions will be examined with both binary-class classification and multi-class classification. Emotions are of great importance in human life. In daily life, we use our emotions intentionally or unintentionally. Therefore, emotion analysis studies are important for understanding emotions and determining their behaviour. References 1Naji M. Firoozabadi M. Azadfallah P.: 'Emotion classification during music listening from forehead biosignals ', Signal. Image. Video. Process., 2015, 9, pp. 1365--1375, doi: 10.1007/s11760-013-0591-6 2Alakus T.B. Turkoglu I.: 'EEG based emotion analysis systems ', TBV J. Comput. Sci. Eng., 2018, 11, (1 ), pp. 26--39 3Michalopoulos K. Bourbakis N.: 'Application of multiscale entropy on EEG signals for emotion detection '. IEEE EMBS Int. Conf. Information Technology Applications in Biomedicine, Orlando, FL, USA, February 2017, pp. 341--344, doi: 10.1109/BHI.2017.7897275 4Alakus T.B. Turkoglu I.: 'Feature selection with sequential forward selection algorithm from emotion estimation based on EEG signals ', Sakarya Univ. J. Sci., 2019, 23, (6 ), pp. 1096--1105, doi: 10.16984/saufenbilder.501799 5Turnip A. Simbolon A.I. Amri M.F. et al.: 'Backpropagation neural networks training for EEG-SSVEP classification of emotion recognition ', Internetwork. Indonesia J., 2017, pp. 53--57 6Alakus T.B. Turkoglu I.: 'Determination of effective EEG channels for discrimination of positive and negative emotions with wavelet decomposition and support vector machines ', Int. J. Inform. Technol., 12, (3 ), pp. 229--237 7Yan J. Chen S. Deng S.: 'A EEG-based emotion recognition model with rhythm and time characteristics ', Brain. Inform., 6, pp. 1--8, doi: 10.1186/s40708-019-0100-y 8Pan J. Li Y. Wang J.: 'An EEG-based brain-computer interface for emotion recognition '. Int. Joint Conf. Neural Networks, Vancouver, BC, Canada, July 2016, pp. 2063--2067, doi: 10.1109/IJCNN.2016.7727453 9Mason W.A. Capitanio J.P.: 'Basic emotions: a reconstruction ', Emot. Rev., 2012, 4, pp. 238--244, doi: 10.1177/1754073912439763 10Russel A.: 'Core affect and psychological construction of emotion ', Psychol. Rev., 2003, 110, pp. 145--150 11Alhagry S. Fahmy A.A. El-Khoribi R.A.: 'Emotion recognition based on EEG using LSTM recurrent neural network ', Int. J. Adv. Comput. Sci. Appl., 8, (10 ), pp. 355--358, doi: 10.14569/IJACSA.2017.081046 12Pandey P. Seeja K.R.: 'Subject independent emotion recognition from EEG using VMD and deep learning ', J. King Saud Univ. --Comput. Inform. Sci., pp. 53--58, doi: 10.1016/j.jksuci.2019.11.003 13Jirayucharoensak S. Pan-Ngum S. Israsena P.: 'EEG-based emotion recognition using deep learning network with principal component based covariate shift adaptation ', Sci. World J., 2014, pp. 1--10, 627892, doi: 10.1155/2014/627892 14Alakus T.B. Gonen M. Turkoglu I.: 'Database for an emotion recognition system based on EEG signals and various computer games --GAMEEMO ', Biomed. Signal Proc. Control, 2020, 60, pp. 1--12, doi: 10.1016/j.bspc.2020.101951 15Vanluchene A.L.G. Vereecke H. Thas O. et al.: 'Spectral entropy as an electroencephalographic measure of anesthetic drug effect: a comparison with bispectral index and processed midlatency auditory evoked response ', Anesthesiology, 2004, 101, pp. 34--42 16Pan Y.N. Chen J. Li X.L.: 'Spectral entropy: a complementary index for rolling element bearing performance degradation assessment ', Proc. Inst. Mech. Eng. C, J. Mech. Eng. Sci., 2009, 223, (5 ), pp. 1223--1231, doi: 10.1243/09544062JMES1224 17Sharma V. Parey A.: 'A review of gear fault diagnosis using various condition indicators ', Procedia Eng., 2016, 144, pp. 256--263, doi: 10.1016/j.proeng.2016.05.131 18Majstorovic N. Andric M. Mikluc D.: 'Entropy-based algorithm for speech recognition in noisy environment '. Telecommunications Forum, Belgrade, Serbia, November 2011, pp. 667--670, doi: 10.1109/TELFOR.2011.6143635 19Vakkuri A. Yli-Hankala A. Talja P. et al.: 'Time-frequency balanced spectral entropy as a measure of anesthetic drug effect in central nervous system during sevoflurane, propofol, and thiopental anesthesia ', Acta Anaesthesiol. Scand., 48, (2 ), pp. 145--153, doi: 10.1111/j.0001-5172.2004.00323.x 20Hewamalage H. Bergmeir C. Bandara K.: 'Recurrent neural networks for time series forecasting: current status and future directions ', Int. J. Forecast., pp. 1--40, doi: 10.1016/j.ijforecast.2020.06.008 21Guo T. Xu Z. Yao X. et al.: 'Robust online time series prediction with recurrent neural networks '. Int. Conf. Data Science and Advanced Analytics, Montreal, QC, October 2016, pp. 816--825, doi: 10.1109/DSAA.2016.92 22Connor J.T. Martin R.D. Atlas L.E.: 'Recurrent neural networks and robust time series prediction ', IEEE Trans. Neural Netw., 1994, 5, pp. 240--254, doi: 10.1109/72.279188 23Graves A. Fernandez S. Schmidhuber J.: 'Bidirectional LSTM networks for improved phoneme classification and recognition '. Int. Conf. Artificial Neural Networks: Formal Models and Their Applications, Warsaw, Poland, September 2005, doi: 10.1007/11550907{\_}126 24Mandrekar J.N.: 'Receiver operating characteristic curve in diagnostic test assessment ', J. Thorac. Oncol., 2010, (9 ), pp. 1315--1316, doi: 10.1097/JTO.0b013e3181ec173d Citing Literature Volume56, Issue25December 2020Pages 1364-1367 FiguresReferencesRelatedInformation},
  bdsk-url-1    = {https://doi.org/10.1049/el.2020.2460},
  c1            = {Kirklareli UniversityFaculty of EngineeringDepartment of Software EngineeringKirklareliTurkey; Firat UniversityFaculty of TechnologyDepartment of Software EngineeringElazigTurkey},
  date-added    = {2024-05-13 15:05:25 +0100},
  date-modified = {2024-05-13 15:05:25 +0100},
  doi           = {10.1049/el.2020.2460},
  hasabstract   = {Y},
  isbn          = {0013-5194},
  keywords      = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Speech Emotion},
  la            = {en},
  priority      = {prio1},
  publisher     = {Institution of Engineering and Technology},
  ranking       = {rank2},
  url           = {https://doi.org/10.1049/el.2020.2460},
}

@Article{katsigiannis2018dr,
  author           = {Katsigiannis, Stamos and Ramzan, Naeem},
  journal          = {IEEE journal of biomedical and health informatics},
  title            = {DREAMER: A Database for Emotion Recognition Through EEG and ECG Signals From Wireless Low-cost Off-the-Shelf Devices},
  year             = {2018},
  month            = jan,
  number           = {1},
  pages            = {98--107},
  volume           = {22},
  abstract         = {In this paper, we present DREAMER, a multimodal database consisting of electroencephalogram (EEG) and electrocardiogram (ECG) signals recorded during affect elicitation by means of audio-visual stimuli. Signals from 23 participants were recorded along with the participants self-assessment of their affective state after each stimuli, in terms of valence, arousal, and dominance. All the signals were captured using portable, wearable, wireless, low-cost, and off-the-shelf equipment that has the potential to allow the use of affective computing methods in everyday applications. A baseline for participant-wise affect recognition using EEG and ECG-based features, as well as their fusion, was established through supervised classification experiments using support vector machines (SVMs). The self-assessment of the participants was evaluated through comparison with the self-assessments from another study using the same audio-visual stimuli. Classification results for valence, and dominance of the proposed database are comparable to the ones achieved for other databases that use nonportable, expensive, medical grade devices. These results indicate the prospects of using low-cost devices for affect recognition applications. The proposed database will be made publicly available in order to allow researchers to achieve a more thorough evaluation of the suitability of these capturing devices for affect recognition applications.},
  bdsk-url-1       = {https://doi.org/10.1109/jbhi.2017.2688239},
  c1               = {School of Engineering and Computing, University of the West of Scotland, Paisley, U.K.; School of Engineering and Computing, University of the West of Scotland, Paisley, U.K.},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1109/jbhi.2017.2688239},
  hasabstract      = {Y},
  isbn             = {2168-2194},
  keywords         = {Emotion Recognition; Affective Computing; Deep Learning for EEG; EEG Analysis; Emotion Regulation},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Institute of Electrical and Electronics Engineers},
  url              = {https://doi.org/10.1109/jbhi.2017.2688239},
}

@Article{trigeorgis2016ad,
  author           = {Trigeorgis, George and Ringeval, Fabien and Brueckner, Raymond and Marchi, Erik and Nicolaou, Mihalis and Schuller, Bj{\"o}rn and Zafeiriou, Stefanos},
  title            = {Adieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network},
  year             = {2016},
  month            = mar,
  abstract         = {The automatic recognition of spontaneous emotions from speech is a challenging task. On the one hand, acoustic features need to be robust enough to capture the emotional content for various styles of speaking, and while on the other, machine learning algorithms need to be insensitive to outliers while being able to model the context. Whereas the latter has been tackled by the use of Long Short-Term Memory (LSTM) networks, the former is still under very active investigations, even though more than a decade of research has provided a large set of acoustic descriptors. In this paper, we propose a solution to the problem of `context-aware'emotional relevant feature extraction, by combining Convolutional Neural Networks (CNNs) with LSTM networks, in order to automatically learn the best representation of the speech signal directly from the raw time representation. In this novel work on the so-called end-to-end speech emotion recognition, we show that the use of the proposed topology significantly outperforms the traditional approaches based on signal processing techniques for the prediction of spontaneous and natural emotions on the RECOLA database.},
  bdsk-url-1       = {https://doi.org/10.1109/icassp.2016.7472669},
  c1               = {Department of Computing, Imperial College London, London, UK; Universitat Passau, Passau, Bayern, DE; MMK, Technische Universitat Munchen, Munich, Germany; Nuance Communications Deutschland GmbH, Germany; MMK, Technische Universitat Munchen, Munich, Germany; Department of Computing, Goldsmiths, University of London, UK; AudEERING UG, Germany; Chair of Complex \& Intelligent Systems, University of Passau, Passau, Germany; Department of Computing, Imperial College London, London, UK; Department of Computing, Imperial College London, London, UK},
  comment          = {no mention of music},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1109/icassp.2016.7472669},
  hasabstract      = {Y},
  keywords         = {Emotion Recognition; Affective Computing; Feature Extraction; Audio-Visual Speech Recognition; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.1109/icassp.2016.7472669},
}

@Article{zheng2019id,
  author           = {Zheng, Wei‐Long and Zhu, Jie and Lu, Bao‐Liang},
  journal          = {IEEE transactions on affective computing},
  title            = {Identifying Stable Patterns over Time for Emotion Recognition from EEG},
  year             = {2019},
  month            = jul,
  number           = {3},
  pages            = {417--429},
  volume           = {10},
  abstract         = {In this paper, we investigate stable patterns of electroencephalogram (EEG) over time for emotion recognition using a machine learning approach. Up to now, various findings of activated patterns associated with different emotions have been reported. However, their stability over time has not been fully investigated yet. In this paper, we focus on identifying EEG stability in emotion recognition. We systematically evaluate the performance of various popular feature extraction, feature selection, feature smoothing and pattern classification methods with the DEAP dataset and a newly developed dataset called SEED for this study. Discriminative Graph regularized Extreme Learning Machine with differential entropy features achieves the best average accuracies of 69.67 and 91.07 percent on the DEAP and SEED datasets, respectively. The experimental results indicate that stable patterns exhibit consistency across sessions; the lateral temporal areas activate more for positive emotions than negative emotions in beta and gamma bands; the neural patterns of neutral emotions have higher alpha responses at parietal and occipital sites; and for negative emotions, the neural patterns have significant higher delta responses at parietal and occipital sites and higher gamma responses at prefrontal sites. The performance of our emotion recognition models shows that the neural patterns are relatively stable within and between sessions.},
  bdsk-url-1       = {https://doi.org/10.1109/taffc.2017.2712143},
  c1               = {Department of Computer Science and Engineering, Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1109/taffc.2017.2712143},
  hasabstract      = {Y},
  isbn             = {1949-3045},
  keywords         = {Deep Learning for EEG; Emotion Recognition; EEG Analysis; Epilepsy Detection; Affective Computing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Institute of Electrical and Electronics Engineers},
  url              = {https://doi.org/10.1109/taffc.2017.2712143},
}

@Article{jirayucharoensak2014ee,
  author           = {Jirayucharoensak, Suwicha and Pan-ngum, Setha and Israsena, Pasin},
  journal          = {The scientific world journal/TheScientificWorldjournal},
  title            = {EEG-Based Emotion Recognition Using Deep Learning Network with Principal Component Based Covariate Shift Adaptation},
  year             = {2014},
  month            = jan,
  pages            = {1--10},
  volume           = {2014},
  abstract         = {Automatic emotion recognition is one of the most challenging tasks. To detect emotion from nonstationary EEG signals, a sophisticated learning algorithm that can represent high-level abstraction is required. This study proposes the utilization of a deep learning network (DLN) to discover unknown feature correlation between input signals that is crucial for the learning task. The DLN is implemented with a stacked autoencoder (SAE) using hierarchical feature learning approach. Input features of the network are power spectral densities of 32-channel EEG signals from 32 subjects. To alleviate overfitting problem, principal component analysis (PCA) is applied to extract the most important components of initial input features. Furthermore, covariate shift adaptation of the principal components is implemented to minimize the nonstationary effect of EEG signals. Experimental results show that the DLN is capable of classifying three different levels of valence and arousal with accuracy of 49.52{\%} and 46.03{\%}, respectively. Principal component based covariate shift adaptation enhances the respective classification accuracy by 5.55{\%} and 6.53{\%}. Moreover, DLN provides better performance compared to SVM and naive Bayes classifiers.},
  bdsk-url-1       = {https://doi.org/10.1155/2014/627892},
  c1               = {Department of Computer Engineering, Faculty of Engineering, Chulalongkorn University, Bangkok 10330, Thailand; National Electronics and Computer Technology Center, Thailand Science Park, Khlong Luang, Pathum Thani 12120, Thailand; Department of Computer Engineering, Faculty of Engineering, Chulalongkorn University, Bangkok 10330, Thailand; National Electronics and Computer Technology Center, Thailand Science Park, Khlong Luang, Pathum Thani 12120, Thailand},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1155/2014/627892},
  hasabstract      = {Y},
  isbn             = {1537-744X},
  keywords         = {Deep Learning for EEG; Emotion Recognition; EEG Analysis; Deep Learning; Affective Computing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Hindawi Publishing Corporation},
  url              = {https://doi.org/10.1155/2014/627892},
}

@Article{zhao2016em,
  author           = {Zhao, M. and Adib, Fadel and Katabi, Dina},
  title            = {Emotion recognition using wireless signals},
  year             = {2016},
  month            = oct,
  abstract         = {This paper demonstrates a new technology that can infer a person's emotions from RF signals reflected off his body. EQ-Radio transmits an RF signal and analyzes its reflections off a person's body to recognize his emotional state (happy, sad, etc.). The key enabler underlying EQ-Radio is a new algorithm for extracting the individual heartbeats from the wireless signal at an accuracy comparable to on-body ECG monitors. The resulting beats are then used to compute emotion-dependent features which feed a machine-learning emotion classifier. We describe the design and implementation of EQ-Radio, and demonstrate through a user study that its emotion recognition accuracy is on par with state-of-the-art emotion recognition systems that require a person to be hooked to an ECG monitor.},
  bdsk-url-1       = {https://doi.org/10.1145/2973750.2973762},
  c1               = {Massachusetts Institute of Technology; Massachusetts Institute of Technology; Massachusetts Institute of Technology},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1145/2973750.2973762},
  hasabstract      = {Y},
  keywords         = {Emotion Recognition; Deep Learning for EEG; Affective Computing; EEG Analysis; ECG Signal},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.1145/2973750.2973762},
}

@Article{baveye2015li,
  author           = {Baveye, Yoann and Dellandr{\'e}a, Emmanuel and Chamaret, Christel and Chen, Liming},
  journal          = {IEEE transactions on affective computing},
  title            = {LIRIS-ACCEDE: A Video Database for Affective Content Analysis},
  year             = {2015},
  month            = jan,
  number           = {1},
  pages            = {43--55},
  volume           = {6},
  abstract         = {Research in affective computing requires ground truth data for training and benchmarking computational models for machine-based emotion understanding.In this paper, we propose a large video database, namely LIRIS-ACCEDE, for affective content analysis and related applications, including video indexing, summarization or browsing.In contrast to existing datasets with very few video resources and limited accessibility due to copyright constraints, LIRIS-ACCEDE consists of 9, 800 good quality video excerpts with a large content diversity.All excerpts are shared under Creative Commons licenses and can thus be freely distributed without copyright issues.Affective annotations were achieved using crowdsourcing through a pair-wise video comparison protocol, thereby ensuring that annotations are fully consistent, as testified by a high inter-annotator agreement, despite the large diversity of raters' cultural backgrounds.In addition, to enable fair comparison and landmark progresses of future affective computational models, we further provide four experimental protocols and a baseline for prediction of emotions using a large set of both visual and audio features.The dataset (the video clips, annotations, features and protocols) is publicly},
  bdsk-url-1       = {https://doi.org/10.1109/taffc.2015.2396531},
  c1               = {Extraction de Caract{\'e}ristiques et Identification; Technicolor R \& I {$[$}Cesson S{\'e}vign{\'e}{$]$}; Extraction de Caract{\'e}ristiques et Identification; Technicolor R \& I {$[$}Cesson S{\'e}vign{\'e}{$]$}; Extraction de Caract{\'e}ristiques et Identification},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1109/taffc.2015.2396531},
  hasabstract      = {Y},
  isbn             = {1949-3045},
  keywords         = {Affective Computing; Emotion Recognition; Databases; Video Summarization; Music Information Retrieval},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Institute of Electrical and Electronics Engineers},
  url              = {https://doi.org/10.1109/taffc.2015.2396531},
}

@Article{zhuang2017em,
  author           = {Zhuang, Ning and Zeng, Ying and Li, Tong and Zhang, Chi and Zhang, Hanming and Yan, Bin},
  journal          = {BioMed research international},
  title            = {Emotion Recognition from EEG Signals Using Multidimensional Information in EMD Domain},
  year             = {2017},
  month            = jan,
  pages            = {1--9},
  volume           = {2017},
  abstract         = {This paper introduces a method for feature extraction and emotion recognition based on empirical mode decomposition (EMD). By using EMD, EEG signals are decomposed into Intrinsic Mode Functions (IMFs) automatically. Multidimensional information of IMF is utilized as features, the first difference of time series, the first difference of phase, and the normalized energy. The performance of the proposed method is verified on a publicly available emotional database. The results show that the three features are effective for emotion recognition. The role of each IMF is inquired and we find that high frequency component IMF1 has significant effect on different emotional states detection. The informative electrodes based on EMD strategy are analyzed. In addition, the classification accuracy of the proposed method is compared with several classical techniques, including fractal dimension (FD), sample entropy, differential entropy, and discrete wavelet transform (DWT). Experiment results on DEAP datasets demonstrate that our method can improve emotion recognition performance.},
  bdsk-url-1       = {https://doi.org/10.1155/2017/8317357},
  c1               = {China National Digital Switching System Engineering and Technological Research Center, Zhengzhou 450002, China; China National Digital Switching System Engineering and Technological Research Center, Zhengzhou 450002, China; Key Laboratory for NeuroInformation of Ministry of Education, School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, China; China National Digital Switching System Engineering and Technological Research Center, Zhengzhou 450002, China; China National Digital Switching System Engineering and Technological Research Center, Zhengzhou 450002, China; China National Digital Switching System Engineering and Technological Research Center, Zhengzhou 450002, China; China National Digital Switching System Engineering and Technological Research Center, Zhengzhou 450002, China},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1155/2017/8317357},
  hasabstract      = {Y},
  isbn             = {2314-6133},
  keywords         = {Emotion Recognition; Feature Extraction; Deep Learning for EEG; Affective Computing; Signal Decomposition},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Hindawi Publishing Corporation},
  url              = {https://doi.org/10.1155/2017/8317357},
}

@Article{dzedzickis2020hu,
  author           = {Dzedzickis, Andrius and Kaklauskas, Art{\=u}ras and Bu{\v c}inskas, Vytautas},
  journal          = {Sensors},
  title            = {Human Emotion Recognition: Review of Sensors and Methods},
  year             = {2020},
  month            = jan,
  number           = {3},
  pages            = {592--592},
  volume           = {20},
  abstract         = {Automated emotion recognition (AEE) is an important issue in various fields of activities which use human emotional reactions as a signal for marketing, technical equipment, or human--robot interaction. This paper analyzes scientific research and technical papers for sensor use analysis, among various methods implemented or researched. This paper covers a few classes of sensors, using contactless methods as well as contact and skin-penetrating electrodes for human emotion detection and the measurement of their intensity. The results of the analysis performed in this paper present applicable methods for each type of emotion and their intensity and propose their classification. The classification of emotion sensors is presented to reveal area of application and expected outcomes from each method, as well as their limitations. This paper should be relevant for researchers using human emotion evaluation and analysis, when there is a need to choose a proper method for their purposes or to find alternative decisions. Based on the analyzed human emotion recognition sensors and methods, we developed some practical applications for humanizing the Internet of Things (IoT) and affective computing systems.},
  bdsk-url-1       = {https://doi.org/10.3390/s20030592},
  c1               = {Faculty of Mechanics, Vilnius Gediminas Technical University, J. Basanaviciaus g. 28, LT-03224 Vilnius, Lithuania; Faculty of Civil engineering, Vilnius Gediminas Technical University, Sauletekio ave. 11, LT-10223 Vilnius, Lithuania; Faculty of Mechanics, Vilnius Gediminas Technical University, J. Basanaviciaus g. 28, LT-03224 Vilnius, Lithuania},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.3390/s20030592},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Emotion Recognition; Affective Computing; Head Gesture Recognition; Speech Emotion; Eye Movement Analysis},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s20030592},
}

@Article{schuller2018sp,
  author           = {Schuller, Bj{\"o}rn},
  journal          = {Communications of the ACM},
  title            = {Speech emotion recognition},
  year             = {2018},
  month            = apr,
  number           = {5},
  pages            = {90--99},
  volume           = {61},
  abstract         = {Tracing 20 years of progress in making machines hear our emotions based on speech signal properties.},
  bdsk-url-1       = {https://doi.org/10.1145/3129340},
  c1               = {University of Augsburg, Germany},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1145/3129340},
  hasabstract      = {Y},
  isbn             = {0001-0782},
  keywords         = {Emotion Recognition; Speech Emotion; Audio-Visual Speech Recognition; Affective Computing; Facial Expression Analysis},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Association for Computing Machinery},
  url              = {https://doi.org/10.1145/3129340},
}

@Article{muehl2014as,
  author           = {M{\"u}hl, Christian and Allison, Brendan and Nijholt, Anton and Chanel, Guillaume},
  journal          = {Brain computer interfaces},
  title            = {A survey of affective brain computer interfaces: principles, state-of-the-art, and challenges},
  year             = {2014},
  month            = apr,
  number           = {2},
  pages            = {66--84},
  volume           = {1},
  abstract         = {Affective states, moods and emotions, are an integral part of human nature: they shape our thoughts, govern the behavior of the individual, and influence our interpersonal relationships. The last decades have seen a growing interest in the automatic detection of such states from voice, facial expression, and physiological signals, primarily with the goal of enhancing human-computer interaction with an affective component. With the advent of brain-computer interface research, the idea of affective brain-computer interfaces (aBCI), enabling affect detection from brain signals, arose. In this article, we set out to survey the field of neurophysiology-based affect detection. We outline possible applications of aBCI in a general taxonomy of brain-computer interface approaches and introduce the core concepts of affect and their neurophysiological fundamentals. We show that there is a growing body of literature that evidences the capabilities, but also the limitations and challenges of affect detection from neurophysiological activity.},
  bdsk-url-1       = {https://doi.org/10.1080/2326263x.2014.912881},
  c1               = {Inria Bordeaux - Sud-Ouest, Talence, France; ASPEN Lab, Electrical and Computer Engineering Department, Old Dominion University, Norfolk, VA, USA; Department of Cognitive Science, University of California at San Diego, La Jolla, CA, USA; Faculty EEMCS, Human Media Interaction, University of Twente, Enschede, The Netherlands; Swiss Center for Affective Sciences --University of Geneva, Campus Biotech, Gen{\`e}ve, Switzerland},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1080/2326263x.2014.912881},
  hasabstract      = {Y},
  isbn             = {2326-2621},
  keywords         = {Affective Computing; Brain-Computer Interfaces; Emotion Recognition; Human-Computer Interaction},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Taylor \& Francis},
  url              = {https://doi.org/10.1080/2326263x.2014.912881},
}

@Article{mirandacorrea2021am,
  author        = {Miranda-Correa, Juan and Abadi, Mojtaba and Sebe, Nicu and Patras, Ioannis},
  journal       = {IEEE transactions on affective computing},
  title         = {AMIGOS: A Dataset for Affect, Personality and Mood Research on Individuals and Groups},
  year          = {2021},
  month         = apr,
  number        = {2},
  pages         = {479--493},
  volume        = {12},
  abstract      = {We present AMIGOS- A dataset for Multimodal research of affect, personality traits and mood on Individuals and GrOupS. Different to other databases, we elicited affect using both short and long videos in two social contexts, one with individual viewers and one with groups of viewers. The database allows the multimodal study of the affective responses, by means of neuro-physiological signals of individuals in relation to their personality and mood, and with respect to the social context and videos' duration. The data is collected in two experimental settings. In the first one, 40 participants watched 16 short emotional videos. In the second one, the participants watched 4 long videos, some of them alone and the rest in groups. The participants' signals, namely, Electroencephalogram (EEG), Electrocardiogram (ECG) and Galvanic Skin Response (GSR), were recorded using wearable sensors. Participants' frontal HD video and both RGB and depth full body videos were also recorded. Participants emotions have been annotated with both self-assessment of affective levels (valence, arousal, control, familiarity, liking and basic emotions) felt during the videos as well as external-assessment of levels of valence and arousal. We present a detailed correlation analysis of the different dimensions as well as baseline methods and results for single-trial classification of valence and arousal, personality traits, mood and social context. The database is made publicly available.},
  bdsk-url-1    = {https://doi.org/10.1109/taffc.2018.2884461},
  c1            = {School of Computer Science and Electronic Engineering, Queen Mary University of London, London, United Kingdom; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; School of Computer Science and Electronic Engineering, Queen Mary University of London, London, United Kingdom},
  comment       = {videos, EEG, ECG, GSR},
  date-added    = {2024-05-13 15:05:25 +0100},
  date-modified = {2024-05-13 15:05:25 +0100},
  doi           = {10.1109/taffc.2018.2884461},
  hasabstract   = {Y},
  isbn          = {1949-3045},
  keywords      = {Affective Computing; Emotion Recognition; Personality Data; Emotion Dynamics; Multimodal Data},
  la            = {en},
  priority      = {prio3},
  publisher     = {Institute of Electrical and Electronics Engineers},
  url           = {https://doi.org/10.1109/taffc.2018.2884461},
}

@Article{marinmorales2018af,
  author           = {Mar{\'\i}n‐Morales, Javier and Higuera-Trujillo, Juan and Greco, Alberto and Guixeres, Jaime and Llinares, Carmen and Scilingo, Enzo and Alca{\~n}{\'\i}z, Mariano and Valenza, Gaetano},
  journal          = {Scientific reports},
  title            = {Affective computing in virtual reality: emotion recognition from brain and heartbeat dynamics using wearable sensors},
  year             = {2018},
  month            = sep,
  number           = {1},
  volume           = {8},
  abstract         = {Abstract Affective Computing has emerged as an important field of study that aims to develop systems that can automatically recognize emotions. Up to the present, elicitation has been carried out with non-immersive stimuli. This study, on the other hand, aims to develop an emotion recognition system for affective states evoked through Immersive Virtual Environments. Four alternative virtual rooms were designed to elicit four possible arousal-valence combinations, as described in each quadrant of the Circumplex Model of Affects. An experiment involving the recording of the electroencephalography (EEG) and electrocardiography (ECG) of sixty participants was carried out. A set of features was extracted from these signals using various state-of-the-art metrics that quantify brain and cardiovascular linear and nonlinear dynamics, which were input into a Support Vector Machine classifier to predict the subject's arousal and valence perception. The model's accuracy was 75.00{\%} along the arousal dimension and 71.21{\%} along the valence dimension. Our findings validate the use of Immersive Virtual Environments to elicit and automatically recognize different emotional states from neural and cardiac dynamics; this development could have novel applications in fields as diverse as Architecture, Health, Education and Videogames.},
  bdsk-url-1       = {https://doi.org/10.1038/s41598-018-32063-4},
  c1               = {Instituto de Investigaci{\'o}n e Innovaci{\'o}n en Bioingenier{\'\i}a, Universitat Polit{\`e}cnica de Val{\`e}ncia, Val{\`e}ncia, Spain; Instituto de Investigaci{\'o}n e Innovaci{\'o}n en Bioingenier{\'\i}a, Universitat Polit{\`e}cnica de Val{\`e}ncia, Val{\`e}ncia, Spain; Bioengineering and Robotics Research Centre E Piaggio \& Department of Information Engineering, University of Pisa, Pisa, Italy; Instituto de Investigaci{\'o}n e Innovaci{\'o}n en Bioingenier{\'\i}a, Universitat Polit{\`e}cnica de Val{\`e}ncia, Val{\`e}ncia, Spain; Instituto de Investigaci{\'o}n e Innovaci{\'o}n en Bioingenier{\'\i}a, Universitat Polit{\`e}cnica de Val{\`e}ncia, Val{\`e}ncia, Spain; Bioengineering and Robotics Research Centre E Piaggio \& Department of Information Engineering, University of Pisa, Pisa, Italy; Instituto de Investigaci{\'o}n e Innovaci{\'o}n en Bioingenier{\'\i}a, Universitat Polit{\`e}cnica de Val{\`e}ncia, Val{\`e}ncia, Spain; Bioengineering and Robotics Research Centre E Piaggio \& Department of Information Engineering, University of Pisa, Pisa, Italy},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1038/s41598-018-32063-4},
  hasabstract      = {Y},
  isbn             = {2045-2322},
  keywords         = {Affective Computing; Emotion Recognition; Emotion Regulation; Human-Computer Interaction; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-17T04:30:12},
  priority         = {prio3},
  publisher        = {Nature Portfolio},
  url              = {https://doi.org/10.1038/s41598-018-32063-4},
}

@Article{santamariagranados2019us,
  author           = {Santamar{\'\i}a-Granados, Luz and Mu{\~n}oz-Organero, Mario and Ram{\'\i}rez-Gonz{\'a}lez, Gustavo and Abdulhay, Enas and Arunkumar, N.},
  journal          = {IEEE access},
  title            = {Using Deep Convolutional Neural Network for Emotion Detection on a Physiological Signals Dataset (AMIGOS)},
  year             = {2019},
  month            = jan,
  pages            = {57--67},
  volume           = {7},
  abstract         = {Recommender systems have been based on context and content, and now the technological challenge of making personalized recommendations based on the user emotional state arises through physiological signals that are obtained from devices or sensors. This paper applies the deep learning approach using a deep convolutional neural network on a dataset of physiological signals (electrocardiogram and galvanic skin response), in this case, the AMIGOS dataset. The detection of emotions is done by correlating these physiological signals with the data of arousal and valence of this dataset, to classify the affective state of a person. In addition, an application for emotion recognition based on classic machine learning algorithms is proposed to extract the features of physiological signals in the domain of time, frequency, and non-linear. This application uses a convolutional neural network for the automatic feature extraction of the physiological signals, and through fully connected network layers, the emotion prediction is made. The experimental results on the AMIGOS dataset show that the method proposed in this paper achieves a better precision of the classification of the emotional states, in comparison with the originally obtained by the authors of this dataset.},
  bdsk-url-1       = {https://doi.org/10.1109/access.2018.2883213},
  c1               = {Faculty of Systems Engineering, Universidad Santo Tom{\'a}s, Tunja, Colombia; Telematics Engineering Department, UC3M-BS Institute of Financial Big Data, Universidad Carlos III de Madrid, Leganes, Spain; Telematics Department, University of Cauca, Popay{\'a}n, Colombia; Department of Biomedical Engineering, Jordan University of Science and Technology, Irbid, Jordan; Department of Electronics and Instrumentation, SASTRA University, Thanjavur, India},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1109/access.2018.2883213},
  hasabstract      = {Y},
  isbn             = {2169-3536},
  keywords         = {Emotion Recognition; Affective Computing; Deep Learning for EEG; Deep Learning; Affective Design},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Institute of Electrical and Electronics Engineers},
  url              = {https://doi.org/10.1109/access.2018.2883213},
}

@Article{hao2019em,
  author           = {Hao, Chao and Dong, Liang and Liu, Yongli and Lu, Bao-Yun},
  journal          = {Sensors},
  title            = {Emotion Recognition from Multiband EEG Signals Using CapsNet},
  year             = {2019},
  month            = may,
  number           = {9},
  pages            = {2212--2212},
  volume           = {19},
  abstract         = {Emotion recognition based on multi-channel electroencephalograph (EEG) signals is becoming increasingly attractive. However, the conventional methods ignore the spatial characteristics of EEG signals, which also contain salient information related to emotion states. In this paper, a deep learning framework based on a multiband feature matrix (MFM) and a capsule network (CapsNet) is proposed. In the framework, the frequency domain, spatial characteristics, and frequency band characteristics of the multi-channel EEG signals are combined to construct the MFM. Then, the CapsNet model is introduced to recognize emotion states according to the input MFM. Experiments conducted on the dataset for emotion analysis using EEG, physiological, and video signals (DEAP) indicate that the proposed method outperforms most of the common models. The experimental results demonstrate that the three characteristics contained in the MFM were complementary and the capsule network was more suitable for mining and utilizing the three correlation characteristics.},
  bdsk-url-1       = {https://doi.org/10.3390/s19092212},
  c1               = {School of Computer Science and Technology, Henan Polytechnic University, Jiaozuo 454000, China; School of Computer Science and Technology, Henan Polytechnic University, Jiaozuo 454000, China; School of Computer Science and Technology, Henan Polytechnic University, Jiaozuo 454000, China; School of Computer Science and Technology, Henan Polytechnic University, Jiaozuo 454000, China},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.3390/s19092212},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s19092212},
}

@Article{nakisa2018ev,
  author           = {Nakisa, Bahareh and Rastgoo, Mohammad and Tjondronegoro, Dian and Chandran, Vinod},
  journal          = {Expert systems with applications},
  title            = {Evolutionary computation algorithms for feature selection of EEG-based emotion recognition using mobile sensors},
  year             = {2018},
  month            = mar,
  pages            = {143--155},
  volume           = {93},
  abstract         = {There is currently no standard or widely accepted subset of features to effectively classify different emotions based on electroencephalogram (EEG) signals. While combining all possible EEG features may improve the classification performance, it can lead to high dimensionality and worse performance due to redundancy and inefficiency. To solve the high-dimensionality problem, this paper proposes a new framework to automatically search for the optimal subset of EEG features using evolutionary computation (EC) algorithms. The proposed framework has been extensively evaluated using two public datasets (MAHNOB, DEAP) and a new dataset acquired with a mobile EEG sensor. The results confirm that EC algorithms can effectively support feature selection to identify the best EEG features and the best channels to maximize performance over a four-quadrant emotion classification problem. These findings are significant for informing future development of EEG-based emotion classification because low-cost mobile EEG sensors with fewer electrodes are becoming popular for many new applications.},
  bdsk-url-1       = {https://doi.org/10.1016/j.eswa.2017.09.062},
  c1               = {Science and Engineering Faculty, Queensland University of Technology, Brisbane, Qld, Australia; Science and Engineering Faculty, Queensland University of Technology, Brisbane, Qld, Australia; School of Business and Tourism, Southern Cross University, Gold Coast, Qld, Australia; Science and Engineering Faculty, Queensland University of Technology, Brisbane, Qld, Australia},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1016/j.eswa.2017.09.062},
  hasabstract      = {Y},
  isbn             = {0957-4174},
  keywords         = {Deep Learning for EEG; Emotion Recognition; EEG Analysis; Affective Computing; Feature Extraction},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Elsevier BV},
  url              = {https://doi.org/10.1016/j.eswa.2017.09.062},
}

@Article{li2018ex,
  author           = {Li, Xiang and Song, Dawei and Zhang, Peng and Zhang, Yazhou and Hou, Yuexian and Hu, Bin},
  journal          = {Frontiers in neuroscience},
  title            = {Exploring EEG Features in Cross-Subject Emotion Recognition},
  year             = {2018},
  month            = mar,
  volume           = {12},
  abstract         = {Recognizing cross-subject emotions based on brain imaging data, e.g., EEG, has always been difficult due to the poor generalizability of features across subjects. Thus, systematically exploring the ability of different EEG features to identify emotional information across subjects is crucial. Prior related work has explored this question based only on one or two kinds of features, and different findings and conclusions have been presented. In this work, we aim at a more comprehensive investigation on this question with a wider range of feature types, including 18 kinds of linear and nonlinear EEG features. The effectiveness of these features was examined on two publicly accessible datasets, namely, the dataset for emotion analysis using physiological signals (DEAP) and the SJTU emotion EEG dataset (SEED). We adopted the support vector machine (SVM) approach and the `leave-one-subject-out' verification strategy to evaluate recognition performance. Using automatic feature selection methods, the highest mean recognition accuracy of 59.06{$\backslash$}{\%} (AUC{\$}={\$}0.605) on the DEAP dataset and of 83.33{$\backslash$}{\%} (AUC{\$}={\$}0.904) on the SEED dataset were reached. Furthermore, using manually operated feature selection on the SEED dataset, we explored the importance of different EEG features in cross-subject emotion recognition from multiple perspectives, including different channels, brain regions, rhythms and feature types. For example, we found that the Hjorth parameter of mobility in the beta rhythm achieved the best mean recognition accuracy compared to the other features. Through a pilot correlation analysis, we further examined the highly correlated features, for a better understanding of the implications hidden in those features that allow for differentiating cross-subject emotions. Various remarkable observations have been made. The results of this paper validate the possibility of exploring robust EEG features in cross-subject emotion recognition.},
  bdsk-url-1       = {https://doi.org/10.3389/fnins.2018.00162},
  c1               = {Tianjin Key Laboratory of Cognitive Computing and Application, Tianjin University, Tianjin, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Computing and Communications, The Open University, Milton Keynes, United Kingdom; Tianjin Key Laboratory of Cognitive Computing and Application, Tianjin University, Tianjin, China; Tianjin Key Laboratory of Cognitive Computing and Application, Tianjin University, Tianjin, China; Tianjin Key Laboratory of Cognitive Computing and Application, Tianjin University, Tianjin, China; School of Information Science and Engineering, Lanzhou University, Lanzhou, China},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.3389/fnins.2018.00162},
  hasabstract      = {Y},
  isbn             = {1662-453X},
  keywords         = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Affective Computing; Epilepsy Detection},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Frontiers Media},
  url              = {https://doi.org/10.3389/fnins.2018.00162},
}

@Article{xing2019sa,
  author           = {Xing, Xiaofen and Li, Zhenqi and Xu, Tianyuan and Shu, Lin and Hu, Bin and Xu, Xiangmin},
  journal          = {Frontiers in neurorobotics},
  title            = {SAE+LSTM: A New Framework for Emotion Recognition From Multi-Channel EEG},
  year             = {2019},
  month            = jun,
  volume           = {13},
  abstract         = {EEG-based automatic emotion recognition can help brain-inspired robots in improving their interactions with humans. This paper presents a novel framework for emotion recognition using multi-channel electroencephalogram (EEG). The framework consists of a linear EEG mixing model and an emotion timing model. Our proposed framework considerably decomposes the EEG source signals from the collected EEG signals and improves classification accuracy by using the context correlations of the EEG feature sequences. Specially, Stack AutoEncoder (SAE) is used to build and solve the linear EEG mixing model and the emotion timing model is based on the Long Short-Term Memory Recurrent Neural Network (LSTM-RNN). The framework was implemented on the DEAP dataset for an emotion recognition experiment, where the mean accuracy of emotion recognition achieved 81.10{\%} in valence and 74.38{\%} in arousal, and the effectiveness of our framework was verified. Our framework exhibited a better performance in emotion recognition using multi-channel EEG than the compared conventional approaches in the experiments.},
  bdsk-url-1       = {https://doi.org/10.3389/fnbot.2019.00037},
  c1               = {School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; School of Information Science and Engineering, Lanzhou University, Lanzhou, China; School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.3389/fnbot.2019.00037},
  hasabstract      = {Y},
  isbn             = {1662-5218},
  keywords         = {Deep Learning for EEG; Emotion Recognition; EEG Analysis; Affective Computing; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Frontiers Media},
  url              = {https://doi.org/10.3389/fnbot.2019.00037},
}

@Article{lan2019do,
  author           = {Lan, Zirui and Sourina, Olga and Wang, Lipo and Scherer, Reinhold and M{\"u}ller-Putz, Gernot},
  journal          = {IEEE transactions on cognitive and developmental systems},
  title            = {Domain Adaptation Techniques for EEG-Based Emotion Recognition: A Comparative Study on Two Public Datasets},
  year             = {2019},
  month            = mar,
  number           = {1},
  pages            = {85--94},
  volume           = {11},
  abstract         = {Affective brain-computer interface (aBCI) introduces personal affective factors to human-computer interaction. The state-of-the-art aBCI tailors its classifier to each individual user to achieve accurate emotion classification. A subject-independent classifier that is trained on pooled data from multiple subjects generally leads to inferior accuracy, due to the fact that electroencephalography patterns vary from subject to subject. Transfer learning or domain adaptation techniques have been leveraged to tackle this problem. Existing studies have reported successful applications of domain adaptation techniques on SEED dataset. However, little is known about the effectiveness of the domain adaptation techniques on other affective datasets or in a cross-dataset application. In this paper, we focus on a comparative study on several state-of-the-art domain adaptation techniques on two datasets: 1) DEAP and 2) SEED. We demonstrate that domain adaptation techniques can improve the classification accuracy on both datasets, but not so effective on DEAP as on SEED. Then, we explore the efficacy of domain adaptation in a cross-dataset setting when the data are collected under different environments using different devices and experimental protocols. Here, we propose to apply domain adaptation to reduce the intersubject variance as well as technical discrepancies between datasets, and then train a subject-independent classifier on one dataset and test on the other. Experiment results show that using domain adaptation technique in a transductive adaptation setting can improve the accuracy significantly by 7.25{\%}-13.40{\%} compared to the baseline accuracy where no domain adaptation technique is used.},
  bdsk-url-1       = {https://doi.org/10.1109/tcds.2018.2826840},
  c1               = {Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Institute of Neural Engineering, Graz University of Technology, Graz, Austria; Institute of Neural Engineering, Graz University of Technology, Graz, Austria},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1109/tcds.2018.2826840},
  hasabstract      = {Y},
  isbn             = {2379-8920},
  keywords         = {Affective Computing; Emotion Recognition; Brain-Computer Interfaces; Speech Emotion; Human-Computer Interaction},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Institute of Electrical and Electronics Engineers},
  url              = {https://doi.org/10.1109/tcds.2018.2826840},
}

@Article{dominguezjimenez2020am,
  author           = {Dom{\'\i}nguez-Jim{\'e}nez, J. A. and Campo-Landines, Kiara and Mart{\'\i}nez-Santos, Juan and Delahoz, E. and Contreras-Ortiz, Sonia},
  journal          = {Biomedical signal processing and control},
  title            = {A machine learning model for emotion recognition from physiological signals},
  year             = {2020},
  month            = jan,
  pages            = {101646--101646},
  volume           = {55},
  abstract         = {Emotions are affective states related to physiological responses. This study proposes a model for recognition of three emotions: amusement, sadness, and neutral from physiological signals with the purpose of developing a reliable methodology for emotion recognition using wearable devices. Target emotions were elicited in 37 volunteers using video clips while two biosignals were recorded: photoplethysmography, which provides information about heart rate, and galvanic skin response. These signals were analyzed in frequency and time domains to obtain a set of features. Several feature selection techniques and classifiers were evaluated. The best model was obtained with random forest recursive feature elimination, for feature selection, and a support vector machine for classification. The results show that it is possible to detect amusement, and neutral emotions using only galvanic skin response features. The system was able to recognize the three target emotions with accuracy up to 100{\%} when evaluated on the test data set.},
  bdsk-url-1       = {https://doi.org/10.1016/j.bspc.2019.101646},
  c1               = {Universidad Tecnol{\'o}gica de Bol{\'\i}var, Km 1 V{\'\i}a Turbaco, Cartagena de Indias, Colombia; Universidad Tecnol{\'o}gica de Bol{\'\i}var, Km 1 V{\'\i}a Turbaco, Cartagena de Indias, Colombia; Universidad Tecnol{\'o}gica de Bol{\'\i}var, Km 1 V{\'\i}a Turbaco, Cartagena de Indias, Colombia; Universidad Tecnol{\'o}gica de Bol{\'\i}var, Km 1 V{\'\i}a Turbaco, Cartagena de Indias, Colombia; Universidad Tecnol{\'o}gica de Bol{\'\i}var, Km 1 V{\'\i}a Turbaco, Cartagena de Indias, Colombia},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1016/j.bspc.2019.101646},
  hasabstract      = {Y},
  isbn             = {1746-8094},
  keywords         = {Emotion Recognition; Affective Computing; Emotion Regulation; Physiological Signals; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Elsevier BV},
  url              = {https://doi.org/10.1016/j.bspc.2019.101646},
}

@Article{cimtay2020in,
  author           = {{\c C}imtay, Y{\"u}cel and Ekmekcio{\v g}lu, Erhan},
  journal          = {Sensors},
  title            = {Investigating the Use of Pretrained Convolutional Neural Network on Cross-Subject and Cross-Dataset EEG Emotion Recognition},
  year             = {2020},
  month            = apr,
  number           = {7},
  pages            = {2034--2034},
  volume           = {20},
  abstract         = {The electroencephalogram (EEG) has great attraction in emotion recognition studies due to its resistance to deceptive actions of humans. This is one of the most significant advantages of brain signals in comparison to visual or speech signals in the emotion recognition context. A major challenge in EEG-based emotion recognition is that EEG recordings exhibit varying distributions for different people as well as for the same person at different time instances. This nonstationary nature of EEG limits the accuracy of it when subject independency is the priority. The aim of this study is to increase the subject-independent recognition accuracy by exploiting pretrained state-of-the-art Convolutional Neural Network (CNN) architectures. Unlike similar studies that extract spectral band power features from the EEG readings, raw EEG data is used in our study after applying windowing, pre-adjustments and normalization. Removing manual feature extraction from the training system overcomes the risk of eliminating hidden features in the raw data and helps leverage the deep neural network's power in uncovering unknown features. To improve the classification accuracy further, a median filter is used to eliminate the false detections along a prediction interval of emotions. This method yields a mean cross-subject accuracy of 86.56{\%} and 78.34{\%} on the Shanghai Jiao Tong University Emotion EEG Dataset (SEED) for two and three emotion classes, respectively. It also yields a mean cross-subject accuracy of 72.81{\%} on the Database for Emotion Analysis using Physiological Signals (DEAP) and 81.8{\%} on the Loughborough University Multimodal Emotion Dataset (LUMED) for two emotion classes. Furthermore, the recognition model that has been trained using the SEED dataset was tested with the DEAP dataset, which yields a mean prediction accuracy of 58.1{\%} across all subjects and emotion classes. Results show that in terms of classification accuracy, the proposed approach is superior to, or on par with, the reference subject-independent EEG emotion recognition studies identified in literature and has limited complexity due to the elimination of the need for feature extraction.},
  bdsk-url-1       = {https://doi.org/10.3390/s20072034},
  c1               = {Institute for Digital Technologies, Loughborough University London, London E20 3BS, UK;; Institute for Digital Technologies, Loughborough University London, London E20 3BS, UK;},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.3390/s20072034},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; ECG Signal},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s20072034},
}

@Article{kwon2018el,
  author           = {Kwon, Yea and Shin, Sae and Kim, Shin},
  journal          = {Sensors},
  title            = {Electroencephalography Based Fusion Two-Dimensional (2D)-Convolution Neural Networks (CNN) Model for Emotion Recognition System},
  year             = {2018},
  month            = apr,
  number           = {5},
  pages            = {1383--1383},
  volume           = {18},
  abstract         = {The purpose of this study is to improve human emotional classification accuracy using a convolution neural networks (CNN) model and to suggest an overall method to classify emotion based on multimodal data. We improved classification performance by combining electroencephalogram (EEG) and galvanic skin response (GSR) signals. GSR signals are preprocessed using by the zero-crossing rate. Sufficient EEG feature extraction can be obtained through CNN. Therefore, we propose a suitable CNN model for feature extraction by tuning hyper parameters in convolution filters. The EEG signal is preprocessed prior to convolution by a wavelet transform while considering time and frequency simultaneously. We use a database for emotion analysis using the physiological signals open dataset to verify the proposed process, achieving 73.4{\%} accuracy, showing significant performance improvement over the current best practice models.},
  bdsk-url-1       = {https://doi.org/10.3390/s18051383},
  c1               = {Department of Computer Science, Yonsei University, 50 Yonsei-ro, Seodaemun-gu, Seoul 03722, Korea;; Department of Computer Science, Yonsei University, 50 Yonsei-ro, Seodaemun-gu, Seoul 03722, Korea;; Department of Computer Science, Yonsei University, 50 Yonsei-ro, Seodaemun-gu, Seoul 03722, Korea;},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.3390/s18051383},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Emotion Recognition; Deep Learning for EEG; Affective Computing; EEG Analysis; Head Gesture Recognition},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s18051383},
}

@Article{mi2018em,
  author           = {Mi, Li and Xu, Hongpei and Liu, Xingwang and Lu, Shengfu},
  journal          = {Technology and health care},
  title            = {Emotion recognition from multichannel EEG signals using K-nearest neighbor classification},
  year             = {2018},
  month            = jul,
  pages            = {509--519},
  volume           = {26},
  abstract         = {Many studies have been done on the emotion recognition based on multi-channel electroencephalogram (EEG) signals.This paper explores the influence of the emotion recognition accuracy of EEG signals in different frequency bands and different number of channels.We classified the emotional states in the valence and arousal dimensions using different combinations of EEG channels. Firstly, DEAP default preprocessed data were normalized. Next, EEG signals were divided into four frequency bands using discrete wavelet transform, and entropy and energy were calculated as features of K-nearest neighbor Classifier.The classification accuracies of the 10, 14, 18 and 32 EEG channels based on the Gamma frequency band were 89.54{\%}, 92.28{\%}, 93.72{\%} and 95.70{\%} in the valence dimension and 89.81{\%}, 92.24{\%}, 93.69{\%} and 95.69{\%} in the arousal dimension. As the number of channels increases, the classification accuracy of emotional states also increases, the classification accuracy of the gamma frequency band is greater than that of the beta frequency band followed by the alpha and theta frequency bands.This paper provided better frequency bands and channels reference for emotion recognition based on EEG.},
  bdsk-url-1       = {https://doi.org/10.3233/thc-174836},
  c1               = {Department of Automation, Faculty of Information Technology, Beijing University of Technology, Beijing 100124, China; The Beijing International Collaboration Base on Brain Informatics and Wisdom Services, Beijing 100024, China; Department of Automation, Faculty of Information Technology, Beijing University of Technology, Beijing 100124, China; The Beijing International Collaboration Base on Brain Informatics and Wisdom Services, Beijing 100024, China; Department of Automation, Faculty of Information Technology, Beijing University of Technology, Beijing 100124, China; The Beijing International Collaboration Base on Brain Informatics and Wisdom Services, Beijing 100024, China; Department of Automation, Faculty of Information Technology, Beijing University of Technology, Beijing 100124, China; The Beijing International Collaboration Base on Brain Informatics and Wisdom Services, Beijing 100024, China},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.3233/thc-174836},
  hasabstract      = {Y},
  isbn             = {0928-7329},
  keywords         = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Epilepsy Detection; Affective Computing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {IOS Press},
  url              = {https://doi.org/10.3233/thc-174836},
}

@Article{liu2020ee,
  author           = {Liu, Junxiu and Wu, Guopei and Luo, Yuling and Qiu, Senhui and Yang, Su and Li, Wei and Bi, Yifei},
  journal          = {Frontiers in systems neuroscience},
  title            = {EEG-Based Emotion Classification Using a Deep Neural Network and Sparse Autoencoder},
  year             = {2020},
  month            = sep,
  volume           = {14},
  abstract         = {Emotion classification based on brain-computer interface (BCI) systems is an appealing research topic. Recently, deep learning has been employed for the emotion classifications of BCI systems and compared to traditional classification methods improved results have been obtained. In this paper, a novel deep neural network is proposed for emotion classification using EEG systems, which combines the Convolutional Neural Network (CNN), Sparse Autoencoder (SAE), and Deep Neural Network (DNN) together. In the proposed network, the features extracted by the CNN are first sent to SAE for encoding and decoding. Then the data with reduced redundancy are used as the input features of a DNN for classification task. The public datasets of DEAP and SEED are used for testing. Experimental results show that the proposed network is more effective than conventional CNN methods on the emotion recognitions. For the DEAP dataset, the highest recognition accuracies of 89.49{\%} and 92.86{\%} are achieved for valence and arousal, respectively. For the SEED dataset, however, the best recognition accuracy reaches 96.77{\%}. By combining the CNN, SAE, and DNN and training them separately, the proposed network is shown as an efficient method with a faster convergence than the conventional CNN.},
  bdsk-url-1       = {https://doi.org/10.3389/fnsys.2020.00043},
  c1               = {School of Electronic Engineering, Guangxi Normal University, Guilin, China; School of Electronic Engineering, Guangxi Normal University, Guilin, China; School of Electronic Engineering, Guangxi Normal University, Guilin, China; Guangxi Key Laboratory of Wireless Wideband Communication and Signal Processing, Guilin, China; School of Electronic Engineering, Guangxi Normal University, Guilin, China; Department of Computer Science and Software Engineering, Xi'an Jiaotong-Liverpool University, Suzhou, China; Academy for Engineering \& Technology, Fudan University, Shanghai, China; Department of Electronic Engineering, The University of York, York, United Kingdom; College of Foreign Languages, University of Shanghai for Science and Technology, Shanghai, China; Department of Psychology, The University of York, York, United Kingdom},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.3389/fnsys.2020.00043},
  hasabstract      = {Y},
  isbn             = {1662-5137},
  keywords         = {Emotion Recognition; Deep Learning for EEG; Affective Computing; Deep Learning; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Frontiers Media},
  url              = {https://doi.org/10.3389/fnsys.2020.00043},
}

@Article{oezerdem2017em,
  author           = {{\"O}zerdem, Mehmet and Polat, Hasan},
  journal          = {Brain informatics},
  title            = {Emotion recognition based on EEG features in movie clips with channel selection},
  year             = {2017},
  month            = jul,
  number           = {4},
  pages            = {241--252},
  volume           = {4},
  abstract         = {Emotion plays an important role in human interaction. People can explain their emotions in terms of word, voice intonation, facial expression, and body language. However, brain-computer interface (BCI) systems have not reached the desired level to interpret emotions. Automatic emotion recognition based on BCI systems has been a topic of great research in the last few decades. Electroencephalogram (EEG) signals are one of the most crucial resources for these systems. The main advantage of using EEG signals is that it reflects real emotion and can easily be processed by computer systems. In this study, EEG signals related to positive and negative emotions have been classified with preprocessing of channel selection. Self-Assessment Manikins was used to determine emotional states. We have employed discrete wavelet transform and machine learning techniques such as multilayer perceptron neural network (MLPNN) and k-nearest neighborhood (kNN) algorithm to classify EEG signals. The classifier algorithms were initially used for channel selection. EEG channels for each participant were evaluated separately, and five EEG channels that offered the best classification performance were determined. Thus, final feature vectors were obtained by combining the features of EEG segments belonging to these channels. The final feature vectors with related positive and negative emotions were classified separately using MLPNN and kNN algorithms. The classification performance obtained with both the algorithms are computed and compared. The average overall accuracies were obtained as 77.14 and 72.92{\%} by using MLPNN and kNN, respectively.},
  bdsk-url-1       = {https://doi.org/10.1007/s40708-017-0069-3},
  c1               = {Electrical and Electronics Engineering, Dicle University, 21000, Diyarbakır, Turkey; Electrical and Electronics Engineering, Mus Alparslan University, 49000, Mu{\c s}, Turkey},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1007/s40708-017-0069-3},
  hasabstract      = {Y},
  isbn             = {2198-4026},
  keywords         = {Emotion Recognition; EEG Analysis; Affective Computing; Deep Learning for EEG; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Springer Science+Business Media},
  url              = {https://doi.org/10.1007/s40708-017-0069-3},
}

@Article{zhang2016re,
  author           = {Zhang, Jianhai and Chen, Ming and Zhao, Shaokai and Hu, Sanqing and Shi, Zhiguo and Cao, Yu},
  journal          = {Sensors},
  title            = {ReliefF-Based EEG Sensor Selection Methods for Emotion Recognition},
  year             = {2016},
  month            = sep,
  number           = {10},
  pages            = {1558--1558},
  volume           = {16},
  abstract         = {Electroencephalogram (EEG) signals recorded from sensor electrodes on the scalp can directly detect the brain dynamics in response to different emotional states. Emotion recognition from EEG signals has attracted broad attention, partly due to the rapid development of wearable computing and the needs of a more immersive human-computer interface (HCI) environment. To improve the recognition performance, multi-channel EEG signals are usually used. A large set of EEG sensor channels will add to the computational complexity and cause users inconvenience. ReliefF-based channel selection methods were systematically investigated for EEG-based emotion recognition on a database for emotion analysis using physiological signals (DEAP). Three strategies were employed to select the best channels in classifying four emotional states (joy, fear, sadness and relaxation). Furthermore, support vector machine (SVM) was used as a classifier to validate the performance of the channel selection results. The experimental results showed the effectiveness of our methods and the comparison with the similar strategies, based on the F-score, was given. Strategies to evaluate a channel as a unity gave better performance in channel reduction with an acceptable loss of accuracy. In the third strategy, after adjusting channels' weights according to their contribution to the classification accuracy, the number of channels was reduced to eight with a slight loss of accuracy (58.51{\%} $\pm$10.05{\%} versus the best classification accuracy 59.13{\%} $\pm$11.00{\%} using 19 channels). In addition, the study of selecting subject-independent channels, related to emotion processing, was also implemented. The sensors, selected subject-independently from frontal, parietal lobes, have been identified to provide more discriminative information associated with emotion processing, and are distributed symmetrically over the scalp, which is consistent with the existing literature. The results will make a contribution to the realization of a practical EEG-based emotion recognition system.},
  bdsk-url-1       = {https://doi.org/10.3390/s16101558},
  c1               = {College of Computer Science, Hangzhou Dianzi University, Hangzhou 310018, China; College of Computer Science, Hangzhou Dianzi University, Hangzhou 310018, China; College of Computer Science, Hangzhou Dianzi University, Hangzhou 310018, China; College of Computer Science, Hangzhou Dianzi University, Hangzhou 310018, China; Department of Information Science and Electronic Engineering, Zhejiang University, Hangzhou 310012, China; Department of Computer Science, The University of Massachusetts Lowell, Lowell, MA 01854, USA},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.3390/s16101558},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Epilepsy Detection; Affective Computing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s16101558},
}

@Article{yang2019am,
  author           = {Yang, Heekyung and Han, Jongdae and Min, Kyungha},
  journal          = {Sensors},
  title            = {A Multi-Column CNN Model for Emotion Recognition from EEG Signals},
  year             = {2019},
  month            = oct,
  number           = {21},
  pages            = {4736--4736},
  volume           = {19},
  abstract         = {We present a multi-column CNN-based model for emotion recognition from EEG signals. Recently, a deep neural network is widely employed for extracting features and recognizing emotions from various biosignals including EEG signals. A decision from a single CNN-based emotion recognizing module shows improved accuracy than the conventional handcrafted feature-based modules. To further improve the accuracy of the CNN-based modules, we devise a multi-column structured model, whose decision is produced by a weighted sum of the decisions from individual recognizing modules. We apply the model to EEG signals from DEAP dataset for comparison and demonstrate the improved accuracy of our model.},
  bdsk-url-1       = {https://doi.org/10.3390/s19214736},
  c1               = {Industry-Academy Cooperation Foundation, Sangmyung University, Seoul 03016, Korea; Department of Computer Science, Sangmyung University, Seoul 03016, Korea; Department of Computer Science, Sangmyung University, Seoul 03016, Korea},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.3390/s19214736},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Emotion Recognition; Deep Learning for EEG; Affective Computing; EEG Analysis},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s19214736},
}

@Article{li2017hu,
  author           = {Li, Youjun and Huang, Jiajin and Zhou, Hu and Zhong, Ning},
  journal          = {Applied sciences},
  title            = {Human Emotion Recognition with Electroencephalographic Multidimensional Features by Hybrid Deep Neural Networks},
  year             = {2017},
  month            = oct,
  number           = {10},
  pages            = {1060--1060},
  volume           = {7},
  abstract         = {The method presented in this study can be applied in many fields, such as mental health care, entertainment consumption behavior, society safety, and so on.For example, in the mental health care field, an automatic emotion analysis system can be constructed with our method to monitor the emotional variation of the subjects.With accurate and objective emotion analysis results from EEG signals, our method can provide useful treatment effect information to the medical staff.},
  bdsk-url-1       = {https://doi.org/10.3390/app7101060},
  c1               = {Institute of International WIC, Beijing University of Technology, Beijing 100124, China;; Institute of International WIC, Beijing University of Technology, Beijing 100124, China;; Institute of International WIC, Beijing University of Technology, Beijing 100124, China;; Institute of International WIC, Beijing University of Technology, Beijing 100124, China;; Knowledge Information Systems Lab, Department of Life Science and Informatics, Maebashi Institute of Technology, Maebashi 371-0816, Japan},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.3390/app7101060},
  hasabstract      = {Y},
  isbn             = {2076-3417},
  keywords         = {Emotion Recognition; Affective Computing; Emotions; Color Psychology; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/app7101060},
}

@Article{thammasan2016fa,
  author           = {Thammasan, Nattapong and Moriyama, Koichi and Fukui, Kenichi and Numao, Masayuki},
  journal          = {Brain informatics},
  title            = {Familiarity effects in EEG-based emotion recognition},
  year             = {2016},
  month            = apr,
  number           = {1},
  pages            = {39--50},
  volume           = {4},
  abstract         = {Although emotion detection using electroencephalogram (EEG) data has become a highly active area of research over the last decades, little attention has been paid to stimulus familiarity, a crucial subjectivity issue. Using both our experimental data and a sophisticated database (DEAP dataset), we investigated the effects of familiarity on brain activity based on EEG signals. Focusing on familiarity studies, we allowed subjects to select the same number of familiar and unfamiliar songs; both resulting datasets demonstrated the importance of reporting self-emotion based on the assumption that the emotional state when experiencing music is subjective. We found evidence that music familiarity influences both the power spectra of brainwaves and the brain functional connectivity to a certain level. We conducted an additional experiment using music familiarity in an attempt to recognize emotional states; our empirical results suggested that the use of only songs with low familiarity levels can enhance the performance of EEG-based emotion classification systems that adopt fractal dimension or power spectral density features and support vector machine, multilayer perceptron or C4.5 classifier. This suggests that unfamiliar songs are most appropriate for the construction of an emotion recognition system.},
  bdsk-url-1       = {https://doi.org/10.1007/s40708-016-0051-5},
  c1               = {Institute of Scientific and Industrial Research (ISIR), Osaka University, Ibaraki-shi, Osaka, 567-0047, Japan; Department of Computer Science and Engineering, Nagoya Institute of Technology, Showa-ku, Nagoya, 466-8555, Japan; Institute of Scientific and Industrial Research (ISIR), Osaka University, Ibaraki-shi, Osaka, 567-0047, Japan; Institute of Scientific and Industrial Research (ISIR), Osaka University, Ibaraki-shi, Osaka, 567-0047, Japan},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1007/s40708-016-0051-5},
  hasabstract      = {Y},
  isbn             = {2198-4026},
  keywords         = {EEG Analysis; Emotion Recognition; Deep Learning for EEG; Affective Computing; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Springer Science+Business Media},
  url              = {https://doi.org/10.1007/s40708-016-0051-5},
}

@Article{marinmorales2020em,
  author           = {Mar{\'\i}n‐Morales, Javier and Llinares, Carmen and Guixeres, Jaime and Alca{\~n}{\'\i}z, Mariano},
  journal          = {Sensors},
  title            = {Emotion Recognition in Immersive Virtual Reality: From Statistics to Affective Computing},
  year             = {2020},
  month            = sep,
  number           = {18},
  pages            = {5163--5163},
  volume           = {20},
  abstract         = {Emotions play a critical role in our daily lives, so the understanding and recognition of emotional responses is crucial for human research. Affective computing research has mostly used non-immersive two-dimensional (2D) images or videos to elicit emotional states. However, immersive virtual reality, which allows researchers to simulate environments in controlled laboratory conditions with high levels of sense of presence and interactivity, is becoming more popular in emotion research. Moreover, its synergy with implicit measurements and machine-learning techniques has the potential to impact transversely in many research areas, opening new opportunities for the scientific community. This paper presents a systematic review of the emotion recognition research undertaken with physiological and behavioural measures using head-mounted displays as elicitation devices. The results highlight the evolution of the field, give a clear perspective using aggregated analysis, reveal the current open issues and provide guidelines for future research.},
  bdsk-url-1       = {https://doi.org/10.3390/s20185163},
  c1               = {Instituto de Investigaci{\'o}n e Innovaci{\'o}n en Bioingenier{\'\i}a, Universitat Polit{\`e}cnica de Val{\`e}ncia, 46022 Val{\`e}ncia, Spain;; Instituto de Investigaci{\'o}n e Innovaci{\'o}n en Bioingenier{\'\i}a, Universitat Polit{\`e}cnica de Val{\`e}ncia, 46022 Val{\`e}ncia, Spain;; Instituto de Investigaci{\'o}n e Innovaci{\'o}n en Bioingenier{\'\i}a, Universitat Polit{\`e}cnica de Val{\`e}ncia, 46022 Val{\`e}ncia, Spain;; Instituto de Investigaci{\'o}n e Innovaci{\'o}n en Bioingenier{\'\i}a, Universitat Polit{\`e}cnica de Val{\`e}ncia, 46022 Val{\`e}ncia, Spain;},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.3390/s20185163},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Affective Computing; Emotion Recognition; Human-Computer Interaction},
  la               = {en},
  modificationdate = {2024-05-17T04:30:16},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s20185163},
}

@Article{candra2015in,
  author           = {Candra, Henry and Yuwono, Mitchell and Chai, Rifai and Handojoseno, A. and Elamvazuthi, Irraivan and Nguyen, Hung and Su, Steven},
  title            = {Investigation of window size in classification of EEG-emotion signal with wavelet entropy and support vector machine},
  year             = {2015},
  month            = aug,
  abstract         = {When dealing with patients with psychological or emotional symptoms, medical practitioners are often faced with the problem of objectively recognizing their patients' emotional state. In this paper, we approach this problem using a computer program that automatically extracts emotions from EEG signals. We extend the finding of Koelstra et. al {$[$}IEEE trans. affective comput., vol. 3, no. 1, pp. 18-31, 2012{$]$} using the same dataset (i.e. the DEAP: dataset for emotion analysis using electroencephalogram, physiological and video signals), where we observed that the accuracy can be further improved using wavelet features extracted from shorter time segments. More precisely, we achieved accuracy of 65{\%} for both valence and arousal using the wavelet entropy of 3 to 12 seconds signal segments. This improvement in accuracy entails an important discovery that information on emotions contained in the EEG signal may be better described in term of wavelets and in shorter time segments.},
  bdsk-url-1       = {https://doi.org/10.1109/embc.2015.7320065},
  c1               = {Faculty of Engineering and Information Technolnov., Universirv of Technologies, Sydney, New South Wales, Australia; Faculty of Engineering and Information Technolnov., Universirv of Technologies, Sydney, New South Wales, Australia; Faculty of Engineering and Information Technolnov., Universirv of Technologies, Sydney, New South Wales, Australia; Faculty of Engineering and Information Technolnov., Universirv of Technologies, Sydney, New South Wales, Australia; Faculty of Engineering and Information Technolnov., Universirv of Technologies, Sydney, New South Wales, Australia; Faculty of Engineering and Information Technolnov., Universirv of Technologies, Sydney, New South Wales, Australia; Dept. of Electrical and Electronic Engineering Universiti Teknologi PETRONAS, Tronoh, Malaysia},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1109/embc.2015.7320065},
  hasabstract      = {Y},
  keywords         = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Affective Computing; Epilepsy Detection},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.1109/embc.2015.7320065},
}

@Article{kossaifi2021se,
  author           = {Kossaifi, Jean and Walecki, Robert and Panagakis, Yannis and Shen, Jie and Schmitt, Maximilian and Ringeval, Fabien and Han, Jing and Pandit, Vedhas and Toisoul, Antoine and Schuller, Bj{\"o}rn and Star, Kam and Hajiyev, Elnar and Panti{\'c}, Maja},
  journal          = {IEEE transactions on pattern analysis and machine intelligence},
  title            = {SEWA DB: A Rich Database for Audio-Visual Emotion and Sentiment Research in the Wild},
  year             = {2021},
  month            = mar,
  number           = {3},
  pages            = {1022--1040},
  volume           = {43},
  abstract         = {Natural human-computer interaction and audio-visual human behaviour sensing systems, which would achieve robust performance in-the-wild are more needed than ever as digital devices are increasingly becoming an indispensable part of our life. Accurately annotated real-world data are the crux in devising such systems. However, existing databases usually consider controlled settings, low demographic variability, and a single task. In this paper, we introduce the SEWA database of more than 2, 000 minutes of audio-visual data of 398 people coming from six cultures, 50 percent female, and uniformly spanning the age range of 18 to 65 years old. Subjects were recorded in two different contexts: while watching adverts and while discussing adverts in a video chat. The database includes rich annotations of the recordings in terms of facial landmarks, facial action units (FAU), various vocalisations, mirroring, and continuously valued valence, arousal, liking, agreement, and prototypic examples of (dis)liking. This database aims to be an extremely valuable resource for researchers in affective computing and automatic human sensing and is expected to push forward the research in human behaviour analysis, including cultural studies. Along with the database, we provide extensive baseline experiments for automatic FAU detection and automatic valence, and (dis)liking intensity estimation.},
  bdsk-url-1       = {https://doi.org/10.1109/tpami.2019.2944808},
  c1               = {Department of Computing, Imperial College London, London, United Kingdom; Department of Computing, Imperial College London, London, United Kingdom; Department of Computing, Imperial College London, London, United Kingdom; Department of Computing, Imperial College London, London, United Kingdom; Chair Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Augsburg, Bavaria, Germany; Universit{\'e}Grenoble Alpes, France; Chair Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Augsburg, Bavaria, Germany; Chair Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Augsburg, Bavaria, Germany; Department of Computing, Imperial College London, London, United Kingdom; Department of Computing, Imperial College London, London, United Kingdom; Playgen, London, United Kingdom; Real eyes, London, United Kingdom; Department of Computing, Imperial College London, London, United Kingdom},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1109/tpami.2019.2944808},
  hasabstract      = {Y},
  isbn             = {0162-8828},
  keywords         = {Affective Computing; Databases; Audiovisual Interaction; Emotion Recognition; Aesthetic Intelligence},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {IEEE Computer Society},
  url              = {https://doi.org/10.1109/tpami.2019.2944808},
}

@Article{cowen2019maa,
  author        = {Cowen, Alan and Sauter, Disa and Tracy, Jessica and Keltner, Dacher},
  journal       = {Psychological science in the public interest},
  title         = {Mapping the Passions: Toward a High-Dimensional Taxonomy of Emotional Experience and Expression},
  year          = {2019},
  month         = jul,
  number        = {1},
  pages         = {69--90},
  volume        = {20},
  abstract      = {What would a comprehensive atlas of human emotions include? For 50 years, scientists have sought to map emotion-related experience, expression, physiology, and recognition in terms of the "basic six"-anger, disgust, fear, happiness, sadness, and surprise. Claims about the relationships between these six emotions and prototypical facial configurations have provided the basis for a long-standing debate over the diagnostic value of expression (for review and latest installment in this debate, see Barrett et al., p. 1). Building on recent empirical findings and methodologies, we offer an alternative conceptual and methodological approach that reveals a richer taxonomy of emotion. Dozens of distinct varieties of emotion are reliably distinguished by language, evoked in distinct circumstances, and perceived in distinct expressions of the face, body, and voice. Traditional models-both the basic six and affective-circumplex model (valence and arousal)-capture a fraction of the systematic variability in emotional response. In contrast, emotion-related responses (e.g., the smile of embarrassment, triumphant postures, sympathetic vocalizations, blends of distinct expressions) can be explained by richer models of emotion. Given these developments, we discuss why tests of a basic-six model of emotion are not tests of the diagnostic value of facial expression more generally. Determining the full extent of what facial expressions can tell us, marginally and in conjunction with other behavioral and contextual cues, will require mapping the high-dimensional, continuous space of facial, bodily, and vocal signals onto richly multifaceted experiences using large-scale statistical modeling and machine-learning methods.},
  bdsk-url-1    = {https://doi.org/10.1177/1529100619850176},
  c1            = {Department of Psychology, University of California, Berkeley; Faculty of Social and Behavioural Sciences, University of Amsterdam; Department of Psychology, University of British Columbia; Department of Psychology, University of California, Berkeley},
  comment       = {no music},
  date-added    = {2024-05-13 15:05:25 +0100},
  date-modified = {2024-05-13 15:05:25 +0100},
  doi           = {10.1177/1529100619850176},
  hasabstract   = {Y},
  isbn          = {1529-1006},
  keywords      = {Emotional Expressions; Emotion Recognition; Affective Computing; Emotion Regulation; Facial Expression},
  la            = {en},
  priority      = {prio3},
  publisher     = {SAGE Publishing},
  url           = {https://doi.org/10.1177/1529100619850176},
}

@Article{abdelwahab2018do,
  author        = {Abdelwahab, Mohammed and Busso, Carlos},
  journal       = {IEEE/ACM transactions on audio, speech, and language processing},
  title         = {Domain Adversarial for Acoustic Emotion Recognition},
  year          = {2018},
  month         = dec,
  number        = {12},
  pages         = {2423--2435},
  volume        = {26},
  abstract      = {The performance of speech emotion recognition is affected by the differences in data distributions between train (source domain) and test (target domain) sets used to build and evaluate the models. This is a common problem, as multiple studies have shown that the performance of emotional classifiers drop when they are exposed to data that does not match the distribution used to build the emotion classifiers. The difference in data distributions becomes very clear when the training and testing data come from different domains, causing a large performance gap between validation and testing performance. Due to the high cost of annotating new data and the abundance of unlabeled data, it is crucial to extract as much useful information as possible from the available unlabeled data. This study looks into the use of adversarial multitask training to extract a common representation between train and test domains. The primary task is to predict emotional attribute-based descriptors for arousal, valence, or dominance. The secondary task is to learn a common representation where the train and test domains cannot be distinguished. By using a gradient reversal layer, the gradients coming from the domain classifier are used to bring the source and target domain representations closer. We show that exploiting unlabeled data consistently leads to better emotion recognition performance across all emotional dimensions. We visualize the effect of adversarial training on the feature representation across the proposed deep learning architecture. The analysis shows that the data representations for the train and test domains converge as the data is passed to deeper layers of the network. We also evaluate the difference in performance when we use a shallow neural network versus a {$\backslash$}emph{\{}deep neural network{\}} (DNN) and the effect of the number of shared layers used by the task and domain classifiers.},
  bdsk-url-1    = {https://doi.org/10.1109/taslp.2018.2867099},
  c1            = {Department of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, TX, USA; Department of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, TX, USA},
  comment       = {no music},
  date-added    = {2024-05-13 15:05:25 +0100},
  date-modified = {2024-05-13 15:05:25 +0100},
  doi           = {10.1109/taslp.2018.2867099},
  hasabstract   = {Y},
  isbn          = {2329-9290},
  keywords      = {Emotion Recognition; Affective Computing; Speech Emotion; Audio Event Detection; Audio-Visual Speech Recognition},
  la            = {en},
  priority      = {prio3},
  publisher     = {Institute of Electrical and Electronics Engineers},
  url           = {https://doi.org/10.1109/taslp.2018.2867099},
}

@Article{qing2019in,
  author           = {Qing, Chunmei and Qiao, Rui and Xu, Xiangmin and Cheng, Yongqiang},
  journal          = {IEEE access},
  title            = {Interpretable Emotion Recognition Using EEG Signals},
  year             = {2019},
  month            = jan,
  pages            = {94160--94170},
  volume           = {7},
  abstract         = {Electroencephalogram (EEG) signal-based emotion recognition has attracted wide interests in recent years and has been broadly adopted in medical, affective computing, and other relevant fields. However, the majority of the research reported in this field tends to focus on the accuracy of classification whilst neglecting the interpretability of emotion progression. In this paper, we propose a new interpretable emotion recognition approach with the activation mechanism by using machine learning and EEG signals. This paper innovatively proposes the emotional activation curve to demonstrate the activation process of emotions. The algorithm first extracts features from EEG signals and classifies emotions using machine learning techniques, in which different parts of a trial are used to train the proposed model and assess its impact on emotion recognition results. Second, novel activation curves of emotions are constructed based on the classification results, and two emotion coefficients, i.e., the correlation coefficients and entropy coefficients. The activation curve can not only classify emotions but also reveals to a certain extent the emotional activation mechanism. Finally, a weight coefficient is obtained from the two coefficients to improve the accuracy of emotion recognition. To validate the proposed method, experiments have been carried out on the DEAP and SEED dataset. The results support the point that emotions are progressively activated throughout the experiment, and the weighting coefficients based on the correlation coefficient and the entropy coefficient can effectively improve the EEG-based emotion recognition accuracy.},
  bdsk-url-1       = {https://doi.org/10.1109/access.2019.2928691},
  c1               = {School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; Department of Computer Science and Technology, University of Hull, Hull, U.K.},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1109/access.2019.2928691},
  hasabstract      = {Y},
  isbn             = {2169-3536},
  keywords         = {Emotion Recognition; Affective Computing; Emotion Regulation; Deep Learning for EEG; EEG Analysis},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Institute of Electrical and Electronics Engineers},
  url              = {https://doi.org/10.1109/access.2019.2928691},
}

@Article{moon2018co,
  author           = {Moon, Seong-Eun and Jang, Sungil and Lee, Jongseok},
  title            = {Convolutional Neural Network Approach for Eeg-Based Emotion Recognition Using Brain Connectivity and its Spatial Information},
  year             = {2018},
  month            = apr,
  abstract         = {Emotion recognition based on electroencephalography (EEG) has received attention as a way to implement human-centric services. However, there is still much room for improvement, particularly in terms of the recognition accuracy. In this paper, we propose a novel deep learning approach using convolutional neural networks (CNNs) for EEG-based emotion recognition. In particular, we employ brain connectivity features that have not been used with deep learning models in previous studies, which can account for synchronous activations of different brain regions. In addition, we develop a method to effectively capture asymmetric brain activity patterns that are important for emotion recognition. Experimental results confirm the effectiveness of our approach.},
  bdsk-url-1       = {https://doi.org/10.1109/icassp.2018.8461315},
  c1               = {School of Integrated Technology, Yonsei University, Republic of Korea{\#}TAB{\#}; School of Integrated Technology, Yonsei University, Republic of Korea{\#}TAB{\#}; School of Integrated Technology, Yonsei University, Republic of Korea{\#}TAB{\#}},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1109/icassp.2018.8461315},
  hasabstract      = {Y},
  keywords         = {Emotion Recognition; Deep Learning for EEG; Affective Computing; EEG Analysis; Sensory Processing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.1109/icassp.2018.8461315},
}

@Article{piho2020am,
  author           = {Piho, Laura and Tjahjadi, Tardi},
  journal          = {IEEE transactions on affective computing},
  title            = {A Mutual Information Based Adaptive Windowing of Informative EEG for Emotion Recognition},
  year             = {2020},
  month            = oct,
  number           = {4},
  pages            = {722--735},
  volume           = {11},
  abstract         = {Emotion recognition using brain wave signals involves using high dimensional electroencephalogram (EEG) data. In this paper, a window selection method based on mutual information is introduced to select an appropriate signal window to reduce the length of the signals. The motivation of the windowing method comes from EEG emotion recognition being computationally costly and the data having low signal-to-noise ratio. The aim of the windowing method is to find a reduced signal where the emotions are strongest. In this paper, it is suggested, that using only the signal section which best describes emotions improves the classification of emotions. This is achieved by iteratively comparing different-length EEG signals at different time locations using the mutual information between the reduced signal and emotion labels as criterion. The reduced signal with the highest mutual information is used for extracting the features for emotion classification. In addition, a viable framework for emotion recognition is introduced. Experimental results on publicly available datasets, DEAP and MAHNOB-HCI, show significant improvement in emotion recognition accuracy.},
  bdsk-url-1       = {https://doi.org/10.1109/taffc.2018.2840973},
  c1               = {School of Engineering, University of Warwick, Coventry, United Kingdom; School of Engineering, University of Warwick, Coventry, United Kingdom},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1109/taffc.2018.2840973},
  hasabstract      = {Y},
  isbn             = {1949-3045},
  keywords         = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Institute of Electrical and Electronics Engineers},
  url              = {https://doi.org/10.1109/taffc.2018.2840973},
}

@Article{baveye2018af,
  author           = {Baveye, Yoann and Chamaret, Christel and Dellandr{\'e}a, Emmanuel and Chen, Liming},
  journal          = {IEEE transactions on affective computing},
  title            = {Affective Video Content Analysis: A Multidisciplinary Insight},
  year             = {2018},
  month            = oct,
  number           = {4},
  pages            = {396--409},
  volume           = {9},
  abstract         = {In our present society, the cinema has become one of the major forms of entertainment providing unlimited contexts of emotion elicitation for the emotional needs of human beings. Since emotions are universal and shape all aspects of our interpersonal and intellectual experience, they have proved to be a highly multidisciplinary research field, ranging from psychology, sociology, neuroscience, etc., to computer science. However, affective multimedia content analysis work from the computer science community benefits but little from the progress achieved in other research fields. In this paper, a multidisciplinary state-of-the-art for affective movie content analysis is given, in order to promote and encourage exchanges between researchers from a very wide range of fields. In contrast to other state-of-the-art papers on affective video content analysis, this work confronts the ideas and models of psychology, and computer science. The concepts of aesthetic emotions and emotion induction, as well as the different representations of emotions are introduced, based on psychological and sociological theories. Previous global and continuous affective video content analysis work, including video emotion recognition and violence detection, are also presented in order to point out the limitations of affective video content analysis work.},
  bdsk-url-1       = {https://doi.org/10.1109/taffc.2017.2661284},
  c1               = {Institut de Recherche en Communications et en Cybern{\'e}tique de Nantes; Technicolor R \& I {$[$}Cesson S{\'e}vign{\'e}{$]$}; Extraction de Caract{\'e}ristiques et Identification; Extraction de Caract{\'e}ristiques et Identification},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1109/taffc.2017.2661284},
  hasabstract      = {Y},
  isbn             = {1949-3045},
  keywords         = {Affective Computing; Emotion Recognition},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Institute of Electrical and Electronics Engineers},
  url              = {https://doi.org/10.1109/taffc.2017.2661284},
}

@Article{wang2020em,
  author           = {Wang, Fei and Wu, Shichao and Zhang, Weiwei and Xu, Zongfeng and Zhang, Yahui and Wu, Chengdong and Coleman, Sonya},
  journal          = {Neuropsychologia},
  title            = {Emotion recognition with convolutional neural network and EEG-based EFDMs},
  year             = {2020},
  month            = sep,
  pages            = {107506--107506},
  volume           = {146},
  abstract         = {Electroencephalogram (EEG), as a direct response to brain activity, can be used to detect mental states and physical conditions. Among various EEG-based emotion recognition studies, due to the non-linear, non-stationary and the individual difference of EEG signals, traditional recognition methods still have the disadvantages of complicated feature extraction and low recognition rates. Thus, this paper first proposes a novel concept of electrode-frequency distribution maps (EFDMs) with short-time Fourier transform (STFT). Residual block based deep convolutional neural network (CNN) is proposed for automatic feature extraction and emotion classification with EFDMs. Aim at the shortcomings of the small amount of EEG samples and the challenge of differences in individual emotions, which makes it difficult to construct a universal model, this paper proposes a cross-datasets emotion recognition method of deep model transfer learning. Experiments carried out on two publicly available datasets. The proposed method achieved an average classification score of 90.59{\%} based on a short length of EEG data on SEED, which is 4.51{\%} higher than the baseline method. Then, the pre-trained model was applied to DEAP through deep model transfer learning with a few samples, resulted an average accuracy of 82.84{\%}. Finally, this paper adopts the gradient weighted class activation mapping (Grad-CAM) to get a glimpse of what features the CNN has learned during training from EFDMs and concludes that the high frequency bands are more favorable for emotion recognition.},
  bdsk-url-1       = {https://doi.org/10.1016/j.neuropsychologia.2020.107506},
  c1               = {Faculty of Robot Science and Engineering, Northeastern University, Shenyang, 110169, China; Faculty of Robot Science and Engineering, Northeastern University, Shenyang, 110169, China; Faculty of Robot Science and Engineering, Northeastern University, Shenyang, 110169, China; College of Information Science and Engineering, Northeastern University, Shenyang, 110819, China; College of Information Science and Engineering, Northeastern University, Shenyang, 110819, China; Faculty of Robot Science and Engineering, Northeastern University, Shenyang, 110169, China; Intelligent Systems Research Centre, Ulster University, Londonderry, United Kingdom},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1016/j.neuropsychologia.2020.107506},
  hasabstract      = {Y},
  isbn             = {0028-3932},
  keywords         = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Epilepsy Detection},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Elsevier BV},
  url              = {https://doi.org/10.1016/j.neuropsychologia.2020.107506},
}

@Article{rouast2021de,
  author           = {Rouast, Philipp and Adam, Marc and Chiong, Raymond},
  journal          = {IEEE transactions on affective computing},
  title            = {Deep Learning for Human Affect Recognition: Insights and New Developments},
  year             = {2021},
  month            = apr,
  number           = {2},
  pages            = {524--543},
  volume           = {12},
  abstract         = {Automatic human affect recognition is a key step towards more natural human-computer interaction. Recent trends include recognition in the wild using a fusion of audiovisual and physiological sensors, a challenging setting for conventional machine learning algorithms. Since 2010, novel deep learning algorithms have been applied increasingly in this field. In this paper, we review the literature on human affect recognition between 2010 and 2017, with a special focus on approaches using deep neural networks. By classifying a total of 950 studies according to their usage of shallow or deep architectures, we are able to show a trend towards deep learning. Reviewing a subset of 233 studies that employ deep neural networks, we comprehensively quantify their applications in this field. We find that deep learning is used for learning of (i) spatial feature representations, (ii) temporal feature representations, and (iii) joint feature representations for multimodal sensor data. Exemplary state-of-the-art architectures illustrate the progress. Our findings show the role deep architectures will play in human affect recognition, and can serve as a reference point for researchers working on related applications.},
  bdsk-url-1       = {https://doi.org/10.1109/taffc.2018.2890471},
  c1               = {School of Electrical Engineering and Computing, University of Newcastle, Callaghan, NSW, Australia; School of Electrical Engineering and Computing, University of Newcastle, Callaghan, NSW, Australia; School of Electrical Engineering and Computing, University of Newcastle, Callaghan, NSW, Australia},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1109/taffc.2018.2890471},
  hasabstract      = {Y},
  isbn             = {1949-3045},
  keywords         = {Affective Computing; Emotion Recognition; Deep Learning; Human-Computer Interaction},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Institute of Electrical and Electronics Engineers},
  url              = {https://doi.org/10.1109/taffc.2018.2890471},
}

@Article{almachot2019ad,
  author           = {Al Machot, Fadi and Elmachot, Ali and Ali, Mouhannad and Al Machot, Elyan and Kyamakya, Kyandoghere},
  journal          = {Sensors},
  title            = {A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors},
  year             = {2019},
  month            = apr,
  number           = {7},
  pages            = {1659--1659},
  volume           = {19},
  abstract         = {One of the main objectives of Active and Assisted Living (AAL) environments is to ensure that elderly and/or disabled people perform/live well in their immediate environments; this can be monitored by among others the recognition of emotions based on non-highly intrusive sensors such as Electrodermal Activity (EDA) sensors. However, designing a learning system or building a machine-learning model to recognize human emotions while training the system on a specific group of persons and testing the system on a totally a new group of persons is still a serious challenge in the field, as it is possible that the second testing group of persons may have different emotion patterns. Accordingly, the purpose of this paper is to contribute to the field of human emotion recognition by proposing a Convolutional Neural Network (CNN) architecture which ensures promising robustness-related results for both subject-dependent and subject-independent human emotion recognition. The CNN model has been trained using a grid search technique which is a model hyperparameter optimization technique to fine-tune the parameters of the proposed CNN architecture. The overall concept's performance is validated and stress-tested by using MAHNOB and DEAP datasets. The results demonstrate a promising robustness improvement regarding various evaluation metrics. We could increase the accuracy for subject-independent classification to 78{\%} and 82{\%} for MAHNOB and DEAP respectively and to 81{\%} and 85{\%} subject-dependent classification for MAHNOB and DEAP respectively (4 classes/labels). The work shows clearly that while using solely the non-intrusive EDA sensors a robust classification of human emotion is possible even without involving additional/other physiological signals.},
  bdsk-url-1       = {https://doi.org/10.3390/s19071659},
  c1               = {Research Center Borstel-Leibniz Lung Center, 23845 Borstel, Germany; Faculty of Mechanical and Electrical Engineering, University of Damascus, Damascus, Syria;; Institute for Smart Systems Technologies, Alpen-Adira University, 9020 Klagenfurt, Austria;; Faculty of Medicine, Dresden University of Technology, 01069 Dresden, Germany;; Institute for Smart Systems Technologies, Alpen-Adira University, 9020 Klagenfurt, Austria;},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.3390/s19071659},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Emotion Recognition; Deep Learning for EEG; Affective Computing; EEG Analysis; Epilepsy Detection},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s19071659},
}

@Article{cimtay2020cr,
  author        = {{\c C}imtay, Y{\"u}cel and Ekmekcio{\v g}lu, Erhan and Caglar‐Ozhan, Seyma},
  journal       = {IEEE access},
  title         = {Cross-Subject Multimodal Emotion Recognition Based on Hybrid Fusion},
  year          = {2020},
  month         = jan,
  pages         = {168865--168878},
  volume        = {8},
  abstract      = {Multimodal emotion recognition has gained traction in affective computing research community to overcome the limitations posed by the processing a single form of data and to increase recognition robustness.In this study, a novel emotion recognition system is introduced, which is based on multiple modalities including facial expressions, galvanic skin response (GSR) and electroencephalogram (EEG).This method follows a hybrid fusion strategy and yields a maximum one-subject-out accuracy of 81.2{\%} and a mean accuracy of 74.2{\%} on our bespoke multimodal emotion dataset (LUMED-2) for 3 emotion classes: sad, neutral and happy.Similarly, our approach yields a maximum one-subject-out accuracy of 91.5{\%} and a mean accuracy of 53.8{\%} on the Database for Emotion Analysis using Physiological Signals (DEAP) for varying numbers of emotion classes, 4 in average, including angry, disgust, afraid, happy, neutral, sad and surprised.The presented model is particularly useful in determining the correct emotional state in the case of natural deceptive facial expressions.In terms of emotion recognition accuracy, this study is superior to, or on par with, the reference subject-independent multimodal emotion recognition studies introduced in the literature.},
  bdsk-url-1    = {https://doi.org/10.1109/access.2020.3023871},
  c1            = {Institute for Digital Technologies, Loughborough University London, London E20 3BS, U.K.; Institute for Digital Technologies, Loughborough University London, London E20 3BS, U.K.; Department of Computer Education and Instructional Technology, Hacettepe University, 06800 Ankara, Turkey},
  comment       = {EEG, GSR},
  date-added    = {2024-05-13 15:05:25 +0100},
  date-modified = {2024-05-13 15:05:25 +0100},
  doi           = {10.1109/access.2020.3023871},
  hasabstract   = {Y},
  isbn          = {2169-3536},
  keywords      = {Emotion Recognition; Affective Computing; Emotions; Multimodal Data; Speech Emotion},
  la            = {en},
  priority      = {prio3},
  publisher     = {Institute of Electrical and Electronics Engineers},
  url           = {https://doi.org/10.1109/access.2020.3023871},
}

@Article{liu2021re,
  author           = {Liu, Haoran and Zhang, Ying and Li, Yujun and Kong, Xianwen},
  journal          = {Frontiers in computational neuroscience},
  title            = {Review on Emotion Recognition Based on Electroencephalography},
  year             = {2021},
  month            = oct,
  volume           = {15},
  abstract         = {Emotions are closely related to human behavior, family, and society. Changes in emotions can cause differences in electroencephalography (EEG) signals, which show different emotional states and are not easy to disguise. EEG-based emotion recognition has been widely used in human-computer interaction, medical diagnosis, military, and other fields. In this paper, we describe the common steps of an emotion recognition algorithm based on EEG from data acquisition, preprocessing, feature extraction, feature selection to classifier. Then, we review the existing EEG-based emotional recognition methods, as well as assess their classification effect. This paper will help researchers quickly understand the basic theory of emotion recognition and provide references for the future development of EEG. Moreover, emotion is an important representation of safety psychology.},
  bdsk-url-1       = {https://doi.org/10.3389/fncom.2021.758212},
  c1               = {The Boiler and Pressure Vessel Safety Inspection Institute of Henan Province, China; Patent Examination Cooperation (Henan) Center of the Patent Office, CNIPA, China; The Boiler and Pressure Vessel Safety Inspection Institute of Henan Province, China; The Boiler and Pressure Vessel Safety Inspection Institute of Henan Province, China},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.3389/fncom.2021.758212},
  hasabstract      = {Y},
  isbn             = {1662-5188},
  keywords         = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Affective Computing; Head Gesture Recognition},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Frontiers Media},
  url              = {https://doi.org/10.3389/fncom.2021.758212},
}

@Article{zhao2018em,
  author           = {Zhao, M. and Adib, Fadel and Katabi, Dina},
  journal          = {Communications of the ACM},
  title            = {Emotion recognition using wireless signals},
  year             = {2018},
  month            = aug,
  number           = {9},
  pages            = {91--100},
  volume           = {61},
  abstract         = {This paper demonstrates a new technology that can infer a person's emotions from RF signals reflected off his body. EQ-Radio transmits an RF signal and analyzes its reflections off a person's body to recognize his emotional state (happy, sad, etc.). The key enabler underlying EQ-Radio is a new algorithm for extracting the individual heartbeats from the wireless signal at an accuracy comparable to on-body ECG monitors. The resulting beats are then used to compute emotion-dependent features which feed a machine-learning emotion classifier. We describe the design and implementation of EQ-Radio, and demonstrate through a user study that its emotion recognition accuracy is on par with state-of-the-art emotion recognition systems that require a person to be hooked to an ECG monitor.},
  bdsk-url-1       = {https://doi.org/10.1145/3236621},
  c1               = {Massachusetts Institute of Technology, Cambridge, MA; Massachusetts Institute of Technology, Cambridge, MA; Massachusetts Institute of Technology, Cambridge, MA},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1145/3236621},
  hasabstract      = {Y},
  isbn             = {0001-0782},
  keywords         = {Emotion Recognition; Deep Learning for EEG; Affective Computing; EEG Analysis; ECG Signal},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Association for Computing Machinery},
  url              = {https://doi.org/10.1145/3236621},
}

@Article{girardi2017em,
  author           = {Girardi, Daniela and Lanubile, Filippo and Novielli, Nicole},
  title            = {Emotion detection using noninvasive low cost sensors},
  year             = {2017},
  month            = oct,
  abstract         = {Emotion recognition from biometrics is relevant to a wide range of application domains, including healthcare. Existing approaches usually adopt multi-electrodes sensors that could be expensive or uncomfortable to be used in real-life situations. In this study, we investigate whether we can reliably recognize high vs. low emotional valence and arousal by relying on noninvasive low cost EEG, EMG, and GSR sensors. We report the results of an empirical study involving 19 subjects. We achieve state-of-the-art classification performance for both valence and arousal even in a cross-subject classification setting, which eliminates the need for individual training and tuning of classification models.},
  bdsk-url-1       = {https://doi.org/10.1109/acii.2017.8273589},
  c1               = {University of Bari `Aldo Moro'; University of Bari `Aldo Moro'; University of Bari `Aldo Moro'},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1109/acii.2017.8273589},
  hasabstract      = {Y},
  keywords         = {Emotion Recognition; Epilepsy Detection; EEG Analysis; Affective Computing; Deep Learning for EEG},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.1109/acii.2017.8273589},
}

@Article{menezes2017to,
  author           = {Menezes, Maria and Samara, Anas and Galway, Leo and Sant'Anna, Anita and Verikas, Antanas and Alonso‐Fernandez, Fernando and Wang, H. and Bond, Raymond},
  journal          = {Personal and ubiquitous computing},
  title            = {Towards emotion recognition for virtual environments: an evaluation of eeg features on benchmark dataset},
  year             = {2017},
  month            = aug,
  number           = {6},
  pages            = {1003--1013},
  volume           = {21},
  abstract         = {One of the challenges in virtual environments is the difficulty users have in interacting with these increasingly complex systems. Ultimately, endowing machines with the ability to perceive users emotions will enable a more intuitive and reliable interaction. Consequently, using the electroencephalogram as a bio-signal sensor, the affective state of a user can be modelled and subsequently utilised in order to achieve a system that can recognise and react to the user's emotions. This paper investigates features extracted from electroencephalogram signals for the purpose of affective state modelling based on Russell's Circumplex Model. Investigations are presented that aim to provide the foundation for future work in modelling user affect to enhance interaction experience in virtual environments. The DEAP dataset was used within this work, along with a Support Vector Machine and Random Forest, which yielded reasonable classification accuracies for Valence and Arousal using feature vectors based on statistical measurements and band power from the {$\backslash$}'z, {$\backslash$}b{\{}eta{\}}, {$\backslash$}'z, and {$\backslash$}'z{$\backslash$}'z waves and High Order Crossing of the EEG signal.},
  bdsk-url-1       = {https://doi.org/10.1007/s00779-017-1072-7},
  c1               = {Center for Applied Intelligent Systems Research, Halmstad University, Halmstad, Sweden; School of Computing and Mathematics, Ulster University Belfast, Belfast, UK; School of Computing and Mathematics, Ulster University Belfast, Belfast, UK; Center for Applied Intelligent Systems Research, Halmstad University, Halmstad, Sweden; Center for Applied Intelligent Systems Research, Halmstad University, Halmstad, Sweden; Center for Applied Intelligent Systems Research, Halmstad University, Halmstad, Sweden; School of Computing and Mathematics, Ulster University Belfast, Belfast, UK; School of Computing and Mathematics, Ulster University Belfast, Belfast, UK},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1007/s00779-017-1072-7},
  hasabstract      = {Y},
  isbn             = {1617-4909},
  keywords         = {Emotion Recognition; Affective Computing; Deep Learning for EEG; EEG Analysis; Sensory Processing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Springer Science+Business Media},
  url              = {https://doi.org/10.1007/s00779-017-1072-7},
}

@Article{moerland2017em,
  author           = {Moerland, Thomas and Broekens, Joost and Jonker, Catholijn},
  journal          = {Machine learning},
  title            = {Emotion in reinforcement learning agents and robots: a survey},
  year             = {2017},
  month            = aug,
  number           = {2},
  pages            = {443--480},
  volume           = {107},
  abstract         = {This article provides the first survey of computational models of emotion in reinforcement learning (RL) agents. The survey focuses on agent/robot emotions, and mostly ignores human user emotions. Emotions are recognized as functional in decision-making by influencing motivation and action selection. Therefore, computational emotion models are usually grounded in the agent's decision making architecture, of which RL is an important subclass. Studying emotions in RL-based agents is useful for three research fields. For machine learning (ML) researchers, emotion models may improve learning efficiency. For the interactive ML and human--robot interaction community, emotions can communicate state and enhance user investment. Lastly, it allows affective modelling researchers to investigate their emotion theories in a successful AI agent class. This survey provides background on emotion theory and RL. It systematically addresses (1) from what underlying dimensions (e.g. homeostasis, appraisal) emotions can be derived and how these can be modelled in RL-agents, (2) what types of emotions have been derived from these dimensions, and (3) how these emotions may either influence the learning efficiency of the agent or be useful as social signals. We also systematically compare evaluation criteria, and draw connections to important RL sub-domains like (intrinsic) motivation and model-based RL. In short, this survey provides both a practical overview for engineers wanting to implement emotions in their RL agents, and identifies challenges and directions for future emotion-RL research.},
  bdsk-url-1       = {https://doi.org/10.1007/s10994-017-5666-0},
  c1               = {Delft University of Technology, Mekelweg 4, 2628 CD, Delft, The Netherlands; Delft University of Technology, Mekelweg 4, 2628 CD, Delft, The Netherlands; Delft University of Technology, Mekelweg 4, 2628 CD, Delft, The Netherlands},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1007/s10994-017-5666-0},
  hasabstract      = {Y},
  isbn             = {0885-6125},
  keywords         = {Emotion Recognition; Affective Computing; Emotion Perception; Reinforcement Learning; Human Perception of Robots},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Springer Science+Business Media},
  url              = {https://doi.org/10.1007/s10994-017-5666-0},
}

@Article{zhang2020an,
  author           = {Zhang, Yaqing and Chen, Jinling and Tan, Jen and Chen, Yuxuan and Chen, Yunyi and Li, Dihan and Lei, Yang and Su, Jian and Huang, Xin and Che, Wenliang},
  journal          = {Frontiers in neuroscience},
  title            = {An Investigation of Deep Learning Models for EEG-Based Emotion Recognition},
  year             = {2020},
  month            = dec,
  volume           = {14},
  abstract         = {Emotion is the human brain reacting to objective things. In real life, human emotions are complex and changeable, so research into emotion recognition is of great significance in real life applications. Recently, many deep learning and machine learning methods have been widely applied in emotion recognition based on EEG signals. However, the traditional machine learning method has a major disadvantage in that the feature extraction process is usually cumbersome, which relies heavily on human experts. Then, end-to-end deep learning methods emerged as an effective method to address this disadvantage with the help of raw signal features and time-frequency spectrums. Here, we investigated the application of several deep learning models to the research field of EEG-based emotion recognition, including deep neural networks (DNN), convolutional neural networks (CNN), long short-term memory (LSTM), and a hybrid model of CNN and LSTM (CNN-LSTM). The experiments were carried on the well-known DEAP dataset. Experimental results show that the CNN and CNN-LSTM models had high classification performance in EEG-based emotion recognition, and their accurate extraction rate of RAW data reached 90.12 and 94.17{\%}, respectively. The performance of the DNN model was not as accurate as other models, but the training speed was fast. The LSTM model was not as stable as the CNN and CNN-LSTM models. Moreover, with the same number of parameters, the training speed of the LSTM was much slower and it was difficult to achieve convergence. Additional parameter comparison experiments with other models, including epoch, learning rate, and dropout probability, were also conducted in the paper. Comparison results prove that the DNN model converged to optimal with fewer epochs and a higher learning rate. In contrast, the CNN model needed more epochs to learn. As for dropout probability, reducing the parameters by \~{}50{\%} each time was appropriate.},
  bdsk-url-1       = {https://doi.org/10.3389/fnins.2020.622759},
  c1               = {Department of Cardiology, Shanghai Tenth People's Hospital, Tongji University School of Medicine, Shanghai, China; Department of Software Engineering, School of Informatics Xiamen University (National Demonstative Software School), Xiamen, China; Department of Software Engineering, School of Informatics Xiamen University (National Demonstative Software School), Xiamen, China; Department of Computer and Software, Institute of System Science, National University of Singapore, Singapore, Singapore; Department of Software Engineering, School of Informatics Xiamen University (National Demonstative Software School), Xiamen, China; Department of Software Engineering, School of Informatics Xiamen University (National Demonstative Software School), Xiamen, China; Department of Software Engineering, School of Informatics Xiamen University (National Demonstative Software School), Xiamen, China; Department of Software Engineering, School of Informatics Xiamen University (National Demonstative Software School), Xiamen, China; Nanjing University of Information Science and Technology, Nanjing, China; School of Software, Jiangxi Normal University, Nanchang, China; Department of Cardiology, Shanghai Tenth People's Hospital, Tongji University School of Medicine, Shanghai, China},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.3389/fnins.2020.622759},
  hasabstract      = {Y},
  isbn             = {1662-453X},
  keywords         = {Emotion Recognition; Deep Learning for EEG; Affective Computing; EEG Analysis; Deep Learning},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Frontiers Media},
  url              = {https://doi.org/10.3389/fnins.2020.622759},
}

@Article{muszynski2021re,
  author           = {Muszy{\'n}ski, Micha{\l} and Tian, Leimin and Lai, Catherine and Moore, Johanna and Κωστούλας, Θεόδωρος and Lombardo, Patrizia and Pun, Thierry and Chanel, Guillaume},
  journal          = {IEEE transactions on affective computing},
  title            = {Recognizing Induced Emotions of Movie Audiences from Multimodal Information},
  year             = {2021},
  month            = jan,
  number           = {1},
  pages            = {36--52},
  volume           = {12},
  abstract         = {Recognizing emotional reactions of movie audiences to affective movie content is a challenging task in affective computing. Previous research on induced emotion recognition has mainly focused on using audio-visual movie content. Nevertheless, the relationship between the perceptions of the affective movie content (perceived emotions) and the emotions evoked in the audiences (induced emotions) is unexplored. In this work, we studied the relationship between perceived and induced emotions of movie audiences. Moreover, we investigated multimodal modelling approaches to predict movie induced emotions from movie content based features, as well as physiological and behavioral reactions of movie audiences. To carry out analysis of induced and perceived emotions, we first extended an existing database for movie affect analysis by annotating perceived emotions in a crowd-sourced manner. We find that perceived and induced emotions are not always consistent with each other. In addition, we show that perceived emotions, movie dialogues, and aesthetic highlights are discriminative for movie induced emotion recognition besides spectators' physiological and behavioral reactions. Also, our experiments revealed that induced emotion recognition could benefit from including temporal information and performing multimodal fusion. Moreover, our work deeply investigated the gap between affective content analysis and induced emotion recognition by gaining insight into the relationships between aesthetic highlights, induced emotions, and perceived emotions.},
  bdsk-url-1       = {https://doi.org/10.1109/taffc.2019.2902091},
  c1               = {University of Geneva, Geneva, Switzerland; University of Edinburgh and Monash University, Clayton, VIC, Australia; University of Edinburgh, Edinburgh, United Kingdom; University of Edinburgh, Edinburgh, United Kingdom; Bournemouth University, Poole, United Kingdom; University of Geneva, Geneva, Switzerland; University of Geneva, Geneva, Switzerland; University of Geneva, Geneva, Switzerland; University of Geneva, Geneva, Switzerland},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1109/taffc.2019.2902091},
  hasabstract      = {Y},
  isbn             = {1949-3045},
  keywords         = {Emotion Recognition; Affective Computing; Emotions; Speech Emotion; Affective Design},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Institute of Electrical and Electronics Engineers},
  url              = {https://doi.org/10.1109/taffc.2019.2902091},
}

@Article{athavipach2019aw,
  author           = {Athavipach, Chanavit and Pan-ngum, Setha and Israsena, Pasin},
  journal          = {Sensors},
  title            = {A Wearable In-Ear EEG Device for Emotion Monitoring},
  year             = {2019},
  month            = sep,
  number           = {18},
  pages            = {4014--4014},
  volume           = {19},
  abstract         = {For future healthcare applications, which are increasingly moving towards out-of-hospital or home-based caring models, the ability to remotely and continuously monitor patients'conditions effectively are imperative. Among others, emotional state is one of the conditions that could be of interest to doctors or caregivers. This paper discusses a preliminary study to develop a wearable device that is a low cost, single channel, dry contact, in-ear EEG suitable for non-intrusive monitoring. All aspects of the designs, engineering, and experimenting by applying machine learning for emotion classification, are covered. Based on the valence and arousal emotion model, the device is able to classify basic emotion with 71.07{\%} accuracy (valence), 72.89{\%} accuracy (arousal), and 53.72{\%} (all four emotions). The results are comparable to those measured from the more conventional EEG headsets at T7 and T8 scalp positions. These results, together with its earphone-like wearability, suggest its potential usage especially for future healthcare applications, such as home-based or tele-monitoring systems as intended.},
  bdsk-url-1       = {https://doi.org/10.3390/s19184014},
  c1               = {Department of Computer Engineering, Faculty of Engineering, Chulalongkorn University, Phayathai Road, Wang Mai, Pathumwan, Bangkok 10330, Thailand; Department of Computer Engineering, Faculty of Engineering, Chulalongkorn University, Phayathai Road, Wang Mai, Pathumwan, Bangkok 10330, Thailand; National Electronics and Computer Technology Center, 112 Thailand Science Park, Phahonyothin Road, Khlong Nueng, Khlong Luang, Pathumthani 12120, Thailand},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.3390/s19184014},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {EEG Analysis; Deep Learning for EEG; Emotion Recognition; Speech Emotion; Affective Computing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s19184014},
}

@Article{asghar2019ee,
  author           = {Asghar, Muhammad and Khan, Muhammad and Fawad, Fawad and Amin, Yasar and Rizwan, Muhammad and Rahman, MuhibUr and Badnava, Salman and Mirjavadi, Seyed},
  journal          = {Sensors},
  title            = {EEG-Based Multi-Modal Emotion Recognition using Bag of Deep Features: An Optimal Feature Selection Approach},
  year             = {2019},
  month            = nov,
  number           = {23},
  pages            = {5218--5218},
  volume           = {19},
  abstract         = {Much attention has been paid to the recognition of human emotions with the help of electroencephalogram (EEG) signals based on machine learning technology. Recognizing emotions is a challenging task due to the non-linear property of the EEG signal. This paper presents an advanced signal processing method using the deep neural network (DNN) for emotion recognition based on EEG signals. The spectral and temporal components of the raw EEG signal are first retained in the 2D Spectrogram before the extraction of features. The pre-trained AlexNet model is used to extract the raw features from the 2D Spectrogram for each channel. To reduce the feature dimensionality, spatial, and temporal based, bag of deep features (BoDF) model is proposed. A series of vocabularies consisting of 10 cluster centers of each class is calculated using the k-means cluster algorithm. Lastly, the emotion of each subject is represented using the histogram of the vocabulary set collected from the raw-feature of a single channel. Features extracted from the proposed BoDF model have considerably smaller dimensions. The proposed model achieves better classification accuracy compared to the recently reported work when validated on SJTU SEED and DEAP data sets. For optimal classification performance, we use a support vector machine (SVM) and k-nearest neighbor (k-NN) to classify the extracted features for the different emotional states of the two data sets. The BoDF model achieves 93.8{\%} accuracy in the SEED data set and 77.4{\%} accuracy in the DEAP data set, which is more accurate compared to other state-of-the-art methods of human emotion recognition.},
  bdsk-url-1       = {https://doi.org/10.3390/s19235218},
  c1               = {Department of Telecommunication Engineering, University of Engineering and Technology, Taxila 47050, Pakistan; Department of Telecommunication Engineering, University of Engineering and Technology, Taxila 47050, Pakistan; Department of Telecommunication Engineering, University of Engineering and Technology, Taxila 47050, Pakistan; Department of Telecommunication Engineering, University of Engineering and Technology, Taxila 47050, Pakistan; Department of Computer Engineering, University of Engineering and Technology, Taxila 47050, Pakistan; Department of Electrical Engineering, Polytechnique Montreal, Montreal, QC H3T 1J4, Canada; Department of Computer Science and Engineering, College of Engineering, Qatar University, P.O. Box 2713 Doha, Qatar; Department of Mechanical and Industrial Engineering, College of Engineering, Qatar University, P.O. Box 2713 Doha, Qatar},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.3390/s19235218},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; ECG Signal},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s19235218},
}

@Article{han2017fr,
  author        = {Han, Jing and Zhang, Zixing and Schmitt, Maximilian and Panti{\'c}, Maja and Schuller, Bj{\"o}rn},
  title         = {From Hard to Soft},
  year          = {2017},
  month         = oct,
  abstract      = {Over the last decade, automatic emotion recognition has become well established. The gold standard target is thereby usually calculated based on multiple annotations from different raters. All related efforts assume that the emotional state of a human subject can be identified by a 'hard' category or a unique value. This assumption tries to ease the human observer's subjectivity when observing patterns such as the emotional state of others. However, as the number of annotators cannot be infinite, uncertainty remains in the emotion target even if calculated from several, yet few human annotators. The common procedure to use this same emotion target in the learning process thus inevitably introduces noise in terms of an uncertain learning target. In this light, we propose a 'soft' prediction framework to provide a more human-like and comprehensive prediction of emotion. In our novel framework, we provide an additional target to indicate the uncertainty of human perception based on the inter-rater disagreement level, in contrast to the traditional framework which is merely producing one single prediction (category or value). To exploit the dependency between the emotional state and the newly introduced perception uncertainty, we implement a multi-task learning strategy. To evaluate the feasibility and effectiveness of the proposed soft prediction framework, we perform extensive experiments on a time- and value-continuous spontaneous audiovisual emotion database including late fusion results. We show that the soft prediction framework with multi-task learning of the emotional state and its perception uncertainty significantly outperforms the individual tasks in both the arousal and valence dimensions.},
  bdsk-url-1    = {https://doi.org/10.1145/3123266.3123383},
  c1            = {University of Augsburg \& University of Passau, Augsburg, Germany; University of Passau, Passau, Germany; University of Augsburg \& University of Passau, Augsburg, Germany; Imperial College London, London, United Kingdom; University of Augsburg \& University of Passau, Augsburg, Germany},
  comment       = {no music},
  date-added    = {2024-05-13 15:05:25 +0100},
  date-modified = {2024-05-13 15:05:25 +0100},
  doi           = {10.1145/3123266.3123383},
  hasabstract   = {Y},
  keywords      = {Affective Computing; Emotion Recognition; Speech Emotion},
  la            = {en},
  priority      = {prio3},
  url           = {https://doi.org/10.1145/3123266.3123383},
}

@Article{ding2023ts,
  author           = {Ding, Yi and Robinson, Neethu and Zhang, Su and Zeng, Qiuhao and Guan, Cuntai},
  journal          = {IEEE transactions on affective computing},
  title            = {TSception: Capturing Temporal Dynamics and Spatial Asymmetry From EEG for Emotion Recognition},
  year             = {2023},
  month            = jul,
  number           = {3},
  pages            = {2238--2250},
  volume           = {14},
  abstract         = {The high temporal resolution and the asymmetric spatial activations are essential attributes of electroencephalogram (EEG) underlying emotional processes in the brain. To learn the temporal dynamics and spatial asymmetry of EEG towards accurate and generalized emotion recognition, we propose TSception, a multi-scale convolutional neural network that can classify emotions from EEG. TSception consists of dynamic temporal, asymmetric spatial, and high-level fusion layers, which learn discriminative representations in the time and channel dimensions simultaneously. The dynamic temporal layer consists of multi-scale 1D convolutional kernels whose lengths are related to the sampling rate of EEG, which learns the dynamic temporal and frequency representations of EEG. The asymmetric spatial layer takes advantage of the asymmetric EEG patterns for emotion, learning the discriminative global and hemisphere representations. The learned spatial representations will be fused by a high-level fusion layer. Using more generalized cross-validation settings, the proposed method is evaluated on two publicly available datasets DEAP and MAHNOB-HCI. The performance of the proposed network is compared with prior reported methods such as SVM, KNN, FBFgMDM, FBTSC, Unsupervised learning, DeepConvNet, ShallowConvNet, and EEGNet. TSception achieves higher classification accuracies and F1 scores than other methods in most of the experiments. The codes are available at https://github.com/yi-ding-cs/TSception},
  bdsk-url-1       = {https://doi.org/10.1109/taffc.2022.3169001},
  c1               = {School of Computer Science and Engineering, Nanyang Technological University, 54761 Singapore, Singapore, Singapore, 639798; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computing Science and Engineering, Nanyang Technological University, 54761 Singapore, Singapore, Singapore, 639798; School of Computer Science and Engineering, Nanyang Technological University, 54761 Singapore, Singapore, Singapore; School of Computer Science and Engineering, Nanyang Technological University, 54761 Singapore, North West, Singapore, 639798},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1109/taffc.2022.3169001},
  hasabstract      = {Y},
  isbn             = {1949-3045},
  keywords         = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Epilepsy Detection},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Institute of Electrical and Electronics Engineers},
  url              = {https://doi.org/10.1109/taffc.2022.3169001},
}

@Article{he2020ad,
  author           = {He, Zhipeng and Li, Zina and Yang, Fuzhou and Wang, Lei and Li, Jingcong and Zhou, Chengju and Pan, Jiahui},
  journal          = {Brain sciences},
  title            = {Advances in Multimodal Emotion Recognition Based on Brain--Computer Interfaces},
  year             = {2020},
  month            = sep,
  number           = {10},
  pages            = {687--687},
  volume           = {10},
  abstract         = {With the continuous development of portable noninvasive human sensor technologies such as brain--computer interfaces (BCI), multimodal emotion recognition has attracted increasing attention in the area of affective computing. This paper primarily discusses the progress of research into multimodal emotion recognition based on BCI and reviews three types of multimodal affective BCI (aBCI): aBCI based on a combination of behavior and brain signals, aBCI based on various hybrid neurophysiology modalities and aBCI based on heterogeneous sensory stimuli. For each type of aBCI, we further review several representative multimodal aBCI systems, including their design principles, paradigms, algorithms, experimental results and corresponding advantages. Finally, we identify several important issues and research directions for multimodal emotion recognition based on BCI.},
  bdsk-url-1       = {https://doi.org/10.3390/brainsci10100687},
  c1               = {School of Software, South China Normal University, Foshan 528225, China; School of Computer, South China Normal University, Guangzhou 510641, China; School of Software, South China Normal University, Foshan 528225, China; School of Software, South China Normal University, Foshan 528225, China; School of Software, South China Normal University, Foshan 528225, China; School of Software, South China Normal University, Foshan 528225, China; School of Software, South China Normal University, Foshan 528225, China},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.3390/brainsci10100687},
  hasabstract      = {Y},
  isbn             = {2076-3425},
  keywords         = {Emotion Recognition; Affective Computing; Brain-Computer Interfaces; BCI Technology; BCI Communication},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/brainsci10100687},
}

@Article{vinola2015as,
  author           = {Vinola, C and Vimaladevi, K.},
  journal          = {ELCVIA. Electronic letters on computer vision and image analysis},
  title            = {A Survey on Human Emotion Recognition Approaches, Databases and Applications},
  year             = {2015},
  month            = dec,
  number           = {2},
  pages            = {24--24},
  volume           = {14},
  abstract         = {This paper presents the various emotion classification and recognition systems which implement methods aiming at improving Human Machine Interaction. The modalities and approaches used for affect detection vary and contribute to accuracy and efficacy in detecting emotions of human beings. This paper discovers them in a comparison and descriptive manner. Various applications that use the methodologies in different contexts to address the challenges in real time are discussed. This survey also describes the databases that can be used as standard data sets in the process of emotion identification. Thus an integrated discussion of methods, databases used and applications pertaining to the emerging field of Affective Computing (AC) is done and surveyed.This paper presents the various emotion classification and recognition systems which implement methods aiming at improving Human Machine Interaction. The modalities and approaches used for affect detection vary and contribute to accuracy and efficacy in detecting emotions of human beings. This paper discovers them in a comparison and descriptive manner. Various applications that use the methodologies in different contexts to address the challenges in real time are discussed. This survey also describes the databases that can be used as standard data sets in the process of emotion identification. Thus an integrated discussion of methods, databases used and applications pertaining to the emerging field of Affective Computing (AC) is done and surveyed.},
  bdsk-url-1       = {https://doi.org/10.5565/rev/elcvia.795},
  c1               = {Francis Xavier Engineering College; P.S.R.Engineering College},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.5565/rev/elcvia.795},
  hasabstract      = {Y},
  isbn             = {1577-5097},
  keywords         = {Emotion Recognition; Affective Computing; Human-Computer Interaction; Affective Design; Emotions},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Computer Vision Center Press},
  url              = {https://doi.org/10.5565/rev/elcvia.795},
}

@Article{lotfian2019cu,
  author           = {Lotfian, Reza and Busso, Carlos},
  journal          = {IEEE/ACM transactions on audio, speech, and language processing},
  title            = {Curriculum Learning for Speech Emotion Recognition From Crowdsourced Labels},
  year             = {2019},
  month            = apr,
  number           = {4},
  pages            = {815--826},
  volume           = {27},
  abstract         = {This study introduces a method to design a curriculum for machine-learning to maximize the efficiency during the training process of deep neural networks (DNNs) for speech emotion recognition. Previous studies in other machine-learning problems have shown the benefits of training a classifier following a curriculum where samples are gradually presented in increasing level of difficulty. For speech emotion recognition, the challenge is to establish a natural order of difficulty in the training set to create the curriculum. We address this problem by assuming that, ambiguous samples for humans are also ambiguous for computers. Speech samples are often annotated by multiple evaluators to account for differences in emotion perception across individuals. While some sentences with clear emotional content are consistently annotated, sentences with more ambiguous emotional content present important disagreement between individual evaluations. We propose to use the disagreement between evaluators as a measure of difficulty for the classification task. We propose metrics that quantify the inter-evaluation agreement to define the curriculum for regression problems and binary and multi-class classification problems. The experimental results consistently show that relying on a curriculum based on agreement between human judgments leads to statistically significant improvements over baselines trained without a curriculum.},
  bdsk-url-1       = {https://doi.org/10.1109/taslp.2019.2898816},
  c1               = {Erik Jonsson School of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, TX, USA; Erik Jonsson School of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, TX, USA},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1109/taslp.2019.2898816},
  hasabstract      = {Y},
  isbn             = {2329-9290},
  keywords         = {Emotion Recognition; Affective Computing; Speech Emotion; Environmental Sound Recognition; Deep Learning},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Institute of Electrical and Electronics Engineers},
  url              = {https://doi.org/10.1109/taslp.2019.2898816},
}

@Article{gannouni2021em,
  author           = {Gannouni, Sofien and Aledaily, Arwa and Belwafi, Kais and Aboalsamh, Hatim},
  journal          = {Scientific reports},
  title            = {Emotion detection using electroencephalography signals and a zero-time windowing-based epoch estimation and relevant electrode identification},
  year             = {2021},
  month            = mar,
  number           = {1},
  volume           = {11},
  abstract         = {Recognizing emotions using biological brain signals requires accurate and efficient signal processing and feature extraction methods. Existing methods use several techniques to extract useful features from a fixed number of electroencephalography (EEG) channels. The primary objective of this study was to improve the performance of emotion recognition using brain signals by applying a novel and adaptive channel selection method that acknowledges that brain activity has a unique behavior that differs from one person to another and one emotional state to another. Moreover, we propose identifying epochs, which are the instants at which excitation is maximum, during the emotion to improve the system's accuracy. We used the zero-time windowing method to extract instantaneous spectral information using the numerator group-delay function to accurately detect the epochs in each emotional state. Different classification scheme were defined using QDC and RNN and evaluated using the DEAP database. The experimental results showed that the proposed method is highly competitive compared with existing studies of multi-class emotion recognition. The average accuracy rate exceeded 89{\%}. Compared with existing algorithms dealing with 9 emotions, the proposed method enhanced the accuracy rate by 8{\%}. Moreover, experiment shows that the proposed system outperforms similar approaches discriminating between 3 and 4 emotions only. We also found that the proposed method works well, even when applying conventional classification algorithms.},
  bdsk-url-1       = {https://doi.org/10.1038/s41598-021-86345-5},
  c1               = {Computer Science Department, College of Computer and Information Sciences, King Saud University, Riyadh, 11543, Saudi Arabia; Computer Science Department, College of Computer and Information Sciences, King Saud University, Riyadh, 11543, Saudi Arabia; Computer Science Department, College of Computer and Information Sciences, King Saud University, Riyadh, 11543, Saudi Arabia; Computer Science Department, College of Computer and Information Sciences, King Saud University, Riyadh, 11543, Saudi Arabia},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1038/s41598-021-86345-5},
  hasabstract      = {Y},
  isbn             = {2045-2322},
  keywords         = {Emotion Recognition; Affective Computing; Epilepsy Detection; EEG Analysis; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Nature Portfolio},
  url              = {https://doi.org/10.1038/s41598-021-86345-5},
}

@Article{gatti2018em,
  author           = {Gatti, Elia and Calzolari, Elena and Maggioni, Emanuela and Obrist, Marianna},
  journal          = {Scientific data},
  title            = {Emotional ratings and skin conductance response to visual, auditory and haptic stimuli},
  year             = {2018},
  month            = jun,
  number           = {1},
  volume           = {5},
  abstract         = {Abstract The human emotional reactions to stimuli delivered by different sensory modalities is a topic of interest for many disciplines, from Human-Computer-Interaction to cognitive sciences. Different databases of stimuli eliciting emotional reaction are available, tested on a high number of participants. Interestingly, stimuli within one database are always of the same type. In other words, to date, no data was obtained and compared from distinct types of emotion-eliciting stimuli from the same participant. This makes it difficult to use different databases within the same experiment, limiting the complexity of experiments investigating emotional reactions. Moreover, whereas the stimuli and the participants'rating to the stimuli are available, physiological reactions of participants to the emotional stimuli are often recorded but not shared. Here, we test stimuli delivered either through a visual, auditory, or haptic modality in a within participant experimental design. We provide the results of our study in the form of a MATLAB structure including basic demographics on the participants, the participant's self-assessment of his/her emotional state, and his/her physiological reactions (i.e., skin conductance).},
  bdsk-url-1       = {https://doi.org/10.1038/sdata.2018.120},
  c1               = {Sussex Computer Human Interaction (SCHI) Lab, School of Engineering and Informatics, University of Sussex, Brighton, UK; Imperial College, London, UK; Sussex Computer Human Interaction (SCHI) Lab, School of Engineering and Informatics, University of Sussex, Brighton, UK; Sussex Computer Human Interaction (SCHI) Lab, School of Engineering and Informatics, University of Sussex, Brighton, UK},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1038/sdata.2018.120},
  hasabstract      = {Y},
  isbn             = {2052-4463},
  keywords         = {Sensory Analysis; Emotional Responses; Affective Computing; Emotion Recognition; Emotions},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Nature Portfolio},
  url              = {https://doi.org/10.1038/sdata.2018.120},
}

@Article{larradet2020to,
  author           = {Larradet, Fanny and Niewiadomski, Rados{\l}aw and Barresi, Giacinto and Caldwell, Darwin and Mattos, Leonardo},
  journal          = {Frontiers in psychology},
  title            = {Toward Emotion Recognition From Physiological Signals in the Wild: Approaching the Methodological Issues in Real-Life Data Collection},
  year             = {2020},
  month            = jul,
  volume           = {11},
  abstract         = {Emotion, mood, and stress recognition (EMSR) has been studied in laboratory settings for decades. In particular, physiological signals are widely used to detect and classify affective states in lab conditions. However, physiological reactions to emotional stimuli have been found to differ in laboratory and natural settings. Thanks to recent technological progress (e.g., in wearables) the creation of EMSR systems for a large number of consumers during their everyday activities is increasingly possible. Therefore, datasets created in the wild are needed to insure the validity and the exploitability of EMSR models for real-life applications. In this paper, we initially present common techniques used in laboratory settings to induce emotions for the purpose of physiological dataset creation. Next, advantages and challenges of data collection in the wild are discussed. To assess the applicability of existing datasets to real-life applications, we propose a set of categories to guide and compare at a glance different methodologies used by researchers to collect such data. For this purpose, we also introduce a visual tool called Graphical Assessment of Real-life Application-Focused Emotional Dataset (GARAFED). In the last part of the paper, we apply the proposed tool to compare existing physiological datasets for EMSR in the wild and to show possible improvements and future directions of research. We wish for this paper and GARAFED to be used as guidelines for researchers and developers who aim at collecting affect-related data for real-life EMSR-based applications.},
  bdsk-url-1       = {https://doi.org/10.3389/fpsyg.2020.01111},
  c1               = {Advanced Robotics, Istituto Italiano di Tecnologia, Genoa, Italy; Contact Unit, Istituto Italiano di Tecnologia, Genoa, Italy; Department of Psychology and Cognitive Science, University of Trento, Rovereto, Italy; Rehab Technologies, Istituto Italiano di Tecnologia, Genoa, Italy; Advanced Robotics, Istituto Italiano di Tecnologia, Genoa, Italy; Advanced Robotics, Istituto Italiano di Tecnologia, Genoa, Italy},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.3389/fpsyg.2020.01111},
  hasabstract      = {Y},
  isbn             = {1664-1078},
  keywords         = {Emotion Recognition; Emotion Regulation; Affective Computing; Speech Emotion; Physiological Signals},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Frontiers Media},
  url              = {https://doi.org/10.3389/fpsyg.2020.01111},
}

@Article{cheng2019he,
  author           = {Cheng, Xiefeng and Wang, Yue and Dai, Shicheng and Zhao, Pei and Liu, Qifa},
  journal          = {Scientific reports},
  title            = {Heart sound signals can be used for emotion recognition},
  year             = {2019},
  month            = apr,
  number           = {1},
  volume           = {9},
  abstract         = {Abstract This article studies whether heart sound signals can be used for emotion recognition. First, we built a small emotion heart sound database, and simultaneously recorded the participants'ECG for comparative analysis. Second, according to the characteristics of the heart sound signals, two emotion evaluation indicators were proposed: HRV of heart sounds (difference between successive heartbeats) and DSV of heart sounds (the ratio of diastolic to systolic duration variability). Then, we extracted linear and nonlinear features from two emotion evaluation indicators to recognize four kinds of emotions. Moreover, we used valence dimension, arousal dimension and valence-arousal synthesis as evaluation standards. The experimental results demonstrated that heart sound signals can be used for emotion recognition. It was more effective to achieve recognition results by combining the features of HRV and DSV of heart sounds. Finally, the average accuracy of four emotion recognitions on valence dimension, arousal dimension and valence-arousal synthesis was up to 96.875{\%}, 88.5417{\%} and 81.25{\%}, respectively.},
  bdsk-url-1       = {https://doi.org/10.1038/s41598-019-42826-2},
  c1               = {College of Electronic and Optical Engineering, Nanjing University of Posts and Telecommunications, Nanjing, 210003, China; College of Electronic and Optical Engineering, Nanjing University of Posts and Telecommunications, Nanjing, 210003, China; College of Electronic and Optical Engineering, Nanjing University of Posts and Telecommunications, Nanjing, 210003, China; Pediatric Cardiology, Xin Hua Hospital Affiliated to Shanghai Jiao Tong University School of Medicine, Shanghai, 200092, China; College of Telecommunication and Information Engineering, Nanjing University of Posts and Telecommunications, Nanjing, 210003, China},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1038/s41598-019-42826-2},
  hasabstract      = {Y},
  isbn             = {2045-2322},
  keywords         = {Emotion Recognition; Affective Computing; Heart Sound; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Nature Portfolio},
  url              = {https://doi.org/10.1038/s41598-019-42826-2},
}

@Article{zhao2021em,
  author        = {Zhao, Sicheng and Jia, Guoli and Yang, Jufeng and Ding, Guiguang and Keutzer, Kurt},
  journal       = {IEEE signal processing magazine},
  title         = {Emotion Recognition From Multiple Modalities: Fundamentals and methodologies},
  year          = {2021},
  month         = nov,
  number        = {6},
  pages         = {59--73},
  volume        = {38},
  abstract      = {Humans are emotional creatures. Multiple modalities are often involved when we express emotions, whether we do so explicitly (such as through facial expression and speech) or implicitly (e.g., via text or images). Enabling machines to have emotional intelligence, i.e., recognizing, interpreting, processing, and simulating emotions, is becoming increasingly important. In this tutorial, we discuss several key aspects of multimodal emotion recognition (MER).},
  bdsk-url-1    = {https://doi.org/10.1109/msp.2021.3106895},
  c1            = {Research Scientist, Columbia University, New York, New York, USA; College of Computer Science, Nankai University, Tianjin, China; College of Computer Science, Nankai University, Tianjin, China; School of Software, Tsinghua University, Beijing, China; Electrical Engineering and Computer Science, University of California, Berkeley, California, USA},
  comment       = {no music},
  date-added    = {2024-05-13 15:05:25 +0100},
  date-modified = {2024-05-13 15:05:25 +0100},
  doi           = {10.1109/msp.2021.3106895},
  hasabstract   = {Y},
  isbn          = {1053-5888},
  keywords      = {Emotion Recognition; Affective Computing; Speech Emotion; Multimodal Data; Facial Expression},
  la            = {en},
  priority      = {prio3},
  publisher     = {Institute of Electrical and Electronics Engineers},
  url           = {https://doi.org/10.1109/msp.2021.3106895},
}

@Article{braun2019im,
  author           = {Braun, Michael and Schubert, Jonas and Pfleging, Bastian and Alt, Florian},
  journal          = {Multimodal technologies and interaction},
  title            = {Improving Driver Emotions with Affective Strategies},
  year             = {2019},
  month            = mar,
  number           = {1},
  pages            = {21--21},
  volume           = {3},
  abstract         = {Drivers in negative emotional states, such as anger or sadness, are prone to perform bad at driving, decreasing overall road safety for all road users. Recent advances in affective computing, however, allow for the detection of such states and give us tools to tackle the connected problems within automotive user interfaces. We see potential in building a system which reacts upon possibly dangerous driver states and influences the driver in order to drive more safely. We compare different interaction approaches for an affective automotive interface, namely Ambient Light, Visual Notification, a Voice Assistant, and an Empathic Assistant. Results of a simulator study with 60 participants (30 each with induced sadness/anger) indicate that an emotional voice assistant with the ability to empathize with the user is the most promising approach as it improves negative states best and is rated most positively. Qualitative data also shows that users prefer an empathic assistant but also resent potential paternalism. This leads us to suggest that digital assistants are a valuable platform to improve driver emotions in automotive environments and thereby enable safer driving.},
  bdsk-url-1       = {https://doi.org/10.3390/mti3010021},
  c1               = {BMW Group Research, New Technologies, Innovations, 85748 Garching, Germany; LMU Munich, 80337 Munich, Germany; LMU Munich, 80337 Munich, Germany; Eindhoven University of Technology, 5612 Eindhoven, The Netherlands; LMU Munich, 80337 Munich, Germany; CODE Research Institute, Bundeswehr University, 81739 Munich, Germany; LMU Munich, 80337 Munich, Germany},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.3390/mti3010021},
  hasabstract      = {Y},
  isbn             = {2414-4088},
  keywords         = {Affective Computing; Driver Fatigue; Emotion Recognition; Affective Design; Emotions},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/mti3010021},
}

@Article{zhao2019pe,
  author           = {Zhao, Sicheng and Gholaminejad, Amir and Ding, Guiguang and Gao, Yue and Han, Jungong and Keutzer, Kurt},
  journal          = {ACM transactions on multimedia computing, communications and applications/ACM transactions on multimedia computing communications and applications},
  title            = {Personalized Emotion Recognition by Personality-Aware High-Order Learning of Physiological Signals},
  year             = {2019},
  month            = jan,
  number           = {1s},
  pages            = {1--18},
  volume           = {15},
  abstract         = {Due to the subjective responses of different subjects to physical stimuli, emotion recognition methodologies from physiological signals are increasingly becoming personalized. Existing works mainly focused on modeling the involved physiological corpus of each subject, without considering the psychological factors, such as interest and personality. The latent correlation among different subjects has also been rarely examined. In this article, we propose to investigate the influence of personality on emotional behavior in a hypergraph learning framework. Assuming that each vertex is a compound tuple (subject, stimuli), multi-modal hypergraphs can be constructed based on the personality correlation among different subjects and on the physiological correlation among corresponding stimuli. To reveal the different importance of vertices, hyperedges, and modalities, we learn the weights for each of them. As the hypergraphs connect different subjects on the compound vertices, the emotions of multiple subjects can be simultaneously recognized. In this way, the constructed hypergraphs are vertex-weighted multi-modal multi-task ones. The estimated factors, referred to as emotion relevance, are employed for emotion recognition. We carry out extensive experiments on the ASCERTAIN dataset and the results demonstrate the superiority of the proposed method, as compared to the state-of-the-art emotion recognition approaches.},
  bdsk-url-1       = {https://doi.org/10.1145/3233184},
  c1               = {Tsinghua University, China and University of California Berkeley, Berkeley; University of California Berkeley, Berkeley; Tsinghua University, Beijing, China; Tsinghua University, Beijing, China; Lancaster University, Lancaster, UK; University of California Berkeley, Berkeley},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1145/3233184},
  hasabstract      = {Y},
  isbn             = {1551-6857},
  keywords         = {Emotion Recognition; Affective Computing; Physiological Signals; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Association for Computing Machinery},
  url              = {https://doi.org/10.1145/3233184},
}

@Article{liu2016em,
  author           = {Liu, Jingxin and Meng, Hongying and Nandi, Asoke and Li, Maozhen},
  title            = {Emotion detection from EEG recordings},
  year             = {2016},
  month            = aug,
  abstract         = {Human brain behavior is very complex and it is difficult to interpret. Human emotion might come from brain activities. However, the relationship between human emotion and brain activities is far from clear. In recent years, more and more researchers are trying to discover this relationship by recording brain signals such as electroencephalogram (EEG) signals with the associated emotion information extracted from other modalities such as facial expression. In this paper, machine learning based methods are used to model this relationship in the publicly available dataset DEAP (Database for Emotional Analysis using Physiological Signals). Different features are extracted from raw EEG recordings. Then Maximum Relevance Minimum Redundancy (mRMR) was used for feature selection. These features are fed into machine learning methods to build the prediction models to extract the emotion information from EEG signals. The models are evaluated on this dataset and satisfactory results are achieved.},
  bdsk-url-1       = {https://doi.org/10.1109/fskd.2016.7603437},
  c1               = {Department of Electronic and Computer Engineering Brunel University London London United Kingdom; Department of Electronic and Computer Engineering Brunel University London London United Kingdom; Dept of Electronic and Computer Engineering, Brunel University London, London, United Kingdom; The Key Laboratory of Embedded Systems and Service, Tongji University, Shanghai, China; Dept of Electronic and Computer Engineering, Brunel University London, London, United Kingdom; The Key Laboratory of Embedded Systems and Service, Tongji University, Shanghai, China},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1109/fskd.2016.7603437},
  hasabstract      = {Y},
  keywords         = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.1109/fskd.2016.7603437},
}

@Article{yuvaraj2014op,
  author           = {Yuvaraj, Rajamanickam and Murugappan, M. and Ibrahim, Norlinah and Sundaraj, Kenneth and Omar, Mohammad and Mohamad, Khairiyah and Palaniappan, Ramaswamy},
  journal          = {International journal of psychophysiology},
  title            = {Optimal set of EEG features for emotional state classification and trajectory visualization in Parkinson's disease},
  year             = {2014},
  month            = dec,
  number           = {3},
  pages            = {482--495},
  volume           = {94},
  abstract         = {In addition to classic motor signs and symptoms, individuals with Parkinson's disease (PD) are characterized by emotional deficits. Ongoing brain activity can be recorded by electroencephalograph (EEG) to discover the links between emotional states and brain activity. This study utilized machine-learning algorithms to categorize emotional states in PD patients compared with healthy controls (HC) using EEG. Twenty non-demented PD patients and 20 healthy age-, gender-, and education level-matched controls viewed happiness, sadness, fear, anger, surprise, and disgust emotional stimuli while fourteen-channel EEG was being recorded. Multimodal stimulus (combination of audio and visual) was used to evoke the emotions. To classify the EEG-based emotional states and visualize the changes of emotional states over time, this paper compares four kinds of EEG features for emotional state classification and proposes an approach to track the trajectory of emotion changes with manifold learning. From the experimental results using our EEG data set, we found that (a) bispectrum feature is superior to other three kinds of features, namely power spectrum, wavelet packet and nonlinear dynamical analysis; (b) higher frequency bands (alpha, beta and gamma) play a more important role in emotion activities than lower frequency bands (delta and theta) in both groups and; (c) the trajectory of emotion changes can be visualized by reducing subject-independent features with manifold learning. This provides a promising way of implementing visualization of patient's emotional state in real time and leads to a practical system for noninvasive assessment of the emotional impairments associated with neurological disorders.},
  bdsk-url-1       = {https://doi.org/10.1016/j.ijpsycho.2014.07.014},
  c1               = {School of Mechatronic Engineering, University Malaysia Perlis (UniMAP), Campus Ulu Pauh, Arau, 02600, Perlis, Malaysia; School of Mechatronic Engineering, University Malaysia Perlis (UniMAP), Campus Ulu Pauh, Arau, 02600, Perlis, Malaysia; Neurology Unit, Department of Medicine, UKM Medical Center, Jalan Yaacob Latiff, 56000, Bandar Tun Razak, Kuala Lumpur, Malaysia; School of Mechatronic Engineering, University Malaysia Perlis (UniMAP), Campus Ulu Pauh, Arau, 02600, Perlis, Malaysia; School of Mechatronic Engineering, University Malaysia Perlis (UniMAP), Campus Ulu Pauh, Arau, 02600, Perlis, Malaysia; Neurology Unit, Department of Medicine, UKM Medical Center, Jalan Yaacob Latiff, 56000, Bandar Tun Razak, Kuala Lumpur, Malaysia; School of Computing, University of Kent, Medway, UK},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1016/j.ijpsycho.2014.07.014},
  hasabstract      = {Y},
  isbn             = {0167-8760},
  keywords         = {EEG Analysis; Deep Learning for EEG; Emotion Recognition; Epilepsy Detection; Affective Computing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Elsevier BV},
  url              = {https://doi.org/10.1016/j.ijpsycho.2014.07.014},
}

@Article{castillo2016so,
  author           = {Castillo, Jos{\'e} and Castro‐Gonz{\'a}lez, {\'A}lvaro and Fern{\'a}ndez-Caballero, Antonio and Latorre, Jos{\'e} and Pastor, Jos{\'e} and Fern{\'a}ndez-Sotos, Alicia and Salichs, Miguel},
  journal          = {Cognitive computation},
  title            = {Software Architecture for Smart Emotion Recognition and Regulation of the Ageing Adult},
  year             = {2016},
  month            = feb,
  number           = {2},
  pages            = {357--367},
  volume           = {8},
  abstract         = {This paper introduces the architecture of an emotion-aware ambient intelligent and gerontechnological project named "Improvement of the Elderly Quality of Life and Care through Smart Emotion Regulation". The objective of the proposal is to find solutions for improving the quality of life and care of the elderly who can or want to continue living at home by using emotion regulation techniques. A series of sensors is used for monitoring the elderlies' facial and gestural expression, activity and behaviour, as well as relevant physiological data. This way the older people's emotions are inferred and recognized. Music, colour and light are the stimulating means to regulate their emotions towards a positive and pleasant mood. Then, the paper proposes a gerontechnological software architecture that enables real-time, continuous monitoring of the elderly and provides the best-tailored reactions of the ambience in order to regulate the older person's emotions towards a positive mood. After describing the benefits of the approach for emotion recognition and regulation in the elderly, the eight levels that compose the architecture are described.},
  bdsk-url-1       = {https://doi.org/10.1007/s12559-016-9383-y},
  c1               = {Instituto de Investigaci{\'o}n en Inform{\'a}tica de Albacete, Universidad de Castilla-La Mancha, 02071, Albacete, Spain; Robotics Lab, Universidad Carlos III de Madrid, 28911, Madrid, Spain; Instituto de Investigaci{\'o}n en Inform{\'a}tica de Albacete, Universidad de Castilla-La Mancha, 02071, Albacete, Spain; Instituto de Investigaci{\'o}n en Discapacidades Neurol{\'o}gicas, Universidad de Castilla-La Mancha, 02071, Albacete, Spain; Instituto de Tecnolog{\'\i}as Audiovisuales, Universidad de Castilla-La Mancha, 16071, Cuenca, Spain; Facultad de Educaci{\'o}n de Albacete, Universidad de Castilla-La Mancha, 02071, Albacete, Spain; Robotics Lab, Universidad Carlos III de Madrid, 28911, Madrid, Spain},
  date-added       = {2024-05-13 15:05:25 +0100},
  date-modified    = {2024-05-13 15:05:25 +0100},
  doi              = {10.1007/s12559-016-9383-y},
  hasabstract      = {Y},
  isbn             = {1866-9956},
  keywords         = {Emotion Recognition; Emotion Perception; Affective Computing; Emotions},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Springer Science+Business Media},
  url              = {https://doi.org/10.1007/s12559-016-9383-y},
}

@Article{tian2017re,
  author           = {Tian, Leimin and Muszy{\'n}ski, Micha{\l} and Lai, Catherine and Moore, Johanna and Κωστούλας, Θεόδωρος and Lombardo, Patrizia and Pun, Thierry and Chanel, Guillaume},
  title            = {Recognizing induced emotions of movie audiences: Are induced and perceived emotions the same?},
  year             = {2017},
  month            = oct,
  abstract         = {Predicting the emotional response of movie audiences to affective movie content is a challenging task in affective computing. Previous work has focused on using audiovisual movie content to predict movie induced emotions. However, the relationship between the audience's perceptions of the affective movie content (perceived emotions) and the emotions evoked in the audience (induced emotions) remains unexplored. In this work, we address the relationship between perceived and induced emotions in movies, and identify features and modelling approaches effective for predicting movie induced emotions. First, we extend the LIRIS-ACCEDE database by annotating perceived emotions in a crowd-sourced manner, and find that perceived and induced emotions are not always consistent. Second, we show that dialogue events and aesthetic highlights are effective predictors of movie induced emotions. In addition to movie based features, we also study physiological and behavioural measurements of audiences. Our experiments show that induced emotion recognition can benefit from including temporal context and from including multimodal information. Our study bridges the gap between affective content analysis and induced emotion prediction.},
  bdsk-url-1       = {https://doi.org/10.1109/acii.2017.8273575},
  c1               = {School of Informatics, University of Edinburgh; Computer Vision and Multimedia Laboratory, University of Geneva; School of Informatics, University of Edinburgh; School of Informatics, University of Edinburgh; Computer Vision and Multimedia Laboratory \& Swiss Center for Affective Sciences, University of Geneva; Faculty of Science and Technology, Bournemouth University, UK; Department of Modern French, University of Geneva; Computer Vision and Multimedia Laboratory \& Swiss Center for Affective Sciences, University of Geneva; Computer Vision and Multimedia Laboratory \& Swiss Center for Affective Sciences, University of Geneva},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1109/acii.2017.8273575},
  hasabstract      = {Y},
  keywords         = {Affective Computing; Emotion Recognition; Emotions; Aspect-based Sentiment Analysis; Narrative Persuasion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.1109/acii.2017.8273575},
}

@Article{kumar2018co,
  author           = {Kumar, Sunil and Chong, Ilyoung},
  journal          = {International journal of environmental research and public health/International journal of environmental research and public health},
  title            = {Correlation Analysis to Identify the Effective Data in Machine Learning: Prediction of Depressive Disorder and Emotion States},
  year             = {2018},
  month            = dec,
  number           = {12},
  pages            = {2907--2907},
  volume           = {15},
  abstract         = {Correlation analysis is an extensively used technique that identifies interesting relationships in data. These relationships help us realize the relevance of attributes with respect to the target class to be predicted. This study has exploited correlation analysis and machine learning-based approaches to identify relevant attributes in the dataset which have a significant impact on classifying a patient's mental health status. For mental health situations, correlation analysis has been performed in Weka, which involves a dataset of depressive disorder symptoms and situations based on weather conditions, as well as emotion classification based on physiological sensor readings. Pearson's product moment correlation and other different classification algorithms have been utilized for this analysis. The results show interesting correlations in weather attributes for bipolar patients, as well as in features extracted from physiological data for emotional states.},
  bdsk-url-1       = {https://doi.org/10.3390/ijerph15122907},
  c1               = {Department of Information and Communications Engineering, Hankuk University of Foreign Studies, Seoul 02450, Korea; Department of Information and Communications Engineering, Hankuk University of Foreign Studies, Seoul 02450, Korea},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/ijerph15122907},
  hasabstract      = {Y},
  isbn             = {1660-4601},
  keywords         = {Affective Computing; Emotion Recognition; Neuroimaging Data Analysis; Personality Data},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/ijerph15122907},
}

@Article{patel2021ee,
  author           = {Patel, Pragati and Raghunandan, R and Annavarapu, Ramesh},
  journal          = {Brain informatics},
  title            = {EEG-based human emotion recognition using entropy as a feature extraction measure},
  year             = {2021},
  month            = oct,
  number           = {1},
  volume           = {8},
  abstract         = {Abstract Many studies on brain--computer interface (BCI) have sought to understand the emotional state of the user to provide a reliable link between humans and machines. Advanced neuroimaging methods like electroencephalography (EEG) have enabled us to replicate and understand a wide range of human emotions more precisely. This physiological signal, i.e., EEG-based method is in stark comparison to traditional non-physiological signal-based methods and has been shown to perform better. EEG closely measures the electrical activities of the brain (a nonlinear system) and hence entropy proves to be an efficient feature in extracting meaningful information from raw brain waves. This review aims to give a brief summary of various entropy-based methods used for emotion classification hence providing insights into EEG-based emotion recognition. This study also reviews the current and future trends and discusses how emotion identification using entropy as a measure to extract features, can accomplish enhanced identification when using EEG signal.},
  bdsk-url-1       = {https://doi.org/10.1186/s40708-021-00141-5},
  c1               = {Department of Physics, School of Physical, Chemical, and Applied Sciences, Pondicherry University, Puducherry, 605014, India; Department of Physics, School of Physical, Chemical, and Applied Sciences, Pondicherry University, Puducherry, 605014, India; Department of Physics, School of Physical, Chemical, and Applied Sciences, Pondicherry University, Puducherry, 605014, India},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1186/s40708-021-00141-5},
  hasabstract      = {Y},
  isbn             = {2198-4026},
  keywords         = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Affective Computing; Epilepsy Detection},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Springer Science+Business Media},
  url              = {https://doi.org/10.1186/s40708-021-00141-5},
}

@Article{samara2017af,
  author           = {Samara, Anas and Galway, Leo and Bond, Raymond and Wang, Hui},
  journal          = {Journal of ambient intelligence \& humanized computing/Journal of ambient intelligence and humanized computing},
  title            = {Affective state detection via facial expression analysis within a human--computer interaction context},
  year             = {2017},
  month            = dec,
  number           = {6},
  pages            = {2175--2184},
  volume           = {10},
  abstract         = {The advancement in technology indicates that there is an opportunity to enhance human--computer interaction by way of affective state recognition. Affective state recognition is typically based on passive stimuli such as watching video clips, which does not reflect genuine interaction. This paper presents a study on affective state recognition using active stimuli, i.e. facial expressions of users when they attempt computerised tasks, particularly across typical usage of computer systems. A data collection experiment is presented for acquiring data from normal users whilst they interact with software, attempting to complete a set of predefined tasks. In addition, a hierarchical machine learning approach is presented for facial expression-based affective state recognition, which employs an Euclidean distance-based feature representation, conjointly with a customised encoding for users' self-reported affective states. Consequently, the aim is to find the potential relationship between the facial expressions, as defined by Paul Ekman, and the self-reported emotional states specified by users using Russells Circumplex model, in relation to the actual feelings and affective states. The main findings of this study suggest that facial expressions cannot precisely reveal the actual feelings of users whilst interacting with common computerised tasks. Moreover, during active interaction tasks more variation occurs within the facial expressions of participants than occurs within passive interaction.},
  bdsk-url-1       = {https://doi.org/10.1007/s12652-017-0636-8},
  c1               = {School of Computing, Ulster University, Belfast, BT37 0QB, UK; School of Computing, Ulster University, Belfast, BT37 0QB, UK; School of Computing, Ulster University, Belfast, BT37 0QB, UK; School of Computing, Ulster University, Belfast, BT37 0QB, UK},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1007/s12652-017-0636-8},
  hasabstract      = {Y},
  isbn             = {1868-5137},
  keywords         = {Affective Computing; Emotion Recognition; Facial Expression; Human-Computer Interaction; Face Perception},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Springer Science+Business Media},
  url              = {https://doi.org/10.1007/s12652-017-0636-8},
}

@Article{galvao2021pr,
  author           = {Galv{\~a}o, Filipe and Alarc{\~a}o, Soraia and Fonseca, Manuel},
  journal          = {Sensors},
  title            = {Predicting Exact Valence and Arousal Values from EEG},
  year             = {2021},
  month            = may,
  number           = {10},
  pages            = {3414--3414},
  volume           = {21},
  abstract         = {Recognition of emotions from physiological signals, and in particular from electroencephalography (EEG), is a field within affective computing gaining increasing relevance. Although researchers have used these signals to recognize emotions, most of them only identify a limited set of emotional states (e.g., happiness, sadness, anger, etc.) and have not attempted to predict exact values for valence and arousal, which would provide a wider range of emotional states. This paper describes our proposed model for predicting the exact values of valence and arousal in a subject-independent scenario. To create it, we studied the best features, brain waves, and machine learning models that are currently in use for emotion classification. This systematic analysis revealed that the best prediction model uses a KNN regressor (K = 1) with Manhattan distance, features from the alpha, beta and gamma bands, and the differential asymmetry from the alpha band. Results, using the DEAP, AMIGOS and DREAMER datasets, show that our model can predict valence and arousal values with a low error (MAE {$<$} 0.06, RMSE {$<$} 0.16) and a strong correlation between predicted and expected values (PCC {$ > $} 0.80), and can identify four emotional classes with an accuracy of 84.4{\%}. The findings of this work show that the features, brain waves and machine learning models, typically used in emotion classification tasks, can be used in more challenging situations, such as the prediction of exact values for valence and arousal.},
  bdsk-url-1       = {https://doi.org/10.3390/s21103414},
  c1               = {LASIGE, Faculdade de Ci{\^e}ncias, Universidade de Lisboa, 1749-016 Lisboa, Portugal; LASIGE, Faculdade de Ci{\^e}ncias, Universidade de Lisboa, 1749-016 Lisboa, Portugal; LASIGE, Faculdade de Ci{\^e}ncias, Universidade de Lisboa, 1749-016 Lisboa, Portugal},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/s21103414},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Emotion Recognition; Affective Computing; Deep Learning for EEG; EEG Analysis; Epilepsy Detection},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s21103414},
}

@Article{zhuang2018in,
  author           = {Zhuang, Ning and Zeng, Ying and Yang, Kai and Zhang, Chi and Li, Tong and Yan, Bin},
  journal          = {Sensors},
  title            = {Investigating Patterns for Self-Induced Emotion Recognition from EEG Signals},
  year             = {2018},
  month            = mar,
  number           = {3},
  pages            = {841--841},
  volume           = {18},
  abstract         = {Most current approaches to emotion recognition are based on neural signals elicited by affective materials such as images, sounds and videos. However, the application of neural patterns in the recognition of self-induced emotions remains uninvestigated. In this study we inferred the patterns and neural signatures of self-induced emotions from electroencephalogram (EEG) signals. The EEG signals of 30 participants were recorded while they watched 18 Chinese movie clips which were intended to elicit six discrete emotions, including joy, neutrality, sadness, disgust, anger and fear. After watching each movie clip the participants were asked to self-induce emotions by recalling a specific scene from each movie. We analyzed the important features, electrode distribution and average neural patterns of different self-induced emotions. Results demonstrated that features related to high-frequency rhythm of EEG signals from electrodes distributed in the bilateral temporal, prefrontal and occipital lobes have outstanding performance in the discrimination of emotions. Moreover, the six discrete categories of self-induced emotion exhibit specific neural patterns and brain topography distributions. We achieved an average accuracy of 87.36{\%} in the discrimination of positive from negative self-induced emotions and 54.52{\%} in the classification of emotions into six discrete categories. Our research will help promote the development of comprehensive endogenous emotion recognition methods.},
  bdsk-url-1       = {https://doi.org/10.3390/s18030841},
  c1               = {China National Digital Switching System Engineering and Technological Research Center, Zhengzhou 450002, China; China National Digital Switching System Engineering and Technological Research Center, Zhengzhou 450002, China; Key Laboratory for NeuroInformation of Ministry of Education, School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu 611731, China; China National Digital Switching System Engineering and Technological Research Center, Zhengzhou 450002, China; China National Digital Switching System Engineering and Technological Research Center, Zhengzhou 450002, China; China National Digital Switching System Engineering and Technological Research Center, Zhengzhou 450002, China; China National Digital Switching System Engineering and Technological Research Center, Zhengzhou 450002, China},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/s18030841},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Speech Emotion; Affective Computing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s18030841},
}

@Article{zhao2022af,
  author           = {Zhao, Sicheng and Yao, Xingxu and Yang, Jufeng and Jia, Guoli and Ding, Guiguang and Chua, Tat-Seng and Schuller, Bj{\"o}rn and Keutzer, Kurt},
  journal          = {IEEE transactions on pattern analysis and machine intelligence},
  title            = {Affective Image Content Analysis: Two Decades Review and New Perspectives},
  year             = {2022},
  month            = oct,
  number           = {10},
  pages            = {6729--6751},
  volume           = {44},
  abstract         = {Images can convey rich semantics and induce various emotions in viewers. Recently, with the rapid advancement of emotional intelligence and the explosive growth of visual data, extensive research efforts have been dedicated to affective image content analysis (AICA). In this survey, we will comprehensively review the development of AICA in the recent two decades, especially focusing on the state-of-the-art methods with respect to three main challenges - the affective gap, perception subjectivity, and label noise and absence. We begin with an introduction to the key emotion representation models that have been widely employed in AICA and description of available datasets for performing evaluation with quantitative comparison of label noise and dataset bias. We then summarize and compare the representative approaches on (1) emotion feature extraction, including both handcrafted and deep features, (2) learning methods on dominant emotion recognition, personalized emotion prediction, emotion distribution learning, and learning from noisy data or few labels, and (3) AICA based applications. Finally, we discuss some challenges and promising research directions in the future, such as image content and context understanding, group emotion clustering, and viewer-image interaction.},
  bdsk-url-1       = {https://doi.org/10.1109/tpami.2021.3094362},
  c1               = {BNRist, Tsinghua University, Beijing, China; College of Computer Science, Nankai University, Tianjin, China; College of Computer Science, Nankai University, Tianjin, China; College of Computer Science, Nankai University, Tianjin, China; BNRist, Tsinghua University, Beijing, China; School of Computing, National University of Singapore, Singapore, Singapore; Department of Computing, Imperial College London, London, U.K.; Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, Berkeley, CA, USA},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1109/tpami.2021.3094362},
  hasabstract      = {Y},
  isbn             = {0162-8828},
  keywords         = {Affective Computing; Emotion Recognition},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {IEEE Computer Society},
  url              = {https://doi.org/10.1109/tpami.2021.3094362},
}

@Article{baenziger2015pa,
  author        = {B{\"a}nziger, Tanja and Hosoya, Georg and Scherer, Klaus},
  journal       = {PloS one},
  title         = {Path Models of Vocal Emotion Communication},
  year          = {2015},
  month         = sep,
  number        = {9},
  pages         = {e0136675--e0136675},
  volume        = {10},
  abstract      = {We propose to use a comprehensive path model of vocal emotion communication, encompassing encoding, transmission, and decoding processes, to empirically model data sets on emotion expression and recognition. The utility of the approach is demonstrated for two data sets from two different cultures and languages, based on corpora of vocal emotion enactment by professional actors and emotion inference by na{\"\i}ve listeners. Lens model equations, hierarchical regression, and multivariate path analysis are used to compare the relative contributions of objectively measured acoustic cues in the enacted expressions and subjective voice cues as perceived by listeners to the variance in emotion inference from vocal expressions for four emotion families (fear, anger, happiness, and sadness). While the results confirm the central role of arousal in vocal emotion communication, the utility of applying an extended path modeling framework is demonstrated by the identification of unique combinations of distal cues and proximal percepts carrying information about specific emotion families, independent of arousal. The statistical models generated show that more sophisticated acoustic parameters need to be developed to explain the distal underpinnings of subjective voice quality percepts that account for much of the variance in emotion inference, in particular voice instability and roughness. The general approach advocated here, as well as the specific results, open up new research strategies for work in psychology (specifically emotion and social perception research) and engineering and computer science (specifically research and development in the domain of affective computing, particularly on automatic emotion detection and synthetic emotion expression in avatars).},
  bdsk-url-1    = {https://doi.org/10.1371/journal.pone.0136675},
  c1            = {Department of Psychology, Mid Sweden University, {\"O}stersund, Sweden; Department of Educational Science and Psychology, Freie Universit{\"a}t, Berlin, Germany; Swiss Centre for Affective Sciences, University of Geneva, Geneva, Switzerland},
  comment       = {no music},
  date-added    = {2024-05-13 15:05:26 +0100},
  date-modified = {2024-05-13 15:05:26 +0100},
  doi           = {10.1371/journal.pone.0136675},
  hasabstract   = {Y},
  isbn          = {1932-6203},
  keywords      = {Emotion Recognition; Speech Emotion; Affective Computing; Audio-Visual Speech Recognition; Facial Expression},
  la            = {en},
  priority      = {prio3},
  publisher     = {Public Library of Science},
  url           = {https://doi.org/10.1371/journal.pone.0136675},
}

@Article{tarnowski2020ey,
  author           = {Tarnowski, Pawe{\l} and Ko{\l}odziej, Marcin and Majkowski, Andrzej and Rak, Remigiusz},
  journal          = {Computational intelligence and neuroscience},
  title            = {Eye-Tracking Analysis for Emotion Recognition},
  year             = {2020},
  month            = sep,
  pages            = {1--13},
  volume           = {2020},
  abstract         = {This article reports the results of the study related to emotion recognition by using eye-tracking. Emotions were evoked by presenting a dynamic movie material in the form of 21 video fragments. Eye-tracking signals recorded from 30 participants were used to calculate 18 features associated with eye movements (fixations and saccades) and pupil diameter. To ensure that the features were related to emotions, we investigated the influence of luminance and the dynamics of the presented movies. Three classes of emotions were considered: high arousal and low valence, low arousal and moderate valence, and high arousal and high valence. A maximum of 80{\%} classification accuracy was obtained using the support vector machine (SVM) classifier and leave-one-subject-out validation method.},
  bdsk-url-1       = {https://doi.org/10.1155/2020/2909267},
  c1               = {Institute of Theory of Electrical Engineering, Measurement, and Information Systems, Warsaw University of Technology, Warsaw 00-662, Poland; Institute of Theory of Electrical Engineering, Measurement, and Information Systems, Warsaw University of Technology, Warsaw 00-662, Poland; Institute of Theory of Electrical Engineering, Measurement, and Information Systems, Warsaw University of Technology, Warsaw 00-662, Poland; Institute of Theory of Electrical Engineering, Measurement, and Information Systems, Warsaw University of Technology, Warsaw 00-662, Poland},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1155/2020/2909267},
  hasabstract      = {Y},
  isbn             = {1687-5265},
  keywords         = {Emotion Recognition; Eye Movement Analysis; Eye Tracking; Affective Computing; Head Gesture Recognition},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Hindawi Publishing Corporation},
  url              = {https://doi.org/10.1155/2020/2909267},
}

@Article{rukavina2016af,
  author        = {Rukavina, Stefanie and Gruss, Sascha and Hoffmann, Holger and Tan, Jun-Wen and Walter, Steffen and Traue, Harald},
  journal       = {PloS one},
  title         = {Affective Computing and the Impact of Gender and Age},
  year          = {2016},
  month         = mar,
  number        = {3},
  pages         = {e0150584--e0150584},
  volume        = {11},
  abstract      = {Affective computing aims at the detection of users' mental states, in particular, emotions and dispositions during human-computer interactions. Detection can be achieved by measuring multimodal signals, namely, speech, facial expressions and/or psychobiology. Over the past years, one major approach was to identify the best features for each signal using different classification methods. Although this is of high priority, other subject-specific variables should not be neglected. In our study, we analyzed the effect of gender, age, personality and gender roles on the extracted psychobiological features (derived from skin conductance level, facial electromyography and heart rate variability) as well as the influence on the classification results. In an experimental human-computer interaction, five different affective states with picture material from the International Affective Picture System and ULM pictures were induced. A total of 127 subjects participated in the study. Among all potentially influencing variables (gender has been reported to be influential), age was the only variable that correlated significantly with psychobiological responses. In summary, the conducted classification processes resulted in 20{\%} classification accuracy differences according to age and gender, especially when comparing the neutral condition with four other affective states. We suggest taking age and gender specifically into account for future studies in affective computing, as these may lead to an improvement of emotion recognition accuracy.},
  bdsk-url-1    = {https://doi.org/10.1371/journal.pone.0150584},
  c1            = {Department of Psychosomatic Medicine and Psychotherapy, Medical Psychology, Ulm University, Ulm, Germany; Department of Psychosomatic Medicine and Psychotherapy, Medical Psychology, Ulm University, Ulm, Germany; Department of Psychosomatic Medicine and Psychotherapy, Medical Psychology, Ulm University, Ulm, Germany; College of Teacher Education, Lishui University, Lishui, P.R. China; Department of Psychosomatic Medicine and Psychotherapy, Medical Psychology, Ulm University, Ulm, Germany; Department of Psychosomatic Medicine and Psychotherapy, Medical Psychology, Ulm University, Ulm, Germany},
  comment       = {physiological},
  date-added    = {2024-05-13 15:05:26 +0100},
  date-modified = {2024-05-13 15:05:26 +0100},
  doi           = {10.1371/journal.pone.0150584},
  hasabstract   = {Y},
  isbn          = {1932-6203},
  keywords      = {Affective Computing; Emotion Recognition; Affective Design; Human-Computer Interaction; Color Psychology},
  la            = {en},
  priority      = {prio3},
  publisher     = {Public Library of Science},
  url           = {https://doi.org/10.1371/journal.pone.0150584},
}

@Article{han2017st,
  author           = {Han, Jing and Zhang, Zixing and Cummins, Nicholas and Ringeval, Fabien and Schuller, Bj{\"o}rn},
  journal          = {Image and vision computing},
  title            = {Strength modelling for real-worldautomatic continuous affect recognition from audiovisual signals},
  year             = {2017},
  month            = sep,
  pages            = {76--86},
  volume           = {65},
  abstract         = {Automatic continuous affect recognition from audiovisual cues is arguably one of the most active research areas in machine learning. In addressing this regression problem, the advantages of the models, such as the global-optimisation capability of Support Vector Machine for Regression and the context-sensitive capability of memory-enhanced neural networks, have been frequently explored, but in an isolated way. Motivated to leverage the individual advantages of these techniques, this paper proposes and explores a novel framework, Strength Modelling, where two models are concatenated in a hierarchical framework. In doing this, the strength information of the first model, as represented by its predictions, is joined with the original features, and this expanded feature space is then utilised as the input by the successive model. A major advantage of Strength Modelling, besides its ability to hierarchically explore the strength of different machine learning algorithms, is that it can work together with the conventional feature- and decision-level fusion strategies for multimodal affect recognition. To highlight the effectiveness and robustness of the proposed approach, extensive experiments have been carried out on two time- and value-continuous spontaneous emotion databases (RECOLA and SEMAINE) using audio and video signals. The experimental results indicate that employing Strength Modelling can deliver a significant performance improvement for both arousal and valence in the unimodal and bimodal settings. The results further show that the proposed systems is competitive or outperform the other state-of-the-art approaches, but being with a simple implementation.},
  bdsk-url-1       = {https://doi.org/10.1016/j.imavis.2016.11.020},
  c1               = {Chair of Complex and Intelligent Systems, University of Passau, Innstr. 41, Passau 94032, Germany; Chair of Complex and Intelligent Systems, University of Passau, Innstr. 41, Passau 94032, Germany; Chair of Complex and Intelligent Systems, University of Passau, Innstr. 41, Passau 94032, Germany; Chair of Complex and Intelligent Systems, University of Passau, Innstr. 41, Passau 94032, Germany; Laboratoire d'Informatique de Grenoble, Universit{\'e}Grenoble Alpes, 700 Avenue Centrale, Grenoble 38058, France; Chair of Complex and Intelligent Systems, University of Passau, Innstr. 41, Passau 94032, Germany; Department of Computing, Imperial College London, 180 Queens' Gate, London SW7 2AZ, UK},
  comment          = {no music, discussed},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1016/j.imavis.2016.11.020},
  hasabstract      = {Y},
  isbn             = {0262-8856},
  keywords         = {Feature Extraction; Affective Computing; Environmental Sound Recognition; Emotion Recognition},
  la               = {en},
  modificationdate = {2024-05-16T12:06:31},
  priority         = {prio3},
  publisher        = {Elsevier BV},
  url              = {https://doi.org/10.1016/j.imavis.2016.11.020},
}

@Article{aydin2016wa,
  author           = {Aydin, Seda and Kaya, Turgay and G{\"u}ler, Hasan},
  journal          = {Brain informatics},
  title            = {Wavelet-based study of valence--arousal model of emotions on EEG signals with LabVIEW},
  year             = {2016},
  month            = jan,
  number           = {2},
  pages            = {109--117},
  volume           = {3},
  abstract         = {This paper illustrates the wavelet-based feature extraction for emotion assessment using electroencephalogram (EEG) signal through graphical coding design. Two-dimensional (valence--arousal) emotion model was studied. Different emotions (happy, joy, melancholy, and disgust) were studied for assessment. These emotions were stimulated by video clips. EEG signals obtained from four subjects were decomposed into five frequency bands (gamma, beta, alpha, theta, and delta) using ``db5''wavelet function. Relative features were calculated to obtain further information. Impact of the emotions according to valence value was observed to be optimal on power spectral density of gamma band. The main objective of this work is not only to investigate the influence of the emotions on different frequency bands but also to overcome the difficulties in the text-based program. This work offers an alternative approach for emotion evaluation through EEG processing. There are a number of methods for emotion recognition such as wavelet transform-based, Fourier transform-based, and Hilbert--Huang transform-based methods. However, the majority of these methods have been applied with the text-based programming languages. In this study, we proposed and implemented an experimental feature extraction with graphics-based language, which provides great convenience in bioelectrical signal processing.},
  bdsk-url-1       = {https://doi.org/10.1007/s40708-016-0031-9},
  c1               = {Department of Electrical-Electronics Engineering, Faculty of Engineering, University of Firat, Elazig, Turkey; Department of Electrical-Electronics Engineering, Faculty of Engineering, University of Firat, Elazig, Turkey; Department of Electrical-Electronics Engineering, Faculty of Engineering, University of Firat, Elazig, Turkey},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1007/s40708-016-0031-9},
  hasabstract      = {Y},
  isbn             = {2198-4026},
  keywords         = {Emotion Recognition; Affective Computing; EEG Analysis; Deep Learning for EEG; Feature Extraction},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Springer Science+Business Media},
  url              = {https://doi.org/10.1007/s40708-016-0031-9},
}

@Article{liu2018mu,
  author           = {Liu, Ningjie and Fang, Yuchun and Li, Ling and Hou, Limin and Yang, Fenglei and Guo, Yike},
  title            = {Multiple Feature Fusion for Automatic Emotion Recognition Using EEG Signals},
  year             = {2018},
  month            = apr,
  abstract         = {Automatic emotion recognition based on electroencephalo-graphic (EEG) signals has received increasing attention in recent years. The Deep Residual Networks (ResNets) can solve vanishing gradient problem and exploding gradient problem well in computer vision and can learn more profound semantic information. And for traditional methods, frequency features often play important role in signal processing area. Thus, in this paper, we use the pre-trained ResNets to extract deep semantic information and the linear-frequency cepstral coefficients (LFCC) as features from raw EEG signals. Then the two features are fused to improve the emotion classification performance of our approach. Moreover, several classifiers are used for our fused features to evaluate the performance and it shows that the proposed approach is effective for emotion classification. We find that the best performance is achieved when use k-nearst neighbor (KNN) as classifier, and we provide a detailed discussion for the reason.},
  bdsk-url-1       = {https://doi.org/10.1109/icassp.2018.8462518},
  c1               = {School of Computer Engineering and Science, University of Kent; School of Computer Engineering and Science, University of Kent; School of Computer Engineering and Science, University of Kent; School of Computer Engineering and Science, University of Kent; School of Computer Engineering and Science, University of Kent; School of Computer Engineering and Science, University of Kent},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1109/icassp.2018.8462518},
  hasabstract      = {Y},
  keywords         = {Emotion Recognition; Deep Learning for EEG; Affective Computing; Head Gesture Recognition; EEG Analysis},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.1109/icassp.2018.8462518},
}

@Article{ali2018ag,
  author           = {Ali, Mouhannad and Al Machot, Fadi and Mosa, Ahmad and Jdeed, Midhat and Al Machot, Elyan and Kyamakya, Kyandoghere},
  journal          = {Sensors},
  title            = {A Globally Generalized Emotion Recognition System Involving Different Physiological Signals},
  year             = {2018},
  month            = jun,
  number           = {6},
  pages            = {1905--1905},
  volume           = {18},
  abstract         = {Machine learning approaches for human emotion recognition have recently demonstrated high performance. However, only/mostly for subject-dependent approaches, in a variety of applications like advanced driver assisted systems, smart homes and medical environments. Therefore, now the focus is shifted more towards subject-independent approaches, which are more universal and where the emotion recognition system is trained using a specific group of subjects and then tested on totally new persons and thereby possibly while using other sensors of same physiological signals in order to recognize their emotions. In this paper, we explore a novel robust subject-independent human emotion recognition system, which consists of two major models. The first one is an automatic feature calibration model and the second one is a classification model based on Cellular Neural Networks (CNN). The proposed system produces state-of-the-art results with an accuracy rate between 80{\%} and 89{\%} when using the same elicitation materials and physiological sensors brands for both training and testing and an accuracy rate of 71.05{\%} when the elicitation materials and physiological sensors brands used in training are different from those used in training. Here, the following physiological signals are involved: ECG (Electrocardiogram), EDA (Electrodermal activity) and ST (Skin-Temperature).},
  bdsk-url-1       = {https://doi.org/10.3390/s18061905},
  c1               = {Department of Smart Systems Technologies, Alpen-Adira University, Klagenfurt 9020, Austria;; Research Center Borstel-Leibniz Center for Medicine and Biosciences, Borstel 23845, Germany;; Department of Smart Systems Technologies, Alpen-Adira University, Klagenfurt 9020, Austria;; Department of Smart Systems Technologies, Alpen-Adira University, Klagenfurt 9020, Austria;; Carl Gustav Carus Faculty of Medicine, Dresden University of Technology, Dresden 01069, Germany;; Department of Smart Systems Technologies, Alpen-Adira University, Klagenfurt 9020, Austria;},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/s18061905},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Emotion Recognition; Affective Computing; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s18061905},
}

@Article{nezami2018sh,
  author           = {Nezami, Omid and Lou, Paria and Karami, Mansoureh},
  journal          = {Language resources and evaluation},
  title            = {ShEMO: a large-scale validated database for Persian speech emotion detection},
  year             = {2018},
  month            = oct,
  number           = {1},
  pages            = {1--16},
  volume           = {53},
  abstract         = {This paper introduces a large-scale, validated database for Persian called Sharif Emotional Speech Database (ShEMO). The database includes 3000 semi-natural utterances, equivalent to 3 h and 25 min of speech data extracted from online radio plays. The ShEMO covers speech samples of 87 native-Persian speakers for five basic emotions including anger, fear, happiness, sadness and surprise, as well as neutral state. Twelve annotators label the underlying emotional state of utterances and majority voting is used to decide on the final labels. According to the kappa measure, the inter-annotator agreement is 64{\%} which is interpreted as ``substantial agreement''. We also present benchmark results based on common classification methods in speech emotion detection task. According to the experiments, support vector machine achieves the best results for both gender-independent (58.2{\%}) and gender-dependent models (female = 59.4{\%}, male = 57.6{\%}). The ShEMO will be available for academic purposes free of charge to provide a baseline for further research on Persian emotional speech.},
  bdsk-url-1       = {https://doi.org/10.1007/s10579-018-9427-x},
  c1               = {Bijar Branch, Islamic Azad University, Bijar, Iran; Sharif University of Technology, Tehran, Iran; Sharif University of Technology, Tehran, Iran},
  comment          = {no music},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1007/s10579-018-9427-x},
  hasabstract      = {Y},
  isbn             = {1574-020X},
  keywords         = {Speech Emotion; Affective Computing; Emotion Recognition},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Springer Science+Business Media},
  url              = {https://doi.org/10.1007/s10579-018-9427-x},
}

@Article{nemati2019ah,
  author           = {Nemati, Shahla and Rohani, Reza and Basiri, Mohammad and Abdar, Moloud and Yen, Neil and Makarenkov, Vladimir},
  journal          = {IEEE access},
  title            = {A Hybrid Latent Space Data Fusion Method for Multimodal Emotion Recognition},
  year             = {2019},
  month            = jan,
  pages            = {172948--172964},
  volume           = {7},
  abstract         = {Multimodal emotion recognition is an emerging interdisciplinary field of research in the area of affective computing and sentiment analysis. It aims at exploiting the information carried by signals of different nature to make emotion recognition systems more accurate. This is achieved by employing a powerful multimodal fusion method. In this study, a hybrid multimodal data fusion method is proposed in which the audio and visual modalities are fused using a latent space linear map and then, their projected features into the cross-modal space are fused with the textual modality using a Dempster-Shafer (DS) theory-based evidential fusion method. The evaluation of the proposed method on the videos of the DEAP dataset shows its superiority over both decision-level and non-latent space fusion methods. Furthermore, the results reveal that employing Marginal Fisher Analysis (MFA) for feature-level audio-visual fusion results in higher improvement in comparison to cross-modal factor analysis (CFA) and canonical correlation analysis (CCA). Also, the implementation results show that exploiting textual users' comments with the audiovisual content of movies improves the performance of the system.},
  bdsk-url-1       = {https://doi.org/10.1109/access.2019.2955637},
  c1               = {Department of Computer Engineering, Shahrekord University, Shahrekord, Iran; Department of Computer Engineering, Shahrekord University, Shahrekord, Iran; Department of Computer Engineering, Shahrekord University, Shahrekord, Iran; Department of Computer Science, Universit{\'e}du Qu{\'e}bec {\`a}Montr{\'e}al, Canada; School of Computer Science and Engineering, University of Aizu, Aizu, Japan; Department of Computer Science, Universit{\'e}du Qu{\'e}bec {\`a}Montr{\'e}al, Canada},
  comment          = {movies, text, physiological - discussed},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1109/access.2019.2955637},
  hasabstract      = {Y},
  isbn             = {2169-3536},
  keywords         = {Emotion Recognition; Multisensory Integration; Multimodal Data; Affective Computing; Crossmodal Processing},
  la               = {en},
  modificationdate = {2024-05-16T11:03:51},
  priority         = {prio3},
  publisher        = {Institute of Electrical and Electronics Engineers},
  url              = {https://doi.org/10.1109/access.2019.2955637},
}

@Article{makantasis2023th,
  author           = {Makantasis, Konstantinos and Liapis, Antonios and Yannakakis, Georgios},
  journal          = {IEEE transactions on affective computing},
  title            = {The Pixels and Sounds of Emotion: General-Purpose Representations of Arousal in Games},
  year             = {2023},
  month            = jan,
  number           = {1},
  pages            = {680--693},
  volume           = {14},
  abstract         = {What if emotion could be captured in a general and subject-agnostic fashion? Is it possible, for instance, to design general-purpose representations that detect affect solely from the pixels and audio of a human-computer interaction video? In this paper we address the above questions by evaluating the capacity of deep learned representations to predict affect by relying only on audiovisual information of videos. We assume that the pixels and audio of an interactive session embed the necessary information required to detect affect. We test our hypothesis in the domain of digital games and evaluate the degree to which deep classifiers and deep preference learning algorithms can learn to predict the arousal of players based only on the video footage of their gameplay. Our results from four dissimilar games suggest that general-purpose representations can be built across games as the arousal models obtain average accuracies as high as 85{\%} using the challenging leave-one-video-out cross-validation scheme. The dissimilar audiovisual characteristics of the tested games showcase the strengths and limitations of the proposed method.},
  bdsk-url-1       = {https://doi.org/10.1109/taffc.2021.3060877},
  c1               = {Institute of Digital Games, University of Malta, Msida, Malta; Institute of Digital Games, University of Malta, Msida, Malta; Institute of Digital Games, University of Malta, Msida, Malta},
  comment          = {classification based on video},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1109/taffc.2021.3060877},
  hasabstract      = {Y},
  isbn             = {1949-3045},
  keywords         = {Affective Computing; Emotion Recognition; General Game Playing; Player Modeling; Deep Learning},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Institute of Electrical and Electronics Engineers},
  url              = {https://doi.org/10.1109/taffc.2021.3060877},
}

@Article{zhang2020rc,
  author           = {Zhang, Tianyi and Ali, Abdallah and Wang, Chen and Hanjalic, Alan and C{\'e}sar, Pablo},
  title            = {RCEA: Real-time, Continuous Emotion Annotation for Collecting Precise Mobile Video Ground Truth Labels},
  year             = {2020},
  month            = apr,
  abstract         = {Collecting accurate and precise emotion ground truth labels for mobile video watching is essential for ensuring meaningful predictions. However, video-based emotion annotation techniques either rely on post-stimulus discrete self-reports, or allow real-time, continuous emotion annotations (RCEA) only for desktop settings. Following a user-centric approach, we designed an RCEA technique for mobile video watching, and validated its usability and reliability in a controlled, indoor (N=12) and later outdoor (N=20) study. Drawing on physiological measures, interaction logs, and subjective workload reports, we show that (1) RCEA is perceived to be usable for annotating emotions while mobile video watching, without increasing users' mental workload (2) the resulting time-variant annotations are comparable with intended emotion attributes of the video stimuli (classification error for valence: 8.3{\%}; arousal: 25{\%}). We contribute a validated annotation technique and associated annotation fusion method, that is suitable for collecting fine-grained emotion annotations while users watch mobile videos.},
  bdsk-url-1       = {https://doi.org/10.1145/3313831.3376808},
  c1               = {Centrum Wiskunde \& Informatica and Delft University of Technology, Amsterdam \& Delft, Netherlands; Centrum Wiskunde \& Informatica (CWI), Amsterdam, Netherlands; Xinhuanet, Beijing, China; Delft University of Technology, Delft, Netherlands; Centrum Wiskunde \& Informatica and Delft University of Technology, Amsterdam \& Delft, Netherlands},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1145/3313831.3376808},
  hasabstract      = {Y},
  keywords         = {Emotion Recognition; Affective Computing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.1145/3313831.3376808},
}

@Article{candra2015re,
  author           = {Candra, Henry and Yuwono, Mitchell and Handojoseno, A. and Chai, Rifai and Su, Steven and Nguyen, Hung},
  title            = {Recognizing emotions from EEG subbands using wavelet analysis},
  year             = {2015},
  month            = aug,
  abstract         = {Objectively recognizing emotions is a particularly important task to ensure that patients with emotional symptoms are given the appropriate treatments. The aim of this study was to develop an emotion recognition system using Electroencephalogram (EEG) signals to identify four emotions including happy, sad, angry, and relaxed. We approached this objective by firstly investigating the relevant EEG frequency band followed by deciding the appropriate feature extraction method. Two features were considered namely: 1. Wavelet Energy, and 2. Wavelet Entropy. EEG Channels reduction was then implemented to reduce the complexity of the features. The ground truth emotional states of each subject were inferred using Russel's circumplex model of emotion, that is, by mapping the subjectively reported degrees of valence (pleasure) and arousal to the appropriate emotions - for example, an emotion with high valence and high arousal is equivalent to a `happy' emotional state, while low valence and low arousal is equivalent to a `sad' emotional state. The Support Vector Machine (SVM) classifier was then used for mapping each feature vector into corresponding discrete emotions. The results presented in this study indicated thatWavelet features extracted from alpha, beta and gamma bands seem to provide the necessary information for describing the aforementioned emotions. Using the DEAP (Dataset for Emotion Analysis using electroencephalogram, Physiological and Video Signals), our proposed method achieved an average sensitivity and specificity of 77.4{\%} $\pm$14.1{\%} and 69.1{\%} $\pm$12.8{\%}, respectively.},
  bdsk-url-1       = {https://doi.org/10.1109/embc.2015.7319766},
  c1               = {Centre for Health Technologies, Faculty of Engineering and Information Technology, University of Technology, Sydney, New South Wales, Australia; Centre for Health Technologies, Faculty of Engineering and Information Technology, University of Technology, Sydney, New South Wales, Australia; Centre for Health Technologies, Faculty of Engineering and Information Technology, University of Technology, Sydney, New South Wales, Australia; Centre for Health Technologies, Faculty of Engineering and Information Technology, University of Technology, Sydney, New South Wales, Australia; Centre for Health Technologies, Faculty of Engineering and Information Technology, University of Technology, Sydney, New South Wales, Australia; Centre for Health Technologies, Faculty of Engineering and Information Technology, University of Technology, Sydney, New South Wales, Australia},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1109/embc.2015.7319766},
  hasabstract      = {Y},
  keywords         = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Epilepsy Detection; Affective Computing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.1109/embc.2015.7319766},
}

@Article{pan2021em,
  author           = {Pan, Bo and Wei, Zheng},
  journal          = {Computational and mathematical methods in medicine},
  title            = {Emotion Recognition Based on EEG Using Generative Adversarial Nets and Convolutional Neural Network},
  year             = {2021},
  month            = oct,
  pages            = {1--11},
  volume           = {2021},
  abstract         = {Emotion recognition plays an important role in the field of human-computer interaction (HCI). Automatic emotion recognition based on EEG is an important topic in brain-computer interface (BCI) applications. Currently, deep learning has been widely used in the field of EEG emotion recognition and has achieved remarkable results. However, due to the cost of data collection, most EEG datasets have only a small amount of EEG data, and the sample categories are unbalanced in these datasets. These problems will make it difficult for the deep learning model to predict the emotional state. In this paper, we propose a new sample generation method using generative adversarial networks to solve the problem of EEG sample shortage and sample category imbalance. In experiments, we explore the performance of emotion recognition with the frequency band correlation and frequency band separation computational models before and after data augmentation on standard EEG-based emotion datasets. Our experimental results show that the method of generative adversarial networks for data augmentation can effectively improve the performance of emotion recognition based on the deep learning model. And we find that the frequency band correlation deep learning model is more conducive to emotion recognition.},
  bdsk-url-1       = {https://doi.org/10.1155/2021/2520394},
  c1               = {School of Electronics and Information, Jiangsu University of Science and Technology, Zhenjiang 212100, China; School of Electronics and Information, Jiangsu University of Science and Technology, Zhenjiang 212100, China},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1155/2021/2520394},
  hasabstract      = {Y},
  isbn             = {1748-670X},
  keywords         = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Hindawi Publishing Corporation},
  url              = {https://doi.org/10.1155/2021/2520394},
}

@Article{romeo2022mu,
  author           = {Romeo, Luca and Cavallo, Andrea and Pepa, Lucia and Bianchi‐Berthouze, Nadia and Pontil, Massimiliano},
  journal          = {IEEE transactions on affective computing},
  title            = {Multiple Instance Learning for Emotion Recognition Using Physiological Signals},
  year             = {2022},
  month            = jan,
  number           = {1},
  pages            = {389--407},
  volume           = {13},
  abstract         = {The problem of continuous emotion recognition has been the subject of several studies. The proposed affective computing approaches employ sequential machine learning algorithms for improving the classification stage, accounting for the time ambiguity of emotional responses. Modeling and predicting the affective state over time is not a trivial problem because continuous data labeling is costly and not always feasible. This is a crucial issue in real-life applications, where data labeling is sparse and possibly captures only the most important events rather than the typical continuous subtle affective changes that occur. In this work, we introduce a framework from the machine learning literature called Multiple Instance Learning, which is able to model time intervals by capturing the presence or absence of relevant states, without the need to label the affective responses continuously (as required by standard sequential learning approaches). This choice offers a viable and natural solution for learning in a weakly supervised setting, taking into account the ambiguity of affective responses. We demonstrate the reliability of the proposed approach in a gold-standard scenario and towards real-world usage by employing an existing dataset (DEAP) and a purposely built one (Consumer). We also outline the advantages of this method with respect to standard supervised machine learning algorithms.},
  bdsk-url-1       = {https://doi.org/10.1109/taffc.2019.2954118},
  c1               = {Cognition, Motion and Neuroscience and Computational Statistics and Machine Learning, Fondazione Istituto Italiano di Tecnologia, Genova, Italy; Department of Information Engineering, Universit{\`a}Politecnica delle Marche, Ancona, Italy; Dipartimento di Psicologia, Fondazione Istituto Italiano di Tecnologia, Universit{\`a}di Torino, and C'MoN Unit, Genoa, Italy; Department of Information Engineering, Universit{\`a}Politecnica delle Marche, Ancona, Italy; UCL Interaction Centre, University College London, London, United Kingdom; Department of Computer Science, University College London, London, United Kingdom; Fondazione Istituto Italiano di Tecnologia, Genova, Italy},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1109/taffc.2019.2954118},
  hasabstract      = {Y},
  isbn             = {1949-3045},
  keywords         = {Emotion Recognition; Affective Computing; Sensory Processing; Multimodal Data},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Institute of Electrical and Electronics Engineers},
  url              = {https://doi.org/10.1109/taffc.2019.2954118},
}

@Article{abadi2015in,
  author           = {Abadi, Mojtaba and Correa, Juan and Wache, Julia and Yang, Heng and Patras, Ioannis and Sebe, Nicu},
  title            = {Inference of personality traits and affect schedule by analysis of spontaneous reactions to affective videos},
  year             = {2015},
  month            = may,
  abstract         = {This paper presents a method for inferring the Positive and Negative Affect Schedule (PANAS) and the BigFive personality traits of 35 participants through the analysis of their implicit responses to 16 emotional videos. The employed modalities to record the implicit responses are (i) EEG, (ii) peripheral physiological signals (ECG, GSR), and (iii) facial landmark trajectories. The predictions of personality traits/PANAS are done using linear regression models that are trained independently on each modality. The main findings of this study are that: (i) PANAS and personality traits of individuals can be predicted based on the users' implicit responses to affective video content, (ii) ECG+GSR signals yield 70{\%}$\pm$8{\%} F1-score on the distinction between extroverts/introverts, (iii) EEG signals yield 69{\%}$\pm$6{\%} F1-score on the distinction between creative/non creative people, and finally (iv) for the prediction of agreeableness, emotional stability, and baseline affective states we achieved significantly higher than chance-level results.},
  bdsk-url-1       = {https://doi.org/10.1109/fg.2015.7163100},
  c1               = {Semantic, Knowledge, and Innovation Lab(SKIL), Telecom Italia; University of Trento, Italy; Queen Mary University of London, London, London, GB; University of Trento, Italy; Queen Mary University of London, UK; Queen Mary University of London, UK; University of Trento, Italy},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1109/fg.2015.7163100},
  hasabstract      = {Y},
  keywords         = {Personality Data; Affective Computing; Emotion Recognition; Color Psychology; Cognitive Performance},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.1109/fg.2015.7163100},
}

@Article{samara2016fe,
  author           = {Samara, Anas and Menezes, Maria and Galway, Leo},
  title            = {Feature Extraction for Emotion Recognition and Modelling Using Neurophysiological Data},
  year             = {2016},
  month            = dec,
  abstract         = {The ubiquitous computing paradigm is becoming a reality; we are reaching a level of automation and computing in which people and devices interact seamlessly. However, one of the main challenges is the difficulty users have in interacting with these increasingly complex systems. Ultimately, endowing machines with the ability to perceive users' emotions will enable a more intuitive and reliable interaction. Consequently, using the electroencephalogram (EEG) as a bio-signal sensor, the affective state of a user can be modelled and subsequently utilised in order to achieve a system that can recognise and react to the users emotions. In this context, this paper investigates feature vector generation from EEG signals for the purpose of affective state modelling based on Russells Circumplex Model. Investigations are presented that aim to provide the foundation for future work in modelling user affect and interaction experiences through exploitation of different input modalities. The DEAP dataset was used within this work, along with a Support Vector Machine, which yielded reasonable classification accuracies for Valence and Arousal using feature vectors based on statistical measurements, band power from the, β, δand θwaves, and High Order Crossing of the EEG signal.},
  bdsk-url-1       = {https://doi.org/10.1109/iucc-css.2016.027},
  c1               = {School of Computing and Mathematics, Ulster University, Belfast, United Kingdom},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1109/iucc-css.2016.027},
  hasabstract      = {Y},
  keywords         = {Emotion Recognition; Affective Computing; Deep Learning for EEG; Feature Extraction; EEG Analysis},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.1109/iucc-css.2016.027},
}

@Article{yan2019ae,
  author           = {Yan, Jianzhuo and Chen, Shangbin and Deng, Sinuo},
  journal          = {Brain informatics},
  title            = {A EEG-based emotion recognition model with rhythm and time characteristics},
  year             = {2019},
  month            = sep,
  number           = {1},
  volume           = {6},
  abstract         = {As an advanced function of the human brain, emotion has a significant influence on human studies, works, and other aspects of life. Artificial Intelligence has played an important role in recognizing human emotion correctly. EEG-based emotion recognition (ER), one application of Brain Computer Interface (BCI), is becoming more popular in recent years. However, due to the ambiguity of human emotions and the complexity of EEG signals, the EEG-ER system which can recognize emotions with high accuracy is not easy to achieve. Based on the time scale, this paper chooses the recurrent neural network as the breakthrough point of the screening model. According to the rhythmic characteristics and temporal memory characteristics of EEG, this research proposes a Rhythmic Time EEG Emotion Recognition Model (RT-ERM) based on the valence and arousal of Long-Short-Term Memory Network (LSTM). By applying this model, the classification results of different rhythms and time scales are different. The optimal rhythm and time scale of the RT-ERM model are obtained through the results of the classification accuracy of different rhythms and different time scales. Then, the classification of emotional EEG is carried out by the best time scales corresponding to different rhythms. Finally, by comparing with other existing emotional EEG classification methods, it is found that the rhythm and time scale of the model can contribute to the accuracy of RT-ERM.},
  bdsk-url-1       = {https://doi.org/10.1186/s40708-019-0100-y},
  c1               = {Faculty of Information Technology, Beijing University of Technology, Beijing, 100124, China; Faculty of Information Technology, Beijing University of Technology, Beijing, 100124, China; Faculty of Information Technology, Beijing University of Technology, Beijing, 100124, China},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1186/s40708-019-0100-y},
  hasabstract      = {Y},
  isbn             = {2198-4026},
  keywords         = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Neuroimaging Data Analysis},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Springer Science+Business Media},
  url              = {https://doi.org/10.1186/s40708-019-0100-y},
}

@Article{nandi2021re,
  author           = {Nandi, Arijit and Xhafa, Fatos and Subirats, Laia and Fort, Santi},
  journal          = {Sensors},
  title            = {Real-Time Emotion Classification Using EEG Data Stream in E-Learning Contexts},
  year             = {2021},
  month            = feb,
  number           = {5},
  pages            = {1589--1589},
  volume           = {21},
  abstract         = {In face-to-face and online learning, emotions and emotional intelligence have an influence and play an essential role. Learners'emotions are crucial for e-learning system because they promote or restrain the learning. Many researchers have investigated the impacts of emotions in enhancing and maximizing e-learning outcomes. Several machine learning and deep learning approaches have also been proposed to achieve this goal. All such approaches are suitable for an offline mode, where the data for emotion classification are stored and can be accessed infinitely. However, these offline mode approaches are inappropriate for real-time emotion classification when the data are coming in a continuous stream and data can be seen to the model at once only. We also need real-time responses according to the emotional state. For this, we propose a real-time emotion classification system (RECS)-based Logistic Regression (LR) trained in an online fashion using the Stochastic Gradient Descent (SGD) algorithm. The proposed RECS is capable of classifying emotions in real-time by training the model in an online fashion using an EEG signal stream. To validate the performance of RECS, we have used the DEAP data set, which is the most widely used benchmark data set for emotion classification. The results show that the proposed approach can effectively classify emotions in real-time from the EEG data stream, which achieved a better accuracy and F1-score than other offline and online approaches. The developed real-time emotion classification system is analyzed in an e-learning context scenario.},
  bdsk-url-1       = {https://doi.org/10.3390/s21051589},
  c1               = {Department of Computer Science, Universitat Polit{\`e}cnica de Catalunya (BarcelonaTech), 08034 Barcelona, Spain;; Eurecat, Centre Tecnol{\`o}gic de Catalunya, 08005 Barcelona, Spain;; Department of Computer Science, Universitat Polit{\`e}cnica de Catalunya (BarcelonaTech), 08034 Barcelona, Spain;; ADaS Lab, Universitat Oberta de Catalunya, 08018 Barcelona, Spain; Eurecat, Centre Tecnol{\`o}gic de Catalunya, 08005 Barcelona, Spain;; Eurecat, Centre Tecnol{\`o}gic de Catalunya, 08005 Barcelona, Spain;},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/s21051589},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Emotion Recognition; Affective Computing; Online Learning; Deep Learning for EEG; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s21051589},
}

@Article{balan2020an,
  author           = {B{\u a}lan, Oana and Moise, Gabriela and Moldoveanu, Alin and Leordeanu, Marius and Moldoveanu, Florica},
  journal          = {Sensors},
  title            = {An Investigation of Various Machine and Deep Learning Techniques Applied in Automatic Fear Level Detection and Acrophobia Virtual Therapy},
  year             = {2020},
  month            = jan,
  number           = {2},
  pages            = {496--496},
  volume           = {20},
  abstract         = {In this paper, we investigate various machine learning classifiers used in our Virtual Reality (VR) system for treating acrophobia. The system automatically estimates fear level based on multimodal sensory data and a self-reported emotion assessment. There are two modalities of expressing fear ratings: the 2-choice scale, where 0 represents relaxation and 1 stands for fear; and the 4-choice scale, with the following correspondence: 0-relaxation, 1-low fear, 2-medium fear and 3-high fear. A set of features was extracted from the sensory signals using various metrics that quantify brain (electroencephalogram-EEG) and physiological linear and non-linear dynamics (Heart Rate-HR and Galvanic Skin Response-GSR). The novelty consists in the automatic adaptation of exposure scenario according to the subject's affective state. We acquired data from acrophobic subjects who had undergone an in vivo pre-therapy exposure session, followed by a Virtual Reality therapy and an in vivo evaluation procedure. Various machine and deep learning classifiers were implemented and tested, with and without feature selection, in both a user-dependent and user-independent fashion. The results showed a very high cross-validation accuracy on the training set and good test accuracies, ranging from 42.5{\%} to 89.5{\%}. The most important features of fear level classification were GSR, HR and the values of the EEG in the beta frequency range. For determining the next exposure scenario, a dominant role was played by the target fear level, a parameter computed by taking into account the patient's estimated fear level.},
  bdsk-url-1       = {https://doi.org/10.3390/s20020496},
  c1               = {Faculty of Automatic Control and Computers, University POLITEHNICA of Bucharest, Bucharest 060042, Romania;; Department of Computer Science, Information Technology, Mathematics and Physics, Petroleum-Gas University of Ploiesti, Ploiesti 100680, Romania;; Faculty of Automatic Control and Computers, University POLITEHNICA of Bucharest, Bucharest 060042, Romania;; Faculty of Automatic Control and Computers, University POLITEHNICA of Bucharest, Bucharest 060042, Romania;; Faculty of Automatic Control and Computers, University POLITEHNICA of Bucharest, Bucharest 060042, Romania;},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/s20020496},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Emotion Recognition; Affective Computing; Deep Learning},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s20020496},
}

@Article{costa2019em,
  author           = {Costa, {\^A}ngelo and Rincon, J. and Carrascosa, Carlos and Juli{\'a}n, Vicente and Nov{\'a}is, Paulo},
  journal          = {Future generation computer systems},
  title            = {Emotions detection on an ambient intelligent system using wearable devices},
  year             = {2019},
  month            = mar,
  pages            = {479--489},
  volume           = {92},
  abstract         = {This paper presents the Emotional Smart Wristband and its integration with the iGenda. The aim is to detect emotional states of a group of entities through the wristband and send the social emotion value to the iGenda so it may change the home environment and notify the caregivers. This project is advantageous to communities of elderly people, like retirement homes, where a harmonious environment is imperative and where the number of inhabitants keeps increasing. The iGenda provides the visual interface and the information center, receiving the information from the Emotional Smart Wristband and tries achieve a specific emotion (such as calm or excitement). Thus, the goal is to provide an affective system that directly interacts with humans by discreetly improving their lifestyle. In this paper, it is described the wristband in depth and the data models, and is provided an evaluation of them performed by real individuals and the validation of this evaluation.},
  bdsk-url-1       = {https://doi.org/10.1016/j.future.2018.03.038},
  c1               = {Centro ALGORITMI, University of Minho, Braga, Portugal; D. Sistemas Inform{\'a}ticos y Computaci{\'o}n, Universitat Polit{\`e}cnica de Val{\`e}ncia, Valencia, Spain; D. Sistemas Inform{\'a}ticos y Computaci{\'o}n, Universitat Polit{\`e}cnica de Val{\`e}ncia, Valencia, Spain; D. Sistemas Inform{\'a}ticos y Computaci{\'o}n, Universitat Polit{\`e}cnica de Val{\`e}ncia, Valencia, Spain; Centro ALGORITMI, University of Minho, Braga, Portugal},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1016/j.future.2018.03.038},
  hasabstract      = {Y},
  isbn             = {0167-739X},
  keywords         = {Emotion Recognition; Emotions; Affective Computing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Elsevier BV},
  url              = {https://doi.org/10.1016/j.future.2018.03.038},
}

@Article{zamanian2018an,
  author           = {Zamanian, Hanieh and Farsi, Hassan},
  journal          = {ELCVIA. Electronic letters on computer vision and image analysis},
  title            = {A New feature extraction method to Improve Emotion Detection Using EEG Signals},
  year             = {2018},
  month            = nov,
  number           = {1},
  pages            = {29--29},
  volume           = {17},
  abstract         = {Since emotion plays an important role in human life, demand and importance of automatic emotion detection have grown with increasing role of human computer interface applications. In this research, the focus is on the emotion detection from the electroencephalogram (EEG) signals. The system derives a mechanism of quantification of basic emotions using. So far, several methods have been reported, which generally use different processing algorithms, evolutionary algorithms, neural networks and classification algorithms. The aim of this paper is to develop a smart method to improve the accuracy of emotion detection by discrete signal processing techniques and applying optimized support vector machine classifier with genetic evolutionary algorithm. The obtained results show that the proposed method provides the accuracy of 93.86{\%} in detection of 4 emotions which is higher than state-of-the-art methods.},
  bdsk-url-1       = {https://doi.org/10.5565/rev/elcvia.1045},
  c1               = {University of Birjand; University of Birjand},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.5565/rev/elcvia.1045},
  hasabstract      = {Y},
  isbn             = {1577-5097},
  keywords         = {Emotion Recognition; Deep Learning for EEG; Affective Computing; EEG Analysis; Epilepsy Detection},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Computer Vision Center Press},
  url              = {https://doi.org/10.5565/rev/elcvia.1045},
}

@Article{arevalilloherraez2019co,
  author           = {Arevalillo‐Herr{\'a}ez, Miguel and Cobos, M{\'a}ximo and Roger, Sandra and Garc{\'\i}a-Pineda, Miguel},
  journal          = {Sensors},
  title            = {Combining Inter-Subject Modeling with a Subject-Based Data Transformation to Improve Affect Recognition from EEG Signals},
  year             = {2019},
  month            = jul,
  number           = {13},
  pages            = {2999--2999},
  volume           = {19},
  abstract         = {Existing correlations between features extracted from Electroencephalography (EEG) signals and emotional aspects have motivated the development of a diversity of EEG-based affect detection methods. Both intra-subject and inter-subject approaches have been used in this context. Intra-subject approaches generally suffer from the small sample problem, and require the collection of exhaustive data for each new user before the detection system is usable. On the contrary, inter-subject models do not account for the personality and physiological influence of how the individual is feeling and expressing emotions. In this paper, we analyze both modeling approaches, using three public repositories. The results show that the subject's influence on the EEG signals is substantially higher than that of the emotion and hence it is necessary to account for the subject's influence on the EEG signals. To do this, we propose a data transformation that seamlessly integrates individual traits into an inter-subject approach, improving classification results.},
  bdsk-url-1       = {https://doi.org/10.3390/s19132999},
  c1               = {Departament d'Inform{\`a}tica, Universitat de Val{\`e}ncia, Avda. de la Universidad, s/n, 46100-Burjasot, Spain. miguel.arevalillo{\char64}uv.es.; Departament d'Inform{\`a}tica, Universitat de Val{\`e}ncia, Avda. de la Universidad, s/n, 46100-Burjasot, Spain.; Departament d'Inform{\`a}tica, Universitat de Val{\`e}ncia, Avda. de la Universidad, s/n, 46100-Burjasot, Spain.; Departament d'Inform{\`a}tica, Universitat de Val{\`e}ncia, Avda. de la Universidad, s/n, 46100-Burjasot, Spain.},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/s19132999},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Emotion Regulation; Affective Computing},
  la               = {en},
  modificationdate = {2024-05-17T04:30:20},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s19132999},
}

@Article{zhao2018pe,
  author           = {Zhao, Sicheng and Ding, Guiguang and Han, Jungong and Gao, Yue},
  title            = {Personality-Aware Personalized Emotion Recognition from Physiological Signals},
  year             = {2018},
  month            = jul,
  abstract         = {Emotion recognition methodologies from physiological signals are increasingly becoming personalized, due to the subjective responses of different subjects to physical stimuli. Existing works mainly focused on modelling the involved physiological corpus of each subject, without considering the psychological factors. The latent correlation among different subjects has also been rarely examined. We propose to investigate the influence of personality on emotional behavior in a hypergraph learning framework. Assuming that each vertex is a compound tuple (subject, stimuli), multi-modal hypergraphs can be constructed based on the personality correlation among different subjects and on the physiological correlation among corresponding stimuli. To reveal the different importance of vertices, hyperedges, and modalities, we assign each of them with weights. The emotion relevance learned on the vertex-weighted multi-modal multi-task hypergraphs is employed for emotion recognition. We carry out extensive experiments on the ASCERTAIN dataset and the results demonstrate the superiority of the proposed method.},
  bdsk-url-1       = {https://doi.org/10.24963/ijcai.2018/230},
  c1               = {Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, USA; School of Computing and Communications, Lancaster University, UK; School of Software, Tsinghua University, China; School of Software, Tsinghua University, China; School of Computing and Communications, Lancaster University, UK; School of Software, Tsinghua University, China},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.24963/ijcai.2018/230},
  hasabstract      = {Y},
  keywords         = {Emotion Recognition; Affective Computing; Physiological Signals; Speech Emotion; Aspect-based Sentiment Analysis},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.24963/ijcai.2018/230},
}

@Article{apicella2021ee,
  author           = {Apicella, Andrea and Arpa{\"\i}a, Pasquale and Mastrati, Giovanna and Moccaldi, Nicola},
  journal          = {Scientific reports},
  title            = {EEG-based detection of emotional valence towards a reproducible measurement of emotions},
  year             = {2021},
  month            = nov,
  number           = {1},
  volume           = {11},
  abstract         = {Abstract A methodological contribution to a reproducible Measurement of Emotions for an EEG-based system is proposed. Emotional Valence detection is the suggested use case. Valence detection occurs along the interval scale theorized by the Circumplex Model of emotions. The binary choice, positive valence vs negative valence, represents a first step towards the adoption of a metric scale with a finer resolution. EEG signals were acquired through a 8-channel dry electrode cap. An implicit-more controlled EEG paradigm was employed to elicit emotional valence through the passive view of standardized visual stimuli (i.e., Oasis dataset) in 25 volunteers without depressive disorders. Results from the Self Assessment Manikin questionnaire confirmed the compatibility of the experimental sample with that of Oasis . Two different strategies for feature extraction were compared: (i) based on a-priory knowledge (i.e., Hemispheric Asymmetry Theories), and (ii) automated (i.e., a pipeline of a custom 12-band Filter Bank and Common Spatial Pattern). An average within-subject accuracy of 96.1 {\%}, was obtained by a shallow Artificial Neural Network, while k -Nearest Neighbors allowed to obtain a cross-subject accuracy equal to 80.2{\%}.},
  bdsk-url-1       = {https://doi.org/10.1038/s41598-021-00812-7},
  c1               = {Laboratory of Augmented Reality for Health Monitoring (ARHeMLab), Department of Electrical Engineering and Information Technology, University of Naples Federico II, Naples, Italy; Laboratory of Augmented Reality for Health Monitoring (ARHeMLab), Department of Electrical Engineering and Information Technology, University of Naples Federico II, Naples, Italy; Laboratory of Augmented Reality for Health Monitoring (ARHeMLab), Department of Electrical Engineering and Information Technology, University of Naples Federico II, Naples, Italy; Laboratory of Augmented Reality for Health Monitoring (ARHeMLab), Department of Electrical Engineering and Information Technology, University of Naples Federico II, Naples, Italy},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1038/s41598-021-00812-7},
  hasabstract      = {Y},
  isbn             = {2045-2322},
  keywords         = {Emotion Recognition; EEG Analysis; Affective Computing; Deep Learning for EEG; Epilepsy Detection},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Nature Portfolio},
  url              = {https://doi.org/10.1038/s41598-021-00812-7},
}

@Article{keelawat2021ac,
  author           = {Keelawat, Panayu and Thammasan, Nattapong and Numao, Masayuki and Kijsirikul, Boonserm},
  journal          = {Sensors},
  title            = {A Comparative Study of Window Size and Channel Arrangement on EEG-Emotion Recognition Using Deep CNN},
  year             = {2021},
  month            = mar,
  number           = {5},
  pages            = {1678--1678},
  volume           = {21},
  abstract         = {Emotion recognition based on electroencephalograms has become an active research area. Yet, identifying emotions using only brainwaves is still very challenging, especially the subject-independent task. Numerous studies have tried to propose methods to recognize emotions, including machine learning techniques like convolutional neural network (CNN). Since CNN has shown its potential in generalization to unseen subjects, manipulating CNN hyperparameters like the window size and electrode order might be beneficial. To our knowledge, this is the first work that extensively observed the parameter selection effect on the CNN. The temporal information in distinct window sizes was found to significantly affect the recognition performance, and CNN was found to be more responsive to changing window sizes than the support vector machine. Classifying the arousal achieved the best performance with a window size of ten seconds, obtaining 56.85{\%} accuracy and a Matthews correlation coefficient (MCC) of 0.1369. Valence recognition had the best performance with a window length of eight seconds at 73.34{\%} accuracy and an MCC value of 0.4669. Spatial information from varying the electrode orders had a small effect on the classification. Overall, valence results had a much more superior performance than arousal results, which were, perhaps, influenced by features related to brain activity asymmetry between the left and right hemispheres.},
  bdsk-url-1       = {https://doi.org/10.3390/s21051678},
  c1               = {Department of Computer Engineering, Chulalongkorn University, Pathum Wan, Bangkok 10330, Thailand; Department of Computer Science and Engineering, University of California San Diego, La Jolla, CA 92093-0404, USA; Human Media Interaction, Faculty of Electrical Engineering, Mathematics and Computer Science, University of Twente, 7522 NB Enschede, The Netherlands; The Institute of Scientific and Industrial Research, Osaka University, Mihogaoka, Ibaraki, Osaka 567-0047, Japan; Department of Computer Engineering, Chulalongkorn University, Pathum Wan, Bangkok 10330, Thailand},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/s21051678},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Emotion Recognition; Deep Learning for EEG; Affective Computing; Sensory Processing; EEG Analysis},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s21051678},
}

@Article{guo2017ee,
  author           = {Guo, Kairui and Candra, Henry and Yu, Hairong and Li, Huiqi and Nguyen, Hung and Su, Steven},
  title            = {EEG-based emotion classification using innovative features and combined SVM and HMM classifier},
  year             = {2017},
  month            = jul,
  abstract         = {Emotion classification is one of the state-of-the-art topics in biomedical signal research, and yet a significant portion remains unknown. This paper offers a novel approach with a combined classifier to recognise human emotion states based on electroencephalogram (EEG) signal. The objective is to achieve high accuracy using the combined classifier designed, which categorises the extracted features calculated from time domain features and Discrete Wavelet Transform (DWT). Two innovative designs are involved in this project: a novel variable is established as a new feature and a combined SVM and HMM classifier is developed. The result shows that the joined features raise the accuracy by 5{\%} on valence axis and 1.5{\%} on arousal axis. The combined classifier can improve the accuracy by 3{\%} comparing with SVM classifier. One of the important applications for high accuracy emotion classification system is offering a powerful tool for psychologists to diagnose emotion related mental diseases and the system developed in this project has the potential to serve such purpose.},
  bdsk-url-1       = {https://doi.org/10.1109/embc.2017.8036868},
  c1               = {Faculty of Engineering and Information Technology, University of Technology, Sydney, New South Wales, Australia; Faculty of Engineering and Information Technology, University of Technology, Sydney, New South Wales, Australia; Faculty of Engineering and Information Technology, University of Technology, Sydney, New South Wales, Australia; School of Information and Electronics, Beijing Institute of Technology; Faculty of Engineering and Information Technology, University of Technology, Sydney, New South Wales, Australia; Faculty of Engineering and Information Technology, University of Technology, Sydney, New South Wales, Australia},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1109/embc.2017.8036868},
  hasabstract      = {Y},
  keywords         = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Epilepsy Detection},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.1109/embc.2017.8036868},
}

@Article{schuller2019af,
  author           = {Schuller, Bj{\"o}rn and Weninger, Felix and Zhang, Yue and Ringeval, Fabien and Batliner, Anton and Steidl, Stefan and Eyben, Florian and Marchi, Erik and Vinciarelli, Alessandro and Scherer, Klaus and Ch{\'e}touani, Mohamed and Mortillaro, Marcello},
  journal          = {Computer speech \& language},
  title            = {Affective and behavioural computing: Lessons learnt from the First Computational Paralinguistics Challenge},
  year             = {2019},
  month            = jan,
  pages            = {156--180},
  volume           = {53},
  abstract         = {In this article, we review the INTERSPEECH 2013 Computational Paralinguistics ChallengE (ComParE) --the first of its kind--in light of the recent developments in affective and behavioural computing. The impact of the first ComParE instalment is manifold: first, it featured various new recognition tasks including social signals such as laughter and fillers, conflict in dyadic group discussions, and atypical communication due to pervasive developmental disorders, as well as enacted emotion; second, it marked the onset of the ComParE, subsuming all tasks investigated hitherto within the realm of computational paralinguistics; finally, besides providing a unified test-bed under well-defined and strictly comparable conditions, we present the definite feature vector used for computation of the baselines, thus laying the foundation for a successful series of follow-up Challenges. Starting with a review of the preceding INTERSPEECH Challenges, we present the four Sub-Challenges of ComParE 2013. In particular, we provide details of the Challenge databases and a meta-analysis by conducting experiments of logistic regression on single features and evaluating the performances achieved by the participants.},
  bdsk-url-1       = {https://doi.org/10.1016/j.csl.2018.02.004},
  c1               = {Imperial College London; University of Augsburg; Universit{\'e}de Gen{\`e}ve = University of Geneva; Technische Universit{\"a}t Munchen - Universit{\'e}Technique de Munich {$[$}Munich, Allemagne{$]$}; Imperial College London; Technische Universit{\"a}t Munchen - Universit{\'e}Technique de Munich {$[$}Munich, Allemagne{$]$}; Groupe d'{\'E}tude en Traduction Automatique/Traitement Automatis{\'e}des Langues et de la Parole; audEERING GmbH; Friedrich-Alexander Universit{\"a}t Erlangen-N{\"u}rnberg; University of Augsburg; Friedrich-Alexander Universit{\"a}t Erlangen-N{\"u}rnberg; Associate Institute for Signal Processing {$[$}Munich{$]$}; audEERING GmbH; Technische Universit{\"a}t Munchen - Universit{\'e}Technique de Munich {$[$}Munich, Allemagne{$]$}; audEERING GmbH; University of Glasgow; Department of Psychology; Institut des Syst{\`e}mes Intelligents et de Robotique; Laboratoire des Instruments et Syst{\`e}mes d'Ile de France; Sorbonne Universit{\'e}; Universit{\'e}de Gen{\`e}ve = University of Geneva},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1016/j.csl.2018.02.004},
  hasabstract      = {Y},
  isbn             = {0885-2308},
  keywords         = {Affective Computing; Emotion Recognition; Human-Computer Interaction; Coping Mechanisms; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Elsevier BV},
  url              = {https://doi.org/10.1016/j.csl.2018.02.004},
}

@Article{dziezyc2020ca,
  author           = {Dzie{\.z}yc, Maciej and Gjoreski, Martin and Kazienko, Przemys{\l}aw and Saganowski, Stanis{\l}aw and Gams, Matja{\v z}},
  journal          = {Sensors},
  title            = {Can We Ditch Feature Engineering? End-to-End Deep Learning for Affect Recognition from Physiological Sensor Data},
  year             = {2020},
  month            = nov,
  number           = {22},
  pages            = {6535--6535},
  volume           = {20},
  abstract         = {To further extend the applicability of wearable sensors in various domains such as mobile health systems and the automotive industry, new methods for accurately extracting subtle physiological information from these wearable sensors are required. However, the extraction of valuable information from physiological signals is still challenging---smartphones can count steps and compute heart rate, but they cannot recognize emotions and related affective states. This study analyzes the possibility of using end-to-end multimodal deep learning (DL) methods for affect recognition. Ten end-to-end DL architectures are compared on four different datasets with diverse raw physiological signals used for affect recognition, including emotional and stress states. The DL architectures specialized for time-series classification were enhanced to simultaneously facilitate learning from multiple sensors, each having their own sampling frequency. To enable fair comparison among the different DL architectures, Bayesian optimization was used for hyperparameter tuning. The experimental results showed that the performance of the models depends on the intensity of the physiological response induced by the affective stimuli, i.e., the DL models recognize stress induced by the Trier Social Stress Test more successfully than they recognize emotional changes induced by watching affective content, e.g., funny videos. Additionally, the results showed that the CNN-based architectures might be more suitable than LSTM-based architectures for affect recognition from physiological sensors.},
  bdsk-url-1       = {https://doi.org/10.3390/s20226535},
  c1               = {Faculty of Computer Science and Management, Wroc{\l}aw University of Science and Technology, 50-370 Wroc{\l}aw, Poland; Jo{\v z}ef Stefan Institute, 1000 Ljubljana, Slovenia; Jo{\v z}ef Stefan Postgraduate School, 1000 Ljubljana, Slovenia; Faculty of Computer Science and Management, Wroc{\l}aw University of Science and Technology, 50-370 Wroc{\l}aw, Poland; Faculty of Computer Science and Management, Wroc{\l}aw University of Science and Technology, 50-370 Wroc{\l}aw, Poland; Jo{\v z}ef Stefan Institute, 1000 Ljubljana, Slovenia; Jo{\v z}ef Stefan Postgraduate School, 1000 Ljubljana, Slovenia},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/s20226535},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Affective Computing; Emotion Recognition; Deep Learning; Pattern Discovery; Feature Extraction},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s20226535},
}

@Article{anderson2017cl,
  author        = {Anderson, Adam and Hsiao, Thomas and Metsis, Vangelis},
  title         = {Classification of Emotional Arousal During Multimedia Exposure},
  year          = {2017},
  month         = jun,
  abstract      = {In the study of emotion recognition, relatively few efforts have been made to compare classification results across different emotion induction methods. In this study, we attempt to classify emotional arousal using physiological signals collected across three stimulus types -- music, videos, and games. Subjects were exposed to relaxing and exciting music and videos and then asked to play Tetris and Minesweeper. Data from GSR, ECG, EOG, EEG, and PPG signals were analyzed using machine learning algorithms. We were able to successfully detect emotion arousal over a set of contiguous multimedia activities. Furthermore, we found that the patterns of physiological response to each multimedia stimuli are varying enough, that we can guess the stimulus type just by looking at the biosignals.},
  bdsk-url-1    = {https://doi.org/10.1145/3056540.3064956},
  c1            = {Dept. of Computer Science, University of Maryland, MD, USA; Dept. of Statistics Rice, University Houston, TX, USA; Dept. of Computer Science, Texas State University, TX, USA},
  comment       = {EEG, GSR},
  date-added    = {2024-05-13 15:05:26 +0100},
  date-modified = {2024-05-13 15:05:26 +0100},
  doi           = {10.1145/3056540.3064956},
  hasabstract   = {Y},
  keywords      = {Emotion Recognition; Affective Computing; Physiological Signals; Sensory Processing; Speech Emotion},
  la            = {en},
  priority      = {prio3},
  url           = {https://doi.org/10.1145/3056540.3064956},
}

@Article{buitelaar2018mi,
  author        = {Buitelaar, Paul and Wood, I. and Negi, Sapna and Ar{\v c}an, Mihael and McCrae, John and Abele, Andrejs and Robin, C{\'e}cile and Andryushechkin, Vladimir and Ziad, Housam and Sagha, Hesam and Schmitt, Maximilian and Schuller, Bj{\"o}rn and S{\'a}nchez-Rada, J. and Iglesias, Carlos and Navarro, Carlos and Giefer, Andreas and Heise, Nicolaus and Masucci, Vincenzo and Danza, Francesco and Caterino, Ciro and Smr{\v z}, Pavel and Hradi{\v s}, Michal and Povoln{\'y}, Filip and Klimes, Marek and Mat{\v e}jka, Pavel and Tummarello, Giovanni},
  journal       = {IEEE transactions on multimedia},
  title         = {MixedEmotions: An Open-Source Toolbox for Multimodal Emotion Analysis},
  year          = {2018},
  month         = sep,
  number        = {9},
  pages         = {2454--2465},
  volume        = {20},
  abstract      = {Recently, there is an increasing tendency to embed functionalities for recognizing emotions from user-generated media content in automated systems such as call-centre operations, recommendations, and assistive technologies, providing richer and more informative user and content profiles. However, to date, adding these functionalities was a tedious, costly, and time-consuming effort, requiring identification and integration of diverse tools with diverse interfaces as required by the use case at hand. The MixedEmotions Toolbox leverages the need for such functionalities by providing tools for text, audio, video, and linked data processing within an easily integrable plug-and-play platform. These functionalities include: 1) for text processing: emotion and sentiment recognition; 2) for audio processing: emotion, age, and gender recognition; 3) for video processing: face detection and tracking, emotion recognition, facial landmark localization, head pose estimation, face alignment, and body pose estimation; and 4) for linked data: knowledge graph integration. Moreover, the MixedEmotions Toolbox is open-source and free. In this paper, we present this toolbox in the context of the existing landscape, and provide a range of detailed benchmarks on standard test-beds showing its state-of-the-art performance. Furthermore, three real-world use cases show its effectiveness, namely, emotion-driven smart TV, call center monitoring, and brand reputation analysis.},
  bdsk-url-1    = {https://doi.org/10.1109/tmm.2018.2798287},
  c1            = {National University of Ireland Galway, Galway, Ireland; National University of Ireland Galway, Galway, Ireland; National University of Ireland Galway, Galway, Ireland; National University of Ireland Galway, Galway, Ireland; National University of Ireland Galway, Galway, Ireland; National University of Ireland Galway, Galway, Ireland; National University of Ireland Galway, Galway, Ireland; National University of Ireland Galway, Galway, Ireland; National University of Ireland Galway, Galway, Ireland; Chair of Complex \& Intelligent Systems, University of Passau, Passau, Germany; Chair of Complex \& Intelligent Systems, University of Passau, Passau, Germany; Chair of Complex \& Intelligent Systems, University of Passau, Passau, Germany; GSI Universidad Polit-{\'e}cnica de Madrid, Madrid, Spain; GSI Universidad Polit-{\'e}cnica de Madrid, Madrid, Spain; Paradigma Digital, Madrid, Spain; Deutsche Welle, Bonn, Germany; Deutsche Welle, Bonn, Germany; Expert Systems, Modena, Italy; Expert Systems, Modena, Italy; Expert Systems, Modena, Italy; Brno University of Technology, Brno-st{\v r}ed, Czech Republic; Brno University of Technology, Brno-st{\v r}ed, Czech Republic; Phonexia, Brno-Krlovo Pole, Czech Republic; Phonexia, Brno-Krlovo Pole, Czech Republic; Phonexia, Brno-Krlovo Pole, Czech Republic; Siren Solutions, Dublin, Ireland},
  comment       = {no music},
  date-added    = {2024-05-13 15:05:26 +0100},
  date-modified = {2024-05-13 15:05:26 +0100},
  doi           = {10.1109/tmm.2018.2798287},
  hasabstract   = {Y},
  isbn          = {1520-9210},
  keywords      = {Emotion Recognition; Emotion Perception; Affective Computing; Multimodal Data; Speech Emotion},
  la            = {en},
  priority      = {prio3},
  publisher     = {Institute of Electrical and Electronics Engineers},
  url           = {https://doi.org/10.1109/tmm.2018.2798287},
}

@Article{kortelainen2015hi,
  author           = {Kortelainen, Jukka and V{\"a}yrynen, Eero and Sepp{\"a}nen, Tapio},
  journal          = {Computational intelligence and neuroscience},
  title            = {High-Frequency Electroencephalographic Activity in Left Temporal Area Is Associated with Pleasant Emotion Induced by Video Clips},
  year             = {2015},
  month            = jan,
  pages            = {1--14},
  volume           = {2015},
  abstract         = {Recent findings suggest that specific neural correlates for the key elements of basic emotions do exist and can be identified by neuroimaging techniques. In this paper, electroencephalogram (EEG) is used to explore the markers for video-induced emotions. The problem is approached from a classifier perspective: the features that perform best in classifying person's valence and arousal while watching video clips with audiovisual emotional content are searched from a large feature set constructed from the EEG spectral powers of single channels as well as power differences between specific channel pairs. The feature selection is carried out using a sequential forward floating search method and is done separately for the classification of valence and arousal, both derived from the emotional keyword that the subject had chosen after seeing the clips. The proposed classifier-based approach reveals a clear association between the increased high-frequency (15-32 Hz) activity in the left temporal area and the clips described as "pleasant" in the valence and "medium arousal" in the arousal scale. These clips represent the emotional keywords amusement and joy/happiness. The finding suggests the occurrence of a specific neural activation during video-induced pleasant emotion and the possibility to detect this from the left temporal area using EEG.},
  bdsk-url-1       = {https://doi.org/10.1155/2015/762769},
  c1               = {Department of Computer Science and Engineering, University of Oulu, P.O. Box 4500, 90014 Oulu, Finland; Department of Computer Science and Engineering, University of Oulu, P.O. Box 4500, 90014 Oulu, Finland; Department of Computer Science and Engineering, University of Oulu, P.O. Box 4500, 90014 Oulu, Finland},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1155/2015/762769},
  hasabstract      = {Y},
  isbn             = {1687-5265},
  keywords         = {Emotion Recognition; Affective Computing; Emotion Regulation; Speech Emotion; Neural Activity},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Hindawi Publishing Corporation},
  url              = {https://doi.org/10.1155/2015/762769},
}

@Article{li2017de,
  author           = {Li, Xiang and Song, Dawei and Zhang, Peng and Hou, Yuexian and Hu, Bin},
  journal          = {International journal of data mining and bioinformatics},
  title            = {Deep fusion of multi-channel neurophysiological signal for emotion recognition and monitoring},
  year             = {2017},
  month            = jan,
  number           = {1},
  pages            = {1--1},
  volume           = {18},
  abstract         = {How to fuse multi-channel neurophysiological signals for emotion recognition is emerging as a hot research topic in community of Computational Psychophysiology. Nevertheless, prior feature engineering based approaches require extracting various domain knowledge related features at a high time cost. Moreover, traditional fusion method cannot fully utilise correlation information between different channels and frequency components. In this paper, we design a hybrid deep learning model, in which the 'Convolutional Neural Network (CNN)' is utilised for extracting task-related features, as well as mining inter-channel and inter-frequency correlation, besides, the 'Recurrent Neural Network (RNN)' is concatenated for integrating contextual information from the frame cube sequence. Experiments are carried out in a trial-level emotion recognition task, on the DEAP benchmarking dataset. Experimental results demonstrate that the proposed framework outperforms the classical methods, with regard to both of the emotional dimensions of Valence and Arousal.},
  bdsk-url-1       = {https://doi.org/10.1504/ijdmb.2017.086097},
  c1               = {Tianjin Key Laboratory of Cognitive Computing and Application, School of Computer Science and Technology, Tianjin University, Tianjin 300350, China; School of Computing and Communications, The Open University, Milton Keynes MK76AA, UK; Tianjin Key Laboratory of Cognitive Computing and Application, School of Computer Science and Technology, Tianjin University, Tianjin 300350, China; Tianjin Key Laboratory of Cognitive Computing and Application, School of Computer Science and Technology, Tianjin University, Tianjin 300350, China; Tianjin Key Laboratory of Cognitive Computing and Application, School of Computer Science and Technology, Tianjin University, Tianjin 300350, China; The Ubiquitous Awareness and Intelligent Solutions Lab, School of Information Science and Engineering, Lanzhou University, Lanzhou 730000, China},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1504/ijdmb.2017.086097},
  hasabstract      = {Y},
  isbn             = {1748-5673},
  keywords         = {Emotion Recognition; Affective Computing; Deep Learning for EEG; Deep Learning; Signal Processing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Inderscience Publishers},
  url              = {https://doi.org/10.1504/ijdmb.2017.086097},
}

@Article{lan2020sa,
  author           = {Lan, Zirui and Liu, Yisi and Sourina, Olga and Wang, Lipo and Scherer, Reinhold and M{\"u}ller-Putz, Gernot},
  journal          = {Advanced engineering informatics},
  title            = {SAFE: An EEG dataset for stable affective feature selection},
  year             = {2020},
  month            = apr,
  pages            = {101047--101047},
  volume           = {44},
  abstract         = {An affective brain-computer interface (aBCI) is a direct communication pathway between human brain and computer, via which the computer tries to recognize the affective states of its user and respond accordingly. As aBCI introduces personal affective factors into human-computer interaction, it could potentially enrich the user's experience during the interaction. Successful emotion recognition plays a key role in such a system. The state-of-the-art aBCIs leverage machine learning techniques which consist in acquiring affective electroencephalogram (EEG) signals from the user and calibrating the classifier to the affective patterns of the user. Many studies have reported satisfactory recognition accuracy using this paradigm. However, affective neural patterns are volatile over time even for the same subject. The recognition accuracy cannot be maintained if the usage of aBCI prolongs without recalibration. Existing studies have overlooked the performance evaluation of aBCI during long-term use. In this paper, we propose SAFE---an EEG dataset for stable affective feature selection. The dataset includes multiple recording sessions spanning across several days for each subject. Multiple sessions across different days were recorded so that the long-term recognition performance of aBCI can be evaluated. Based on this dataset, we demonstrate that the recognition accuracy of aBCIs deteriorates when re-calibration is ruled out during long-term usage. Then, we propose a stable feature selection method to choose the most stable affective features, for mitigating the accuracy deterioration to a lesser extent and maximizing the aBCI performance in the long run. We invite other researchers to test the performance of their aBCI algorithms on this dataset, and especially to evaluate the long-term performance of their methods.},
  bdsk-url-1       = {https://doi.org/10.1016/j.aei.2020.101047},
  c1               = {Fraunhofer Singapore, Singapore; Nanyang Technological University, Singapore; Fraunhofer Singapore, Singapore; Fraunhofer Singapore, Singapore; Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Computer Science and Electronic Engineering, University of Essex, UK; Institute of Neural Engineering, Graz University of Technology, Graz, Austria},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1016/j.aei.2020.101047},
  hasabstract      = {Y},
  isbn             = {1474-0346},
  keywords         = {Affective Computing; Brain-Computer Interfaces; Emotion Recognition; Human-Computer Interaction; BCI Technology},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Elsevier BV},
  url              = {https://doi.org/10.1016/j.aei.2020.101047},
}

@Article{zhang2014re,
  author           = {Zhang, Ligang and Tjondronegoro, Dian and Chandran, Vinod},
  journal          = {Image and vision computing},
  title            = {Representation of facial expression categories in continuous arousal--valence space: Feature and correlation},
  year             = {2014},
  month            = dec,
  number           = {12},
  pages            = {1067--1079},
  volume           = {32},
  abstract         = {Representation of facial expressions using continuous dimensions has shown to be inherently more expressive and psychologically meaningful than using categorized emotions, and thus has gained increasing attention over recent years.Many sub-problems have arisen in this new field that remain only partially understood.A comparison of the regression performance of different texture and geometric features and investigation of the correlations between continuous dimensional axes and basic categorized emotions are two of these.This paper presents empirical studies addressing these problems, and it reports results from an evaluation of different methods for detecting spontaneous facial expressions within the arousal-valence dimensional space (AV).The evaluation compares the performance of texture features (SIFT, Gabor, LBP) against geometric features (FAP-based distances), and the fusion of the two.It also compares the prediction of arousal and valence, obtained using the best fusion method, to the corresponding ground truths.Spatial distribution, shift, similarity, and correlation are considered for the six basic categorized emotions (i.e.anger, disgust, fear, happiness, sadness, surprise).Using the NVIE database, results show that the fusion of LBP and FAP features performs the best.The results from the NVIE and FEEDTUM databases reveal novel findings about the correlations of arousal and valence dimensions to each of six basic emotion categories.},
  bdsk-url-1       = {https://doi.org/10.1016/j.imavis.2014.09.005},
  c1               = {Faculty of Computer Science and Engineering, Xi'an University of Technology, 5 South Jinhua Road, Xi'an 710048, China; Science and Engineering Faculty, Queensland University of Technology, 2 George St, Brisbane 4000, Australia; Science and Engineering Faculty, Queensland University of Technology, 2 George St, Brisbane 4000, Australia; Science and Engineering Faculty, Queensland University of Technology, 2 George St, Brisbane 4000, Australia},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1016/j.imavis.2014.09.005},
  hasabstract      = {Y},
  isbn             = {0262-8856},
  keywords         = {Facial Expression; Emotion Recognition; Affective Computing; Face Perception; Emotional Expressions},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Elsevier BV},
  url              = {https://doi.org/10.1016/j.imavis.2014.09.005},
}

@Article{gupta2018en,
  author           = {Gupta, Akash and Sahu, Harsh and Nanecha, Nihal and Kumar, Pradeep and Roy, Partha and Chang, Victor},
  journal          = {Journal of grid computing},
  title            = {Enhancing Text Using Emotion Detected from EEG Signals},
  year             = {2018},
  month            = aug,
  number           = {2},
  pages            = {325--340},
  volume           = {17},
  abstract         = {Often people might not be able to express themselves properly on social media, like not being able to think of appropriate words representative of their emotional state. In this paper, we propose an end to end system which aims to enhance user-input sentence according to his/her current emotional state. It works by a) detecting the emotion of the user and b) enhancing the input sentence by inserting emotive words to make the sentence more representative of the emotional state of the user. The emotional state of the user is recognized by analyzing the Electroencephalogram (EEG) signals from the brain. For text enhancement, we modify the words corresponding to the detected emotion using correlation finder scheme. Next, the verification of the sentence correctness has been performed using Long Short Term Memory (LSTM) Networks based Language Modeling framework. An accuracy of 74.95{\%} has been recorded for the classification of five emotional states in a dataset of 25 participants using EEG signals. Similarly, promising results have been obtained for the task text enhancement and overall end-to-end system. To the best of our knowledge, this work is the first of its kind to enhance text according to the emotional state detected by EEG brainwaves. The system also releases an individual from thinking and typing words, which might be a complicated procedure sometimes.},
  bdsk-url-1       = {https://doi.org/10.1007/s10723-018-9462-2},
  c1               = {Department of Computer Science \& Engineering, IIT Roorkee, Roorkee, India; Department of Computer Science \& Engineering, IIT Roorkee, Roorkee, India; Department of Computer Science \& Engineering, IIT Roorkee, Roorkee, India; Department of Computer Science \& Engineering, IIT Roorkee, Roorkee, India; Department of Computer Science \& Engineering, IIT Roorkee, Roorkee, India; International Business School Suzhou and Research Institute of Big Data Analytics, Xi'an Jiaotong-Liverpool University, Suzhou, China},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1007/s10723-018-9462-2},
  hasabstract      = {Y},
  isbn             = {1570-7873},
  keywords         = {Emotion Recognition; Speech Emotion; EEG Analysis; Deep Learning for EEG; Affective Computing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Springer Science+Business Media},
  url              = {https://doi.org/10.1007/s10723-018-9462-2},
}

@Article{arnaugonzalez2021on,
  author           = {Arnau-Gonz{\'a}lez, Pablo and Arevalillo‐Herr{\'a}ez, Miguel and Katsigiannis, Stamos and Ramzan, Naeem},
  journal          = {IEEE transactions on affective computing},
  title            = {On the Influence of Affect in EEG-Based Subject Identification},
  year             = {2021},
  month            = apr,
  number           = {2},
  pages            = {391--401},
  volume           = {12},
  abstract         = {Biometric signals have been extensively used for user identification and authentication due to their inherent characteristics that are unique to each person. The variation exhibited between the brain signals (EEG) of different people makes such signals especially suitable for biometric user identification. However, the characteristics of these signals are also influenced by the user's current condition, including his/her affective state. In this paper, we analyze the significance of the affect-related component of brain signals within the subject identification context. Consistent results are obtained across three different public datasets, suggesting that the dominant component of the signal is subject-related, but the affective state also has a contribution that affects identification accuracy. Results show that identification accuracy increases when the system has been trained with EEG recordings that refer to similar affective states as the sample that is to be identified. This improvement holds independently of the features and classification algorithm used, and it is generally above 10 percent under a rigorous setting, when the training and validation datasets do not share data from the same recording days. This finding emphasizes the potential benefits of considering affective information in applications that require subject identification, such as user authentication.},
  bdsk-url-1       = {https://doi.org/10.1109/taffc.2018.2877986},
  c1               = {School of Engineering and Computing, University of the West of Scotland, Paisley, United Kingdom; Departament d'Inform{\`a}tica, Universitat de Val{\`e}ncia. Avda. de la Universidad s/n., Burjasot, Spain; School of Engineering and Computing, University of the West of Scotland, Paisley, United Kingdom; School of Engineering and Computing, University of the West of Scotland, Paisley, United Kingdom},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1109/taffc.2018.2877986},
  hasabstract      = {Y},
  isbn             = {1949-3045},
  keywords         = {Emotion Recognition; Affective Computing; Biometrics; EEG Analysis; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Institute of Electrical and Electronics Engineers},
  url              = {https://doi.org/10.1109/taffc.2018.2877986},
}

@Article{pan2020em,
  author           = {Pan, Lizheng and Yin, Zeming and She, Shigang and Song, Aiguo},
  journal          = {Entropy},
  title            = {Emotional State Recognition from Peripheral Physiological Signals Using Fused Nonlinear Features and Team-Collaboration Identification Strategy},
  year             = {2020},
  month            = apr,
  number           = {5},
  pages            = {511--511},
  volume           = {22},
  abstract         = {Emotion recognition realizing human inner perception has a very important application prospect in human-computer interaction. In order to improve the accuracy of emotion recognition, a novel method combining fused nonlinear features and team-collaboration identification strategy was proposed for emotion recognition using physiological signals. Four nonlinear features, namely approximate entropy (ApEn), sample entropy (SaEn), fuzzy entropy (FuEn) and wavelet packet entropy (WpEn) are employed to reflect emotional states deeply with each type of physiological signal. Then the features of different physiological signals are fused to represent the emotional states from multiple perspectives. Each classifier has its own advantages and disadvantages. In order to make full use of the advantages of other classifiers and avoid the limitation of single classifier, the team-collaboration model is built and the team-collaboration decision-making mechanism is designed according to the proposed team-collaboration identification strategy which is based on the fusion of support vector machine (SVM), decision tree (DT) and extreme learning machine (ELM). Through analysis, SVM is selected as the main classifier with DT and ELM as auxiliary classifiers. According to the designed decision-making mechanism, the proposed team-collaboration identification strategy can effectively employ different classification methods to make decision based on the characteristics of the samples through SVM classification. For samples which are easy to be identified by SVM, SVM directly determines the identification results, whereas SVM-DT-ELM collaboratively determines the identification results, which can effectively utilize the characteristics of each classifier and improve the classification accuracy. The effectiveness and universality of the proposed method are verified by Augsburg database and database for emotion analysis using physiological (DEAP) signals. The experimental results uniformly indicated that the proposed method combining fused nonlinear features and team-collaboration identification strategy presents better performance than the existing methods.},
  bdsk-url-1       = {https://doi.org/10.3390/e22050511},
  c1               = {Remote Measurement and Control Key Lab of Jiangsu Province, School of Instrument Science and Engineering, Southeast University, Nanjing 210096, China; School of Mechanical Engineering, Changzhou University, Changzhou 213164, China; School of Mechanical Engineering, Changzhou University, Changzhou 213164, China; School of Mechanical Engineering, Changzhou University, Changzhou 213164, China; Remote Measurement and Control Key Lab of Jiangsu Province, School of Instrument Science and Engineering, Southeast University, Nanjing 210096, China},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/e22050511},
  hasabstract      = {Y},
  isbn             = {1099-4300},
  keywords         = {Emotion Recognition; Affective Computing; Head Gesture Recognition; Physiological Signals; Neural Ensemble Physiology},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/e22050511},
}

@Article{chen2017su,
  author           = {Chen, Jing and Hu, Bin and Wang, Yue and Moore, Philip and Dai, Yongqiang and Feng, Lei and Ding, Zhijie},
  journal          = {BMC medical informatics and decision making},
  title            = {Subject-independent emotion recognition based on physiological signals: a three-stage decision method},
  year             = {2017},
  month            = dec,
  number           = {S3},
  volume           = {17},
  abstract         = {Collaboration between humans and computers has become pervasive and ubiquitous, however current computer systems are limited in that they fail to address the emotional component. An accurate understanding of human emotions is necessary for these computers to trigger proper feedback. Among multiple emotional channels, physiological signals are synchronous with emotional responses; therefore, analyzing physiological changes is a recognized way to estimate human emotions. In this paper, a three-stage decision method is proposed to recognize four emotions based on physiological signals in the multi-subject context. Emotion detection is achieved by using a stage-divided strategy in which each stage deals with a fine-grained goal.The decision method consists of three stages. During the training process, the initial stage transforms mixed training subjects to separate groups, thus eliminating the effect of individual differences. The second stage categorizes four emotions into two emotion pools in order to reduce recognition complexity. The third stage trains a classifier based on emotions in each emotion pool. During the testing process, a test case or test trial will be initially classified to a group followed by classification into an emotion pool in the second stage. An emotion will be assigned to the test trial in the final stage. In this paper we consider two different ways of allocating four emotions into two emotion pools. A comparative analysis is also carried out between the proposal and other methods.An average recognition accuracy of 77.57{\%} was achieved on the recognition of four emotions with the best accuracy of 86.67{\%} to recognize the positive and excited emotion. Using differing ways of allocating four emotions into two emotion pools, we found there is a difference in the effectiveness of a classifier on learning each emotion. When compared to other methods, the proposed method demonstrates a significant improvement in recognizing four emotions in the multi-subject context.The proposed three-stage decision method solves a crucial issue which is 'individual differences' in multi-subject emotion recognition and overcomes the suboptimal performance with respect to direct classification of multiple emotions. Our study supports the observation that the proposed method represents a promising methodology for recognizing multiple emotions in the multi-subject context.},
  bdsk-url-1       = {https://doi.org/10.1186/s12911-017-0562-x},
  c1               = {F. Joseph Halcomb III, M.D. Department of Biomedical Engineering, University of Kentucky, Lexington, 40506, USA; School of Information Science and Engineering, Lanzhou University, Lanzhou, 730000, China; School of Information Science and Engineering, Lanzhou University, Lanzhou, 730000, China; School of Information Science and Engineering, Lanzhou University, Lanzhou, 730000, China; School of Information Science and Engineering, Lanzhou University, Lanzhou, 730000, China; Beijing Anding Hospital of Capital Medical University, Beijing, 100088, China; The Third People's Hospital of Tianshui, Tianshui, 741020, China},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1186/s12911-017-0562-x},
  hasabstract      = {Y},
  isbn             = {1472-6947},
  keywords         = {Emotion Recognition; Affective Computing; Physiological Signals; Signal Processing; Human-Computer Interaction},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {BioMed Central},
  url              = {https://doi.org/10.1186/s12911-017-0562-x},
}

@Article{mendozapalechor2018af,
  author           = {Mendoza-Palechor, Fabio and Menezes, Maria and Sant'Anna, Anita and Ort{\'\i}z‐Barrios, Miguel and Samara, Anas and Galway, Leo},
  journal          = {Journal of ambient intelligence \& humanized computing/Journal of ambient intelligence and humanized computing},
  title            = {Affective recognition from EEG signals: an integrated data-mining approach},
  year             = {2018},
  month            = sep,
  number           = {10},
  pages            = {3955--3974},
  volume           = {10},
  abstract         = {Emotions play an important role in human communication, interaction, and decision making processes. Therefore, considerable efforts have been made towards the automatic identification of human emotions, in particular electroencephalogram (EEG) signals and Data Mining (DM) techniques have been then used to create models recognizing the affective states of users. However, most previous works have used clinical grade EEG systems with at least 32 electrodes. These systems are expensive and cumbersome, and therefore unsuitable for usage during normal daily activities. Smaller EEG headsets such as the Emotiv are now available and can be used during daily activities. This paper investigates the accuracy and applicability of previous affective recognition methods on data collected with an Emotiv headset while participants used a personal computer to fulfill several tasks. Several features were extracted from four channels only (AF3, AF4, F3 and F4 in accordance with the 10--20 system). Both Support Vector Machine and Na{\"\i}ve Bayes were used for emotion classification. Results demonstrate that such methods can be used to accurately detect emotions using a small EEG headset during a normal daily activity.},
  bdsk-url-1       = {https://doi.org/10.1007/s12652-018-1065-z},
  c1               = {Department of Electronic and Systems Engineering, Universidad de la Costa CUC, Barranquilla, Colombia; Center for Applied Intelligent Systems Research, Halmstad University, Halmstad, Sweden; Center for Applied Intelligent Systems Research, Halmstad University, Halmstad, Sweden; Department of Industrial Management, Agroindustry and Operations, Universidad de la Costa CUC, Barranquilla, Colombia; School of Computing, Computer Science Research Institute, Ulster University, Belfast, UK; School of Computing, Computer Science Research Institute, Ulster University, Belfast, UK},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1007/s12652-018-1065-z},
  hasabstract      = {Y},
  isbn             = {1868-5137},
  keywords         = {Emotion Recognition; Deep Learning for EEG; Affective Computing; EEG Analysis; Head Gesture Recognition},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Springer Science+Business Media},
  url              = {https://doi.org/10.1007/s12652-018-1065-z},
}

@Article{shen2021mu,
  author           = {Shen, Fangyao and Peng, Yong and Kong, Wanzeng and Dai, Guojun},
  journal          = {Sensors},
  title            = {Multi-Scale Frequency Bands Ensemble Learning for EEG-Based Emotion Recognition},
  year             = {2021},
  month            = feb,
  number           = {4},
  pages            = {1262--1262},
  volume           = {21},
  abstract         = {Emotion recognition has a wide range of potential applications in the real world. Among the emotion recognition data sources, electroencephalography (EEG) signals can record the neural activities across the human brain, providing us a reliable way to recognize the emotional states. Most of existing EEG-based emotion recognition studies directly concatenated features extracted from all EEG frequency bands for emotion classification. This way assumes that all frequency bands share the same importance by default; however, it cannot always obtain the optimal performance. In this paper, we present a novel multi-scale frequency bands ensemble learning (MSFBEL) method to perform emotion recognition from EEG signals. Concretely, we first re-organize all frequency bands into several local scales and one global scale. Then we train a base classifier on each scale. Finally we fuse the results of all scales by designing an adaptive weight learning method which automatically assigns larger weights to more important scales to further improve the performance. The proposed method is validated on two public data sets. For the ``SEED IV''data set, MSFBEL achieves average accuracies of 82.75{\%}, 87.87{\%}, and 78.27{\%} on the three sessions under the within-session experimental paradigm. For the ``DEAP''data set, it obtains average accuracy of 74.22{\%} for four-category classification under 5-fold cross validation. The experimental results demonstrate that the scale of frequency bands influences the emotion recognition rate, while the global scale that directly concatenating all frequency bands cannot always guarantee to obtain the best emotion recognition performance. Different scales provide complementary information to each other, and the proposed adaptive weight learning method can effectively fuse them to further enhance the performance.},
  bdsk-url-1       = {https://doi.org/10.3390/s21041262},
  c1               = {School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou 310018, China; MoE Key Laboratory of Advanced Perception and Intelligent Control of High-End Equipment, Anhui Polytechnic University, Wuhu 241000, China; School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou 310018, China; Key Laboratory of Brain Machine Collaborative Intelligence of Zhejiang Province, Hangzhou 310018, China; School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou 310018, China; Key Laboratory of Brain Machine Collaborative Intelligence of Zhejiang Province, Hangzhou 310018, China; School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou 310018, China},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/s21041262},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Neural Ensemble Physiology},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s21041262},
}

@Article{nandi2022af,
  author           = {Nandi, Arijit and Xhafa, Fatos},
  journal          = {Methods},
  title            = {A federated learning method for real-time emotion state classification from multi-modal streaming},
  year             = {2022},
  month            = aug,
  pages            = {340--347},
  volume           = {204},
  abstract         = {Emotional and physical health are strongly connected and should be taken care of simultaneously to ensure completely healthy persons. A person's emotional health can be determined by detecting emotional states from various physiological measurements (EDA, RB, EEG, etc.). Affective Computing has become the field of interest, which uses software and hardware to detect emotional states. In the IoT era, wearable sensor-based real-time multi-modal emotion state classification has become one of the hottest topics. In such setting, a data stream is generated from wearable-sensor devices, data accessibility is restricted to those devices only and usually a high data generation rate should be processed to achieve real-time emotion state responses. Additionally, protecting the users' data privacy makes the processing of such data even more challenging. Traditional classifiers have limitations to achieve high accuracy of emotional state detection under demanding requirements of decentralized data and protecting users' privacy of sensitive information as such classifiers need to see all data. Here comes the federated learning, whose main idea is to create a global classifier without accessing the users' local data. Therefore, we have developed a federated learning framework for real-time emotion state classification using multi-modal physiological data streams from wearable sensors, called Fed-ReMECS. The main findings of our Fed-ReMECS framework are the development of an efficient and scalable real-time emotion classification system from distributed multimodal physiological data streams, where the global classifier is built without accessing (privacy protection) the users' data in an IoT environment. The experimental study is conducted using the popularly used multi-modal benchmark DEAP dataset for emotion classification. The results show the effectiveness of our developed approach in terms of accuracy, efficiency, scalability and users' data privacy protection.},
  bdsk-url-1       = {https://doi.org/10.1016/j.ymeth.2022.03.005},
  c1               = {Eurecat, Centre Tecnol{\`o}gic de Catalunya, 08005 Barcelona, Spain; Universitat Polit{\`e}cnica de Catalunya (BarcelonaTech), 08034 Barcelona, Spain; Universitat Polit{\`e}cnica de Catalunya (BarcelonaTech), 08034 Barcelona, Spain},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1016/j.ymeth.2022.03.005},
  hasabstract      = {Y},
  isbn             = {1046-2023},
  keywords         = {Affective Computing; Emotion Recognition; Emotion Regulation; Speech Emotion; Multimodal Data},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Elsevier BV},
  url              = {https://doi.org/10.1016/j.ymeth.2022.03.005},
}

@Article{kapucu2018tu,
  author           = {Kapucu, Aycan and Kılı{\c c}, Aslı and {\"O}zkılı{\c c}, Yıldız and Sarıbaz, Bengisu},
  journal          = {Psychological reports},
  title            = {Turkish Emotional Word Norms for Arousal, Valence, and Discrete Emotion Categories},
  year             = {2018},
  month            = dec,
  number           = {1},
  pages            = {188--209},
  volume           = {124},
  abstract         = {The present study combined dimensional and categorical approaches to emotion to develop normative ratings for a large set of Turkish words on two major dimensions of emotion: arousal and valence, as well as on five basic emotion categories of happiness, sadness, anger, fear, and disgust. A set of 2031 Turkish words obtained by translating Affective Norms for English Words to Turkish and pooling from the Turkish Word Norms were rated by a large sample of 1527 participants. This is the first comprehensive and standardized word set in Turkish offering discrete emotional ratings in addition to dimensional ratings along with concreteness judgments. Consistent with Affective Norms for English Words and word databases in several other languages, arousal increased as valence became more positive or more negative. As expected, negative emotions (anger, and disgust) were positively correlated with each other, whereas the positive emotion, happiness, was negatively correlated with the negative emotion categories. Data further showed that the valence dimension was strongly correlated with happiness, and the arousal dimension was mostly correlated with fear. These findings show highly similar and consistent patterns with word sets provided in other languages in terms of the relationships between arousal and valence dimensions, relationships between dimensions and specific emotion categories, relationships among specific emotions, and further support the stability of the relationship between basic discrete emotions at the word level across different cultures.},
  bdsk-url-1       = {https://doi.org/10.1177/0033294118814722},
  c1               = {Department of Psychology, Ege University, {\.I}zmir, Turkey; Department of Psychology, Middle East Technical University, Ankara, Turkey; Department of Psychology, Uludag University, Bursa, Turkey; Department of Psychology, Ege University, {\.I}zmir, Turkey},
  comment          = {no music},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1177/0033294118814722},
  hasabstract      = {Y},
  isbn             = {0033-2941},
  keywords         = {Emotion Recognition; Emotion Regulation; Affective Computing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {SAGE Publishing},
  url              = {https://doi.org/10.1177/0033294118814722},
}

@Article{mou2016au,
  author           = {Mou, Wenxuan and G{\"u}ne{\c s}, Hatice and Patras, Ioannis},
  title            = {Automatic Recognition of Emotions and Membership in Group Videos},
  year             = {2016},
  month            = jun,
  abstract         = {Automatic affect analysis and understanding has become a well established research area in the last two decades. However, little attention has been paid to the analysis of the affect expressed in group settings, either in the form of affect expressed by the whole group collectively or affect expressed by each individual member of the group. This paper presents a framework which, in group settings automatically classifies the affect expressed by each individual group member along both arousal and valence dimensions. We first introduce a novel Volume Quantised Local Zernike Moments Fisher Vectors (vQLZM-FV) descriptor to represent the facial behaviours of individuals in the spatio-temporal domain and then propose a method to recognize the group membership of each individual (i.e., which group the individual in question is part of) by using their face and body behavioural cues. We conduct a set of experiments on a newly collected dataset that contains fourteen recordings of four groups, each consisting of four people watching affective movie stimuli. Our experimental results show that (1) the proposed vQLZM-FV outperforms the other feature representations in affect recognition, and (2) group membership can be recognized using the non-verbal face and body features, indicating that individuals influence each other's behaviours within a group setting.},
  bdsk-url-1       = {https://doi.org/10.1109/cvprw.2016.185},
  c1               = {Queen Mary University of London, UK; University of Cambridge, Cambridge, UK; Queen Mary University of London, UK},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1109/cvprw.2016.185},
  hasabstract      = {Y},
  keywords         = {Facial Expression Analysis; Affective Computing; Emotion Recognition; Feature Learning; Facial Landmark Detection},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.1109/cvprw.2016.185},
}

@Article{shao2020us,
  author           = {Shao, Mingyang and Snyder, Matt and Nejat, Goldie and Benhabib, B.},
  journal          = {Robotics},
  title            = {User Affect Elicitation with a Socially Emotional Robot},
  year             = {2020},
  month            = jun,
  number           = {2},
  pages            = {44--44},
  volume           = {9},
  abstract         = {To effectively communicate with people, social robots must be capable of detecting, interpreting, and responding to human affect during human--robot interactions (HRIs). In order to accurately detect user affect during HRIs, affect elicitation techniques need to be developed to create and train appropriate affect detection models. In this paper, we present such a novel affect elicitation and detection method for social robots in HRIs. Non-verbal emotional behaviors of the social robot were designed to elicit user affect, which was directly measured through electroencephalography (EEG) signals. HRI experiments with both younger and older adults were conducted to evaluate our affect elicitation technique and compare the two types of affect detection models we developed and trained utilizing multilayer perceptron neural networks (NNs) and support vector machines (SVMs). The results showed that; on average, the self-reported valence and arousal were consistent with the intended elicited affect. Furthermore, it was also noted that the EEG data obtained could be used to train affect detection models with the NN models achieving higher classification rates},
  bdsk-url-1       = {https://doi.org/10.3390/robotics9020044},
  c1               = {Department of Mechanical and Industrial Engineering, University of Toronto, Toronto, ON M5S 3G8, Canada;; Yee Hong Centre for Geriatric Care, Mississauga, ON L5V 2X5, Canada;; Department of Mechanical and Industrial Engineering, University of Toronto, Toronto, ON M5S 3G8, Canada;; Department of Mechanical and Industrial Engineering, University of Toronto, Toronto, ON M5S 3G8, Canada;},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/robotics9020044},
  hasabstract      = {Y},
  isbn             = {2218-6581},
  keywords         = {Emotion Recognition; Human Perception of Robots; Affective Computing; Human-Robot Interaction; Emotion Perception},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/robotics9020044},
}

@Article{phan2021ee,
  author           = {Phan, Tran-Dac-Thinh and Kim, Soo-Hyung and Yang, Hyung-Jeong and Lee, Guee-Sang},
  journal          = {Sensors},
  title            = {EEG-Based Emotion Recognition by Convolutional Neural Network with Multi-Scale Kernels},
  year             = {2021},
  month            = jul,
  number           = {15},
  pages            = {5092--5092},
  volume           = {21},
  abstract         = {Besides facial or gesture-based emotion recognition, Electroencephalogram (EEG) data have been drawing attention thanks to their capability in countering the effect of deceptive external expressions of humans, like faces or speeches. Emotion recognition based on EEG signals heavily relies on the features and their delineation, which requires the selection of feature categories converted from the raw signals and types of expressions that could display the intrinsic properties of an individual signal or a group of them. Moreover, the correlation or interaction among channels and frequency bands also contain crucial information for emotional state prediction, and it is commonly disregarded in conventional approaches. Therefore, in our method, the correlation between 32 channels and frequency bands were put into use to enhance the emotion prediction performance. The extracted features chosen from the time domain were arranged into feature-homogeneous matrices, with their positions following the corresponding electrodes placed on the scalp. Based on this 3D representation of EEG signals, the model must have the ability to learn the local and global patterns that describe the short and long-range relations of EEG channels, along with the embedded features. To deal with this problem, we proposed the 2D CNN with different kernel-size of convolutional layers assembled into a convolution block, combining features that were distributed in small and large regions. Ten-fold cross validation was conducted on the DEAP dataset to prove the effectiveness of our approach. We achieved the average accuracies of 98.27{\%} and 98.36{\%} for arousal and valence binary classification, respectively.},
  bdsk-url-1       = {https://doi.org/10.3390/s21155092},
  c1               = {Department of Artificial Intelligence Convergence, Chonnam National University, 77 Yongbong-ro, Gwangju 500-757, Korea;; Department of Artificial Intelligence Convergence, Chonnam National University, 77 Yongbong-ro, Gwangju 500-757, Korea;; Department of Artificial Intelligence Convergence, Chonnam National University, 77 Yongbong-ro, Gwangju 500-757, Korea;; Department of Artificial Intelligence Convergence, Chonnam National University, 77 Yongbong-ro, Gwangju 500-757, Korea;},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/s21155092},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Affective Computing; Head Gesture Recognition},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s21155092},
}

@Article{tiwari2019fu,
  author           = {Tiwari, Abhishek and Falk, Tiago},
  journal          = {Computational intelligence and neuroscience},
  title            = {Fusion of Motif- and Spectrum-Related Features for Improved EEG-Based Emotion Recognition},
  year             = {2019},
  month            = jan,
  pages            = {1--14},
  volume           = {2019},
  abstract         = {Emotion recognition is a burgeoning field allowing for more natural human-machine interactions and interfaces. Electroencephalography (EEG) has shown to be a useful modality with which user emotional states can be measured and monitored, particularly primitives such as valence and arousal. In this paper, we propose the use of ordinal pattern analysis, also called motifs, for improved EEG-based emotion recognition. Motifs capture recurring structures in time series and are inherently robust to noise, thus are well suited for the task at hand. Several connectivity, asymmetry, and graph-theoretic features are proposed and extracted from the motifs to be used for affective state recognition. Experiments with a widely used public database are conducted, and results show the proposed features outperforming benchmark spectrum-based features, as well as other more recent nonmotif-based graph-theoretic features and amplitude modulation-based connectivity/asymmetry measures. Feature and score-level fusion suggest complementarity between the proposed and benchmark spectrum-based measures. When combined, the fused models can provide up to 9{\%} improvement relative to benchmark features alone and up to 16{\%} to nonmotif-based graph-theoretic features.},
  bdsk-url-1       = {https://doi.org/10.1155/2019/3076324},
  c1               = {Institut National de la Research Scientifique, Universit{\'e}du Qu{\'e}bec, Montr{\'e}al, Qu{\'e}bec, Canada; Institut National de la Research Scientifique, Universit{\'e}du Qu{\'e}bec, Montr{\'e}al, Qu{\'e}bec, Canada},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1155/2019/3076324},
  hasabstract      = {Y},
  isbn             = {1687-5265},
  keywords         = {Emotion Recognition; EEG Analysis; Emotion Regulation; Affective Computing; Deep Learning for EEG},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Hindawi Publishing Corporation},
  url              = {https://doi.org/10.1155/2019/3076324},
}

@Article{zhang2020re,
  author           = {Zhang, Jing and Wen, Xingyu and Whang, Mincheol},
  journal          = {Sensors},
  title            = {Recognition of Emotion According to the Physical Elements of the Video},
  year             = {2020},
  month            = jan,
  number           = {3},
  pages            = {649--649},
  volume           = {20},
  abstract         = {The increasing interest in the effects of emotion on cognitive, social, and neural processes creates a constant need for efficient and reliable techniques for emotion elicitation. Emotions are important in many areas, especially in advertising design and video production. The impact of emotions on the audience plays an important role. This paper analyzes the physical elements in a two-dimensional emotion map by extracting the physical elements of a video (color, light intensity, sound, etc.). We used k-nearest neighbors (K-NN), support vector machine (SVM), and multilayer perceptron (MLP) classifiers in the machine learning method to accurately predict the four dimensions that express emotions, as well as summarize the relationship between the two-dimensional emotion space and physical elements when designing and producing video.},
  bdsk-url-1       = {https://doi.org/10.3390/s20030649},
  c1               = {Department of Emotion Engineering, University of Sangmyung, Seoul 03016, Korea.; Department of Emotion Engineering, University of Sangmyung, Seoul 03016, Korea.; Department of Human-Centered Artificial Intelligence, University of Sangmyung, Seoul 03016, Korea.},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/s20030649},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Emotion Recognition; Affective Computing; Emotions; Feature Extraction; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s20030649},
}

@Article{saganowski2022em,
  author           = {Saganowski, Stanis{\l}aw and Komoszy{\'n}ska, Joanna and Behnke, Maciej and Perz, Bartosz and Kunc, Dominika and Klich, Bart{\l}omiej and Kaczmarek, {\L}ukasz and Kazienko, Przemys{\l}aw},
  journal          = {Scientific data},
  title            = {Emognition dataset: emotion recognition with self-reports, facial expressions, and physiology using wearables},
  year             = {2022},
  month            = apr,
  number           = {1},
  volume           = {9},
  abstract         = {Abstract The Emognition dataset is dedicated to testing methods for emotion recognition (ER) from physiological responses and facial expressions. We collected data from 43 participants who watched short film clips eliciting nine discrete emotions: amusement, awe, enthusiasm, liking, surprise, anger, disgust, fear, and sadness. Three wearables were used to record physiological data: EEG, BVP (2x), HR, EDA, SKT, ACC (3x), and GYRO (2x); in parallel with the upper-body videos. After each film clip, participants completed two types of self-reports: (1) related to nine discrete emotions and (2) three affective dimensions: valence, arousal, and motivation. The obtained data facilitates various ER approaches, e.g., multimodal ER, EEG- vs. cardiovascular-based ER, discrete to dimensional representation transitions. The technical validation indicated that watching film clips elicited the targeted emotions. It also supported signals'high quality.},
  bdsk-url-1       = {https://doi.org/10.1038/s41597-022-01262-0},
  c1               = {Wroclaw University of Science and Technology, Faculty of Information and Communication Technology, Department of Artificial Intelligence, Wroc{\l}aw, 50-370, Poland; Wroclaw University of Science and Technology, Faculty of Information and Communication Technology, Department of Artificial Intelligence, Wroc{\l}aw, 50-370, Poland; Wroclaw University of Science and Technology, Faculty of Information and Communication Technology, Department of Artificial Intelligence, Wroc{\l}aw, 50-370, Poland; Wroclaw University of Science and Technology, Faculty of Information and Communication Technology, Department of Artificial Intelligence, Wroc{\l}aw, 50-370, Poland; Wroclaw University of Science and Technology, Faculty of Information and Communication Technology, Department of Artificial Intelligence, Wroc{\l}aw, 50-370, Poland; Wroclaw University of Science and Technology, Faculty of Information and Communication Technology, Department of Artificial Intelligence, Wroc{\l}aw, 50-370, Poland; Adam Mickiewicz University, Faculty of Psychology and Cognitive Science, Poznan, 61-664, Poland; Wroclaw University of Science and Technology, Faculty of Information and Communication Technology, Department of Artificial Intelligence, Wroc{\l}aw, 50-370, Poland},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1038/s41597-022-01262-0},
  hasabstract      = {Y},
  isbn             = {2052-4463},
  keywords         = {Emotion Recognition; Affective Computing; Emotion Regulation; Speech Emotion; EEG Analysis},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Nature Portfolio},
  url              = {https://doi.org/10.1038/s41597-022-01262-0},
}

@Article{li2017ch,
  author           = {Li, Xian and Yan, Jianzhuo and Chen, Jianhui},
  journal          = {ITM web of conferences},
  title            = {Channel Division Based Multiple Classifiers Fusion for Emotion Recognition Using EEG signals},
  year             = {2017},
  month            = jan,
  pages            = {07006--07006},
  volume           = {11},
  abstract         = {With the rapid development of computer technology, pervasive computing and wearable devices, EEG-based emotion recognition has gradually attracted much attention in affecting computing (AC) domain. In this paper, we propose an approach of emotion recognition using EEG signals based on the weighted fusion of multiple base classifiers. These base classifiers based on SVM are constructed using a channel division mechanism according to the neuropsychological theory that different brain areas are differ in processing intensity of emotional information. The outputs of channel base classifiers are integrated by a weighted fusion strategy which is based on the confidence estimation on each emotional label by each base classifier. The evaluation on the DEAP dataset shows that our proposed multiple classifiers fusion method outperforms individual channel base classifiers and the feature fusion method for EEG-based emotion recognition.},
  bdsk-url-1       = {https://doi.org/10.1051/itmconf/20171107006},
  c1               = {Faculty of Information Technology, Beijing University of Technology, Beijing 100124, China; Faculty of Information Technology, Beijing University of Technology, Beijing 100124, China; Faculty of Information Technology, Beijing University of Technology, Beijing 100124, China},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1051/itmconf/20171107006},
  hasabstract      = {Y},
  isbn             = {2271-2097},
  keywords         = {Emotion Recognition; Deep Learning for EEG; Affective Computing; EEG Analysis; Head Gesture Recognition},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {EDP Sciences},
  url              = {https://doi.org/10.1051/itmconf/20171107006},
}

@Article{valderas2019mu,
  author           = {Valderas, Mar{\'\i}a and Bolea, Juan and Laguna, Pablo and Bail{\'o}n, Raquel and Vallverd{\'u}, Montserrat},
  journal          = {Physiological measurement},
  title            = {Mutual information between heart rate variability and respiration for emotion characterization},
  year             = {2019},
  month            = sep,
  number           = {8},
  pages            = {084001--084001},
  volume           = {40},
  abstract         = {Interest in emotion recognition has increased in recent years as a useful tool for diagnosing psycho-neural illnesses. In this study, the auto-mutual and the cross-mutual information function, AMIF and CMIF respectively, are used for human emotion recognition.The AMIF technique was applied to heart rate variability (HRV) signals to study complex interdependencies, and the CMIF technique was considered to quantify the complex coupling between HRV and respiratory signals. Both algorithms were adapted to short-term RR time series. Traditional band pass filtering was applied to the RR series at low frequency (LF) and high frequency (HF) bands, and a respiration-based filter bandwidth was also investigated ({$[$}Formula: see text{$]$}). Both the AMIF and the CMIF algorithms were calculated with regard to different time scales as specific complexity measures. The ability of the parameters derived from the AMIF and the CMIF to discriminate emotions was evaluated on a database of video-induced emotion elicitation. Five elicited states i.e. relax (neutral), joy (positive valence), as well as fear, sadness and anger (negative valences) were considered.The results revealed that the AMIF applied to the RR time series filtered in the {$[$}Formula: see text{$]$} band was able to discriminate between the following: relax and joy and fear, joy and each negative valence conditions, and finally fear and sadness and anger, all with a statistical significance level p -value {$[$}Formula: see text{$]$} 0.05, sensitivity, specificity and accuracy higher than 70{\%} and area under the receiver operating characteristic curve index AUC {$[$}Formula: see text{$]$}0.70. Furthermore, the parameters derived from the AMIF and the CMIF allowed the low signal complexity presented during fear to be characterized in front of any of the studied elicited states.Based on these results, human emotion manifested in the HRV and respiratory signal responses could be characterized by means of the information-content complexity.},
  bdsk-url-1       = {https://doi.org/10.1088/1361-6579/ab310a},
  c1               = {Arag{\'o}n Institute for Engineering Research (I3A), IIS Arag{\'o}n, University of Zaragoza, 50018 Zaragoza, Spain; Biomedical Signal Interpretation and Computational Simulation (BSICoS), Arag{\'o}n Institute for Engineering Research (I3A), IIS Arag{\'o}n, University of Zaragoza, Spain, Mar{\'\i}a de Luna, 1, 50015 Zaragoza, Spain; Department ESAII, Centre for Biomedical Engineering Research, Universitat Polit{\`e}cnica de Catalunya, Barcelona, 08028, Spain; Biomedical Signal Interpretation and Computational Simulation (BSICoS), Arag{\'o}n Institute for Engineering Research (I3A), IIS Arag{\'o}n, University of Zaragoza, Spain, Mar{\'\i}a de Luna, 1, 50015 Zaragoza, Spain; CIBER de Bioingenier{\'\i}a, Biomateriales y Nanomedicina (CIBER-BBN), Madrid, Spain; Biomedical Signal Interpretation and Computational Simulation (BSICoS), Arag{\'o}n Institute for Engineering Research (I3A), IIS Arag{\'o}n, University of Zaragoza, Spain, Mar{\'\i}a de Luna, 1, 50015 Zaragoza, Spain; CIBER de Bioingenier{\'\i}a, Biomateriales y Nanomedicina (CIBER-BBN), Madrid, Spain; Biomedical Signal Interpretation and Computational Simulation (BSICoS), Arag{\'o}n Institute for Engineering Research (I3A), IIS Arag{\'o}n, University of Zaragoza, Spain, Mar{\'\i}a de Luna, 1, 50015 Zaragoza, Spain; CIBER de Bioingenier{\'\i}a, Biomateriales y Nanomedicina (CIBER-BBN), Madrid, Spain; CIBER de Bioingenier{\'\i}a, Biomateriales y Nanomedicina (CIBER-BBN), Madrid, Spain; Department ESAII, Centre for Biomedical Engineering Research, Universitat Polit{\`e}cnica de Catalunya, Barcelona, 08028, Spain},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1088/1361-6579/ab310a},
  hasabstract      = {Y},
  isbn             = {0967-3334},
  keywords         = {Heart Rate Variability; Emotion Recognition; Affective Computing; Emotion Regulation; Human-Computer Interaction},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {IOP Publishing},
  url              = {https://doi.org/10.1088/1361-6579/ab310a},
}

@Article{kong2021ee,
  author           = {Kong, Tianjiao and Shao, Jie and Hu, Jiuyuan and Yang, Xin and Yang, Shiyiling and Malekian, Reza},
  journal          = {Sensors},
  title            = {EEG-Based Emotion Recognition Using an Improved Weighted Horizontal Visibility Graph},
  year             = {2021},
  month            = mar,
  number           = {5},
  pages            = {1870--1870},
  volume           = {21},
  abstract         = {Emotion recognition, as a challenging and active research area, has received considerable awareness in recent years. In this study, an attempt was made to extract complex network features from electroencephalogram (EEG) signals for emotion recognition. We proposed a novel method of constructing forward weighted horizontal visibility graphs (FWHVG) and backward weighted horizontal visibility graphs (BWHVG) based on angle measurement. The two types of complex networks were used to extract network features. Then, the two feature matrices were fused into a single feature matrix to classify EEG signals. The average emotion recognition accuracies based on complex network features of proposed method in the valence and arousal dimension were 97.53{\%} and 97.75{\%}. The proposed method achieved classification accuracies of 98.12{\%} and 98.06{\%} for valence and arousal when combined with time-domain features.},
  bdsk-url-1       = {https://doi.org/10.3390/s21051870},
  c1               = {College of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing 210016, China; Key Laboratory of Underwater Acoustic Signal Processing, Ministry of Education, Southeast University, Nanjing 210096, China; College of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing 210016, China; Key Laboratory of Underwater Acoustic Signal Processing, Ministry of Education, Southeast University, Nanjing 210096, China; College of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing 210016, China; Key Laboratory of Underwater Acoustic Signal Processing, Ministry of Education, Southeast University, Nanjing 210096, China; College of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing 210016, China; Key Laboratory of Underwater Acoustic Signal Processing, Ministry of Education, Southeast University, Nanjing 210096, China; College of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing 210016, China; Key Laboratory of Underwater Acoustic Signal Processing, Ministry of Education, Southeast University, Nanjing 210096, China; Department of Computer Science and Media Technology, Malm{\"o}University, 20506 Malm{\"o}, Sweden},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/s21051870},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Head Gesture Recognition; Affective Computing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s21051870},
}

@Article{abaalkhail2018su,
  author           = {Abaalkhail, Rana and Guthier, Benjamin and Alharthi, Rajwa and Saddik, Abdulmotaleb},
  journal          = {Semantic web},
  title            = {Survey on ontologies for affective states and their influences},
  year             = {2018},
  month            = jun,
  number           = {4},
  pages            = {441--458},
  volume           = {9},
  abstract         = {Human behavior is impacted by emotion, mood, personality, needs and subjective well-being. Emotion and mood are human affective states while personality, needs and subjective well-being are influences on those affective states. Ontologies are a method of representing real-world knowledge, such as h uman affective states and their influences, in a format that a computer can process. They allow researchers to build systems that harness affective states. By unifying terms and meanings, ontologies enable these systems to communicate and share knowledge with each other. In this paper, we survey existing ontologies on affective states and their influences. We also provide the psychological background of affective states, their influences and representational models. The paper discusses a total of 20 ontologies on emotion, one ontology on mood, one ontology on needs, and 11 general purpose ontologies and lexicons. Based on the analysis of existing ontologies, we summarize and discuss the current state of the art in the field.},
  bdsk-url-1       = {https://doi.org/10.3233/sw-170270},
  c1               = {College of Computer Science, Information System Department, King Saud University, Kingdom of Saudi Arabia; Multimedia Communications Research Laboratory, University of Ottawa, 800 King Edward Ave, K1N 6N5, Ottawa, ON, Canada. E-mails: rabaa006{\char64}uottawa.ca, ralha081{\char64}uottawa.ca, elsaddik{\char64}uottawa.ca; Department of Computer Science IV, University of Mannheim, Germany. E-mail: guthier{\char64}informatik.uni-mannheim.de; College of Computer and Information Technology, Taif University, Kingdom of Saudi Arabia; Multimedia Communications Research Laboratory, University of Ottawa, 800 King Edward Ave, K1N 6N5, Ottawa, ON, Canada. E-mails: rabaa006{\char64}uottawa.ca, ralha081{\char64}uottawa.ca, elsaddik{\char64}uottawa.ca; Multimedia Communications Research Laboratory, University of Ottawa, 800 King Edward Ave, K1N 6N5, Ottawa, ON, Canada. E-mails: rabaa006{\char64}uottawa.ca, ralha081{\char64}uottawa.ca, elsaddik{\char64}uottawa.ca},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3233/sw-170270},
  hasabstract      = {Y},
  isbn             = {1570-0844},
  keywords         = {Affective Computing; Emotion Recognition; Emotion Dynamics; Speech Emotion; Personality Data},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {IOS Press},
  url              = {https://doi.org/10.3233/sw-170270},
}

@Article{chen2022em,
  author           = {Chen, J. and Ro, Tony and Zhu, Zhigang},
  journal          = {IEEE access},
  title            = {Emotion Recognition With Audio, Video, EEG, and EMG: A Dataset and Baseline Approaches},
  year             = {2022},
  month            = jan,
  pages            = {13229--13242},
  volume           = {10},
  abstract         = {This paper describes a new posed multimodal emotional dataset and compares human emotion classification based on four different modalities -audio, video, electromyography (EMG), and electroencephalography (EEG).The results are reported with several baseline approaches using various feature extraction techniques and machine-learning algorithms.First, we collected a dataset from 11 human subjects expressing six basic emotions and one neutral emotion.We then extracted features from each modality using principal component analysis, autoencoder, convolution network, and mel-frequency cepstral coefficient (MFCC), some unique to individual modalities.A number of baseline models have been applied to compare the classification performance in emotion recognition, including k-nearest neighbors (KNN), support vector machines (SVM), random forest, multilayer perceptron (MLP), long short-term memory (LSTM) model, and convolutional neural network (CNN).Our results show that bootstrapping the biosensor signals (i.e., EMG and EEG) can greatly increase emotion classification performance by reducing noise.In contrast, the best classification results were obtained by a traditional KNN, whereas audio and image sequences of human emotions could be better classified using LSTM.},
  bdsk-url-1       = {https://doi.org/10.1109/access.2022.3146729},
  c1               = {Computer Science Department, The City College of New York (CUNY), New York, NY 10031, USA; Programs in Psychology, Biology, and Cognitive Neuroscience, The Graduate Center, CUNY, New York, NY 10016, USA; Computer Science Department, The City College of New York (CUNY), New York, NY 10031, USA; Doctoral Program in Computer Science, The Graduate Center, CUNY, New York, NY 10016, USA},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1109/access.2022.3146729},
  hasabstract      = {Y},
  isbn             = {2169-3536},
  keywords         = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Affective Computing; Environmental Sound Recognition},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Institute of Electrical and Electronics Engineers},
  url              = {https://doi.org/10.1109/access.2022.3146729},
}

@Article{tang2021en,
  author        = {Tang, Duowei and Kuppens, Peter and Geurts, Luc and van Waterschoot, Toon},
  journal       = {EURASIP Journal on Audio, Speech and Music Processing},
  title         = {End-to-end speech emotion recognition using a novel context-stacking dilated convolution neural network},
  year          = {2021},
  month         = may,
  number        = {1},
  volume        = {2021},
  abstract      = {Amongst the various characteristics of a speech signal, the expression of emotion is one of the characteristics that exhibits the slowest temporal dynamics. Hence, a performant speech emotion recognition (SER) system requires a predictive model that is capable of learning sufficiently long temporal dependencies in the analysed speech signal. Therefore, in this work, we propose a novel end-to-end neural network architecture based on the concept of dilated causal convolution with context stacking. Firstly, the proposed model consists only of parallelisable layers and is hence suitable for parallel processing, while avoiding the inherent lack of parallelisability occurring with recurrent neural network (RNN) layers. Secondly, the design of a dedicated dilated causal convolution block allows the model to have a receptive field as large as the input sequence length, while maintaining a reasonably low computational cost. Thirdly, by introducing a context stacking structure, the proposed model is capable of exploiting long-term temporal dependencies hence providing an alternative to the use of RNN layers. We evaluate the proposed model in SER regression and classification tasks and provide a comparison with a state-of-the-art end-to-end SER model. Experimental results indicate that the proposed model requires only 1/3 of the number of model parameters used in the state-of-the-art model, while also significantly improving SER performance. Further experiments are reported to understand the impact of using various types of input representations (i.e. raw audio samples vs log mel-spectrograms) and to illustrate the benefits of an end-to-end approach over the use of hand-crafted audio features. Moreover, we show that the proposed model can efficiently learn intermediate embeddings preserving speech emotion information.},
  bdsk-url-1    = {https://doi.org/10.1186/s13636-021-00208-5},
  c1            = {Department of Electrical Engineering (ESAT), STADIUS Center for Dynamical Systems, Signal Processing, and Data Analytics, KU Leuven, Leuven, Belgium; Faculty of Psychology and Educational Sciences, KU Leuven, Leuven, Belgium; e-Media Research Lab, KU Leuven, Leuven, Belgium; Department of Electrical Engineering (ESAT), STADIUS Center for Dynamical Systems, Signal Processing, and Data Analytics, KU Leuven, Leuven, Belgium},
  comment       = {no music},
  date-added    = {2024-05-13 15:05:26 +0100},
  date-modified = {2024-05-13 15:05:26 +0100},
  doi           = {10.1186/s13636-021-00208-5},
  hasabstract   = {Y},
  isbn          = {1687-4714},
  keywords      = {Emotion Recognition; Speech Emotion; Audio-Visual Speech Recognition; Affective Computing; Audio Event Detection},
  la            = {en},
  priority      = {prio3},
  publisher     = {Springer Nature},
  url           = {https://doi.org/10.1186/s13636-021-00208-5},
}

@Article{amiriparian2017se,
  author           = {Amiriparian, Shahin and Cummins, Nicholas and Ottl, Sandra and Gerczuk, Maurice and Schuller, Bj{\"o}rn},
  title            = {Sentiment analysis using image-based deep spectrum features},
  year             = {2017},
  month            = oct,
  abstract         = {We test the suitability of our novel deep spectrum feature representation for performing speech-based sentiment analysis. Deep spectrum features are formed by passing spectrograms through a pre-trained image convolutional neural network (CNN) and have been shown to capture useful emotion information in speech; however, their usefulness for sentiment analysis is yet to be investigated. Using a data set of movie reviews collected from YouTube, we compare deep spectrum features combined with the bag-of-audio-words (BoAW) paradigm with a state-of-the-art Mel Frequency Cepstral Coefficients (MFCC) based BoAW system when performing a binary sentiment classification task. Key results presented indicate the suitability of both features for the proposed task. The deep spectrum features achieve an unweighted average recall of 74.5 {\%}. The results provide further evidence for the effectiveness of deep spectrum features as a robust feature representation for speech analysis.},
  bdsk-url-1       = {https://doi.org/10.1109/aciiw.2017.8272618},
  c1               = {Chair of Complex \& Intelligent Systems, Universit{\"a}t Passau, Germany; Chair of Embedded Intelligence for Health Care \& Wellbeing, Augsburg University, Augsburg, Germany; Machine Intelligence \& Signal Processing Group, Technische Universit{\"a}at M{\"u}unchen, Germany; Chair of Complex \& Intelligent Systems, Universit{\"a}t Passau, Germany; Chair of Embedded Intelligence for Health Care \& Wellbeing, Augsburg University, Augsburg, Germany; Chair of Complex \& Intelligent Systems, Universit{\"a}t Passau, Germany{\#}TAB{\#}; Chair of Complex \& Intelligent Systems, Universit{\"a}t Passau, Germany{\#}TAB{\#}; Chair of Complex \& Intelligent Systems, Universit{\"a}t Passau, Germany; Imperial College London, GLAM--Group on Language, Audio \& Music, London, UK},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1109/aciiw.2017.8272618},
  hasabstract      = {Y},
  keywords         = {Emotion Recognition; Affective Computing; Feature Extraction; Aspect-based Sentiment Analysis; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.1109/aciiw.2017.8272618},
}

@Article{hazerrau2020th,
  author           = {Hazer-Rau, Dilana and Meudt, Sascha and Daucher, Andreas and Spohrs, Jennifer and Hoffmann, Holger and Schwenker, Friedhelm and Traue, Harald},
  journal          = {Sensors},
  title            = {The uulmMAC Database---A Multimodal Affective Corpus for Affective Computing in Human-Computer Interaction},
  year             = {2020},
  month            = apr,
  number           = {8},
  pages            = {2308--2308},
  volume           = {20},
  abstract         = {In this paper, we present a multimodal dataset for affective computing research acquired in a human-computer interaction (HCI) setting. An experimental mobile and interactive scenario was designed and implemented based on a gamified generic paradigm for the induction of dialog-based HCI relevant emotional and cognitive load states. It consists of six experimental sequences, inducing Interest, Overload, Normal, Easy, Underload, and Frustration. Each sequence is followed by subjective feedbacks to validate the induction, a respiration baseline to level off the physiological reactions, and a summary of results. Further, prior to the experiment, three questionnaires related to emotion regulation (ERQ), emotional control (TEIQue-SF), and personality traits (TIPI) were collected from each subject to evaluate the stability of the induction paradigm. Based on this HCI scenario, the University of Ulm Multimodal Affective Corpus (uulmMAC), consisting of two homogenous samples of 60 participants and 100 recording sessions was generated. We recorded 16 sensor modalities including 4 ×video, 3 ×audio, and 7 ×biophysiological, depth, and pose streams. Further, additional labels and annotations were also collected. After recording, all data were post-processed and checked for technical and signal quality, resulting in the final uulmMAC dataset of 57 subjects and 95 recording sessions. The evaluation of the reported subjective feedbacks shows significant differences between the sequences, well consistent with the induced states, and the analysis of the questionnaires shows stable results. In summary, our uulmMAC database is a valuable contribution for the field of affective computing and multimodal data analysis: Acquired in a mobile interactive scenario close to real HCI, it consists of a large number of subjects and allows transtemporal investigations. Validated via subjective feedbacks and checked for quality issues, it can be used for affective computing and machine learning applications.},
  bdsk-url-1       = {https://doi.org/10.3390/s20082308},
  c1               = {Section Medical Psychology, University of Ulm, Frauensteige 6, 89075 Ulm, Germany; Institute of Neural Information Processing, University of Ulm, James-Frank-Ring, 89081 Ulm, Germany; Section Medical Psychology, University of Ulm, Frauensteige 6, 89075 Ulm, Germany; Section Medical Psychology, University of Ulm, Frauensteige 6, 89075 Ulm, Germany; Section Medical Psychology, University of Ulm, Frauensteige 6, 89075 Ulm, Germany; Institute of Neural Information Processing, University of Ulm, James-Frank-Ring, 89081 Ulm, Germany; Section Medical Psychology, University of Ulm, Frauensteige 6, 89075 Ulm, Germany},
  comment          = {no music},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/s20082308},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Affective Computing; Human-Computer Interaction; Emotion Recognition; Affective Design; Multimodal Data},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s20082308},
}

@Article{wang2022ee,
  author           = {Wang, Yuqi and Zhang, Lijun and Xia, Pan and Wang, Peng and Chen, Xianxiang and Du, Lidong and Fang, Zhen and Du, Mingyan},
  journal          = {Bioengineering},
  title            = {EEG-Based Emotion Recognition Using a 2D CNN with Different Kernels},
  year             = {2022},
  month            = may,
  number           = {6},
  pages            = {231--231},
  volume           = {9},
  abstract         = {Emotion recognition is receiving significant attention in research on health care and Human-Computer Interaction (HCI). Due to the high correlation with emotion and the capability to affect deceptive external expressions such as voices and faces, Electroencephalogram (EEG) based emotion recognition methods have been globally accepted and widely applied. Recently, great improvements have been made in the development of machine learning for EEG-based emotion detection. However, there are still some major disadvantages in previous studies. Firstly, traditional machine learning methods require extracting features manually which is time-consuming and rely heavily on human experts. Secondly, to improve the model accuracies, many researchers used user-dependent models that lack generalization and universality. Moreover, there is still room for improvement in the recognition accuracies in most studies. Therefore, to overcome these shortcomings, an EEG-based novel deep neural network is proposed for emotion classification in this article. The proposed 2D CNN uses two convolutional kernels of different sizes to extract emotion-related features along both the time direction and the spatial direction. To verify the feasibility of the proposed model, the pubic emotion dataset DEAP is used in experiments. The results show accuracies of up to 99.99{\%} and 99.98 for arousal and valence binary classification, respectively, which are encouraging for research and applications in the emotion recognition field.},
  bdsk-url-1       = {https://doi.org/10.3390/bioengineering9060231},
  c1               = {Institute of Microelectronics of Chinese Academy of Sciences, Beijing 100029, China;; University of Chinese Academy of Sciences, Beijing 100049, China;; Institute of Microelectronics of Chinese Academy of Sciences, Beijing 100029, China;; University of Chinese Academy of Sciences, Beijing 100049, China;; Aerospace Information Research Institute, Chinese Academy of Sciences (AIRCAS), Beijing 100190, China; University of Chinese Academy of Sciences, Beijing 100049, China;; Aerospace Information Research Institute, Chinese Academy of Sciences (AIRCAS), Beijing 100190, China; Aerospace Information Research Institute, Chinese Academy of Sciences (AIRCAS), Beijing 100190, China; University of Chinese Academy of Sciences, Beijing 100049, China;; Aerospace Information Research Institute, Chinese Academy of Sciences (AIRCAS), Beijing 100190, China; University of Chinese Academy of Sciences, Beijing 100049, China;; Aerospace Information Research Institute, Chinese Academy of Sciences (AIRCAS), Beijing 100190, China; Personalized Management of Chronic Respiratory Disease, Chinese Academy of Medical Sciences, Beijing 100190, China; University of Chinese Academy of Sciences, Beijing 100049, China;; China Beijing Luhe Hospital, Capital Medical University, Beijing 101199, China; University of Chinese Academy of Sciences, Beijing 100049, China;},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/bioengineering9060231},
  hasabstract      = {Y},
  isbn             = {2306-5354},
  keywords         = {Emotion Recognition; Deep Learning for EEG; Affective Computing; EEG Analysis; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/bioengineering9060231},
}

@Article{tu2020am,
  author        = {Tu, Guoyun and Fu, Yanwei and Li, Boyang and Gao, Jiarui and Jiang, Yu-Gang and Xue, Xiangyang},
  journal       = {IEEE transactions on multimedia},
  title         = {A Multi-Task Neural Approach for Emotion Attribution, Classification, and Summarization},
  year          = {2020},
  month         = jan,
  number        = {1},
  pages         = {148--159},
  volume        = {22},
  abstract      = {Emotional content is a crucial ingredient in user-generated videos. However, the sparsity of emotional expressions in the videos poses an obstacle to visual emotion analysis. In this paper, we propose a new neural approach, Bi-stream Emotion Attribution-Classification Network (BEAC-Net), to solve three related emotion analysis tasks: emotion recognition, emotion attribution, and emotion-oriented summarization, in a single integrated framework. BEAC-Net has two major constituents, an attribution network and a classification network. The attribution network extracts the main emotional segment that classification should focus on in order to mitigate the sparsity issue. The classification network utilizes both the extracted segment and the original video in a bi-stream architecture. We contribute a new dataset for the emotion attribution task with human-annotated ground-truth labels for emotion segments. Experiments on two video datasets demonstrate superior performance of the proposed framework and the complementary nature of the dual classification streams.},
  bdsk-url-1    = {https://doi.org/10.1109/tmm.2019.2922129},
  c1            = {Fudan University, Shanghai, China; School of Data Science, Fudan University, Shanghai, China; Big Data Laboratory, Baidu Research, Sunnyvale, US; Fudan University, Shanghai, China; Jilian Technology Group (Video++), Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; Fudan University, Shanghai, China},
  comment       = {video},
  date-added    = {2024-05-13 15:05:26 +0100},
  date-modified = {2024-05-13 15:05:26 +0100},
  doi           = {10.1109/tmm.2019.2922129},
  hasabstract   = {Y},
  isbn          = {1520-9210},
  keywords      = {Affective Computing; Emotion Recognition; Video Summarization; Action Recognition; Feature Extraction},
  la            = {en},
  priority      = {prio3},
  publisher     = {Institute of Electrical and Electronics Engineers},
  url           = {https://doi.org/10.1109/tmm.2019.2922129},
}

@Article{su2019ad,
  author           = {Su, Yuanyuan and Li, Wenchao and Bi, Ning and Lv, Zhao},
  journal          = {Frontiers in neurorobotics},
  title            = {Adolescents Environmental Emotion Perception by Integrating EEG and Eye Movements},
  year             = {2019},
  month            = jun,
  volume           = {13},
  abstract         = {Giving a robot the ability to perceive emotion in its environment can improve human-robot interaction (HRI), thereby facilitating more human-like communication. To achieve emotion recognition in different built environments for adolescents, we propose a multi-modal emotion intensity perception method using an integration of electroencephalography (EEG) and eye movement information. Specifically, we first develop a new stimulus video selection method based on computation of normalized arousal and valence scores according to subjective feedback from participants. Then, we establish a valence perception sub-model and an arousal sub-model by collecting and analyzing emotional EEG and eye movement signals, respectively. We employ this dual recognition method to perceive emotional intensities synchronously in two dimensions. In the laboratory environment, the best recognition accuracies of the modality fusion for the arousal and valence dimensions are 72.8{\%} and 69.3{\%}. The experimental results validate the feasibility of the proposed multi-modal emotion recognition method for environment emotion intensity perception. This promising tool not only achieves more accurate emotion perception for HRI systems but also provides an alternative approach to quantitatively assess environmental psychology.},
  bdsk-url-1       = {https://doi.org/10.3389/fnbot.2019.00046},
  c1               = {College of Design, Iowa State University, Ames, IA, United States; Department of Design, Anhui University, Hefei, China; School of Computer Science and Technology, Anhui University, Hefei, China; School of Computer Science, Georgia Institute of Technology, Atlanta, GA, United States; School of Computer Science and Technology, Anhui University, Hefei, China},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3389/fnbot.2019.00046},
  hasabstract      = {Y},
  isbn             = {1662-5218},
  keywords         = {Emotion Recognition; Eye Movement Analysis; Deep Learning for EEG; EEG Analysis; Affective Computing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Frontiers Media},
  url              = {https://doi.org/10.3389/fnbot.2019.00046},
}

@Article{maeng2020de,
  author        = {Maeng, Junho and Kang, Dong and Kim, Deok‐Hwan},
  journal       = {Electronics},
  title         = {Deep Learning Method for Selecting Effective Models and Feature Groups in Emotion Recognition Using an Asian Multimodal Database},
  year          = {2020},
  month         = nov,
  number        = {12},
  pages         = {1988--1988},
  volume        = {9},
  abstract      = {Emotional awareness is vital for advanced interactions between humans and computer systems. This paper introduces a new multimodal dataset called MERTI-Apps based on Asian physiological signals and proposes a genetic algorithm (GA)---long short-term memory (LSTM) deep learning model to derive the active feature groups for emotion recognition. This study developed an annotation labeling program for observers to tag the emotions of subjects by their arousal and valence during dataset creation. In the learning phase, a GA was used to select effective LSTM model parameters and determine the active feature group from 37 features and 25 brain lateralization features extracted from the electroencephalogram (EEG) time, frequency, and time--frequency domains. The proposed model achieved a root-mean-square error (RMSE) of 0.0156 in terms of the valence regression performance in the MAHNOB-HCI dataset, and RMSE performances of 0.0579 and 0.0287 in terms of valence and arousal regression performance, and 65.7{\%} and 88.3{\%} in terms of valence and arousal accuracy in the in-house MERTI-Apps dataset, which uses Asian-population-specific 12-channel EEG data and adds an additional brain lateralization (BL) feature. The results revealed 91.3{\%} and 94.8{\%} accuracy in the valence and arousal domain in the DEAP dataset owing to the effective model selection of a GA.},
  bdsk-url-1    = {https://doi.org/10.3390/electronics9121988},
  c1            = {Department of Electronic Engineering, Inha University, Incheon 22212, Korea;; Department of Electronic Engineering, Inha University, Incheon 22212, Korea;; Department of Electronic Engineering, Inha University, Incheon 22212, Korea;},
  comment       = {physiological},
  date-added    = {2024-05-13 15:05:26 +0100},
  date-modified = {2024-05-13 15:05:26 +0100},
  doi           = {10.3390/electronics9121988},
  hasabstract   = {Y},
  isbn          = {2079-9292},
  keywords      = {Emotion Recognition; Affective Computing; Head Gesture Recognition; Deep Learning for EEG; Deep Learning},
  la            = {en},
  priority      = {prio3},
  publisher     = {Multidisciplinary Digital Publishing Institute},
  url           = {https://doi.org/10.3390/electronics9121988},
}

@Article{marinmorales2021he,
  author           = {Mar{\'\i}n‐Morales, Javier and Higuera-Trujillo, Juan and Guixeres, Jaime and Llinares, Carmen and Alca{\~n}{\'\i}z, Mariano and Valenza, Gaetano},
  journal          = {PloS one},
  title            = {Heart rate variability analysis for the assessment of immersive emotional arousal using virtual reality: Comparing real and virtual scenarios},
  year             = {2021},
  month            = jul,
  number           = {7},
  pages            = {e0254098--e0254098},
  volume           = {16},
  abstract         = {Many affective computing studies have developed automatic emotion recognition models, mostly using emotional images, audio and videos. In recent years, virtual reality (VR) has been also used as a method to elicit emotions in laboratory environments. However, there is still a need to analyse the validity of VR in order to extrapolate the results it produces and to assess the similarities and differences in physiological responses provoked by real and virtual environments. We investigated the cardiovascular oscillations of 60 participants during a free exploration of a real museum and its virtualisation viewed through a head-mounted display. The differences between the heart rate variability features in the high and low arousal stimuli conditions were analysed through statistical hypothesis testing; and automatic arousal recognition models were developed across the real and the virtual conditions using a support vector machine algorithm with recursive feature selection. The subjects' self-assessments suggested that both museums elicited low and high arousal levels. In addition, the real museum showed differences in terms of cardiovascular responses, differences in vagal activity, while arousal recognition reached 72.92{\%} accuracy. However, we did not find the same arousal-based autonomic nervous system change pattern during the virtual museum exploration. The results showed that, while the direct virtualisation of a real environment might be self-reported as evoking psychological arousal, it does not necessarily evoke the same cardiovascular changes as a real arousing elicitation. These contribute to the understanding of the use of VR in emotion recognition research; future research is needed to study arousal and emotion elicitation in immersive VR.},
  bdsk-url-1       = {https://doi.org/10.1371/journal.pone.0254098},
  c1               = {Instituto de Investigaci{\'o}n e Innovaci{\'o}n en Bioingenier{\'\i}a, Universitat Polit{\`e}cnica de Val{\`e}ncia, Val{\`e}ncia, Spain; Instituto de Investigaci{\'o}n e Innovaci{\'o}n en Bioingenier{\'\i}a, Universitat Polit{\`e}cnica de Val{\`e}ncia, Val{\`e}ncia, Spain; Instituto de Investigaci{\'o}n e Innovaci{\'o}n en Bioingenier{\'\i}a, Universitat Polit{\`e}cnica de Val{\`e}ncia, Val{\`e}ncia, Spain; Instituto de Investigaci{\'o}n e Innovaci{\'o}n en Bioingenier{\'\i}a, Universitat Polit{\`e}cnica de Val{\`e}ncia, Val{\`e}ncia, Spain; Instituto de Investigaci{\'o}n e Innovaci{\'o}n en Bioingenier{\'\i}a, Universitat Polit{\`e}cnica de Val{\`e}ncia, Val{\`e}ncia, Spain; Bioengineering and Robotics Research Centre E Piaggio \& Department of Information Engineering, University of Pisa, Pisa, Italy},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1371/journal.pone.0254098},
  hasabstract      = {Y},
  isbn             = {1932-6203},
  keywords         = {Heart Rate Variability; Affective Computing; Emotion Recognition; Emotion Regulation},
  la               = {en},
  modificationdate = {2024-05-17T04:30:23},
  priority         = {prio3},
  publisher        = {Public Library of Science},
  url              = {https://doi.org/10.1371/journal.pone.0254098},
}

@Article{zhang2018em,
  author           = {Zhang, Xingxing and Xu, Chao and Xue, Wanli and Hu, Jing and He, Yongchuan and Gao, Mengxin},
  journal          = {Sensors},
  title            = {Emotion Recognition Based on Multichannel Physiological Signals with Comprehensive Nonlinear Processing},
  year             = {2018},
  month            = nov,
  number           = {11},
  pages            = {3886--3886},
  volume           = {18},
  abstract         = {Multichannel physiological datasets are usually nonlinear and separable in the field of emotion recognition. Many researchers have applied linear or partial nonlinear processing in feature reduction and classification, but these applications did not work well. Therefore, this paper proposed a comprehensive nonlinear method to solve this problem. On the one hand, as traditional feature reduction may cause the loss of significant amounts of feature information, Kernel Principal Component Analysis (KPCA) based on radial basis function (RBF) was introduced to map the data into a high-dimensional space, extract the nonlinear information of the features, and then reduce the dimension. This method can provide many features carrying information about the structure in the physiological dataset. On the other hand, considering its advantages of predictive power and feature selection from a large number of features, Gradient Boosting Decision Tree (GBDT) was used as a nonlinear ensemble classifier to improve the recognition accuracy. The comprehensive nonlinear processing method had a great performance on our physiological dataset. Classification accuracy of four emotions in 29 participants achieved 93.42{\%}.},
  bdsk-url-1       = {https://doi.org/10.3390/s18113886},
  c1               = {College of Intelligence and Computing, Tianjin University, Tianjin 300350, China;; College of Intelligence and Computing, Tianjin University, Tianjin 300350, China;; College of Intelligence and Computing, Tianjin University, Tianjin 300350, China;; School of Computer Science and Engineering, Tianjin University of Technology, Tianjin 300384, China; College of Intelligence and Computing, Tianjin University, Tianjin 300350, China;; Shenzhen Graduate School, Peking University, Shenzhen 518055, China;; Department of Economics, Pennsylvania State University, State College, PA 16803, USA;},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/s18113886},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Emotion Recognition; Affective Computing; Feature Extraction; Multimodal Data},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s18113886},
}

@Article{rincon2016ad,
  author        = {Rincon, J. and Poza, J. L. and Posadas, Ju{\'a}n and Juli{\'a}n, Vicente and Carrascosa, Carlos},
  journal       = {Advances in distributed computing and artificial intelligence journal},
  title         = {Adding real data to detect emotions by means of smart resource artifacts in MAS},
  year          = {2016},
  month         = nov,
  number        = {4},
  pages         = {85--92},
  volume        = {5},
  abstract      = {This article proposes an application of a social emotional model, which allows to extract, analyse, represent and manage the social emotion of a group of entities. Specifically, the application is based on how music can influence in a positive or negative way over emotional states. The proposed approach employs the JaCalIVE framework, which facilitates the development of this kind of environments. A physical device called smart resource offers to agents processed sensor data as a service. So that, agents obtain real data from a smart resource. MAS uses the smart resource as an artifact by means of a specific communications protocol. The framework includes a design method and a physical simulator. In this way, the social emotional model allows the creation of simulations over JaCalIVE, in which the emotional states are used in the decision-making of the agents.},
  bdsk-url-1    = {https://doi.org/10.14201/adcaij2016548592},
  c1            = {Valencia Polytechnic University; Valencia Polytechnic University; Valencia Polytechnic University; Valencia Polytechnic University; Valencia Polytechnic University},
  comment       = {no music},
  date-added    = {2024-05-13 15:05:26 +0100},
  date-modified = {2024-05-13 15:05:26 +0100},
  doi           = {10.14201/adcaij2016548592},
  hasabstract   = {Y},
  isbn          = {2255-2863},
  keywords      = {Affective Computing; Emotion Recognition},
  la            = {en},
  priority      = {prio3},
  publisher     = {Ediciones Universidad de Salamanca},
  url           = {https://doi.org/10.14201/adcaij2016548592},
}

@Article{kalashami2022ee,
  author           = {Kalashami, Mahsa and Pedram, Mir and Sadr, Hossein},
  journal          = {Computational intelligence and neuroscience},
  title            = {EEG Feature Extraction and Data Augmentation in Emotion Recognition},
  year             = {2022},
  month            = mar,
  pages            = {1--16},
  volume           = {2022},
  abstract         = {Emotion recognition is a challenging problem in Brain-Computer Interaction (BCI). Electroencephalogram (EEG) gives unique information about brain activities that are created due to emotional stimuli. This is one of the most substantial advantages of brain signals in comparison to facial expression, tone of voice, or speech in emotion recognition tasks. However, the lack of EEG data and high dimensional EEG recordings lead to difficulties in building effective classifiers with high accuracy. In this study, data augmentation and feature extraction techniques are proposed to solve the lack of data problem and high dimensionality of data, respectively. In this study, the proposed method is based on deep generative models and a data augmentation strategy called Conditional Wasserstein GAN (CWGAN), which is applied to the extracted features to regenerate additional EEG features. DEAP dataset is used to evaluate the effectiveness of the proposed method. Finally, a standard support vector machine and a deep neural network with different tunes were implemented to build effective models. Experimental results show that using the additional augmented data enhances the performance of EEG-based emotion recognition models. Furthermore, the mean accuracy of classification after data augmentation is increased 6.5{\%} for valence and 3.0{\%} for arousal, respectively.},
  bdsk-url-1       = {https://doi.org/10.1155/2022/7028517},
  c1               = {Department of Electrical and Computer Engineering, Faculty of Engineering, Kharazmi University, Tehran 15719-14911, Iran; Department of Electrical and Computer Engineering, Faculty of Engineering, Kharazmi University, Tehran 15719-14911, Iran; Department of Computer Engineering, Rahbord Shomal Institute of Higher Education, Rasht, Iran},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1155/2022/7028517},
  hasabstract      = {Y},
  isbn             = {1687-5265},
  keywords         = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Feature Extraction; Affective Computing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Hindawi Publishing Corporation},
  url              = {https://doi.org/10.1155/2022/7028517},
}

@Article{arano2021wh,
  author           = {Ara{\~n}o, Keith and Gloor, Peter and Orsenigo, Carlotta and Vercellis, Carlo},
  journal          = {Cognitive computation},
  title            = {When Old Meets New: Emotion Recognition from Speech Signals},
  year             = {2021},
  month            = apr,
  number           = {3},
  pages            = {771--783},
  volume           = {13},
  abstract         = {Abstract Speech is one of the most natural communication channels for expressing human emotions. Therefore, speech emotion recognition (SER) has been an active area of research with an extensive range of applications that can be found in several domains, such as biomedical diagnostics in healthcare and human--machine interactions. Recent works in SER have been focused on end-to-end deep neural networks (DNNs). However, the scarcity of emotion-labeled speech datasets inhibits the full potential of training a deep network from scratch. In this paper, we propose new approaches for classifying emotions from speech by combining conventional mel-frequency cepstral coefficients (MFCCs) with image features extracted from spectrograms by a pretrained convolutional neural network (CNN). Unlike prior studies that employ end-to-end DNNs, our methods eliminate the resource-intensive network training process. By using the best prediction model obtained, we also build an SER application that predicts emotions in real time. Among the proposed methods, the hybrid feature set fed into a support vector machine (SVM) achieves an accuracy of 0.713 in a 6-class prediction problem evaluated on the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) dataset, which is higher than the previously published results. Interestingly, MFCCs taken as unique input into a long short-term memory (LSTM) network achieve a slightly higher accuracy of 0.735. Our results reveal that the proposed approaches lead to an improvement in prediction accuracy. The empirical findings also demonstrate the effectiveness of using a pretrained CNN as an automatic feature extractor for the task of emotion prediction. Moreover, the success of the MFCC-LSTM model is evidence that, despite being conventional features, MFCCs can still outperform more sophisticated deep-learning feature sets.},
  bdsk-url-1       = {https://doi.org/10.1007/s12559-021-09865-2},
  c1               = {Department of Management, Economics and Industrial Engineering, Politecnico di Milano, Milan, 20156, Italy; Center for Collective Intelligence, Massachusetts Institute of Technology, Boston, 02139, USA; Department of Management, Economics and Industrial Engineering, Politecnico di Milano, Milan, 20156, Italy; Department of Management, Economics and Industrial Engineering, Politecnico di Milano, Milan, 20156, Italy},
  comment          = {no music},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1007/s12559-021-09865-2},
  hasabstract      = {Y},
  isbn             = {1866-9956},
  keywords         = {Emotion Recognition; Speech Emotion; Affective Computing; Audio-Visual Speech Recognition; Environmental Sound Recognition},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Springer Science+Business Media},
  url              = {https://doi.org/10.1007/s12559-021-09865-2},
}

@Article{yuvaraj2023co,
  author           = {Yuvaraj, Rajamanickam and Thangavel, Prasanth and Thomas, John and Fogarty, Jack and Ali, Farhan},
  journal          = {Sensors},
  title            = {Comprehensive Analysis of Feature Extraction Methods for Emotion Recognition from Multichannel EEG Recordings},
  year             = {2023},
  month            = jan,
  number           = {2},
  pages            = {915--915},
  volume           = {23},
  abstract         = {Advances in signal processing and machine learning have expedited electroencephalogram (EEG)-based emotion recognition research, and numerous EEG signal features have been investigated to detect or characterize human emotions. However, most studies in this area have used relatively small monocentric data and focused on a limited range of EEG features, making it difficult to compare the utility of different sets of EEG features for emotion recognition. This study addressed that by comparing the classification accuracy (performance) of a comprehensive range of EEG feature sets for identifying emotional states, in terms of valence and arousal. The classification accuracy of five EEG feature sets were investigated, including statistical features, fractal dimension (FD), Hjorth parameters, higher order spectra (HOS), and those derived using wavelet analysis. Performance was evaluated using two classifier methods, support vector machine (SVM) and classification and regression tree (CART), across five independent and publicly available datasets linking EEG to emotional states: MAHNOB-HCI, DEAP, SEED, AMIGOS, and DREAMER. The FD-CART feature-classification method attained the best mean classification accuracy for valence (85.06{\%}) and arousal (84.55{\%}) across the five datasets. The stability of these findings across the five different datasets also indicate that FD features derived from EEG data are reliable for emotion recognition. The results may lead to the possible development of an online feature extraction framework, thereby enabling the development of an EEG-based emotion recognition system in real time.},
  bdsk-url-1       = {https://doi.org/10.3390/s23020915},
  c1               = {National Institute of Education, Nanyang Technological University, Singapore 637616, Singapore; Interdisciplinary Graduate School, Nanyang Technological University, Singapore 639798, Singapore; Montreal Neurological Institute, McGill University, Montreal, QC H3A 2B4, Canada; National Institute of Education, Nanyang Technological University, Singapore 637616, Singapore; National Institute of Education, Nanyang Technological University, Singapore 637616, Singapore},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/s23020915},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Feature Extraction; Affective Computing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s23020915},
}

@Article{martineztejada2021ex,
  author           = {Mart{\'\i}nez-Tejada, Laura and Puertas-Gonz{\'a}lez, Alex and Yoshimura, Natsue and Koike, Yasuharu},
  journal          = {Brain sciences},
  title            = {Exploring EEG Characteristics to Identify Emotional Reactions under Videogame Scenarios},
  year             = {2021},
  month            = mar,
  number           = {3},
  pages            = {378--378},
  volume           = {11},
  abstract         = {In this article we present the study of electroencephalography (EEG) traits for emotion recognition process using a videogame as a stimuli tool, and considering two different kind of information related to emotions: arousal--valence self-assesses answers from participants, and game events that represented positive and negative emotional experiences under the videogame context. We performed a statistical analysis using Spearman's correlation between the EEG traits and the emotional information. We found that EEG traits had strong correlation with arousal and valence scores; also, common EEG traits with strong correlations, belonged to the theta band of the central channels. Then, we implemented a regression algorithm with feature selection to predict arousal and valence scores using EEG traits. We achieved better result for arousal regression, than for valence regression. EEG traits selected for arousal and valence regression belonged to time domain (standard deviation, complexity, mobility, kurtosis, skewness), and frequency domain (power spectral density---PDS, and differential entropy---DE from theta, alpha, beta, gamma, and all EEG frequency spectrum). Addressing game events, we found that EEG traits related with the theta, alpha and beta band had strong correlations. In addition, distinctive event-related potentials where identified in the presence of both types of game events. Finally, we implemented a classification algorithm to discriminate between positive and negative events using EEG traits to identify emotional information. We obtained good classification performance using only two traits related with frequency domain on the theta band and on the full EEG spectrum.},
  bdsk-url-1       = {https://doi.org/10.3390/brainsci11030378},
  c1               = {FIRST Institute of Innovative Research, Tokyo Institute of Technology, Yokohama, Kanagawa 226-8503, Japan; System Engineering and Computation School, Universidad Pedag{\'o}gica y Tecnol{\'o}gica de Colombia, Santiago de Tunja 150007, Colombia; Department of Advanced Neuroimaging, Integrative Brain Imaging Center, National Center of Neurology and Psychiatry, Kodaira, Tokyo 187-8551, Japan; FIRST Institute of Innovative Research, Tokyo Institute of Technology, Yokohama, Kanagawa 226-8503, Japan; Neural Information Analysis Laboratories, ATR, Kyoto 619-0288, Japan; PRESTO, JST, Kawaguchi, Saitama 332-0012, Japan; Department of Advanced Neuroimaging, Integrative Brain Imaging Center, National Center of Neurology and Psychiatry, Kodaira, Tokyo 187-8551, Japan; FIRST Institute of Innovative Research, Tokyo Institute of Technology, Yokohama, Kanagawa 226-8503, Japan},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/brainsci11030378},
  hasabstract      = {Y},
  isbn             = {2076-3425},
  keywords         = {EEG Analysis; Emotion Recognition; Deep Learning for EEG; Emotion Regulation; Affective Computing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/brainsci11030378},
}

@Article{quispe2022ap,
  author           = {Quispe, Kevin and Utyiama, Daniel and dos Santos, Eulanda and Oliveira, Hor{\'a}cio and Souto, Eduardo},
  journal          = {Sensors},
  title            = {Applying Self-Supervised Representation Learning for Emotion Recognition Using Physiological Signals},
  year             = {2022},
  month            = nov,
  number           = {23},
  pages            = {9102--9102},
  volume           = {22},
  abstract         = {The use of machine learning (ML) techniques in affective computing applications focuses on improving the user experience in emotion recognition. The collection of input data (e.g., physiological signals), together with expert annotations are part of the established standard supervised learning methodology used to train human emotion recognition models. However, these models generally require large amounts of labeled data, which is expensive and impractical in the healthcare context, in which data annotation requires even more expert knowledge. To address this problem, this paper explores the use of the self-supervised learning (SSL) paradigm in the development of emotion recognition methods. This approach makes it possible to learn representations directly from unlabeled signals and subsequently use them to classify affective states. This paper presents the key concepts of emotions and how SSL methods can be applied to recognize affective states. We experimentally analyze and compare self-supervised and fully supervised training of a convolutional neural network designed to recognize emotions. The experimental results using three emotion datasets demonstrate that self-supervised representations can learn widely useful features that improve data efficiency, are widely transferable, are competitive when compared to their fully supervised counterparts, and do not require the data to be labeled for learning.},
  bdsk-url-1       = {https://doi.org/10.3390/s22239102},
  c1               = {Computer Institute, Federal University of Amazonas, Manaus 69080-900, Brazil; Computer Institute, Federal University of Amazonas, Manaus 69080-900, Brazil; Computer Institute, Federal University of Amazonas, Manaus 69080-900, Brazil; Computer Institute, Federal University of Amazonas, Manaus 69080-900, Brazil; Computer Institute, Federal University of Amazonas, Manaus 69080-900, Brazil},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/s22239102},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Emotion Recognition; Affective Computing; Sensory Processing; Speech Emotion; Physiological Signals},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s22239102},
}

@Article{barathi2020af,
  author           = {Barathi, Soumya and Proulx, Michael and O'Neill, Eamonn and Lutteroth, Christof},
  title            = {Affect Recognition using Psychophysiological Correlates in High Intensity VR Exergaming},
  year             = {2020},
  month            = apr,
  abstract         = {User experience estimation of VR exergame players by recognising their affective state could enable us to personalise and optimise their experience. Affect recognition based on psychophysiological measurements has been successful for moderate intensity activities. High intensity VR exergames pose challenges as the effects of exercise and VR headsets interfere with those measurements. We present two experiments that investigate the use of different sensors for affect recognition in a VR exergame. The first experiment compares the impact of physical exertion and gamification on psychophysiological measurements during rest, conventional exercise, VR exergaming, and sedentary VR gaming. The second experiment compares underwhelming, overwhelming and optimal VR exergaming scenarios. We identify gaze fixations, eye blinks, pupil diameter and skin conductivity as psychophysiological measures suitable for affect recognition in VR exergaming and analyse their utility in determining affective valence and arousal. Our findings provide guidelines for researchers of affective VR exergames.},
  bdsk-url-1       = {https://doi.org/10.1145/3313831.3376596},
  c1               = {University of Bath, Bath, United Kingdom; University of Bath, Bath, United Kingdom; University of Bath, Bath, United Kingdom; University of Bath, Bath, United Kingdom},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1145/3313831.3376596},
  hasabstract      = {Y},
  keywords         = {Emotion Recognition; Affective Computing; Attention Lapses},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.1145/3313831.3376596},
}

@Article{pinilla2021af,
  author           = {Pinilla, Andr{\'e}s and Garc{\'\i}a, Jaime and Raffe, William and Voigt-Antons, Jan‐Niklas and Spang, Robert and M{\"o}ller, Sebastian},
  journal          = {Frontiers in virtual reality},
  title            = {Affective Visualization in Virtual Reality: An Integrative Review},
  year             = {2021},
  month            = aug,
  volume           = {2},
  abstract         = {A cluster of research in Affective Computing suggests that it is possible to infer some characteristics of users'affective states by analyzing their electrophysiological activity in real-time. However, it is not clear how to use the information extracted from electrophysiological signals to create visual representations of the affective states of Virtual Reality (VR) users. Visualization of users'affective states in VR can lead to biofeedback therapies for mental health care. Understanding how to visualize affective states in VR requires an interdisciplinary approach that integrates psychology, electrophysiology, and audio-visual design. Therefore, this review aims to integrate previous studies from these fields to understand how to develop virtual environments that can automatically create visual representations of users'affective states. The manuscript addresses this challenge in four sections: First, theories related to emotion and affect are summarized. Second, evidence suggesting that visual and sound cues tend to be associated with affective states are discussed. Third, some of the available methods for assessing affect are described. The fourth and final section contains five practical considerations for the development of virtual reality environments for affect visualization.},
  bdsk-url-1       = {https://doi.org/10.3389/frvir.2021.630731},
  c1               = {Quality and Usability Lab, Institute for Software Technology and Theoretical Computer Science, Faculty of Electrical Engineering and Computer Science, Technische Universit?t Berlin, Germany; UTS Games Studio, Faculty of Engineering and IT, University of Technology Sydney UTS, Australia; UTS Games Studio, Faculty of Engineering and IT, University of Technology Sydney UTS, Australia; UTS Games Studio, Faculty of Engineering and IT, University of Technology Sydney UTS, Australia; German Research Center for Artificial Intelligence (DFKI), Germany; Quality and Usability Lab, Institute for Software Technology and Theoretical Computer Science, Faculty of Electrical Engineering and Computer Science, Technische Universit?t Berlin, Germany; Quality and Usability Lab, Institute for Software Technology and Theoretical Computer Science, Faculty of Electrical Engineering and Computer Science, Technische Universit?t Berlin, Germany; German Research Center for Artificial Intelligence (DFKI), Germany; Quality and Usability Lab, Institute for Software Technology and Theoretical Computer Science, Faculty of Electrical Engineering and Computer Science, Technische Universit?t Berlin, Germany},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3389/frvir.2021.630731},
  hasabstract      = {Y},
  isbn             = {2673-4192},
  keywords         = {Affective Computing; Emotion Recognition; Multisensory Integration; Affective Design},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Frontiers Media},
  url              = {https://doi.org/10.3389/frvir.2021.630731},
}

@Article{shumailov2017co,
  author           = {Shumailov, Ilia and G{\"u}ne{\c s}, Hatice},
  title            = {Computational analysis of valence and arousal in virtual reality gaming using lower arm electromyograms},
  year             = {2017},
  month            = oct,
  abstract         = {Progress in the affective computing field has led to the creation of affect-aware games that aim to adapt to the emotions experienced by the players. In this paper we focus on affect recognition in virtual reality (VR) gaming, a problem that to the best of our knowledge has not yet been sufficiently explored. We aim to answer two research questions: (i) Is it possible to reliably capture and recognize the affective state of a person based on EMG sensors placed on their lower arms, while they interact with the virtual environment? and (ii) Is EMG signal from one arm sufficient for detecting affect? We conducted a study in which 8 people were playing a set of VR games with two EMG sensors placed on their arms. We analysed the EMG signals and extracted a number of features to infer the affective states of the players. Our experimental results show that the EMG measures from left and right arms provide sufficient information to detect emotions experienced by a player of a VR game. Our results also show that classifying a DWT-dbl signal with Support Vector Machine (SVM) yields F1=0.91 for predicting low/high arousal and F1=0.85 for predicting positive/negative valence when using just the left-arm EMG signal. To the best of our knowledge, this is the first work that uses EMG data from arm movements as a single source of affective information and addresses affect recognition in VR gaming.},
  bdsk-url-1       = {https://doi.org/10.1109/acii.2017.8273595},
  c1               = {Department of Computer Science \& Technology, University of Cambridge; Department of Computer Science \& Technology, University of Cambridge},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1109/acii.2017.8273595},
  hasabstract      = {Y},
  keywords         = {Affective Computing; Emotion Recognition; Sensory Feedback; EEG Analysis},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.1109/acii.2017.8273595},
}

@Article{petrescu2021ma,
  author           = {Petrescu, Livia and Petrescu, C{\u a}t{\u a}lin and Oprea, Ana and Mitruț, Oana and Moise, Gabriela and Moldoveanu, Alin and Moldoveanu, Florica},
  journal          = {Sensors},
  title            = {Machine Learning Methods for Fear Classification Based on Physiological Features},
  year             = {2021},
  month            = jul,
  number           = {13},
  pages            = {4519--4519},
  volume           = {21},
  abstract         = {This paper focuses on the binary classification of the emotion of fear, based on the physiological data and subjective responses stored in the DEAP dataset. We performed a mapping between the discrete and dimensional emotional information considering the participants'ratings and extracted a substantial set of 40 types of features from the physiological data, which represented the input to various machine learning algorithms---Decision Trees, k-Nearest Neighbors, Support Vector Machine and artificial networks---accompanied by dimensionality reduction, feature selection and the tuning of the most relevant hyperparameters, boosting classification accuracy. The methodology we approached included tackling different situations, such as resolving the problem of having an imbalanced dataset through data augmentation, reducing overfitting, computing various metrics in order to obtain the most reliable classification scores and applying the Local Interpretable Model-Agnostic Explanations method for interpretation and for explaining predictions in a human-understandable manner. The results show that fear can be predicted very well (accuracies ranging from 91.7{\%} using Gradient Boosting Trees to 93.5{\%} using dimensionality reduction and Support Vector Machine) by extracting the most relevant features from the physiological data and by searching for the best parameters which maximize the machine learning algorithms'classification scores.},
  bdsk-url-1       = {https://doi.org/10.3390/s21134519},
  c1               = {Faculty of Biology, University of Bucharest, 050095 Bucharest, Romania; Faculty of Automatic Control and Computers, University Politehnica of Bucharest, 060042 Bucharest, Romania; Faculty of Automatic Control and Computers, University Politehnica of Bucharest, 060042 Bucharest, Romania; Faculty of Automatic Control and Computers, University Politehnica of Bucharest, 060042 Bucharest, Romania; Faculty of Letters and Sciences, Petroleum-Gas University of Ploiesti, 100680 Ploiesti, Romania; Faculty of Automatic Control and Computers, University Politehnica of Bucharest, 060042 Bucharest, Romania; Faculty of Automatic Control and Computers, University Politehnica of Bucharest, 060042 Bucharest, Romania},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/s21134519},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Affective Computing; Emotion Recognition; Feature Extraction; Emotion Regulation},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s21134519},
}

@Article{kumano2015an,
  author           = {Kumano, Shiro and Otsuka, Kazuhiro and Mikami, Dan and Matsuda, Masafumi and Yamato, Junji},
  journal          = {IEEE transactions on affective computing},
  title            = {Analyzing Interpersonal Empathy via Collective Impressions},
  year             = {2015},
  month            = oct,
  number           = {4},
  pages            = {324--336},
  volume           = {6},
  abstract         = {This paper presents a research framework for understanding the empathy that arises between people while they are conversing. By focusing on the process by which empathy is perceived by other people, this paper aims to develop a computational model that automatically infers perceived empathy from participant behavior. To describe such perceived empathy objectively, we introduce the idea of using the collective impressions of external observers. In particular, we focus on the fact that the perception of other's empathy varies from person to person, and take the standpoint that this individual difference itself is an essential attribute of human communication for building, for example, successful human relationships and consensus. This paper describes a probabilistic model of the process that we built based on the Bayesian network, and that relates the empathy perceived by observers to how the gaze and facial expressions of participants co-occur between a pair. In this model, the probability distribution represents the diversity of observers' impression, which reflects the individual differences in the schema when perceiving others' empathy from their behaviors, and the ambiguity of the behaviors. Comprehensive experiments demonstrate that the inferred distributions are similar to those made by observers.},
  bdsk-url-1       = {https://doi.org/10.1109/taffc.2015.2417561},
  c1               = {{$[$}Nippon Telegraph and Telephone Corporation, Kanagawa, Japan{$]$}; {$[$}Nippon Telegraph and Telephone Corporation, Kanagawa, Japan{$]$}; {$[$}Nippon Telegraph and Telephone Corporation, Kanagawa, Japan{$]$}; {$[$}Nippon Telegraph and Telephone Corporation, Kanagawa, Japan{$]$}; {$[$}Nippon Telegraph and Telephone Corporation, Kanagawa, Japan{$]$}},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1109/taffc.2015.2417561},
  hasabstract      = {Y},
  isbn             = {1949-3045},
  keywords         = {Affective Computing; Emotion Perception; Intention Understanding; Emotion Recognition; Interpersonal Synchrony},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Institute of Electrical and Electronics Engineers},
  url              = {https://doi.org/10.1109/taffc.2015.2417561},
}

@Article{chaudhari2023fa,
  author           = {Chaudhari, Aayushi and Bhatt, Chintan and Krishna, Achyut and Gonz{\'a}lez, Carmelo},
  journal          = {Electronics},
  title            = {Facial Emotion Recognition with Inter-Modality-Attention-Transformer-Based Self-Supervised Learning},
  year             = {2023},
  month            = jan,
  number           = {2},
  pages            = {288--288},
  volume           = {12},
  abstract         = {Emotion recognition is a very challenging research field due to its complexity, as individual differences in cognitive--emotional cues involve a wide variety of ways, including language, expressions, and speech. If we use video as the input, we can acquire a plethora of data for analyzing human emotions. In this research, we use features derived from separately pretrained self-supervised learning models to combine text, audio (speech), and visual data modalities. The fusion of features and representation is the biggest challenge in multimodal emotion classification research. Because of the large dimensionality of self-supervised learning characteristics, we present a unique transformer and attention-based fusion method for incorporating multimodal self-supervised learning features that achieved an accuracy of 86.40{\%} for multimodal emotion classification.},
  bdsk-url-1       = {https://doi.org/10.3390/electronics12020288},
  c1               = {CHARUSAT Campus, Charotar University of Science and Technology, Changa 388421, India; U \& P U. Patel Department of Computer Engineering, Chandubhai S Patel Institute of Technology (CSPIT),; Department of Computer Science and Engineering, School of Technology, Pandit Deendayal Energy University, Gandhinagar 382007, India; CHARUSAT Campus, Charotar University of Science and Technology, Changa 388421, India; U \& P U. Patel Department of Computer Engineering, Chandubhai S Patel Institute of Technology (CSPIT),; Signals and Communications Department, IDeTIC, University of Las Palmas de Gran Canaria, 35001 Las Palmas, Spain},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/electronics12020288},
  hasabstract      = {Y},
  isbn             = {2079-9292},
  keywords         = {Emotion Recognition; Audio-Visual Speech Recognition; Affective Computing; Speech Emotion; Multimodal Data},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/electronics12020288},
}

@Article{wosiak2020hy,
  author           = {Wosiak, Agnieszka and Dura, Aleksandra},
  journal          = {Sensors},
  title            = {Hybrid Method of Automated EEG Signals'Selection Using Reversed Correlation Algorithm for Improved Classification of Emotions},
  year             = {2020},
  month            = dec,
  number           = {24},
  pages            = {7083--7083},
  volume           = {20},
  abstract         = {Based on the growing interest in encephalography to enhance human--computer interaction (HCI) and develop brain--computer interfaces (BCIs) for control and monitoring applications, efficient information retrieval from EEG sensors is of great importance. It is difficult due to noise from the internal and external artifacts and physiological interferences. The enhancement of the EEG-based emotion recognition processes can be achieved by selecting features that should be taken into account in further analysis. Therefore, the automatic feature selection of EEG signals is an important research area. We propose a multistep hybrid approach incorporating the Reversed Correlation Algorithm for automated frequency band---electrode combinations selection. Our method is simple to use and significantly reduces the number of sensors to only three channels. The proposed method has been verified by experiments performed on the DEAP dataset. The obtained effects have been evaluated regarding the accuracy of two emotions---valence and arousal. In comparison to other research studies, our method achieved classification results that were 4.20--8.44{\%} greater. Moreover, it can be perceived as a universal EEG signal classification technique, as it belongs to unsupervised methods.},
  bdsk-url-1       = {https://doi.org/10.3390/s20247083},
  c1               = {Institute of Information Technology, Lodz University of Technology, W{\'o}lcza {\'n}ska 215, 90-924 {\L}{\'o}d{\'z}, Poland;; Institute of Information Technology, Lodz University of Technology, W{\'o}lcza {\'n}ska 215, 90-924 {\L}{\'o}d{\'z}, Poland;},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/s20247083},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {EEG Analysis; Emotion Recognition; Deep Learning for EEG; Affective Computing; Sensory Processing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s20247083},
}

@Article{zhang2023we,
  author           = {Zhang, Tianyi and Ali, Abdallah and Wang, Chen and Hanjalic, Alan and C{\'e}sar, Pablo},
  journal          = {IEEE transactions on affective computing},
  title            = {Weakly-Supervised Learning for Fine-Grained Emotion Recognition Using Physiological Signals},
  year             = {2023},
  month            = jul,
  number           = {3},
  pages            = {2304--2322},
  volume           = {14},
  abstract         = {Instead of predicting just one emotion for one activity (e.g., video watching), fine-grained emotion recognition enables more temporally precise recognition. Previous works on fine-grained emotion recognition require segment-by-segment, fine-grained emotion labels to train the recognition algorithm. However, experiments to collect these labels are costly and time-consuming compared with only collecting one emotion label after the user watched that stimulus (i.e., the post-stimuli emotion labels). To recognize emotions at a finer granularity level when trained with only post-stimuli labels, we propose an emotion recognition algorithm based on Deep Multiple Instance Learning ( <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" > EDMIL</i > ) using physiological signals. <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" > EDMIL</i > recognizes fine-grained valence and arousal (V-A) labels by identifying which instances represent the post-stimuli V-A annotated by users after watching the videos. Instead of fully-supervised training, the instances are weakly-supervised by the post-stimuli labels in the training stage. The V-A of instances are estimated by the instance gains, which indicate the probability of instances to predict the post-stimuli labels. We tested <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" > EDMIL</i > on three different datasets, <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" > CASE</i > , <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" > MERCA</i > and <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" > CEAP-360VR</i > , collected in three different environments: desktop, mobile and HMD-based Virtual Reality, respectively. Recognition results validated with the fine-grained V-A self-reports show that for subject-independent 3-class classification (high/neutral/low), <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" > EDMIL</i > obtains promising recognition accuracies: 75.63{\%} and 79.73{\%} for V-A on <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" > CASE</i > , 70.51{\%} and 67.62{\%} for V-A on <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" > MERCA</i > and 65.04{\%} and 67.05{\%} for V-A on <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" > CEAP-360VR</i > . Our ablation study shows that all components of <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" > EDMIL</i > contribute to both the classification and regression tasks. Our experiments also show that (1) compared with fully-supervised learning, weakly-supervised learning can reduce the problem of overfitting caused by the temporal mismatch between fine-grained annotations and physiological signals, (2) instance segment lengths between 1-2 s result in the highest recognition accuracies and (3) <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" > EDMIL</i > performs best if post-stimuli annotations consist of less than 30{\%} or more than 60{\%} of the entire video watching.},
  bdsk-url-1       = {https://doi.org/10.1109/taffc.2022.3158234},
  c1               = {Distributed and Interactive Systems, Centrum Wiskunde \&{\#}x0026; Informatica (CWI), Amsterdam, The Netherlands; Multimedia Computing Group, Delft University of Technology, Delft, The Netherlands; Distributed and Interactive Systems, Centrum Wiskunde \&{\#}x0026; Informatica (CWI), Amsterdam, The Netherlands; State Key Laboratory of Media Convergence Production Technology and Systems, Xinhua News Agency \&{\#}x0026; Future Media and Convergence Institute, Beijing, China; Multimedia Computing Group, Delft University of Technology, Delft, The Netherlands; Distributed and Interactive Systems, Centrum Wiskunde \&{\#}x0026; Informatica (CWI), Amsterdam, The Netherlands; Multimedia Computing Group, Delft University of Technology, Delft, The Netherlands},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1109/taffc.2022.3158234},
  hasabstract      = {Y},
  isbn             = {1949-3045},
  keywords         = {Emotion Recognition; Affective Computing; Deep Learning for EEG; Deep Learning; Physiological Signals},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Institute of Electrical and Electronics Engineers},
  url              = {https://doi.org/10.1109/taffc.2022.3158234},
}

@Article{vujic2020go,
  author           = {Vujic, Angela and Tong, Stephanie and Picard, Rosalind and Maes, Pattie},
  title            = {Going with our Guts},
  year             = {2020},
  month            = oct,
  abstract         = {A hard challenge for wearable systems is to measure differences in emotional valence, i.e. positive and negative affect via physiology. However, the stomach or gastric signal is an unexplored modality that could offer new affective information. We created a wearable device and software to record gastric signals, known as electrogastrography (EGG). An in-laboratory study was conducted to compare EGG with electrodermal activity (EDA) in 33 individuals viewing affective stimuli. We found that negative stimuli attenuate EGG's indicators of parasympathetic activation, or "rest and digest" activity. We compare EGG to the remaining physiological signals and describe implications for affect detection. Further, we introduce how wearable EGG may support future applications in areas as diverse as reducing nausea in virtual reality and helping treat emotion-related eating disorders.},
  bdsk-url-1       = {https://doi.org/10.1145/3382507.3418882},
  c1               = {Massachusetts Institute of Technology, Cambridge, MA, USA; Harvard University, Cambridge, MA, USA; Massachusetts Institute of Technology, Cambridge, MA, USA; Massachusetts Institute of Technology, Cambridge, MA, USA},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1145/3382507.3418882},
  hasabstract      = {Y},
  keywords         = {Sensory Expectations; Emotion Recognition; Affective Computing; Physiological Signals; Multisensory Integration},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.1145/3382507.3418882},
}

@Article{glowinski2015to,
  author           = {Glowinski, Donald and Mortillaro, Marcello and Scherer, Klaus and Dael, Nele and Camurri, Antonio},
  title            = {Towards a minimal representation of affective gestures (Extended abstract)},
  year             = {2015},
  month            = sep,
  abstract         = {How efficiently decoding affective information when computational resources and sensor systems are limited?This paper presents a framework for analysis of affective behavior starting with a reduced amount of visual information related to human upper-body movements.The main goal is to individuate a minimal representation of emotional displays based on non-verbal gesture features.The GEMEP (Geneva multimodal emotion portrayals) corpus was used to validate this framework.Twelve emotions expressed by ten actors form the selected data set of emotion portrayals.Visual tracking of trajectories of head and hands was performed from a frontal and a lateral view.Postural/shape and dynamic expressive gesture features were identified and analyzed.A feature reduction procedure was carried out, resulting in a four-dimensional model of emotion expression, that effectively classified/grouped emotions according to their valence (positive, negative) and arousal (high, low).These results show that emotionally relevant information can be detected/measured/obtained from the dynamic qualities of gesture.The framework was implemented as software modules (plug-ins) extending the EyesWeb XMI Expressive Gesture Processing Library and was tested as a component for a multimodal search engine in collaboration with Google within the EU-ICT I-SEARCH project.},
  bdsk-url-1       = {https://doi.org/10.1109/acii.2015.7344616},
  c1               = {Swiss Center for Affective Sciences University of Geneva Switzerland CH-1205; Swiss Center for Affective Sciences University of Geneva Switzerland CH-1205; Swiss Center for Affective Sciences University of Geneva Switzerland CH-1205; Institute of Psychology University of Lausanne Lausanne, CH 1015; Casa Paganini -InfoMus (DIBRIS) University of Genoa Genoa, IT-16145},
  comment          = {no music},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1109/acii.2015.7344616},
  hasabstract      = {Y},
  keywords         = {Affective Computing; Emotion Recognition; Gesture; Facial Expression; Feature Extraction},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.1109/acii.2015.7344616},
}

@Article{cunningham2019au,
  author        = {Cunningham, Stuart and Ridley, Harrison and Weinel, Jonathan and Picking, Richard},
  title         = {Audio Emotion Recognition using Machine Learning to support Sound Design},
  year          = {2019},
  month         = sep,
  abstract      = {In recent years, the field of Music Emotion Recognition has become established. Less attention has been directed towards the counterpart domain of Audio Emotion Recognition, which focuses upon detection of emotional stimuli resulting from non-musical sound. By better understanding how sounds provoke emotional responses in an audience it may be possible to enhance the work of sound designers.},
  bdsk-url-1    = {https://doi.org/10.1145/3356590.3356609},
  c1            = {Manchester Metropolitan University, Manchester, UK; Manchester Metropolitan University, Manchester, UK; Coventry University, Coventry, UK; Wrexham Glynd{\^w}r University, Wrexham, UK},
  comment       = {no music},
  date-added    = {2024-05-13 15:05:26 +0100},
  date-modified = {2024-05-13 15:05:26 +0100},
  doi           = {10.1145/3356590.3356609},
  hasabstract   = {Y},
  keywords      = {Emotion Recognition; Environmental Sound Recognition; Audio Event Detection; Speech Emotion; Affective Computing},
  la            = {en},
  priority      = {prio3},
  url           = {https://doi.org/10.1145/3356590.3356609},
}

@Article{wang2023de,
  author           = {Wang, Xiaohu and Ren, Yongmei and Luo, Ze and He, Wei and Hong, Jun and Huang, Yue},
  journal          = {Frontiers in psychology},
  title            = {Deep learning-based EEG emotion recognition: Current trends and future perspectives},
  year             = {2023},
  month            = feb,
  volume           = {14},
  abstract         = {Automatic electroencephalogram (EEG) emotion recognition is a challenging component of human-computer interaction (HCI). Inspired by the powerful feature learning ability of recently-emerged deep learning techniques, various advanced deep learning models have been employed increasingly to learn high-level feature representations for EEG emotion recognition. This paper aims to provide an up-to-date and comprehensive survey of EEG emotion recognition, especially for various deep learning techniques in this area. We provide the preliminaries and basic knowledge in the literature. We review EEG emotion recognition benchmark data sets briefly. We review deep learning techniques in details, including deep belief networks, convolutional neural networks, and recurrent neural networks. We describe the state-of-the-art applications of deep learning techniques for EEG emotion recognition in detail. We analyze the challenges and opportunities in this field and point out its future directions.},
  bdsk-url-1       = {https://doi.org/10.3389/fpsyg.2023.1126994},
  c1               = {School of Intelligent Manufacturing and Mechanical Engineering, Hunan Institute of Technology, Hengyang, China; School of Electrical and Information Engineering, Hunan Institute of Technology, Hengyang, China; School of Intelligent Manufacturing and Mechanical Engineering, Hunan Institute of Technology, Hengyang, China; School of Electrical and Information Engineering, Hunan Institute of Technology, Hengyang, China; School of Intelligent Manufacturing and Mechanical Engineering, Hunan Institute of Technology, Hengyang, China; School of Computer and Information Engineering, Hunan Institute of Technology, Hengyang, China},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3389/fpsyg.2023.1126994},
  hasabstract      = {Y},
  isbn             = {1664-1078},
  keywords         = {Deep Learning for EEG; Emotion Recognition; EEG Analysis; Affective Computing; Head Gesture Recognition},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Frontiers Media},
  url              = {https://doi.org/10.3389/fpsyg.2023.1126994},
}

@Article{kim2022ac,
  author           = {Kim, Sungkyu and Kim, Tae‐Seong and Lee, Won},
  journal          = {Sensors},
  title            = {Accelerating 3D Convolutional Neural Network with Channel Bottleneck Module for EEG-Based Emotion Recognition},
  year             = {2022},
  month            = sep,
  number           = {18},
  pages            = {6813--6813},
  volume           = {22},
  abstract         = {Deep learning-based emotion recognition using EEG has received increasing attention in recent years. The existing studies on emotion recognition show great variability in their employed methods including the choice of deep learning approaches and the type of input features. Although deep learning models for EEG-based emotion recognition can deliver superior accuracy, it comes at the cost of high computational complexity. Here, we propose a novel 3D convolutional neural network with a channel bottleneck module (CNN-BN) model for EEG-based emotion recognition, with the aim of accelerating the CNN computation without a significant loss in classification accuracy. To this end, we constructed a 3D spatiotemporal representation of EEG signals as the input of our proposed model. Our CNN-BN model extracts spatiotemporal EEG features, which effectively utilize the spatial and temporal information in EEG. We evaluated the performance of the CNN-BN model in the valence and arousal classification tasks. Our proposed CNN-BN model achieved an average accuracy of 99.1{\%} and 99.5{\%} for valence and arousal, respectively, on the DEAP dataset, while significantly reducing the number of parameters by 93.08{\%} and FLOPs by 94.94{\%}. The CNN-BN model with fewer parameters based on 3D EEG spatiotemporal representation outperforms the state-of-the-art models. Our proposed CNN-BN model with a better parameter efficiency has excellent potential for accelerating CNN-based emotion recognition without losing classification performance.},
  bdsk-url-1       = {https://doi.org/10.3390/s22186813},
  c1               = {Department of Software Convergence, Kyung Hee University, Yongin 17104, Korea; Department of Biomedical Engineering, Kyung Hee University, Yongin 17104, Korea; Department of Software Convergence, Kyung Hee University, Yongin 17104, Korea},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/s22186813},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Emotion Recognition; Deep Learning for EEG; Affective Computing; EEG Analysis; Deep Learning},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s22186813},
}

@Article{aung2022en,
  author           = {Aung, Si and Hassan, Mehedi and Brady, Mark and Mannan, Zubaer and Azam, Sami and Karim, Asif and Zaman, Sadika and Wongsawat, Yodchanan},
  journal          = {Computational intelligence and neuroscience},
  title            = {Entropy-Based Emotion Recognition from Multichannel EEG Signals Using Artificial Neural Network},
  year             = {2022},
  month            = oct,
  pages            = {1--13},
  volume           = {2022},
  abstract         = {Humans experience a variety of emotions throughout the course of their daily lives, including happiness, sadness, and rage. As a result, an effective emotion identification system is essential for electroencephalography (EEG) data to accurately reflect emotion in real-time. Although recent studies on this problem can provide acceptable performance measures, it is still not adequate for the implementation of a complete emotion recognition system. In this research work, we propose a new approach for an emotion recognition system, using multichannel EEG calculation with our developed entropy known as multivariate multiscale modified-distribution entropy (MM-mDistEn) which is combined with a model based on an artificial neural network (ANN) to attain a better outcome over existing methods. The proposed system has been tested with two different datasets and achieved better accuracy than existing methods. For the GAMEEMO dataset, we achieved an average accuracy $\pm$standard deviation of 95.73{\%} $\pm$0.67 for valence and 96.78{\%} $\pm$0.25 for arousal. Moreover, the average accuracy percentage for the DEAP dataset reached 92.57{\%} $\pm$1.51 in valence and 80.23{\%} $\pm$1.83 in arousal.},
  bdsk-url-1       = {https://doi.org/10.1155/2022/6000989},
  c1               = {Department of Biomedical Engineering, Faculty of Engineering, Mahidol University, Salaya, Thailand; Computer Science and Engineering, North Western University, Khulna, Bangladesh; Asia Pacific College of Business and Law, Charles Darwin University, Casuarina, NT, Australia; Department of Smart Computing, Kyungdong University, Global Campus, Goseong-Gun, Republic of Korea; College of Engineering IT and Environment, Charles Darwin University, Casuarina, NT, Australia; College of Engineering IT and Environment, Charles Darwin University, Casuarina, NT, Australia; Computer Science and Engineering, North Western University, Khulna, Bangladesh; Department of Biomedical Engineering, Faculty of Engineering, Mahidol University, Salaya, Thailand},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1155/2022/6000989},
  hasabstract      = {Y},
  isbn             = {1687-5265},
  keywords         = {Emotion Recognition; Affective Computing; Deep Learning for EEG; EEG Analysis; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Hindawi Publishing Corporation},
  url              = {https://doi.org/10.1155/2022/6000989},
}

@Article{laracabrera2019at,
  author        = {Lara-Cabrera, Ra{\'u}l and Camacho, David},
  journal       = {Future generation computer systems},
  title         = {A taxonomy and state of the art revision on affective games},
  year          = {2019},
  month         = mar,
  pages         = {516--525},
  volume        = {92},
  abstract      = {Affective games are a sub-field of affective computing that tries to study how to design videogames that are able to react to the emotions expressed by the player, as well as provoking desired emotions to them. To achieve those goals it is necessary to research on how to measure and detect human emotions using a computer, and how to adapt videogames to the perceived emotions to finally provoke them to the players. This work presents a taxonomy for research on affective games centring on the aforementioned issues. Here we devise as well a revision of the most relevant published works known to the authors on this area. Finally, we analyse and discuss which important research problem are yet open and might be tackled by future investigations in the area of affective games.},
  bdsk-url-1    = {https://doi.org/10.1016/j.future.2017.12.056},
  c1            = {Computer Science Department, Universidad Aut{\'o}noma de Madrid, Spain; Computer Science Department, Universidad Aut{\'o}noma de Madrid, Spain},
  comment       = {no music},
  date-added    = {2024-05-13 15:05:26 +0100},
  date-modified = {2024-05-13 15:05:26 +0100},
  doi           = {10.1016/j.future.2017.12.056},
  hasabstract   = {Y},
  isbn          = {0167-739X},
  keywords      = {Affective Computing; Emotion Recognition; Emotion Perception},
  la            = {en},
  priority      = {prio3},
  publisher     = {Elsevier BV},
  url           = {https://doi.org/10.1016/j.future.2017.12.056},
}

@Article{wampfler2022af,
  author           = {Wampfler, Rafael and Klingler, Severin and Solenthaler, Barbara and Schinazi, Victor and Gro{\ss}, Markus and Holz, Christian},
  journal          = {CHI Conference on Human Factors in Computing Systems},
  title            = {Affective State Prediction from Smartphone Touch and Sensor Data in the Wild},
  year             = {2022},
  month            = apr,
  abstract         = {Knowledge of users'affective states can improve their interaction with smartphones by providing more personalized experiences (e.g., search results and news articles). We present an affective state classification model based on data gathered on smartphones in real-world environments. From touch events during keystrokes and the signals from the inertial sensors, we extracted two-dimensional heat maps as input into a convolutional neural network to predict the affective states of smartphone users. For evaluation, we conducted a data collection in the wild with 82 participants over 10 weeks. Our model accurately predicts three levels (low, medium, high) of valence (AUC up to 0.83), arousal (AUC up to 0.85), and dominance (AUC up to 0.84). We also show that using the inertial sensor data alone, our model achieves a similar performance (AUC up to 0.83), making our approach less privacy-invasive. By personalizing our model to the user, we show that performance increases by an additional 0.07 AUC.},
  bdsk-url-1       = {https://doi.org/10.1145/3491102.3501835},
  c1               = {Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland; Department of Psychology, Bond University, Australia and Future Health Technologies, Singapore-ETH Centre, Singapore; Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1145/3491102.3501835},
  hasabstract      = {Y},
  keywords         = {Affective Computing; Emotion Recognition; Emotional Design},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.1145/3491102.3501835},
}

@Article{chen2022el,
  author           = {Chen, Jingxia and Min, Chongdan and Wang, Changhao and Tang, Zhezhe and Liu, Yang and Hu, Xiuwen},
  journal          = {Frontiers in neuroscience},
  title            = {Electroencephalograph-Based Emotion Recognition Using Brain Connectivity Feature and Domain Adaptive Residual Convolution Model},
  year             = {2022},
  month            = jun,
  volume           = {16},
  abstract         = {In electroencephalograph (EEG) emotion recognition research, obtaining high-level emotional features with more discriminative information has become the key to improving the classification performance. This study proposes a new end-to-end emotion recognition method based on brain connectivity (BC) features and domain adaptive residual convolutional network (short for BC-DA-RCNN), which could effectively extract the spatial connectivity information related to the emotional state of the human brain and introduce domain adaptation to achieve accurate emotion recognition within and across the subject's EEG signals. The BC information is represented by the global brain network connectivity matrix. The DA-RCNN is used to extract high-level emotional features between different dimensions of EEG signals, reduce the domain offset between different subjects, and strengthen the common features between different subjects. The experimental results on the large public DEAP data set show that the accuracy of the subject-dependent and subject-independent binary emotion classification in valence reaches 95.15 and 88.28{\%}, respectively, which outperforms all the benchmark methods. The proposed method is proven to have lower complexity, better generalization ability, and domain robustness that help to lay a solid foundation for the development of high-performance affective brain-computer interface applications.},
  bdsk-url-1       = {https://doi.org/10.3389/fnins.2022.878146},
  c1               = {School of Electronic Information and Artificial Intelligence, Shaanxi University of Science and Technology, China; School of Electronic Information and Artificial Intelligence, Shaanxi University of Science and Technology, China; School of Electronic Information and Artificial Intelligence, Shaanxi University of Science and Technology, China; School of Electronic Information and Artificial Intelligence, Shaanxi University of Science and Technology, China; School of Electronic Information and Artificial Intelligence, Shaanxi University of Science and Technology, China; School of Electronic Information and Artificial Intelligence, Shaanxi University of Science and Technology, China},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3389/fnins.2022.878146},
  hasabstract      = {Y},
  isbn             = {1662-453X},
  keywords         = {Emotion Recognition; Affective Computing; Deep Learning for EEG; EEG Analysis; Epilepsy Detection},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Frontiers Media},
  url              = {https://doi.org/10.3389/fnins.2022.878146},
}

@Article{moontaha2023on,
  author           = {Moontaha, Sidratul and Schumann, Franziska and Arnrich, Bert},
  journal          = {Sensors},
  title            = {Online Learning for Wearable EEG-Based Emotion Classification},
  year             = {2023},
  month            = feb,
  number           = {5},
  pages            = {2387--2387},
  volume           = {23},
  abstract         = {Giving emotional intelligence to machines can facilitate the early detection and prediction of mental diseases and symptoms. Electroencephalography (EEG)-based emotion recognition is widely applied because it measures electrical correlates directly from the brain rather than indirect measurement of other physiological responses initiated by the brain. Therefore, we used non-invasive and portable EEG sensors to develop a real-time emotion classification pipeline. The pipeline trains different binary classifiers for Valence and Arousal dimensions from an incoming EEG data stream achieving a 23.9{\%} (Arousal) and 25.8{\%} (Valence) higher F1-Score on the state-of-art AMIGOS dataset than previous work. Afterward, the pipeline was applied to the curated dataset from 15 participants using two consumer-grade EEG devices while watching 16 short emotional videos in a controlled environment. Mean F1-Scores of 87{\%} (Arousal) and 82{\%} (Valence) were achieved for an immediate label setting. Additionally, the pipeline proved to be fast enough to achieve predictions in real-time in a live scenario with delayed labels while continuously being updated. The significant discrepancy from the readily available labels on the classification scores leads to future work to include more data. Thereafter, the pipeline is ready to be used for real-time applications of emotion classification.},
  bdsk-url-1       = {https://doi.org/10.3390/s23052387},
  c1               = {Digital Health---Connected Healthcare, Hasso Plattner Institute, University of Potsdam, 14482 Potsdam, Germany; Digital Health---Connected Healthcare, Hasso Plattner Institute, University of Potsdam, 14482 Potsdam, Germany; Digital Health---Connected Healthcare, Hasso Plattner Institute, University of Potsdam, 14482 Potsdam, Germany},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/s23052387},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Epilepsy Detection},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s23052387},
}

@Article{elalamy2021mu,
  author           = {Elalamy, Rayan and Fanourakis, Marios and Chanel, Guillaume},
  title            = {Multi-modal emotion recognition using recurrence plots and transfer learning on physiological signals},
  year             = {2021},
  month            = sep,
  abstract         = {In this paper we propose to use Recurrence Plots (RP) to generate 2D representations of physiological activity which should be less subject dependent and better suited for non-stationary signals such as EDA. The performance of spectrograms and RPs are compared on two publicly available datasets: AMIGOS and DEAP. Transfer learning is employed by using a pre-trained ResNet-50 model to recognize emotional states (high vs low arousal and high vs low valence) from the two types of representations. Results show that RPs reach a similar performance to spectrograms on periodic signals such as ECG and plethysmography (F1 of 0.76 for valence and 0.74 for arousal on the AMIGOS dataset) while they outperform spectrograms on EDA (F1 of 0.74 for valence and 0.75 for arousal). By combining the two sources of information we were able to reach a F1 of 0.76 for valence and 0.75 for arousal.},
  bdsk-url-1       = {https://doi.org/10.1109/acii52823.2021.9597442},
  c1               = {University of Geneva, Geneva, Switzerland; University of Geneva, Geneva, Switzerland; University of Geneva, Geneva, Switzerland},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1109/acii52823.2021.9597442},
  hasabstract      = {Y},
  keywords         = {Emotion Recognition; Affective Computing; Deep Learning for EEG; Signal Processing; Neural Ensemble Physiology},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.1109/acii52823.2021.9597442},
}

@Article{saffaryazdi2022us,
  author           = {Saffaryazdi, Nastaran and Wasim, Syed and Dileep, Kuldeep and Nia, Alireza and Nanayakkara, Suranga and Broadbent, Elizabeth and Billinghurst, Mark},
  journal          = {Frontiers in psychology},
  title            = {Using Facial Micro-Expressions in Combination With EEG and Physiological Signals for Emotion Recognition},
  year             = {2022},
  month            = jun,
  volume           = {13},
  abstract         = {Emotions are multimodal processes that play a crucial role in our everyday lives. Recognizing emotions is becoming more critical in a wide range of application domains such as healthcare, education, human-computer interaction, Virtual Reality, intelligent agents, entertainment, and more. Facial macro-expressions or intense facial expressions are the most common modalities in recognizing emotional states. However, since facial expressions can be voluntarily controlled, they may not accurately represent emotional states. Earlier studies have shown that facial micro-expressions are more reliable than facial macro-expressions for revealing emotions. They are subtle, involuntary movements responding to external stimuli that cannot be controlled. This paper proposes using facial micro-expressions combined with brain and physiological signals to more reliably detect underlying emotions. We describe our models for measuring arousal and valence levels from a combination of facial micro-expressions, Electroencephalography (EEG) signals, galvanic skin responses (GSR), and Photoplethysmography (PPG) signals. We then evaluate our model using the DEAP dataset and our own dataset based on a subject-independent approach. Lastly, we discuss our results, the limitations of our work, and how these limitations could be overcome. We also discuss future directions for using facial micro-expressions and physiological signals in emotion recognition.},
  bdsk-url-1       = {https://doi.org/10.3389/fpsyg.2022.864047},
  c1               = {Empathic Computing Laboratory, Auckland Bioengineering Institute, The University of Auckland, Auckland, New Zealand; Empathic Computing Laboratory, Auckland Bioengineering Institute, The University of Auckland, Auckland, New Zealand; Empathic Computing Laboratory, Auckland Bioengineering Institute, The University of Auckland, Auckland, New Zealand; Empathic Computing Laboratory, Auckland Bioengineering Institute, The University of Auckland, Auckland, New Zealand; Augmented Human Laboratory, Auckland Bioengineering Institute, The University of Auckland, Auckland, New Zealand; Department of Psychological Medicine, The University of Auckland, Auckland, New Zealand; Empathic Computing Laboratory, Auckland Bioengineering Institute, The University of Auckland, Auckland, New Zealand},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3389/fpsyg.2022.864047},
  hasabstract      = {Y},
  isbn             = {1664-1078},
  keywords         = {Emotion Recognition; Facial Expression; Affective Computing; Speech Emotion; Head Gesture Recognition},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Frontiers Media},
  url              = {https://doi.org/10.3389/fpsyg.2022.864047},
}

@Article{jia2022cr,
  author           = {Jia, Jingjing and Zhang, Bofeng and Lv, Hehe and Xu, Zhikang and Hu, Shengxiang and Li, Haiyan},
  journal          = {Brain sciences},
  title            = {CR-GCN: Channel-Relationships-Based Graph Convolutional Network for EEG Emotion Recognition},
  year             = {2022},
  month            = jul,
  number           = {8},
  pages            = {987--987},
  volume           = {12},
  abstract         = {Electroencephalography (EEG) is recorded by electrodes from different areas of the brain and is commonly used to measure neuronal activity. EEG-based methods have been widely used for emotion recognition recently. However, most current methods for EEG-based emotion recognition do not fully exploit the relationship of EEG channels, which affects the precision of emotion recognition. To address the issue, in this paper, we propose a novel method for EEG-based emotion recognition called CR-GCN: Channel-Relationships-based Graph Convolutional Network. Specifically, topological structure of EEG channels is distance-based and tends to capture local relationships, and brain functional connectivity tends to capture global relationships among EEG channels. Therefore, we construct EEG channel relationships using an adjacency matrix in graph convolutional network where the adjacency matrix captures both local and global relationships among different EEG channels. Extensive experiments demonstrate that CR-GCN method significantly outperforms the state-of-the-art methods. In subject-dependent experiments, the average classification accuracies of 94.69{\%} and 93.95{\%} are achieved for valence and arousal. In subject-independent experiments, the average classification accuracies of 94.78{\%} and 93.46{\%} are obtained for valence and arousal.},
  bdsk-url-1       = {https://doi.org/10.3390/brainsci12080987},
  c1               = {School of Computer Engineering and Science, Shanghai University, Shanghai 200444, China; School of Computer Science and Technology, Kashi University, Kashi 844008, China; School of Computer and Communication Engineering, Shanghai Polytechnic University, Shanghai 201209, China; School of Computer Engineering and Science, Shanghai University, Shanghai 200444, China; School of Computer Engineering and Science, Shanghai University, Shanghai 200444, China; School of Computer Engineering and Science, Shanghai University, Shanghai 200444, China; School of Computer Science and Technology, Kashi University, Kashi 844008, China},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/brainsci12080987},
  hasabstract      = {Y},
  isbn             = {2076-3425},
  keywords         = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Brain-Computer Interfaces},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/brainsci12080987},
}

@Article{wirawan2022th,
  author           = {Wirawan, I and Wardoyo, Retantyo and Lelono, Danang},
  journal          = {International Journal of Power Electronics and Drive Systems/International Journal of Electrical and Computer Engineering},
  title            = {The challenges of emotion recognition methods based on electroencephalogram signals: a literature review},
  year             = {2022},
  month            = apr,
  number           = {2},
  pages            = {1508--1508},
  volume           = {12},
  abstract         = {Electroencephalogram (EEG) signals in recognizing emotions have several advantages. Still, the success of this study, however, is strongly influenced by: i) the distribution of the data used, ii) consider of differences in participant characteristics, and iii) consider the characteristics of the EEG signals. In response to these issues, this study will examine three important points that affect the success of emotion recognition packaged in several research questions: i) What factors need to be considered to generate and distribute EEG data?, ii) How can EEG signals be generated with consideration of differences in participant characteristics?, and iii) How do EEG signals with characteristics exist among its features for emotion recognition? The results, therefore, indicate some important challenges to be studied further in EEG signals-based emotion recognition research. These include i) determine robust methods for imbalanced EEG signals data, ii) determine the appropriate smoothing method to eliminate disturbances on the baseline signals, iii) determine the best baseline reduction methods to reduce the differences in the characteristics of the participants on the EEG signals, iv) determine the robust architecture of the capsule network method to overcome the loss of knowledge information and apply it in more diverse data set.},
  bdsk-url-1       = {https://doi.org/10.11591/ijece.v12i2.pp1508-1519},
  c1               = {Universitas Gadjah Mada; Universitas Gadjah Mada; Universitas Gadjah Mada},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.11591/ijece.v12i2.pp1508-1519},
  hasabstract      = {Y},
  isbn             = {2722-256X},
  keywords         = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Affective Computing; ECG Signal},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Institute of Advanced Engineering and Science (IAES)},
  url              = {https://doi.org/10.11591/ijece.v12i2.pp1508-1519},
}

@Article{gu2021op,
  author           = {Gu, Xiaoqing and Yu, Fan and Zhou, Jie and Zhu, Jiaqun},
  journal          = {Frontiers in psychology},
  title            = {Optimized Projection and Fisher Discriminative Dictionary Learning for EEG Emotion Recognition},
  year             = {2021},
  month            = jun,
  volume           = {12},
  abstract         = {Electroencephalogram (EEG)-based emotion recognition (ER) has drawn increasing attention in the brain--computer interface (BCI) due to its great potentials in human--machine interaction applications. According to the characteristics of rhythms, EEG signals usually can be divided into several different frequency bands. Most existing methods concatenate multiple frequency band features together and treat them as a single feature vector. However, it is often difficult to utilize band-specific information in this way. In this study, an optimized projection and Fisher discriminative dictionary learning (OPFDDL) model is proposed to efficiently exploit the specific discriminative information of each frequency band. Using subspace projection technology, EEG signals of all frequency bands are projected into a subspace. The shared dictionary is learned in the projection subspace such that the specific discriminative information of each frequency band can be utilized efficiently, and simultaneously, the shared discriminative information among multiple bands can be preserved. In particular, the Fisher discrimination criterion is imposed on the atoms to minimize within-class sparse reconstruction error and maximize between-class sparse reconstruction error. Then, an alternating optimization algorithm is developed to obtain the optimal solution for the projection matrix and the dictionary. Experimental results on two EEG-based ER datasets show that this model can achieve remarkable results and demonstrate its effectiveness.},
  bdsk-url-1       = {https://doi.org/10.3389/fpsyg.2021.705528},
  c1               = {School of Computer Science and Artificial Intelligence, Changzhou University, China; Viterbi School of Engineering, University of Southern California, United States; School of Electrical and Mechanical Engineering, Shaoxing University, China; School of Computer Science and Artificial Intelligence, Changzhou University, China},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3389/fpsyg.2021.705528},
  hasabstract      = {Y},
  isbn             = {1664-1078},
  keywords         = {Deep Learning for EEG; Emotion Recognition; EEG Analysis; Epilepsy Detection; Affective Computing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Frontiers Media},
  url              = {https://doi.org/10.3389/fpsyg.2021.705528},
}

@Article{can2023ap,
  author           = {Can, Yekta and Mahesh, Bhargavi and Andr{\'e}, Elisabeth},
  journal          = {Proceedings of the IEEE},
  title            = {Approaches, Applications, and Challenges in Physiological Emotion Recognition---A Tutorial Overview},
  year             = {2023},
  month            = oct,
  number           = {10},
  pages            = {1287--1313},
  volume           = {111},
  abstract         = {An automatic emotion recognition system can serve as a fundamental framework for various applications in daily life from monitoring emotional well-being to improving the quality of life through better emotion regulation. Understanding the process of emotion manifestation becomes crucial for building emotion recognition systems. An emotional experience results in changes not only in interpersonal behavior but also in physiological responses. Physiological signals are one of the most reliable means for recognizing emotions since individuals cannot consciously manipulate them for a long duration. These signals can be captured by medical-grade wearable devices, as well as commercial smart watches and smart bands. With the shift in research direction from laboratory to unrestricted daily life, commercial devices have been employed ubiquitously. However, this shift has introduced several challenges, such as low data quality, dependency on subjective self-reports, unlimited movement-related changes, and artifacts in physiological signals. This tutorial provides an overview of practical aspects of emotion recognition, such as experiment design, properties of different physiological modalities, existing datasets, suitable machine learning algorithms for physiological data, and several applications. It aims to provide the necessary psychological and physiological backgrounds through various emotion theories and the physiological manifestation of emotions, thereby laying a foundation for emotion recognition. Finally, the tutorial discusses open research directions and possible solutions.},
  bdsk-url-1       = {https://doi.org/10.1109/jproc.2023.3286445},
  c1               = {Chair for Human-Centered Artificial Intelligence, University of Augsburg, Augsburg, Germany; Chair for Human-Centered Artificial Intelligence, University of Augsburg, Augsburg, Germany; Chair for Human-Centered Artificial Intelligence, University of Augsburg, Augsburg, Germany},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1109/jproc.2023.3286445},
  hasabstract      = {Y},
  isbn             = {0018-9219},
  keywords         = {Emotion Recognition; Physiological Signals; Affective Computing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Institute of Electrical and Electronics Engineers},
  url              = {https://doi.org/10.1109/jproc.2023.3286445},
}

@Article{li2021id,
  author           = {Li, Siyu and Lyu, Xiaotong and Zhao, Lei and Chen, Zhuangfei and Gong, Anmin and Fu, Yunfa},
  journal          = {Frontiers in computational neuroscience},
  title            = {Identification of Emotion Using Electroencephalogram by Tunable Q-Factor Wavelet Transform and Binary Gray Wolf Optimization},
  year             = {2021},
  month            = sep,
  volume           = {15},
  abstract         = {Emotional brain-computer interface based on electroencephalogram (EEG) is a hot issue in the field of human-computer interaction, and is also an important part of the field of emotional computing. Among them, the recognition of EEG induced by emotion is a key problem. Firstly, the preprocessed EEG is decomposed by tunable-Q wavelet transform. Secondly, the sample entropy, second-order differential mean, normalized second-order differential mean, and Hjorth parameter (mobility and complexity) of each sub-band are extracted. Then, the binary gray wolf optimization algorithm is used to optimize the feature matrix. Finally, support vector machine is used to train the classifier. The five types of emotion signal samples of 32 subjects in the database for emotion analysis using physiological signal dataset is identified by the proposed algorithm. After 6-fold cross-validation, the maximum recognition accuracy is 90.48{\%}, the sensitivity is 70.25{\%}, the specificity is 82.01{\%}, and the Kappa coefficient is 0.603. The results show that the proposed method has good performance indicators in the recognition of multiple types of EEG emotion signals, and has a better performance improvement compared with the traditional methods.},
  bdsk-url-1       = {https://doi.org/10.3389/fncom.2021.732763},
  c1               = {School of Information Engineering and Automation, Kunming University of Science and Technology, Kunming, China; School of Information Engineering and Automation, Kunming University of Science and Technology, Kunming, China; Brain Cognition and Brain-Computer Intelligence Integration Group, Kunming University of Science and Technology, Kunming, China; Brain Cognition and Brain-Computer Intelligence Integration Group, Kunming University of Science and Technology, Kunming, China; College of Information Engineering, Engineering University of PAP, Xi'an, China; Computer Technology Application Key Lab of Yunnan Province, Kunming University of Science and Technology, Kunming, China},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3389/fncom.2021.732763},
  hasabstract      = {Y},
  isbn             = {1662-5188},
  keywords         = {Emotion Recognition; Affective Computing; Deep Learning for EEG; EEG Analysis; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Frontiers Media},
  url              = {https://doi.org/10.3389/fncom.2021.732763},
}

@Article{santamariagranados2021to,
  author           = {Santamar{\'\i}a-Granados, Luz and Mendoza-Moreno, Juan and Chantre-Astaiza, {\'A}ngela and Mu{\~n}oz-Organero, Mario and Ram{\'\i}rez-Gonz{\'a}lez, Gustavo},
  journal          = {Sensors},
  title            = {Tourist Experiences Recommender System Based on Emotion Recognition with Wearable Data},
  year             = {2021},
  month            = nov,
  number           = {23},
  pages            = {7854--7854},
  volume           = {21},
  abstract         = {The collection of physiological data from people has been facilitated due to the mass use of cheap wearable devices. Although the accuracy is low compared to specialized healthcare devices, these can be widely applied in other contexts. This study proposes the architecture for a tourist experiences recommender system (TERS) based on the user's emotional states who wear these devices. The issue lies in detecting emotion from Heart Rate (HR) measurements obtained from these wearables. Unlike most state-of-the-art studies, which have elicited emotions in controlled experiments and with high-accuracy sensors, this research's challenge consisted of emotion recognition (ER) in the daily life context of users based on the gathering of HR data. Furthermore, an objective was to generate the tourist recommendation considering the emotional state of the device wearer. The method used comprises three main phases: The first was the collection of HR measurements and labeling emotions through mobile applications. The second was emotional detection using deep learning algorithms. The final phase was the design and validation of the TERS-ER. In this way, a dataset of HR measurements labeled with emotions was obtained as results. Among the different algorithms tested for ER, the hybrid model of Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) networks had promising results. Moreover, concerning TERS, Collaborative Filtering (CF) using CNN showed better performance.},
  bdsk-url-1       = {https://doi.org/10.3390/s21237854},
  c1               = {GIDINT, Faculty of Systems Engineering, Universidad Santo Tom{\'a}s Seccional Tunja, Calle 19, No. 11-64, Tunja 150001, Colombia.; GIDINT, Faculty of Systems Engineering, Universidad Santo Tom{\'a}s Seccional Tunja, Calle 19, No. 11-64, Tunja 150001, Colombia.; SysT{\'e}mico Research Group, Department of Tourism Sciences, Universidad del Cauca, Calle 5, No. 4-70, Popay{\'a}n 190002, Colombia.; GAST, Telematics Engineering Department, Universidad Carlos III de Madrid, Av. de la Universidad, 30, 28911 Madrid, Spain.; GIT, Telematics Department, Universidad del Cauca, Calle 5, No. 4-70, Popay{\'a}n 190002, Colombia.},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/s21237854},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Emotion Recognition; Affective Computing; Emotion Regulation; Epilepsy Detection},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s21237854},
}

@Article{costantini2022th,
  author           = {Costantini, Giovanni and Parada-Cabaleiro, Emilia and Casali, Daniele and Cesarini, Valerio},
  journal          = {Sensors},
  title            = {The Emotion Probe: On the Universality of Cross-Linguistic and Cross-Gender Speech Emotion Recognition via Machine Learning},
  year             = {2022},
  month            = mar,
  number           = {7},
  pages            = {2461--2461},
  volume           = {22},
  abstract         = {Machine Learning (ML) algorithms within a human--computer framework are the leading force in speech emotion recognition (SER). However, few studies explore cross-corpora aspects of SER; this work aims to explore the feasibility and characteristics of a cross-linguistic, cross-gender SER. Three ML classifiers (SVM, Na{\"\i}ve Bayes and MLP) are applied to acoustic features, obtained through a procedure based on Kononenko's discretization and correlation-based feature selection. The system encompasses five emotions (disgust, fear, happiness, anger and sadness), using the Emofilm database, comprised of short clips of English movies and the respective Italian and Spanish dubbed versions, for a total of 1115 annotated utterances. The results see MLP as the most effective classifier, with accuracies higher than 90{\%} for single-language approaches, while the cross-language classifier still yields accuracies higher than 80{\%}. The results show cross-gender tasks to be more difficult than those involving two languages, suggesting greater differences between emotions expressed by male versus female subjects than between different languages. Four feature domains, namely, RASTA, F0, MFCC and spectral energy, are algorithmically assessed as the most effective, refining existing literature and approaches based on standard sets. To our knowledge, this is one of the first studies encompassing cross-gender and cross-linguistic assessments on SER.},
  bdsk-url-1       = {https://doi.org/10.3390/s22072461},
  c1               = {Department of Electronic Engineering, University of Rome Tor Vergata, 00133 Rome, Italy; Institute of Computational Perception, Johannes Kepler University, 4040 Linz, Austria; Department of Electronic Engineering, University of Rome Tor Vergata, 00133 Rome, Italy; Department of Electronic Engineering, University of Rome Tor Vergata, 00133 Rome, Italy},
  comment          = {no music},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/s22072461},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Emotion Recognition; Speech Emotion; End-to-End Speech Recognition; Affective Computing; Audio-Visual Speech Recognition},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s22072461},
}

@Article{vempala2024pr,
  author        = {Vempala, Naresh and Russo, Frank and Lab, Smart},
  title         = {Predicting Emotion from Music Audio Features Using Neural Networks},
  year          = {2024},
  month         = mar,
  abstract      = {{$<$}p{$ > $}We describe our implementation of two neural networks: a static feedforward network, and an Elman network, for predicting mean valence/arousal ratings of participants for musical excerpts based on audio features. Thirteen audio features were extracted from 12 classical music excerpts (3 from each emotion quadrant). Valence/arousal ratings were collected from 45 participants for the static network, and 9 participants for the Elman network. For the Elman network, each excerpt was temporally segmented into four, sequential chunks of equal duration. Networks were trained on eight of the 12 excerpts and tested on the remaining four. The static network predicted values that closely matched mean participant ratings of valence and arousal. The Elman network did a good job of predicting the arousal trend but not the valence trend. Our study indicates that neural networks can be trained to identify statistical consistencies across audio features to predict valence/arousal values.{$<$}/p{$ > $}},
  bdsk-url-1    = {https://doi.org/10.32920/25413184},
  c1            = {<div class=page title="Page 1><div class="layoutArea><div class="column><p><span>Ryerson University </span></p></div></div></div><span style="white-space: pre;> </span>},
  date-added    = {2024-05-13 15:05:26 +0100},
  date-modified = {2024-05-13 15:05:26 +0100},
  doi           = {10.32920/25413184},
  hasabstract   = {Y},
  keywords      = {Audio Event Detection; Melody Extraction; Emotion Recognition; Affective Computing; Speech Emotion},
  la            = {en},
  priority      = {prio1},
  ranking       = {rank2},
  url           = {https://doi.org/10.32920/25413184},
}

@Article{morgan2019ca,
  author        = {Morgan, Shae},
  journal       = {Journal of speech, language, and hearing research},
  title         = {Categorical and Dimensional Ratings of Emotional Speech: Behavioral Findings From the Morgan Emotional Speech Set},
  year          = {2019},
  month         = nov,
  number        = {11},
  pages         = {4015--4029},
  volume        = {62},
  abstract      = {Purpose Emotion classification for auditory stimuli typically employs 1 of 2 approaches (discrete categories or emotional dimensions). This work presents a new emotional speech set, compares these 2 classification methods for emotional speech stimuli, and emphasizes the need to consider the entire communication model (i.e., the talker, message, and listener) when studying auditory emotion portrayal and perception. Method Emotional speech from male and female talkers was evaluated using both categorical and dimensional rating methods. Ten young adult listeners (ages 19-28 years) evaluated stimuli recorded in 4 emotional speaking styles (Angry, Calm, Happy, and Sad). Talker and listener factors were examined for potential influences on emotional ratings using categorical and dimensional rating methods. Listeners rated stimuli by selecting an emotion category, rating the activation and pleasantness, and indicating goodness of category fit. Results Discrete ratings were generally consistent with dimensional ratings for speech, with accuracy for emotion recognition well above chance. As stimuli approached dimensional extremes of activation and pleasantness, listeners were more confident in their category selection, indicative of a hybrid approach to emotion classification. Female talkers were rated as more activated than male talkers, and female listeners gave higher ratings of activation compared to male listeners, confirming gender differences in emotion perception. Conclusion A hybrid model for auditory emotion classification is supported by the data. Talker and listener factors, such as gender, were found to impact the ratings of emotional speech and must be considered alongside stimulus factors in the design of future studies of emotion.},
  bdsk-url-1    = {https://doi.org/10.1044/2019_jslhr-s-19-0144},
  bdsk-url-2    = {https://doi.org/10.1044/2019%7B%5C_%7Djslhr-s-19-0144},
  c1            = {Department of Communication Sciences and Disorders, University of Utah, Salt Lake City; Program in Audiology, Department of Otolaryngology Head and Neck Surgery and Communicative Disorders, School of Medicine, University of Louisville, KY},
  comment       = {no music},
  date-added    = {2024-05-13 15:05:26 +0100},
  date-modified = {2024-05-13 15:05:26 +0100},
  doi           = {10.1044/2019{\_}jslhr-s-19-0144},
  hasabstract   = {Y},
  isbn          = {1092-4388},
  keywords      = {Speech Emotion; Emotion Recognition; Affective Computing; Sensory Expectations},
  la            = {en},
  priority      = {prio3},
  publisher     = {American Speech--Language--Hearing Association},
  url           = {https://doi.org/10.1044/2019_jslhr-s-19-0144},
}

@Article{cui2023em,
  author        = {Cui, Gaochao and Li, Xueyuan and Touyama, Hideaki},
  journal       = {Scientific reports},
  title         = {Emotion recognition based on group phase locking value using convolutional neural network},
  year          = {2023},
  month         = mar,
  number        = {1},
  volume        = {13},
  abstract      = {Electroencephalography (EEG)-based emotion recognition is an important technology for human-computer interactions. In the field of neuromarketing, emotion recognition based on group EEG can be used to analyze the emotional states of multiple users. Previous emotion recognition experiments have been based on individual EEGs; therefore, it is difficult to use them for estimating the emotional states of multiple users. The purpose of this study is to find a data processing method that can improve the efficiency of emotion recognition. In this study, the DEAP dataset was used, which comprises EEG signals of 32 participants that were recorded as they watched 40 videos with different emotional themes. This study compared emotion recognition accuracy based on individual and group EEGs using the proposed convolutional neural network model. Based on this study, we can see that the differences of phase locking value (PLV) exist in different EEG frequency bands when subjects are in different emotional states. The results showed that an emotion recognition accuracy of up to 85{\%} can be obtained for group EEG data by using the proposed model. It means that using group EEG data can effectively improve the efficiency of emotion recognition. Moreover, the significant emotion recognition accuracy for multiple users achieved in this study can contribute to research on handling group human emotional states.},
  bdsk-url-1    = {https://doi.org/10.1038/s41598-023-30458-6},
  c1            = {Graduate School of Engineering, Toyama Prefectural University, Imizu, 9390398, Japan; Graduate School of Engineering, Toyama Prefectural University, Imizu, 9390398, Japan; Graduate School of Engineering, Toyama Prefectural University, Imizu, 9390398, Japan},
  comment       = {EEG},
  date-added    = {2024-05-13 15:05:26 +0100},
  date-modified = {2024-05-13 15:05:26 +0100},
  doi           = {10.1038/s41598-023-30458-6},
  hasabstract   = {Y},
  isbn          = {2045-2322},
  keywords      = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Emotions},
  la            = {en},
  priority      = {prio3},
  publisher     = {Nature Portfolio},
  url           = {https://doi.org/10.1038/s41598-023-30458-6},
}

@Article{sripian2021th,
  author           = {Sripian, Peeraya and Anuardi, Muhammad and Yu, Jirong and Sugaya, Midori},
  journal          = {Sensors},
  title            = {The Implementation and Evaluation of Individual Preference in Robot Facial Expression Based on Emotion Estimation Using Biological Signals},
  year             = {2021},
  month            = sep,
  number           = {18},
  pages            = {6322--6322},
  volume           = {21},
  abstract         = {Recently, robot services have been widely applied in many fields. To provide optimum service, it is essential to maintain good acceptance of the robot for more effective interaction with users. Previously, we attempted to implement facial expressions by synchronizing an estimated human emotion on the face of a robot. The results revealed that the robot could present different perceptions according to individual preferences. In this study, we considered individual differences to improve the acceptance of the robot by changing the robot's expression according to the emotion of its interacting partner. The emotion was estimated using biological signals, and the robot changed its expression according to three conditions: synchronized with the estimated emotion, inversely synchronized, and a funny expression. During the experiment, the participants provided feedback regarding the robot's expression by choosing whether they ``like''or ``dislike''the expression. We investigated individual differences in the acceptance of the robot expression using the Semantic Differential scale method. In addition, logistic regression was used to create a classification model by considering individual differences based on the biological data and feedback from each participant. We found that the robot expression based on inverse synchronization when the participants felt a negative emotion could result in impression differences among individuals. Then, the robot's expression was determined based on the classification model, and the Semantic Differential scale on the impression of the robot was compared with the three conditions. Overall, we found that the participants were most accepting when the robot expression was calculated using the proposed personalized method.},
  bdsk-url-1       = {https://doi.org/10.3390/s21186322},
  c1               = {College of Engineering, Shibaura Institute of Technology, Tokyo 135-8548, Japan; College of Engineering, Shibaura Institute of Technology, Tokyo 135-8548, Japan; College of Engineering, Shibaura Institute of Technology, Tokyo 135-8548, Japan; College of Engineering, Shibaura Institute of Technology, Tokyo 135-8548, Japan},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/s21186322},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Emotion Recognition; Emotional Expressions; Affective Computing; Facial Expression},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s21186322},
}

@Article{li2022mu,
  author           = {Li, Qi and Liu, Yunqing and Liu, Quanyang and Zhang, Qiong and Yan, Fei and Ma, Yanhui and Zhang, Xinyu},
  journal          = {Entropy},
  title            = {Multidimensional Feature in Emotion Recognition Based on Multi-Channel EEG Signals},
  year             = {2022},
  month            = dec,
  number           = {12},
  pages            = {1830--1830},
  volume           = {24},
  abstract         = {As a major daily task for the popularization of artificial intelligence technology, more and more attention has been paid to the scientific research of mental state electroencephalogram (EEG) in recent years. To retain the spatial information of EEG signals and fully mine the EEG timing-related information, this paper proposes a novel EEG emotion recognition method. First, to obtain the frequency, spatial, and temporal information of multichannel EEG signals more comprehensively, we choose the multidimensional feature structure as the input of the artificial neural network. Then, a neural network model based on depthwise separable convolution is proposed, extracting the input structure's frequency and spatial features. The network can effectively reduce the computational parameters. Finally, we modeled using the ordered neuronal long short-term memory (ON-LSTM) network, which can automatically learn hierarchical information to extract deep emotional features hidden in EEG time series. The experimental results show that the proposed model can reasonably learn the correlation and temporal dimension information content between EEG multi-channel and improve emotion classification performance. We performed the experimental validation of this paper in two publicly available EEG emotional datasets. In the experiments on the DEAP dataset (a dataset for emotion analysis using EEG, physiological, and video signals), the mean accuracy of emotion recognition for arousal and valence is 95.02{\%} and 94.61{\%}, respectively. In the experiments on the SEED dataset (a dataset collection for various purposes using EEG signals), the average accuracy of emotion recognition is 95.49{\%}.},
  bdsk-url-1       = {https://doi.org/10.3390/e24121830},
  c1               = {Department of Electronics and Information Engineering, Changchun University of Science and Technology, Changchun 130000, China; Department of Electronics and Information Engineering, Changchun University of Science and Technology, Changchun 130000, China; Department of Electronics and Information Engineering, Changchun University of Science and Technology, Changchun 130000, China; Department of Electronics and Information Engineering, Changchun University of Science and Technology, Changchun 130000, China; Department of Electronics and Information Engineering, Changchun University of Science and Technology, Changchun 130000, China; Department of Electronics and Information Engineering, Changchun University of Science and Technology, Changchun 130000, China; Department of Electronics and Information Engineering, Changchun University of Science and Technology, Changchun 130000, China; Economics School, Jilin University, Changchun 130000, China},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/e24121830},
  hasabstract      = {Y},
  isbn             = {1099-4300},
  keywords         = {Deep Learning for EEG; Emotion Recognition; EEG Analysis; Affective Computing; Emotion Regulation},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/e24121830},
}

@Article{velasco2019ca,
  author           = {de Velasco, Mikel and Justo, Raquel and Zorrilla, Asier and Torres, M.},
  title            = {Can Spontaneous Emotions be Detected from Speech on TV Political Debates?},
  year             = {2019},
  month            = oct,
  abstract         = {Decoding emotional states from multimodal signals is an increasingly active domain, within the framework of affective computing, which aims to a better understanding of Human-Human Communication as well as to improve Human-Computer Interaction.But the automatic recognition of spontaneous emotions from speech is a very complex task due to the lack of a certainty of the speaker states as well as to the difficulty to identify a variety of emotions in real scenarios.In this work we explore the extent to which emotional states can be decoded from speech signals extracted from TV political debates.The labelling procedure was supported by perception experiments where only a small set of emotions has been identified.In addition, some scaled judgements of valence, arousal and dominance were also provided.In this framework the paper shows meaningful comparisons between both, the dimensional and the categorical models of emotions, which is a new contribution when dealing with spontaneous emotions.To this end Support Vector Machines (SVM) as well as Feedforward Neural Networks (FNN) have been proposed to develop classifiers and predictors.The experimental evaluation over a Spanish corpus has shown the ability of both models to be identified in speech segments by the proposed artificial systems.},
  bdsk-url-1       = {https://doi.org/10.1109/coginfocom47531.2019.9089948},
  c1               = {Universidad del Pa{\'\i}s Vasco UPV/EHU Leioa, Spain 48940; Universidad del Pa{\'\i}s Vasco UPV/EHU Leioa, Spain 48940; Universidad del Pa{\'\i}s Vasco UPV/EHU Leioa, Spain 48940; Universidad del Pa{\'\i}s Vasco UPV/EHU Leioa, Spain 48940},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1109/coginfocom47531.2019.9089948},
  hasabstract      = {Y},
  keywords         = {Speech Emotion; Emotion Recognition; Affective Computing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.1109/coginfocom47531.2019.9089948},
}

@Article{miranda2022bi,
  author           = {Miranda, Jos{\'e} and Rituerto-Gonz{\'a}lez, Esther and Luis-Mingueza, Clara and Canabal, Manuel and Barcenas, Alberto and Lanza-Guti{\'e}rrez, Jos{\'e} and Pel{\'a}ez-Moreno, Carmen and L{\'o}pez-Ongil, C.},
  journal          = {IEEE internet of things journal},
  title            = {Bindi: Affective Internet of Things to Combat Gender-Based Violence},
  year             = {2022},
  month            = nov,
  number           = {21},
  pages            = {21174--21193},
  volume           = {9},
  abstract         = {The main research motivation of this article is the fight against gender-based violence and achieving gender equality from a technological perspective. The solution proposed in this work goes beyond currently existing panic buttons, needing to be manually operated by the victims under difficult circumstances. Instead, Bindi, our end-to-end autonomous multimodal system, relies on artificial intelligence methods to automatically identify violent situations, based on detecting fear-related emotions, and trigger a protection protocol, if necessary. To this end, Bindi integrates modern state-of-the-art technologies, such as the Internet of Bodies, affective computing, and cyber--physical systems, leveraging: 1) affective Internet of Things (IoT) with auditory and physiological commercial off-the-shelf smart sensors embedded in wearable devices; 2) hierarchical multisensorial information fusion; and 3) the edge-fog-cloud IoT architecture. This solution is evaluated using our own data set named WEMAC, a very recently collected and freely available collection of data comprising the auditory and physiological responses of 47 women to several emotions elicited by using a virtual reality environment. On this basis, this work provides an analysis of multimodal late fusion strategies to combine the physiological and speech data processing pipelines to identify the best intelligence engine strategy for Bindi. In particular, the best data fusion strategy reports an overall fear classification accuracy of 63.61{\%} for a subject-independent approach. Both a power consumption study and an audio data processing pipeline to detect violent acoustic events complement this analysis. This research is intended as an initial multimodal baseline that facilitates further work with real-life elicited fear in women.},
  bdsk-url-1       = {https://doi.org/10.1109/jiot.2022.3177256},
  c1               = {Department of Electronics, Universidad Carlos III of Madrid, Legan\&{\#}x00E9;s, Spain; Department of Signal Theory and Communications, Universidad Carlos III of Madrid, Legan\&{\#}x00E9;s, Spain; Department of Signal Theory and Communications, Universidad Carlos III of Madrid, Legan\&{\#}x00E9;s, Spain; Department of Electronics, Universidad Carlos III of Madrid, Legan\&{\#}x00E9;s, Spain; Department of Electronics, Universidad Carlos III of Madrid, Legan\&{\#}x00E9;s, Spain; Department of Computer Science, Universidad de Alcal\&{\#}x00E1;, Alcal\&{\#}x00E1; de Henares, Spain; Department of Signal Theory and Communications, Universidad Carlos III of Madrid, Legan\&{\#}x00E9;s, Spain; Department of Electronics, Universidad Carlos III of Madrid, Legan\&{\#}x00E9;s, Spain},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1109/jiot.2022.3177256},
  hasabstract      = {Y},
  isbn             = {2327-4662},
  keywords         = {Affective Computing; Emotion Recognition; Internet-based Interventions},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Institute of Electrical and Electronics Engineers},
  url              = {https://doi.org/10.1109/jiot.2022.3177256},
}

@Article{kutt2022bi,
  author        = {Kutt, Krzysztof and Dr{\k a}{\.z}yk, Dominika and {\.Z}uchowska, Laura and Szel{\k a}{\.z}ek, Maciej and Bobek, Szymon and Nalepa, Grzegorz},
  journal       = {Scientific data},
  title         = {BIRAFFE2, a multimodal dataset for emotion-based personalization in rich affective game environments},
  year          = {2022},
  month         = jun,
  number        = {1},
  volume        = {9},
  abstract      = {Abstract Generic emotion prediction models based on physiological data developed in the field of affective computing apparently are not robust enough. To improve their effectiveness, one needs to personalize them to specific individuals and incorporate broader contextual information. To address the lack of relevant datasets, we propose the 2nd Study in Bio-Reactions and Faces for Emotion-based Personalization for AI Systems (BIRAFFE2) dataset. In addition to the classical procedure in the stimulus-appraisal paradigm, it also contains data from an affective gaming session in which a range of contextual data was collected from the game environment. This is complemented by accelerometer, ECG and EDA signals, participants'facial expression data, together with personality and game engagement questionnaires. The dataset was collected on 102 participants. Its potential usefulness is presented by validating the correctness of the contextual data and indicating the relationships between personality and participants'emotions and between personality and physiological signals.},
  bdsk-url-1    = {https://doi.org/10.1038/s41597-022-01402-6},
  c1            = {Jagiellonian Human-Centered Artificial Intelligence Laboratory (JAHCAI) and Institute of Applied Computer Science, Jagiellonian University, Krak{\'o}w, Poland; Institute of Neuroscience, Universit{\'e}Catholique de Louvain, Louvain-la-Neuve, Belgium; Department of Applied Computer Science, AGH University of Science and Technology, Krak{\'o}w, Poland; Department of Applied Computer Science, AGH University of Science and Technology, Krak{\'o}w, Poland; Jagiellonian Human-Centered Artificial Intelligence Laboratory (JAHCAI) and Institute of Applied Computer Science, Jagiellonian University, Krak{\'o}w, Poland; Jagiellonian Human-Centered Artificial Intelligence Laboratory (JAHCAI) and Institute of Applied Computer Science, Jagiellonian University, Krak{\'o}w, Poland},
  comment       = {physiological},
  date-added    = {2024-05-13 15:05:26 +0100},
  date-modified = {2024-05-13 15:05:26 +0100},
  doi           = {10.1038/s41597-022-01402-6},
  hasabstract   = {Y},
  isbn          = {2052-4463},
  keywords      = {Affective Computing; Emotion Recognition; Personality Data; Emotion Regulation; Multimodal Data},
  la            = {en},
  priority      = {prio3},
  publisher     = {Nature Portfolio},
  url           = {https://doi.org/10.1038/s41597-022-01402-6},
}

@Article{li2022de,
  author           = {Li, Qi and Liu, Yunqing and Shang, Yujie and Zhang, Qiong and Yan, Fei},
  journal          = {Entropy},
  title            = {Deep Sparse Autoencoder and Recursive Neural Network for EEG Emotion Recognition},
  year             = {2022},
  month            = aug,
  number           = {9},
  pages            = {1187--1187},
  volume           = {24},
  abstract         = {Recently, emotional electroencephalography (EEG) has been of great importance in brain-computer interfaces, and it is more urgent to realize automatic emotion recognition. The EEG signal has the disadvantages of being non-smooth, non-linear, stochastic, and susceptible to background noise. Additionally, EEG signal processing network models have the disadvantages of a large number of parameters and long training time. To address the above issues, a novel model is presented in this paper. Initially, a deep sparse autoencoder network (DSAE) was used to remove redundant information from the EEG signal and reconstruct its underlying features. Further, combining a convolutional neural network (CNN) with long short-term memory (LSTM) can extract relevant features from task-related features, mine the correlation between the 32 channels of the EEG signal, and integrate contextual information from these frames. The proposed DSAE + CNN + LSTM (DCRNN) model was experimented with on the public dataset DEAP. The classification accuracies of valence and arousal reached 76.70{\%} and 81.43{\%}, respectively. Meanwhile, we conducted experiments with other comparative methods to further demonstrate the effectiveness of the DCRNN method.},
  bdsk-url-1       = {https://doi.org/10.3390/e24091187},
  c1               = {Department of Electronics and Information Engineering, Changchun University of Science and Technology, Changchun 130012, China; Department of Electronics and Information Engineering, Changchun University of Science and Technology, Changchun 130012, China; Department of Electronics and Information Engineering, Changchun University of Science and Technology, Changchun 130012, China; Department of Electronics and Information Engineering, Changchun University of Science and Technology, Changchun 130012, China; Department of Electronics and Information Engineering, Changchun University of Science and Technology, Changchun 130012, China},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/e24091187},
  hasabstract      = {Y},
  isbn             = {1099-4300},
  keywords         = {Deep Learning for EEG; Emotion Recognition; EEG Analysis; Neural Ensemble Physiology; Affective Computing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/e24091187},
}

@Article{almarri2021au,
  author           = {Almarri, Badar and Rajasekaran, Sanguthevar and Huang, Chun-Hsi},
  journal          = {PloS one},
  title            = {Automatic subject-specific spatiotemporal feature selection for subject-independent affective BCI},
  year             = {2021},
  month            = aug,
  number           = {8},
  pages            = {e0253383--e0253383},
  volume           = {16},
  abstract         = {The dimensionality of the spatially distributed channels and the temporal resolution of electroencephalogram (EEG) based brain-computer interfaces (BCI) undermine emotion recognition models. Thus, prior to modeling such data, as the final stage of the learning pipeline, adequate preprocessing, transforming, and extracting temporal (i.e., time-series signals) and spatial (i.e., electrode channels) features are essential phases to recognize underlying human emotions. Conventionally, inter-subject variations are dealt with by avoiding the sources of variation (e.g., outliers) or turning the problem into a subject-deponent. We address this issue by preserving and learning from individual particularities in response to affective stimuli. This paper investigates and proposes a subject-independent emotion recognition framework that mitigates the subject-to-subject variability in such systems. Using an unsupervised feature selection algorithm, we reduce the feature space that is extracted from time-series signals. For the spatial features, we propose a subject-specific unsupervised learning algorithm that learns from inter-channel co-activation online. We tested this framework on real EEG benchmarks, namely DEAP, MAHNOB-HCI, and DREAMER. We train and test the selection outcomes using nested cross-validation and a support vector machine (SVM). We compared our results with the state-of-the-art subject-independent algorithms. Our results show an enhanced performance by accurately classifying human affection (i.e., based on valence and arousal) by 16{\%}--27{\%} compared to other studies. This work not only outperforms other subject-independent studies reported in the literature but also proposes an online analysis solution to affection recognition.},
  bdsk-url-1       = {https://doi.org/10.1371/journal.pone.0253383},
  c1               = {Dept. of Computer Science and Engineering, University of Connecticut, Storrs, CT, United States of America; Dept. of Computer Science, King Faisal University, Al-Ahsa, Saudi Arabia; Dept. of Computer Science and Engineering, University of Connecticut, Storrs, CT, United States of America; The School of Computing, Southern Illinois University, Carbondale, IL, United States of America},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1371/journal.pone.0253383},
  hasabstract      = {Y},
  isbn             = {1932-6203},
  keywords         = {Emotion Recognition; Affective Computing; Brain-Computer Interfaces; Feature Extraction; Sensory Processing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Public Library of Science},
  url              = {https://doi.org/10.1371/journal.pone.0253383},
}

@Article{hao2019de,
  author           = {Hao, Min and Liu, Guangyuan and Gokhale, Anu and Xu, Yanwu and Chen, Rui},
  journal          = {Computational intelligence and neuroscience},
  title            = {Detecting Happiness Using Hyperspectral Imaging Technology},
  year             = {2019},
  month            = jan,
  pages            = {1--16},
  volume           = {2019},
  abstract         = {Hyperspectral imaging (HSI) technology can be used to detect human emotions based on the power of material discrimination from their faces. In this paper, HSI is used to remotely sense and distinguish blood chromophores in facial tissues and acquire an evaluation indicator (tissue oxygen saturation, StO2) using an optical absorption model. This study explored facial analysis while people were showing spontaneous expressions of happiness during social interaction. Happiness, as a psychological emotion, has been shown to be strongly linked to other activities such as physiological reaction and facial expression. Moreover, facial expression as a communicative motor behavior likely arises from musculoskeletal anatomy, neuromuscular activity, and individual personality. This paper quantified the neuromotor movements of tissues surrounding some regions of interest (ROIs) on smiling happily. Next, we selected six regions-the forehead, eye, nose, cheek, mouth, and chin-according to a facial action coding system (FACS). Nineteen segments were subsequently partitioned from the above ROIs. The affective data (StO2) of 23 young adults were acquired by HSI while the participants expressed emotions (calm or happy), and these were used to compare the significant differences in the variations of StO2 between the different ROIs through repeated measures analysis of variance. Results demonstrate that happiness causes different distributions in the variations of StO2 for the above ROIs; these are explained in depth in the article. This study establishes that facial tissue oxygen saturation is a valid and reliable physiological indicator of happiness and merits further research.},
  bdsk-url-1       = {https://doi.org/10.1155/2019/1965789},
  c1               = {School of Electronic and Information Engineering, Southwest University, Chongqing, China; School of Electronic and Information Engineering, Southwest University, Chongqing, China; Illinois State University, Normal, IL, USA; Center of Technical Support for Network Security, Chongqing Municipal Public Security Bureau, Chongqing, China; College of Computer and Information Science, Southwest University, Chongqing, China},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1155/2019/1965789},
  hasabstract      = {Y},
  isbn             = {1687-5265},
  keywords         = {Emotion Recognition; Affective Computing; Face Perception; Facial Expression; Emotional Expressions},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Hindawi Publishing Corporation},
  url              = {https://doi.org/10.1155/2019/1965789},
}

@Article{pandit2018tr,
  author           = {Pandit, Vedhas and Cummins, Nicholas and Schmitt, Maximilian and Hantke, Simone and Graf, Franz and Paletta, Lucas and Schuller, Bj{\"o}rn},
  title            = {Tracking Authentic and In-the-wild Emotions Using Speech},
  year             = {2018},
  month            = may,
  abstract         = {This first-of-its-kind study aims to track authentic affect representations in-the-wild. We use the `Graz Real-life Affect in the Street and Supermarket (GRAS2)' corpus featuring audiovisual recordings of random participants in non-laboratory conditions. The participants were initially unaware of being recorded. This paradigm enabled us to use a collection of a wide range of authentic, spontaneous and natural affective behaviours. Six raters annotated twenty-eight conversations averaging 2.5 minutes in duration, tracking the arousal and valence levels of the participants. We generate the gold standards through a novel robust Evaluator Weighted Estimator (EWE) formulation. We train Support Vector Regressors (SVR) and Recurrent Neural Networks (RNN) with the low-level-descriptors (LLDs) of the ComParE feature-set in different derived representations including bag-of-audio-words. Despite the challenging nature of this database, a fusion system achieved a highly promising concordance correlation coefficient (CCC) of.372 for arousal dimension, while RNNs achieved a top CCC of.223 in predicting valence, using a bag-of-features representation.},
  bdsk-url-1       = {https://doi.org/10.1109/aciiasia.2018.8470340},
  c1               = {University of Augsburg, Augsburg, Germany; University of Augsburg, Augsburg, Germany; University of Augsburg, Augsburg, Germany; University of Augsburg, Augsburg, Germany; Joanneum Research Forschungsgesellschaft mbH, Graz, Austria; Joanneum Research Forschungsgesellschaft mbH, Graz, Austria; Joanneum Research Forschungsgesellschaft mbH, Graz, Austria},
  comment          = {no music},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1109/aciiasia.2018.8470340},
  hasabstract      = {Y},
  keywords         = {Affective Computing; Emotion Recognition; Speech Emotion; Environmental Sound Recognition},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.1109/aciiasia.2018.8470340},
}

@Article{fouladgar2021cn,
  author           = {Fouladgar, Nazanin and Alirezaie, Marjan and Fr{\"a}mling, Kary},
  journal          = {Neural computing \& applications},
  title            = {CN-waterfall: a deep convolutional neural network for multimodal physiological affect detection},
  year             = {2021},
  month            = sep,
  number           = {3},
  pages            = {2157--2176},
  volume           = {34},
  abstract         = {Abstract Affective computing solutions, in the literature, mainly rely on machine learning methods designed to accurately detect human affective states. Nevertheless, many of the proposed methods are based on handcrafted features, requiring sufficient expert knowledge in the realm of signal processing. With the advent of deep learning methods, attention has turned toward reduced feature engineering and more end-to-end machine learning. However, most of the proposed models rely on late fusion in a multimodal context. Meanwhile, addressing interrelations between modalities for intermediate-level data representation has been largely neglected. In this paper, we propose a novel deep convolutional neural network, called CN-Waterfall, consisting of two modules: Base and General . While the Base module focuses on the low-level representation of data from each single modality, the General module provides further information, indicating relations between modalities in the intermediate- and high-level data representations. The latter module has been designed based on theoretically grounded concepts in the Explainable AI (XAI) domain, consisting of four different fusions. These fusions are mainly tailored to correlation - and non-correlation -based modalities. To validate our model, we conduct an exhaustive experiment on WESAD and MAHNOB-HCI, two publicly and academically available datasets in the context of multimodal affective computing. We demonstrate that our proposed model significantly improves the performance of physiological-based multimodal affect detection.},
  bdsk-url-1       = {https://doi.org/10.1007/s00521-021-06516-3},
  c1               = {Department of Computing Science, Ume{\aa}University, Ume{\aa}, Sweden; Centre for Applied Autonomous Sensor Systems (AASS), {\"O}rebro University, {\"O}rebro, Sweden; Department of Computing Science, Ume{\aa}University, Ume{\aa}, Sweden},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1007/s00521-021-06516-3},
  hasabstract      = {Y},
  isbn             = {0941-0643},
  keywords         = {Affective Computing; Emotion Recognition; Deep Learning; Multimodal Data},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Springer Science+Business Media},
  url              = {https://doi.org/10.1007/s00521-021-06516-3},
}

@Article{vafaei2023ex,
  author           = {Vafaei, Elnaz and Rahatabad, Fereidoun and Setarehdan, Seyed and Azadfallah, Parviz},
  journal          = {Journal of healthcare engineering},
  title            = {Extracting a Novel Emotional EEG Topographic Map Based on a Stacked Autoencoder Network},
  year             = {2023},
  month            = jan,
  pages            = {1--19},
  volume           = {2023},
  abstract         = {Emotion recognition based on brain signals has increasingly become attractive to evaluate human's internal emotional states. Conventional emotion recognition studies focus on developing machine learning and classifiers. However, most of these methods do not provide information on the involvement of different areas of the brain in emotions. Brain mapping is considered as one of the most distinguishing methods of showing the involvement of different areas of the brain in performing an activity. Most mapping techniques rely on projection and visualization of only one of the electroencephalogram (EEG) subband features onto brain regions. The present study aims to develop a new EEG-based brain mapping, which combines several features to provide more complete and useful information on a single map instead of common maps. In this study, the optimal combination of EEG features for each channel was extracted using a stacked autoencoder (SAE) network and visualizing a topographic map. Based on the research hypothesis, autoencoders can extract optimal features for quantitative EEG (QEEG) brain mapping. The DEAP EEG database was employed to extract topographic maps. The accuracy of image classifiers using the convolutional neural network (CNN) was used as a criterion for evaluating the distinction of the obtained maps from a stacked autoencoder topographic map (SAETM) method for different emotions. The average classification accuracy was obtained 0.8173 and 0.8037 in the valence and arousal dimensions, respectively. The extracted maps were also ranked by a team of experts compared to common maps. The results of quantitative and qualitative evaluation showed that the obtained map by SAETM has more information than conventional maps.},
  bdsk-url-1       = {https://doi.org/10.1155/2023/9223599},
  c1               = {Department of Biomedical Engineering, Science and Research Branch, Islamic Azad University, Tehran, Iran; Department of Biomedical Engineering, Science and Research Branch, Islamic Azad University, Tehran, Iran; School of Electrical and Computer Engineering, College of Engineering, University of Tehran, Tehran, Iran; Tarbiat Modares University, Tehran, Iran},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1155/2023/9223599},
  hasabstract      = {Y},
  isbn             = {2040-2295},
  keywords         = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Affective Computing; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Hindawi Publishing Corporation},
  url              = {https://doi.org/10.1155/2023/9223599},
}

@Article{kone2018pe,
  author        = {Kon{\'e}, Chaka and Th{\`a}nh, Nh{\^a}n and Flamary, R{\'e}mi and Belleudy, C{\'e}cile},
  journal       = {Internatinoal Journal of Sensor Networks and Data Communications/Internatinoal Journal of Sensor Networks and Data Communications},
  title         = {Performance Comparison of the KNN and SVM Classification Algorithms in the Emotion Detection System EMOTICA},
  year          = {2018},
  month         = jan,
  number        = {01},
  volume        = {07},
  abstract      = {Emotica (EMOTIon CApture) system is a multimodal emotion recognition system that uses physiological signals.A DLF (Decision Level Fusion) approach with a voting method is used in this system to merge monomodal decisions for a multimodal detection.In this document, on the one hand, we describe how from a physiological signal Emotica can detect an emotional activity and distinguish one emotional activity from others.On the other hand, we present a study about two classification algorithms, KNN and SVM.These algorithms have been implemented on the Emotica system in order to see which one is the best.The experiments show that KNN and SVM allow a high accuracy in emotion recognition, but SVM is more accurate than KNN on the data that was used.Indeed, we obtain a recognition rate of 81.69{\%} and 84{\%} respectively with KNN and SVM algorithms under certain conditions.},
  bdsk-url-1    = {https://doi.org/10.4172/2090-4886.1000153},
  c1            = {Universit´e Cˆote d'Azur, Laboratoire LEAT CNRS UMR 7248, Sophia-Antipolis, France; Universit´e Cˆote d'Azur, Laboratoire I3S CNRS UMR 7271, Sophia-Antipolis, France; Universit´e Cˆote d'Azur, Observatoire de la Cˆote d'Azur, CNRS, Laboratoire Lagrange, Nice, France; Universit´e Cˆote d'Azur, Laboratoire LEAT CNRS UMR 7248, Sophia-Antipolis, France},
  comment       = {physiological signals},
  date-added    = {2024-05-13 15:05:26 +0100},
  date-modified = {2024-05-13 15:05:26 +0100},
  doi           = {10.4172/2090-4886.1000153},
  hasabstract   = {Y},
  isbn          = {2090-4878},
  keywords      = {Emotion Recognition; Affective Computing; Speech Emotion},
  la            = {en},
  priority      = {prio3},
  publisher     = {OMICS Publishing Group},
  url           = {https://doi.org/10.4172/2090-4886.1000153},
}

@Article{zhang2022gr,
  author           = {Zhang, Jing and Zhang, Xueying and Chen, Guijun and Zhao, Qing},
  journal          = {Brain sciences},
  title            = {Granger-Causality-Based Multi-Frequency Band EEG Graph Feature Extraction and Fusion for Emotion Recognition},
  year             = {2022},
  month            = dec,
  number           = {12},
  pages            = {1649--1649},
  volume           = {12},
  abstract         = {Graph convolutional neural networks (GCN) have attracted much attention in the task of electroencephalogram (EEG) emotion recognition. However, most features of current GCNs do not take full advantage of the causal connection between the EEG signals in different frequency bands during the process of constructing the adjacency matrix. Based on the causal connectivity between the EEG channels obtained by Granger causality (GC) analysis, this paper proposes a multi-frequency band EEG graph feature extraction and fusion method for EEG emotion recognition. First, the original GC matrices between the EEG signals at each frequency band are calculated via GC analysis, and then they are adaptively converted to asymmetric binary GC matrices through an optimal threshold. Then, a kind of novel GC-based GCN feature (GC-GCN) is constructed by using differential entropy features and the binary GC matrices as the node values and adjacency matrices, respectively. Finally, on the basis of the GC-GCN features, a new multi-frequency band feature fusion method (GC-F-GCN) is proposed, which integrates the graph information of the EEG signals at different frequency bands for the same node. The experimental results demonstrate that the proposed GC-F-GCN method achieves better recognition performance than the state-of-the-art GCN methods, for which average accuracies of 97.91{\%}, 98.46{\%}, and 98.15{\%} were achieved for the arousal, valence, and arousal-valence classifications, respectively.},
  bdsk-url-1       = {https://doi.org/10.3390/brainsci12121649},
  c1               = {College of Information and Computer, Taiyuan University of Technology, Taiyuan 030024, China; College of Information and Computer, Taiyuan University of Technology, Taiyuan 030024, China; College of Information and Computer, Taiyuan University of Technology, Taiyuan 030024, China; College of Information and Computer, Taiyuan University of Technology, Taiyuan 030024, China},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/brainsci12121649},
  hasabstract      = {Y},
  isbn             = {2076-3425},
  keywords         = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Affective Computing; Feature Extraction},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/brainsci12121649},
}

@Article{huynh2016ja,
  author           = {Huynh, Sinh and Lee, Eun and Park, Taiwoo and Balan, Rajesh},
  title            = {Jasper},
  year             = {2016},
  month            = jun,
  abstract         = {This paper aims to develop a system that evaluates the emotional experience of gamers based on physiological changes. A within-subject experiment with 22 participants has been designed to investigate the effects of difficulty level and social playing mode on player emotions and to examine the correlation between each emotion and the physiological changes. We demonstrate the feasibility of using commodity wearable physiological sensing devices to recognize mobile gamer's emotion. Specifically, our system performs 3-level excitement classification at an accuracy of 77.38{\%} and binary classification of happiness state at an accuracy of 73.21{\%}. These classification results show the potential of using commodity wearable sensing devices as a valuable evaluation tool for game designers to gauge user emotions and develop personalized gaming experience.},
  bdsk-url-1       = {https://doi.org/10.1145/2934646.2934648},
  c1               = {Singapore Management University, Singapore, Singapore; Singapore Management University, Singapore, Singapore; Michigan State University, East Lansing, Michigan, USA; Singapore Management University, Singapore, Singapore},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1145/2934646.2934648},
  hasabstract      = {Y},
  keywords         = {Affective Computing; Emotion Recognition; User Satisfaction; Emotions; Affective Design},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.1145/2934646.2934648},
}

@Article{abdelghaffar2022su,
  author           = {Abdel-Ghaffar, Eman and Wu, Yujin and Daoudi, Mohamed},
  journal          = {IEEE access},
  title            = {Subject-Dependent Emotion Recognition System Based on Multidimensional Electroencephalographic Signals: A Riemannian Geometry Approach},
  year             = {2022},
  month            = jan,
  pages            = {14993--15006},
  volume           = {10},
  abstract         = {Emotion recognition plays an important role in human computer interaction systems as it helps the computer in understanding human behavior and their decision making process.Using Electroencephalographic (EEG) signals in emotion recognition offers a direct assessment on the inner state of human mind.This study aims to build a subject dependent emotion recognition system that differentiate between high and low levels of valance and arousal, using multidimensional EEG signals.Our system offers a transfer learning-minimum distance to Riemannian mean (TL-MDRM) framework.In this work, we perform two pre-processing stages.In the first stage, we analyze the EEG signals to investigate their non-Gaussianity and determine the most appropriate signal distribution.Using several statistical and goodness of fit tests, T-distribution was found to be the most appropriate distribution.Covariance matrix estimations plays a crucial step in manifold learning technique, based on the most suitable signal distribution the covariance matrix estimation technique is chosen.In the second stage, we perform transfer learning to deal with cross-session variability by generating a unique reference point for each participant and performing affine transformation for the covariance matrices on the symmetric positive definite (SPD) manifold around this point.The results show that, TL process improved the performance even when assuming Gaussian distribution, while assuming T-distribution with TL improved the performance further.},
  bdsk-url-1       = {https://doi.org/10.1109/access.2022.3147461},
  c1               = {Engineering Shoubra, Benha Univ; Centre de Recherche en Informatique, Signal et Automatique de Lille - UMR 9189; Centre de Recherche en Informatique, Signal et Automatique de Lille - UMR 9189; Ecole nationale sup{\'e}rieure Mines-T{\'e}l{\'e}com Lille Douai},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1109/access.2022.3147461},
  hasabstract      = {Y},
  isbn             = {2169-3536},
  keywords         = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Institute of Electrical and Electronics Engineers},
  url              = {https://doi.org/10.1109/access.2022.3147461},
}

@Article{romaniszynkania2021af,
  author           = {Romaniszyn-Kania, Patrycja and Pollak, Anita and Bugdo{\l}, Marcin and Bugdol, Monika and Kania, Damian and Ma{\'n}ka, Anna and Danch--Wierzchowska, Marta and Mitas, Andrzej},
  journal          = {Sensors},
  title            = {Affective State during Physiotherapy and Its Analysis Using Machine Learning Methods},
  year             = {2021},
  month            = jul,
  number           = {14},
  pages            = {4853--4853},
  volume           = {21},
  abstract         = {Invasive or uncomfortable procedures especially during healthcare trigger emotions. Technological development of the equipment and systems for monitoring and recording psychophysiological functions enables continuous observation of changes to a situation responding to a situation. The presented study aimed to focus on the analysis of the individual's affective state. The results reflect the excitation expressed by the subjects'statements collected with psychological questionnaires. The research group consisted of 49 participants (22 women and 25 men). The measurement protocol included acquiring the electrodermal activity signal, cardiac signals, and accelerometric signals in three axes. Subjective measurements were acquired for affective state using the JAWS questionnaires, for cognitive skills the DST, and for verbal fluency the VFT. The physiological and psychological data were subjected to statistical analysis and then to a machine learning process using different features selection methods (JMI or PCA). The highest accuracy of the kNN classifier was achieved in combination with the JMI method (81.63{\%}) concerning the division complying with the JAWS test results. The classification sensitivity and specificity were 85.71{\%} and 71.43{\%}.},
  bdsk-url-1       = {https://doi.org/10.3390/s21144853},
  c1               = {Faculty of Biomedical Engineering, Silesian University of Technology, Roosevelta 40, 41-800 Zabrze, Poland; Institute of Psychology, University of Silesia in Katowice, Bankowa 12, 40-007 Katowice, Poland; Faculty of Biomedical Engineering, Silesian University of Technology, Roosevelta 40, 41-800 Zabrze, Poland; Faculty of Biomedical Engineering, Silesian University of Technology, Roosevelta 40, 41-800 Zabrze, Poland; Institute of Physiotherapy and Health Sciences, The Jerzy Kukuczka Academy of Physical Education in Katowice, Miko{\l}owska 72A, 40-065 Katowice, Poland; Faculty of Biomedical Engineering, Silesian University of Technology, Roosevelta 40, 41-800 Zabrze, Poland; Faculty of Biomedical Engineering, Silesian University of Technology, Roosevelta 40, 41-800 Zabrze, Poland; Faculty of Biomedical Engineering, Silesian University of Technology, Roosevelta 40, 41-800 Zabrze, Poland},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/s21144853},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Affective Computing; Emotion Regulation; Emotion Recognition; Speech Emotion; Baroreflex Sensitivity},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s21144853},
}

@Article{brown2014ge,
  author        = {Brown, LaVonda and Howard, Ayanna},
  title         = {Gestural behavioral implementation on a humanoid robotic platform for effective social interaction},
  year          = {2014},
  month         = aug,
  abstract      = {The role of emotions in social scenarios is to provide an inherent mode of communication between two parties. When emotions are properly employed and understood, people are able to respond appropriately, which further enhances the social interaction. Ultimately, effective emotion execution in social settings has the capability to build rapport, improve engagement, optimize learning, provide comfort, and increase overall likability. In this paper, we discuss associating dominant emotions of effective social interaction to gestural behaviors on a humanoid robotic platform. Studies with 13 participants interacting with the robot show that by integrating key principles related to the characteristics of happy and sad emotions, the intended emotion is perceived across all participants with 95.19{\%} and 94.23{\%} sensitivity, respectively.},
  bdsk-url-1    = {https://doi.org/10.1109/roman.2014.6926297},
  c1            = {{$[$}School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, 30332, USA{$]$}; {$[$}School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, 30332, USA{$]$}},
  comment       = {no music},
  date-added    = {2024-05-13 15:05:26 +0100},
  date-modified = {2024-05-13 15:05:26 +0100},
  doi           = {10.1109/roman.2014.6926297},
  hasabstract   = {Y},
  keywords      = {Human Perception of Robots; Emotion Recognition; Emotion Perception; Affective Computing; Human-Robot Interaction},
  la            = {en},
  priority      = {prio3},
  url           = {https://doi.org/10.1109/roman.2014.6926297},
}

@Article{manoj2015en,
  author        = {Manoj, Padmanabhan and Murali, Srinivasan and Rincon, Francisco and Atienza, David},
  title         = {Energy-aware embedded classifier design for real-time emotion analysis},
  year          = {2015},
  month         = aug,
  abstract      = {Detection and classification of human emotions from multiple bio-signals has a wide variety of applications. Though electronic devices are available in the market today that acquire multiple body signals, the classification of human emotions in real-time, adapted to the tight energy budgets of wearable embedded systems is a big challenge. In this paper we present an embedded classifier for real-time emotion classification. We propose a system that operates at different energy budgeted modes, depending on the available energy, where each mode is constrained by an operating energy bound. The classifier has an offline training phase where feature selection is performed for each operating mode, with an energy-budget aware algorithm that we propose. Across the different operating modes, the classification accuracy ranges from 95{\%} - 75{\%} and 89{\%} - 70{\%} for arousal and valence respectively. The accuracy is traded off for less power consumption, which results in an increased battery life of up to 7.7 times (from 146.1 to 1126.9 hours).},
  bdsk-url-1    = {https://doi.org/10.1109/embc.2015.7318846},
  c1            = {Embedded Systems Lab. (ESL), EPFL, Switzerland; SmartCardia Inc. Ltd, Switzerland; SmartCardia Inc. Ltd, Switzerland; Embedded Systems Lab. (ESL), EPFL, Switzerland},
  comment       = {wearable signals},
  date-added    = {2024-05-13 15:05:26 +0100},
  date-modified = {2024-05-13 15:05:26 +0100},
  doi           = {10.1109/embc.2015.7318846},
  hasabstract   = {Y},
  keywords      = {Emotion Recognition; Affective Computing; Deep Learning for EEG; Sensory Processing},
  la            = {en},
  priority      = {prio3},
  url           = {https://doi.org/10.1109/embc.2015.7318846},
}

@Article{bhatt2023ma,
  author        = {Bhatt, Priya and Sethi, Ajay and Tasgaonkar, Vaibhav and Shroff, Jugal and Pendharkar, Isha and Desai, Aditya and Sinha, Pratyush and Deshpande, Aditya and Joshi, Gargi and Rahate, Anil and Jain, Priyanka and Walambe, Rahee and Kotecha, Ketan and Jain, Nidhi},
  journal       = {Brain informatics},
  title         = {Machine learning for cognitive behavioral analysis: datasets, methods, paradigms, and research directions},
  year          = {2023},
  month         = jul,
  number        = {1},
  volume        = {10},
  abstract      = {Abstract Human behaviour reflects cognitive abilities. Human cognition is fundamentally linked to the different experiences or characteristics of consciousness/emotions, such as joy, grief, anger, etc., which assists in effective communication with others. Detection and differentiation between thoughts, feelings, and behaviours are paramount in learning to control our emotions and respond more effectively in stressful circumstances. The ability to perceive, analyse, process, interpret, remember, and retrieve information while making judgments to respond correctly is referred to as Cognitive Behavior. After making a significant mark in emotion analysis, deception detection is one of the key areas to connect human behaviour, mainly in the forensic domain. Detection of lies, deception, malicious intent, abnormal behaviour, emotions, stress, have significant roles in advanced stages of behavioral science. Artificial Intelligence and Machine learning (AI/ML) has helped a great deal in pattern recognition, data extraction and analysis, and interpretations. The goal of using AI and ML in behavioral sciences is to infer human behaviour, mainly for mental health or forensic investigations. The presented work provides an extensive review of the research on cognitive behaviour analysis. A parametric study is presented based on different physical characteristics, emotional behaviours, data collection sensing mechanisms, unimodal and multimodal datasets, modelling AI/ML methods, challenges, and future research directions.},
  bdsk-url-1    = {https://doi.org/10.1186/s40708-023-00196-6},
  c1            = {Symbiosis Institute of Technology, Symbiosis International Deemed University, Pune, India; Symbiosis Institute of Technology, Symbiosis International Deemed University, Pune, India; Symbiosis Institute of Technology, Symbiosis International Deemed University, Pune, India; Symbiosis Institute of Technology, Symbiosis International Deemed University, Pune, India; Symbiosis Institute of Technology, Symbiosis International Deemed University, Pune, India; Symbiosis Institute of Technology, Symbiosis International Deemed University, Pune, India; Symbiosis Institute of Technology, Symbiosis International Deemed University, Pune, India; Symbiosis Institute of Technology, Symbiosis International Deemed University, Pune, India; Symbiosis Institute of Technology, Symbiosis International Deemed University, Pune, India; Symbiosis Institute of Technology, Symbiosis International Deemed University, Pune, India; Centre for Development of Advanced Computing (C-DAC), Delhi, India; Symbiosis Centre for Applied Artificial Intelligence, Symbiosis International Deemed University, Pune, India; Symbiosis Centre for Applied Artificial Intelligence, Symbiosis International Deemed University, Pune, India; Centre for Development of Advanced Computing (C-DAC), Delhi, India},
  comment       = {review},
  date-added    = {2024-05-13 15:05:26 +0100},
  date-modified = {2024-05-13 15:05:26 +0100},
  doi           = {10.1186/s40708-023-00196-6},
  hasabstract   = {Y},
  isbn          = {2198-4026},
  keywords      = {Affective Computing; Emotion Recognition; Detection; Cognitive Processes; Nonverbal Behavior},
  la            = {en},
  priority      = {prio3},
  publisher     = {Springer Science+Business Media},
  url           = {https://doi.org/10.1186/s40708-023-00196-6},
}

@Article{fan2022re,
  author           = {Fan, Chengcheng and Liu, Xiang and Gu, Xiaochen and Zhou, Liang and Li, Xiaoou},
  journal          = {Journal of healthcare engineering},
  title            = {Research on Emotion Recognition of EEG Signal Based on Convolutional Neural Networks and High-Order Cross-Analysis},
  year             = {2022},
  month            = feb,
  pages            = {1--11},
  volume           = {2022},
  abstract         = {Emotion recognition means the automatic identification of a human's emotional state by obtaining his/her physiological or nonphysiological signals. The EEG-based method is an effective mechanism, which is commonly used for the recognition of emotions in real environments. In this paper, the convolutional neural network is used to classify the EEG signal into three and four emotional states under the DEAP dataset, which is defined as a Database for Emotion Analysis using physiological signals. For this purpose, a high-order cross-feature sample is extracted to recognize the emotional state with a single channel. A seven-layer convolutional neural network is used to classify the 32-channel EEG signal, and the average accuracy of four and three emotional states is 65{\%} and 58.62{\%}. The single-channel high-order cross-sample is classified with convolutional neural networks, and the average accuracy of four emotional states is 43.5{\%}. Among all the channels related to emotion recognition, the F4 channel gets the best classification accuracy of 44.25{\%}, and the average accuracy of the even number channel is higher than the odd number channel. The proposed method provides a basis for the real-time application of EEG-based emotion recognition.},
  bdsk-url-1       = {https://doi.org/10.1155/2022/6238172},
  c1               = {Department of Automation, School of Mechatronics Engineering and Automation, Key Laboratory of Power Station Automation Technology, Shanghai University, 333 Nanchen Road, Shanghai 200072, China; Shanghai University of Medicine \& Health Science, School of Medical Instrument, 257 Tianxiong Road, Pudong New District, Shanghai 201318, China; Weihai Municipal Hospital, 70 Heping Road, Huanchui District, Weihai 264200, China; Department of Automation, School of Mechatronics Engineering and Automation, Key Laboratory of Power Station Automation Technology, Shanghai University, 333 Nanchen Road, Shanghai 200072, China; Shanghai University of Medicine \& Health Science, School of Medical Instrument, 257 Tianxiong Road, Pudong New District, Shanghai 201318, China; Shanghai University of Medicine \& Health Science, School of Medical Instrument, 257 Tianxiong Road, Pudong New District, Shanghai 201318, China; Shanghai University of Medicine \& Health Science, School of Medical Instrument, 257 Tianxiong Road, Pudong New District, Shanghai 201318, China},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1155/2022/6238172},
  hasabstract      = {Y},
  isbn             = {2040-2295},
  keywords         = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Signal Processing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Hindawi Publishing Corporation},
  url              = {https://doi.org/10.1155/2022/6238172},
}

@Article{yildirim2023an,
  author           = {Yıldırım, Elif and Akbulut, Fatma and {\c C}atal, {\c C}a{\u g}atay},
  journal          = {Multimedia tools and applications},
  title            = {Analysis of facial emotion expression in eating occasions using deep learning},
  year             = {2023},
  month            = mar,
  number           = {20},
  pages            = {31659--31671},
  volume           = {82},
  abstract         = {Eating is experienced as an emotional social activity in any culture. There are factors that influence the emotions felt during food consumption. The emotion felt while eating has a significant impact on our lives and affects different health conditions such as obesity. In addition, investigating the emotion during food consumption is considered a multidisciplinary problem ranging from neuroscience to anatomy. In this study, we focus on evaluating the emotional experience of different participants during eating activities and aim to analyze them automatically using deep learning models. We propose a facial expression-based prediction model to eliminate user bias in questionnaire-based assessment systems and to minimize false entries to the system. We measured the neural, behavioral, and physical manifestations of emotions with a mobile app and recognize emotional experiences from facial expressions. In this research, we used three different situations to test whether there could be any factor other than the food that could affect a person's mood. We asked users to watch videos, listen to music or do nothing while eating. This way we found out that not only food but also external factors play a role in emotional change. We employed three Convolutional Neural Network (CNN) architectures, fine-tuned VGG16, and Deepface to recognize emotional responses during eating. The experimental results demonstrated that the fine-tuned VGG16 provides remarkable results with an overall accuracy of 77.68{\%} for recognizing the four emotions. This system is an alternative to today's survey-based restaurant and food evaluation systems.},
  bdsk-url-1       = {https://doi.org/10.1007/s11042-023-15008-6},
  c1               = {Department of Computer Engineering, Istanbul K{\"u}lt{\"u}r University, Istanbul, Turkey; Department of Computer Engineering, Istanbul K{\"u}lt{\"u}r University, Istanbul, Turkey; Department of Computer Science and Engineering, Qatar University, Doha, Qatar},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1007/s11042-023-15008-6},
  hasabstract      = {Y},
  isbn             = {1380-7501},
  keywords         = {Emotion Recognition; Affective Computing; Facial Expression; Emotional Responses; Sensory Analysis},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Springer Science+Business Media},
  url              = {https://doi.org/10.1007/s11042-023-15008-6},
}

@Article{wang2023un,
  author           = {Wang, James and Zhao, Sicheng and Wu, Chenyan and Adams, Reginald and Newman, Michelle and Shafir, Tal and Tsachor, Rachelle},
  journal          = {Proceedings of the IEEE},
  title            = {Unlocking the Emotional World of Visual Media: An Overview of the Science, Research, and Impact of Understanding Emotion},
  year             = {2023},
  month            = oct,
  number           = {10},
  pages            = {1236--1286},
  volume           = {111},
  abstract         = {The emergence of artificial emotional intelligence technology is revolutionizing the fields of computers and robotics, allowing for a new level of communication and understanding of human behavior that was once thought impossible. While recent advancements in deep learning have transformed the field of computer vision, automated understanding of evoked or expressed emotions in visual media remains in its infancy. This foundering stems from the absence of a universally accepted definition of "emotion", coupled with the inherently subjective nature of emotions and their intricate nuances. In this article, we provide a comprehensive, multidisciplinary overview of the field of emotion analysis in visual media, drawing on insights from psychology, engineering, and the arts. We begin by exploring the psychological foundations of emotion and the computational principles that underpin the understanding of emotions from images and videos. We then review the latest research and systems within the field, accentuating the most promising approaches. We also discuss the current technological challenges and limitations of emotion analysis, underscoring the necessity for continued investigation and innovation. We contend that this represents a "Holy Grail" research problem in computing and delineate pivotal directions for future inquiry. Finally, we examine the ethical ramifications of emotion-understanding technologies and contemplate their potential societal impacts. Overall, this article endeavors to equip readers with a deeper understanding of the domain of emotion analysis in visual media and to inspire further research and development in this captivating and rapidly evolving field.},
  bdsk-url-1       = {https://doi.org/10.1109/jproc.2023.3273517},
  c1               = {College of Information Sciences and Technology, The Pennsylvania State University, University Park, PA, USA; Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China; College of Information Sciences and Technology, The Pennsylvania State University, University Park, PA, USA; Department of Psychology, The Pennsylvania State University, University Park, PA, USA; Department of Psychology, The Pennsylvania State University, University Park, PA, USA; Emily Sagol Creative Arts Therapies Research Center, University of Haifa, Haifa, Israel; School of Theatre and Music, University of Illinois at Chicago, Chicago, IL, USA},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1109/jproc.2023.3273517},
  hasabstract      = {Y},
  isbn             = {0018-9219},
  keywords         = {Emotion Recognition; Affective Computing; Aesthetic Emotions; Visual Attention; Emotional Responses},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Institute of Electrical and Electronics Engineers},
  url              = {https://doi.org/10.1109/jproc.2023.3273517},
}

@Article{chavezmartinez2015ha,
  author        = {Ch{\'a}vez-Mart{\'\i}nez, Gilberto and Ru{\'\i}z-Correa, Salvador and G{\'a}tica-P{\'e}rez, Daniel},
  title         = {Happy and agreeable?},
  year          = {2015},
  month         = nov,
  abstract      = {The mobile and ubiquitous nature of conversational social video has placed video blogs among the most popular forms of online video. For this reason, there has been an increasing interest in conducting studies of human behavior from video blogs in affective and social computing. In this context, we consider the problem of mood and personality trait impression inference using verbal and nonverbal audio-visual features. Under a multi-label classification framework, we show that for both mood and personality trait binary label sets, not only the simultaneous inference of multiple labels is feasible, but also that classification accuracy increases moderately for several labels, compared to a single-label approach. The multi-label method we consider naturally exploits label correlations, which motivate our approach, and our results are consistent with models proposed in psychology to define human emotional states and personality. Our approach points to the automatic specification of co-occurring emotional states and personality, by inferring several labels at once, compared to single-label approaches. We also propose a new set of facial features, based on emotion valence from facial expressions, and analyze their suitability in the multi-label framework.},
  bdsk-url-1    = {https://doi.org/10.1145/2836041.2836051},
  c1            = {Idiap Research Institute, Switzerland; Instituto Potosino de Investigati{\'o}n Cient{\'\i}fica y Tecnol{\'o}gica, Mexico; Idiap Research Institute, {\'E}cole Polytechnique F{\'e}d{\'e}rale de Lausanne, Switzerland},
  comment       = {video},
  date-added    = {2024-05-13 15:05:26 +0100},
  date-modified = {2024-05-13 15:05:26 +0100},
  doi           = {10.1145/2836041.2836051},
  hasabstract   = {Y},
  keywords      = {Multi-label Learning; Affective Computing; Emotion Recognition; Aspect-based Sentiment Analysis},
  la            = {en},
  priority      = {prio3},
  url           = {https://doi.org/10.1145/2836041.2836051},
}

@Article{jemiolo2022da,
  author           = {Jemio{\l}o, Pawe{\l} and Storman, Dawid and Mamica, Maria and Szymkowski, Mateusz and {\.Z}abicka, Wioletta and Wojtaszek-G{\l}{\'o}wka, Magdalena and Lig{\k e}za, Antoni},
  journal          = {Sensors},
  title            = {Datasets for Automated Affect and Emotion Recognition from Cardiovascular Signals Using Artificial Intelligence---A Systematic Review},
  year             = {2022},
  month            = mar,
  number           = {7},
  pages            = {2538--2538},
  volume           = {22},
  abstract         = {Our review aimed to assess the current state and quality of publicly available datasets used for automated affect and emotion recognition (AAER) with artificial intelligence (AI), and emphasising cardiovascular (CV) signals. The quality of such datasets is essential to create replicable systems for future work to grow. We investigated nine sources up to 31 August 2020, using a developed search strategy, including studies considering the use of AI in AAER based on CV signals. Two independent reviewers performed the screening of identified records, full-text assessment, data extraction, and credibility. All discrepancies were resolved by discussion. We descriptively synthesised the results and assessed their credibility. The protocol was registered on the Open Science Framework (OSF) platform. Eighteen records out of 195 were selected from 4649 records, focusing on datasets containing CV signals for AAER. Included papers analysed and shared data of 812 participants aged 17 to 47. Electrocardiography was the most explored signal (83.33{\%} of datasets). Authors utilised video stimulation most frequently (52.38{\%} of experiments). Despite these results, much information was not reported by researchers. The quality of the analysed papers was mainly low. Researchers in the field should concentrate more on methodology.},
  bdsk-url-1       = {https://doi.org/10.3390/s22072538},
  c1               = {AGH University of Science and Technology, Faculty of Electrical Engineering, Automatics, Computer Science and Biomedical Engineering, al. A. Mickiewicza 30, 30-059 Krakow, Poland; Chair of Epidemiology and Preventive Medicine, Department of Hygiene and Dietetics, Jagiellonian University Medical College, ul. M. Kopernika 7, 31-034 Krakow, Poland; AGH University of Science and Technology, Faculty of Electrical Engineering, Automatics, Computer Science and Biomedical Engineering, al. A. Mickiewicza 30, 30-059 Krakow, Poland; AGH University of Science and Technology, Faculty of Electrical Engineering, Automatics, Computer Science and Biomedical Engineering, al. A. Mickiewicza 30, 30-059 Krakow, Poland; Students'Scientific Research Group of Systematic Reviews, Jagiellonian University Medical College, ul. M. Kopernika 7, 31-034 Krakow, Poland; Students'Scientific Research Group of Systematic Reviews, Jagiellonian University Medical College, ul. M. Kopernika 7, 31-034 Krakow, Poland; AGH University of Science and Technology, Faculty of Electrical Engineering, Automatics, Computer Science and Biomedical Engineering, al. A. Mickiewicza 30, 30-059 Krakow, Poland},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/s22072538},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Affective Computing; Emotion Recognition; Heartbeat Classification; Signal Processing; Arrhythmia Detection},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s22072538},
}

@Article{chakraborty2018to,
  author           = {Chakraborty, Prithwi and Tjondronegoro, Dian and Zhang, Ligang and Chandran, Vinod},
  journal          = {IEEE access},
  title            = {Towards Generic Modelling of Viewer Interest Using Facial Expression and Heart Rate Features},
  year             = {2018},
  month            = jan,
  pages            = {62490--62502},
  volume           = {6},
  abstract         = {Automatic detection of viewer interest while watching video contents can enable multimedia applications, such as online video streaming, to recommend contents in real time. However, there is yet a generic model for detecting viewer interest that is independent of subject and content while using noninvasive sensors in near-natural settings. This paper is the first attempt at solving this issue by investigating the feasibility of a generic model for detecting viewer interest based on facial expression and heart rate features. The proposed model adopts deep learning features, which are trained and tested using multisubjects' data across different video stimuli domains. The experimental results show that the generic model can reach a similar accuracy to a domain-specific model.},
  bdsk-url-1       = {https://doi.org/10.1109/access.2018.2874892},
  c1               = {IT Discipline, School of Business and Tourism, Southern Cross University, Bilinga, QLD, Australia; IT Discipline, School of Business and Tourism, Southern Cross University, Bilinga, QLD, Australia; School of Engineering and Technology, Central Queensland University, Brisbane, QLD, Australia; School of Electrical, Engineering and Computer Science, Queensland University of Technology, Brisbane, QLD, Australia},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1109/access.2018.2874892},
  hasabstract      = {Y},
  isbn             = {2169-3536},
  keywords         = {Visual Attention; Emotion Recognition; Feature Extraction; Video Object Segmentation; Affective Computing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Institute of Electrical and Electronics Engineers},
  url              = {https://doi.org/10.1109/access.2018.2874892},
}

@Article{garciafaura2019em,
  author           = {Garc{\'\i}a-Faura, {\'A}lvaro and Hern{\'a}ndez-Garc{\'\i}a, A. and Fern{\'a}ndez-Mart{\'\i}nez, Fernando and D{\'\i}az-de-Mar{\'\i}a, Fernando and San-Segundo, Rub{\'e}n},
  journal          = {Web intelligence},
  title            = {Emotion and attention: Audiovisual models for group-level skin response recognition in short movies},
  year             = {2019},
  month            = feb,
  number           = {1},
  pages            = {29--40},
  volume           = {17},
  abstract         = {The electrodermal activity (EDA) is a psychophysiological indicator which can be considered a somatic marker of the emotional and attentional reaction of subjects towards stimuli. EDA measurements are not biased by the cognitive process of giving an opinion or a score to characterize the subjective perception, and group-level EDA recordings integrate the reaction of the whole audience, thus reducing the signal noise. This paper contributes to the field of affective video content analysis, extending previous novel work on the use of EDA as ground truth for prediction algorithms. Here, we label short video clips according to the audience's emotion (high vs. low) and attention (increasing vs. decreasing), derived from EDA records. Then, we propose a set of low-level audiovisual descriptors and train binary classifiers that predict the emotion and attention with 75{\%} and 80{\%} accuracy, respectively. These results, along with those of previous works, reinforce the usefulness of such low-level audiovisual descriptors to model video in terms of the induced affective response.},
  bdsk-url-1       = {https://doi.org/10.3233/web-190398},
  c1               = {Department of Electrical Engineering, Universidad Polit{\'e}cnica de Madrid, Madrid, Spain. E-mails: agfaura{\char64}die.upm.es, fernando.fernandezm{\char64}upm.es, ruben.sansegundo{\char64}upm.es; Institute of Cognitive Science, Universit{\"a}t Osnabr{\"u}ck, Osnabr{\"u}ck, Germany. E-mail: ahernandez{\char64}uos.de; Department of Electrical Engineering, Universidad Polit{\'e}cnica de Madrid, Madrid, Spain. E-mails: agfaura{\char64}die.upm.es, fernando.fernandezm{\char64}upm.es, ruben.sansegundo{\char64}upm.es; Signal Theory and Communications, Universidad Carlos III de Madrid, Madrid, Spain. E-mail: fdiaz{\char64}tsc.uc3m.es; Department of Electrical Engineering, Universidad Polit{\'e}cnica de Madrid, Madrid, Spain. E-mails: agfaura{\char64}die.upm.es, fernando.fernandezm{\char64}upm.es, ruben.sansegundo{\char64}upm.es},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3233/web-190398},
  hasabstract      = {Y},
  isbn             = {2405-6456},
  keywords         = {Emotion Recognition; Audio Event Detection; Speech Emotion; Affective Computing; Facial Expression},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {IOS Press},
  url              = {https://doi.org/10.3233/web-190398},
}

@Article{jalilifard2020cl,
  author           = {Jalilifard, Amir and Rastegarnia, Amir and Pizzolato, Ednaldo and Islam, Kafiul},
  journal          = {International Journal of Power Electronics and Drive Systems/International Journal of Electrical and Computer Engineering},
  title            = {Classification of emotions induced by horror and relaxing movies using single-channel EEG recordings},
  year             = {2020},
  month            = aug,
  number           = {4},
  pages            = {3826--3826},
  volume           = {10},
  abstract         = {It has been observed from recent studies that corticolimbic Theta rhythm from EEG recordings perceived as fear or threatening scene during neural processing of visual stimuli. In additions, neural oscillations' patterns in Theta, Alpha and Beta sub-bands also play important role in brain's emotional processing. Inspired from these findings, in this paper we attempt to classify two different emotional states by analyzing single-channel EEG recordings. A video clip that can evoke 3 different emotional states: neutral, relaxation and scary is shown to 19 college-aged subjects and they were asked to score their emotional outcome by giving a number between 0 to 10 (where 0 means not scary at all and 10 means the most scary). First, recorded EEG data were preprocessed by stationary wavelet transform (SWT) based artifact removal algorithm. Then power distribution in simultaneous time-frequency domain was analyzed using short-time Fourier transform (STFT) followed by calculating the average power during each 0.2s time-segment for each brain sub-band. Finally, 46 features, as the mean power of frequency bands between 4 and 50 Hz during each time-segment, containing 689 instances{$\backslash$}textemdash for each subject {$\backslash$}textemdash were collected for classification. We found that relaxation and fear emotions evoked during watching scary and relaxing movies can be classified with average classification rate of 94.208{$\backslash$}{\%} using K-NN by applying methods and materials proposed in this paper. We also classified the dataset using SVM and we found out that K-NN classifier (when {$\backslash$}begin{\{}math{\}} k=1 {$\backslash$}end{\{}math{\}}) outperforms SVM in classifying EEG dynamics induced by horror and relaxing movies, however, for {$\backslash$}begin{\{}math{\}} K \&gt;1 {$\backslash$}end{\{}math{\}} in K-NN, SVM has better average classification rate.},
  bdsk-url-1       = {https://doi.org/10.11591/ijece.v10i4.pp3826-3838},
  c1               = {Federal University of Sao Carlos; Malayer University; Federal University of Sao Carlos; Independent University},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.11591/ijece.v10i4.pp3826-3838},
  hasabstract      = {Y},
  isbn             = {2722-256X},
  keywords         = {EEG Analysis; Emotion Recognition; Speech Emotion; Affective Computing; Deep Learning for EEG},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Institute of Advanced Engineering and Science (IAES)},
  url              = {https://doi.org/10.11591/ijece.v10i4.pp3826-3838},
}

@Article{masuda2023mu,
  author           = {Masuda, Nagisa and Yairi, Ikuko},
  journal          = {Frontiers in psychology},
  title            = {Multi-Input CNN-LSTM deep learning model for fear level classification based on EEG and peripheral physiological signals},
  year             = {2023},
  month            = jun,
  volume           = {14},
  abstract         = {Objective and accurate classification of fear levels is a socially important task that contributes to developing treatments for Anxiety Disorder, Obsessive-compulsive Disorder, Post-Traumatic Stress Disorder (PTSD), and Phobia. This study examines a deep learning model to automatically estimate human fear levels with high accuracy using multichannel EEG signals and multimodal peripheral physiological signals in the DEAP dataset. The Multi-Input CNN-LSTM classification model combining Convolutional Neural Network (CNN) and Long Sort-Term Memory (LSTM) estimated four fear levels with an accuracy of 98.79{\%} and an F1 score of 99.01{\%} in a 10-fold cross-validation. This study contributes to the following; (1) to present the possibility of recognizing fear emotion with high accuracy using a deep learning model from physiological signals without arbitrary feature extraction or feature selection, (2) to investigate effective deep learning model structures for high-accuracy fear recognition and to propose Multi-Input CNN-LSTM, and (3) to examine the model's tolerance to individual differences in physiological signals and the possibility of improving accuracy through additional learning.},
  bdsk-url-1       = {https://doi.org/10.3389/fpsyg.2023.1141801},
  c1               = {Graduate School of Science and Engineering, Sophia University, Japan; Graduate School of Science and Engineering, Sophia University, Japan},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3389/fpsyg.2023.1141801},
  hasabstract      = {Y},
  isbn             = {1664-1078},
  keywords         = {Deep Learning for EEG; Emotion Recognition; Deep Learning; Affective Computing; Epilepsy Detection},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Frontiers Media},
  url              = {https://doi.org/10.3389/fpsyg.2023.1141801},
}

@Article{sridhar2022un,
  author           = {Sridhar, Kusha and Busso, Carlos},
  journal          = {IEEE transactions on affective computing},
  title            = {Unsupervised Personalization of an Emotion Recognition System: The Unique Properties of the Externalization of Valence in Speech},
  year             = {2022},
  month            = oct,
  number           = {4},
  pages            = {1959--1972},
  volume           = {13},
  abstract         = {The prediction of valence from speech is an important, but challenging problem. The expression of valence in speech has speaker-dependent cues, which contribute to performances that are often significantly lower than the prediction of other emotional attributes such as arousal and dominance. A practical approach to improve valence prediction from speech is to adapt the models to the target speakers in the test set. Adapting a <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" > speech emotion recognition</i > (SER) system to a particular speaker is a hard problem, especially with <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" > deep neural networks</i > (DNNs), since it requires optimizing millions of parameters. This study proposes an unsupervised approach to address this problem by searching for speakers in the train set with similar acoustic patterns as the speaker in the test set. Speech samples from the selected speakers are used to create the adaptation set. This approach leverages transfer learning using pre-trained models, which are adapted with these speech samples. We propose three alternative adaptation strategies: unique speaker, oversampling and weighting approaches. These methods differ on the use of the adaptation set in the personalization of the valence models. The results demonstrate that a valence prediction model can be efficiently personalized with these unsupervised approaches, leading to relative improvements as high as 13.52{\%}.},
  bdsk-url-1       = {https://doi.org/10.1109/taffc.2022.3187336},
  c1               = {Erik Jonsson School of Engineering and Computer Science, The University of Texas at Dallas, Richardson, TX, USA; Erik Jonsson School of Engineering and Computer Science, The University of Texas at Dallas, Richardson, TX, USA},
  comment          = {no music},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1109/taffc.2022.3187336},
  hasabstract      = {Y},
  isbn             = {1949-3045},
  keywords         = {Speech Emotion; Speech Enhancement; Emotion Recognition; Audio-Visual Speech Recognition; Affective Computing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Institute of Electrical and Electronics Engineers},
  url              = {https://doi.org/10.1109/taffc.2022.3187336},
}

@Article{branco2022cl,
  author           = {Branco, Luciano and Ehteshami, A. and Azgomi, Hamid and Faghih, Rose},
  journal          = {Frontiers in computational neuroscience},
  title            = {Closed-Loop Tracking and Regulation of Emotional Valence State From Facial Electromyogram Measurements},
  year             = {2022},
  month            = mar,
  volume           = {16},
  abstract         = {Affective studies provide essential insights to address emotion recognition and tracking. In traditional open-loop structures, a lack of knowledge about the internal emotional state makes the system incapable of adjusting stimuli parameters and automatically responding to changes in the brain. To address this issue, we propose to use facial electromyogram measurements as biomarkers to infer the internal hidden brain state as feedback to close the loop. In this research, we develop a systematic way to track and control emotional valence, which codes emotions as being pleasant or obstructive. Hence, we conduct a simulation study by modeling and tracking the subject's emotional valence dynamics using state-space approaches. We employ Bayesian filtering to estimate the person-specific model parameters along with the hidden valence state, using continuous and binary features extracted from experimental electromyogram measurements. Moreover, we utilize a mixed-filter estimator to infer the secluded brain state in a real-time simulation environment. We close the loop with a fuzzy logic controller in two categories of regulation: inhibition and excitation. By designing a control action, we aim to automatically reflect any required adjustments within the simulation and reach the desired emotional state levels. Final results demonstrate that, by making use of physiological data, the proposed controller could effectively regulate the estimated valence state. Ultimately, we envision future outcomes of this research to support alternative forms of self-therapy by using wearable machine interface architectures capable of mitigating periods of pervasive emotions and maintaining daily well-being and welfare.},
  bdsk-url-1       = {https://doi.org/10.3389/fncom.2022.747735},
  c1               = {Department of Electrical and Computer Engineering, United States; Department of Electrical and Computer Engineering, United States; Department of Electrical and Computer Engineering, United States; Department of Neurological Surgery, United States; Department of Biomedical Engineering, United States; Department of Electrical and Computer Engineering, United States},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3389/fncom.2022.747735},
  hasabstract      = {Y},
  isbn             = {1662-5188},
  keywords         = {Affective Computing; Emotion Recognition; EEG Analysis; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Frontiers Media},
  url              = {https://doi.org/10.3389/fncom.2022.747735},
}

@Article{chen2022fe,
  author           = {Chen, Jing and Zhao, Zexian and Shu, Qin‐Fen and Cai, Guoen},
  journal          = {Frontiers in psychology},
  title            = {Feature extraction based on microstate sequences for EEG--based emotion recognition},
  year             = {2022},
  month            = dec,
  volume           = {13},
  abstract         = {Recognizing emotion from Electroencephalography (EEG) is a promising and valuable research issue in the field of affective brain-computer interfaces (aBCI). To improve the accuracy of emotion recognition, an emotional feature extraction method is proposed based on the temporal information in the EEG signal. This study adopts microstate analysis as a spatio-temporal analysis for EEG signals. Microstates are defined as a series of momentary quasi-stable scalp electric potential topographies. Brain electrical activity could be modeled as being composed of a time sequence of microstates. Microstate sequences provide an ideal macroscopic window for observing the temporal dynamics of spontaneous brain activity. To further analyze the fine structure of the microstate sequence, we propose a feature extraction method based on k-mer. K-mer is a k-length substring of a given sequence. It has been widely used in computational genomics and sequence analysis. We extract features that are based on the D2∗statistic of k-mer. In addition, we also extract four parameters (duration, occurrence, time coverage, GEV) of each microstate class as features at the coarse level. We conducted experiments on the DEAP dataset to evaluate the performance of the proposed features. The experimental results demonstrate that the fusion of features in fine and coarse levels can effectively improve classification accuracy.},
  bdsk-url-1       = {https://doi.org/10.3389/fpsyg.2022.1065196},
  c1               = {Faculty of Computing, Harbin Institute of Technology, Harbin, China; School of Computer Science and Engineering, Xi'an University of Technology, Xi'an, China; Department of Neurology, Zhejiang Hospital, Hangzhou, China; Department of Neurology, Zhejiang Hospital, Hangzhou, China; Department of Critical Care Medicine, Zhejiang Hospital, Hangzhou, China},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3389/fpsyg.2022.1065196},
  hasabstract      = {Y},
  isbn             = {1664-1078},
  keywords         = {Emotion Recognition; EEG Analysis; Feature Extraction; Deep Learning for EEG; Affective Computing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Frontiers Media},
  url              = {https://doi.org/10.3389/fpsyg.2022.1065196},
}

@Article{chakraborty2015us,
  author           = {Chakraborty, Prithwi and Li-gang, Zhang and Tjondronegoro, Dian and Chandran, Vinod},
  title            = {Using Viewer's Facial Expression and Heart Rate for Sports Video Highlights Detection},
  year             = {2015},
  month            = jun,
  abstract         = {Viewer interest, evoked by video content, can potentially identify the highlights of the video. This paper explores the use of facial expressions (FE) and heart rate (HR) of viewers captured using camera and non-strapped sensor for identifying interesting video segments. The data from ten subjects with three videos showed that these signals are viewer dependent and not synchronized with the video contents. To address this issue, new algorithms are proposed to effectively combine FE and HR signals for identifying the time when viewer interest is potentially high. The results show that, compared with subjective annotation and match report highlights, 'non-neutral' FE and 'relatively higher and faster' HR is able to capture 60{\%}-80{\%} of goal, foul, and shot-on-goal soccer video events. FE is found to be more indicative than HR of viewer interest, but the fusion of these two modalities outperforms each of them.},
  bdsk-url-1       = {https://doi.org/10.1145/2671188.2749361},
  c1               = {Queensland University of Technology, Brisbane, Australia; Xi'an University of Technology, Xi'an, China; Queensland University of Technology, Brisbane, Australia; Queensland University of Technology, Brisbane, Australia},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1145/2671188.2749361},
  hasabstract      = {Y},
  keywords         = {Event Detection; Feature Extraction; Emotion Recognition; Video Summarization; Affective Computing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.1145/2671188.2749361},
}

@Article{nie2017em,
  author        = {Nie, Yu and Wu, Yang and Yang, Zhong-Yao and Sun, Guangzhi and Yang, Yongjian and Hong, Xuanyi},
  title         = {Emotional Evaluation Based on SVM},
  year          = {2017},
  month         = jan,
  abstract      = {With the continuous development of the modern intelligent household, people found that brain wave can be used for controlling household appliances.EEG signal, as a typical brain wave signal carrying the brain state information, has walked into the researcher's view.Brain wave carries detailed information about the state of the brain.As a result, using computer to collect and analyze EEG signal plays a great role in smart home.This paper uses positive and negative emotional EEG signal as the research object, begins with introducing the research status of brain waves, and then uses the Chinese Affective Picture System (CAPS {$[$}12{$]$}) of Chinese Academy of Sciences, designs the watch pictures to experiment, utilizes machine learning algorithm of support vector machine (SVM) for data analysis, and obtains an accuracy of 58.3{\%} eventually.This paper provides a feasible scheme for the study of EEG in the field of emotion analysis.},
  bdsk-url-1    = {https://doi.org/10.2991/amcce-17.2017.111},
  c1            = {College of Software, Jilin University, Jilin, China; College of Software, Jilin University, Jilin, China; College of Software, Jilin University, Jilin, China; College of Software, Jilin University, Jilin, China; College of Software, Jilin University, Jilin, China; College of Software, Jilin University, Jilin, China},
  comment       = {EEG},
  date-added    = {2024-05-13 15:05:26 +0100},
  date-modified = {2024-05-13 15:05:26 +0100},
  doi           = {10.2991/amcce-17.2017.111},
  hasabstract   = {Y},
  keywords      = {Emotion Recognition; Affective Computing; Support Vector Machines; Speech Emotion; Color Psychology},
  la            = {en},
  priority      = {prio3},
  url           = {https://doi.org/10.2991/amcce-17.2017.111},
}

@Article{yvart2016sy,
  author           = {Yvart, Willy and Delestage, Charles-Alexandre and Leleu-Merviel, Sylvie},
  journal          = {Link{\"o}ping electronic conference proceedings},
  title            = {SYM: Toward a New Tool in User's Mood Determination},
  year             = {2016},
  month            = mar,
  abstract         = {Even though the emotional state is increasingly taken into account in scientific studies aimed at determining user experience of user acceptance, there are still only a few normalized tools.In this article, we decided to focus on mood determination as we consider this affective state to be more pervasive and more understandable by the person who is experiencing it.Thus, we propose a prototypical tool called SYM (Spot Your Mood) as a new tool in user mood determination to be used in many different situations.},
  bdsk-url-1       = {https://doi.org/10.3384/ecp10304},
  c1               = {(UVHC) TCTS (UMONS) Rue Michel Rondet, 59135 Wallers, FRANCE; DeVisu (UVHC) Rue Michel Rondet, 59135 Wallers, FRANCE; DeVisu (UVHC) Rue Michel Rondet, 59135 Wallers, FRANCE},
  comment          = {No relevant material, discussed},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3384/ecp10304},
  hasabstract      = {Y},
  isbn             = {1650-3686},
  keywords         = {Emotion Recognition; Affective Computing; Emotion Regulation; Speech Emotion; Emotions},
  la               = {en},
  modificationdate = {2024-05-16T12:08:55},
  priority         = {prio3},
  publisher        = {Link{\"o}ping University Electronic Press},
  url              = {https://doi.org/10.3384/ecp10304},
}

@Article{yue2021ma,
  author        = {Yue, Hua and Zhong, Xiaolong and Zhang, Bingxue and Yin, Zhong and Zhang, Jianhua},
  journal       = {Brain sciences},
  title         = {Manifold Feature Fusion with Dynamical Feature Selection for Cross-Subject Emotion Recognition},
  year          = {2021},
  month         = oct,
  number        = {11},
  pages         = {1392--1392},
  volume        = {11},
  abstract      = {Affective computing systems can decode cortical activities to facilitate emotional human-computer interaction. However, personalities exist in neurophysiological responses among different users of the brain-computer interface leads to a difficulty for designing a generic emotion recognizer that is adaptable to a novel individual. It thus brings an obstacle to achieve cross-subject emotion recognition (ER). To tackle this issue, in this study we propose a novel feature selection method, manifold feature fusion and dynamical feature selection (MF-DFS), under transfer learning principle to determine generalizable features that are stably sensitive to emotional variations. The MF-DFS framework takes the advantages of local geometrical information feature selection, domain adaptation based manifold learning, and dynamical feature selection to enhance the accuracy of the ER system. Based on three public databases, DEAP, MAHNOB-HCI and SEED, the performance of the MF-DFS is validated according to the leave-one-subject-out paradigm under two types of electroencephalography features. By defining three emotional classes of each affective dimension, the accuracy of the MF-DFS-based ER classifier is achieved at 0.50-0.48 (DEAP) and 0.46-0.50 (MAHNOBHCI) for arousal and valence emotional dimensions, respectively. For the SEED database, it achieves 0.40 for the valence dimension. The corresponding accuracy is significantly superior to several classical feature selection methods on multiple machine learning models.},
  bdsk-url-1    = {https://doi.org/10.3390/brainsci11111392},
  c1            = {Engineering Research Center of Optical Instrument and System, Ministry of Education, Shanghai Key Lab of Modern Optical System, University of Shanghai for Science and Technology, Shanghai 200093, China; Engineering Research Center of Optical Instrument and System, Ministry of Education, Shanghai Key Lab of Modern Optical System, University of Shanghai for Science and Technology, Shanghai 200093, China; School of Optical-Electrical and Computer Engineering, University of Shanghai for Science and Technology, Shanghai 200093, China; Engineering Research Center of Optical Instrument and System, Ministry of Education, Shanghai Key Lab of Modern Optical System, University of Shanghai for Science and Technology, Shanghai 200093, China; OsloMet Artificial Intelligence Lab, Department of Computer Science, Oslo Metropolitan University, N-0130 Oslo, Norway},
  comment       = {EEG},
  date-added    = {2024-05-13 15:05:26 +0100},
  date-modified = {2024-05-13 15:05:26 +0100},
  doi           = {10.3390/brainsci11111392},
  hasabstract   = {Y},
  isbn          = {2076-3425},
  keywords      = {Affective Computing; Emotion Recognition; Feature Extraction; Human-Computer Interaction; Aspect-based Sentiment Analysis},
  la            = {en},
  priority      = {prio3},
  publisher     = {Multidisciplinary Digital Publishing Institute},
  url           = {https://doi.org/10.3390/brainsci11111392},
}

@Article{ni2021ad,
  author           = {Ni, Tongguang and Ni, Yuyao and Xue, Jing and Wang, Suhong},
  journal          = {Frontiers in psychology},
  title            = {A Domain Adaptation Sparse Representation Classifier for Cross-Domain Electroencephalogram-Based Emotion Classification},
  year             = {2021},
  month            = jul,
  volume           = {12},
  abstract         = {The brain-computer interface (BCI) interprets the physiological information of the human brain in the process of consciousness activity. It builds a direct information transmission channel between the brain and the outside world. As the most common non-invasive BCI modality, electroencephalogram (EEG) plays an important role in the emotion recognition of BCI; however, due to the individual variability and non-stationary of EEG signals, the construction of EEG-based emotion classifiers for different subjects, different sessions, and different devices is an important research direction. Domain adaptation utilizes data or knowledge from more than one domain and focuses on transferring knowledge from the source domain (SD) to the target domain (TD), in which the EEG data may be collected from different subjects, sessions, or devices. In this study, a new domain adaptation sparse representation classifier (DASRC) is proposed to address the cross-domain EEG-based emotion classification. To reduce the differences in domain distribution, the local information preserved criterion is exploited to project the samples from SD and TD into a shared subspace. A common domain-invariant dictionary is learned in the projection subspace so that an inherent connection can be built between SD and TD. In addition, both principal component analysis (PCA) and Fisher criteria are exploited to promote the recognition ability of the learned dictionary. Besides, an optimization method is proposed to alternatively update the subspace and dictionary learning. The comparison of CSFDDL shows the feasibility and competitive performance for cross-subject and cross-dataset EEG-based emotion classification problems.},
  bdsk-url-1       = {https://doi.org/10.3389/fpsyg.2021.721266},
  c1               = {School of Computer Science and Artificial Intelligence, Changzhou University, China; School of Electrical Engineering, Xi'an Jiaotong University, China; Department of Nephrology, Affiliated Wuxi People's Hospital of Nanjing Medical University, China; Department of Clinical Psychology, The Third Affiliated Hospital of Soochow University, China},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3389/fpsyg.2021.721266},
  hasabstract      = {Y},
  isbn             = {1664-1078},
  keywords         = {Emotion Recognition; Brain-Computer Interfaces; EEG Analysis; Deep Learning for EEG; Affective Computing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Frontiers Media},
  url              = {https://doi.org/10.3389/fpsyg.2021.721266},
}

@Article{akbulut2022ev,
  author           = {Akbulut, Fatma},
  journal          = {{\.I}stanbul ticaret {\"u}niversitesi fen bilimleri dergisi},
  title            = {EVALUATING THE EFFECTS OF THE AUTONOMIC NERVOUS SYSTEM AND SYMPATHETIC ACTIVITY ON EMOTIONAL STATES},
  year             = {2022},
  month            = jun,
  number           = {41},
  pages            = {156--169},
  volume           = {21},
  abstract         = {Emotion recognition has attracted more interest by being applied in many application areas from different domains such as medical diagnosis, e-commerce, and robotics. This research quantifies the stimulated short-term effect of emotions on the autonomic nervous system and sympathetic activity. The primary purpose of this study is to investigate the responses of 21 adults by attaching a wearable system to measure physiological data such as an electrocardiogram and electrodermal activity in a controlled environment. Cardiovascular effects were evaluated with heart rate variability indices that included HR, HRV triangular-index, rMSSD (ms), pNN5O ({\%}); frequency analysis of the very low frequency (VLF: 0-0, 04 Hz), low frequency (LF: 0, 04-0, 15 Hz), and high frequency (HF: 0, 15-0, 4 Hz) components; nonlinear analysis. The sympathetic activity was evaluated with time-varying and time-invariant spectral analysis results of the EDA. The participants who experience calmness had a 4, 8{\%} lower heart rate (75, 06$\pm$16, 76 and 78, 72$\pm$16, 52) observed compared to happiness. Negative valance with high-arousal emotions like anger was invariably responded to with a peak in skin conductance level. Besides, negative valance with low-arousal emotions like sadness was allied with a drop in conductance level. Anger, in addition to being the most well-known emotion, elicited coherent time-varying spectral responses.},
  bdsk-url-1       = {https://doi.org/10.55071/ticaretfbd.1125431},
  c1               = {ISTANBUL KULTUR UNIVERSITY},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.55071/ticaretfbd.1125431},
  hasabstract      = {Y},
  isbn             = {1305-7820},
  keywords         = {Emotion Regulation; Emotion Recognition; Heart Rate Variability; Affective Computing; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.55071/ticaretfbd.1125431},
}

@Article{hosseini2023pe,
  author           = {Hosseini, Mohammad and Firoozabadi, Seyed and Badie, Kambiz and Azadfallah, Parviz},
  journal          = {Brain sciences},
  title            = {Personality-Based Emotion Recognition Using EEG Signals with a CNN-LSTM Network},
  year             = {2023},
  month            = jun,
  number           = {6},
  pages            = {947--947},
  volume           = {13},
  abstract         = {The accurate detection of emotions has significant implications in healthcare, psychology, and human-computer interaction. Integrating personality information into emotion recognition can enhance its utility in various applications. The present study introduces a novel deep learning approach to emotion recognition, which utilizes electroencephalography (EEG) signals and the Big Five personality traits. The study recruited 60 participants and recorded their EEG data while they viewed unique sequence stimuli designed to effectively capture the dynamic nature of human emotions and personality traits. A pre-trained convolutional neural network (CNN) was used to extract emotion-related features from the raw EEG data. Additionally, a long short-term memory (LSTM) network was used to extract features related to the Big Five personality traits. The network was able to accurately predict personality traits from EEG data. The extracted features were subsequently used in a novel network to predict emotional states within the arousal and valence dimensions. The experimental results showed that the proposed classifier outperformed common classifiers, with a high accuracy of 93.97{\%}. The findings suggest that incorporating personality traits as features in the designed network, for emotion recognition, leads to higher accuracy, highlighting the significance of examining these traits in the analysis of emotions.},
  bdsk-url-1       = {https://doi.org/10.3390/brainsci13060947},
  c1               = {Department of Biomedical Engineering, Science and Research Branche, Islamic Azad University, Tehran 14778-93855, Iran; Department of Medical Physics, Faculty of Medicine, Tarbiat Modares University, Tehran 14117-13116, Iran; Content \& E-Services Research Group, IT Research Faculty, ICT Research Institute, Tehran 14399-55471, Iran; Department of Psychology, Faculty of Humanities, Tarbiat Modares University, Tehran 14117-13116, Iran},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/brainsci13060947},
  hasabstract      = {Y},
  isbn             = {2076-3425},
  keywords         = {Emotion Recognition; Deep Learning for EEG; Affective Computing; EEG Analysis; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/brainsci13060947},
}

@Article{kadiri2022su,
  author           = {Kadiri, Sudarsana and Alku, Paavo},
  journal          = {Sensors},
  title            = {Subjective Evaluation of Basic Emotions from Audio--Visual Data},
  year             = {2022},
  month            = jun,
  number           = {13},
  pages            = {4931--4931},
  volume           = {22},
  abstract         = {Understanding of the perception of emotions or affective states in humans is important to develop emotion-aware systems that work in realistic scenarios. In this paper, the perception of emotions in naturalistic human interaction (audio-visual data) is studied using perceptual evaluation. For this purpose, a naturalistic audio-visual emotion database collected from TV broadcasts such as soap-operas and movies, called the IIIT-H Audio-Visual Emotion (IIIT-H AVE) database, is used. The database consists of audio-alone, video-alone, and audio-visual data in English. Using data of all three modes, perceptual tests are conducted for four basic emotions (angry, happy, neutral, and sad) based on category labeling and for two dimensions, namely arousal (active or passive) and valence (positive or negative), based on dimensional labeling. The results indicated that the participants' perception of emotions was remarkably different between the audio-alone, and audio-video data. This finding emphasizes the importance of emotion-specific features compared to commonly used features in the development of emotion-aware systems.},
  bdsk-url-1       = {https://doi.org/10.3390/s22134931},
  c1               = {Department of Signal Processing and Acoustics, Aalto University, Otakaari 3, FI-00076 Espoo, Finland; Department of Signal Processing and Acoustics, Aalto University, Otakaari 3, FI-00076 Espoo, Finland},
  comment          = {no music, video, discussed},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/s22134931},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Emotion Recognition; Affective Computing; Sensory Expectations; Speech Emotion; Audio Event Detection},
  la               = {en},
  modificationdate = {2024-05-16T12:07:34},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s22134931},
}

@Article{oates2019ro,
  author           = {Oates, C. and Triantafyllopoulos, Andreas and Steiner, Ingmar and Schuller, Bj{\"o}rn},
  title            = {Robust Speech Emotion Recognition Under Different Encoding Conditions},
  year             = {2019},
  month            = sep,
  abstract         = {In an era where large speech corpora annotated for emotion are hard to come by, and especially ones where emotion is expressed freely instead of being acted, the importance of using free online sources for collecting such data cannot be overstated.Most of those sources, however, contain encoded audio due to storage and bandwidth constraints, often in very low bitrates.In addition, with the increased industry interest on voice-based applications, it is inevitable that speech emotion recognition (SER) algorithms will soon find their way into production environments, where the audio might be encoded in a different bitrate than the one available during training.Our contribution is threefold.First, we show that encoded audio still contains enough relevant information for robust SER.Next, we investigate the effects of mismatched encoding conditions in the training and test set both for traditional machine learning algorithms built on hand-crafted features and modern end-toend methods.Finally, we investigate the robustness of those algorithms in the multi-condition scenario, where the training set is augmented with encoded audio, but still differs from the training set.Our results indicate that end-to-end methods are more robust even in the more challenging scenario of mismatched conditions.},
  bdsk-url-1       = {https://doi.org/10.21437/interspeech.2019-1658},
  c1               = {audEERING GmbH, Gilching, Germany; audEERING GmbH, Gilching, Germany; audEERING GmbH, Gilching, Germany; GLAM -Group on Language, Audio \& Music, Imperial College London, UK; ZD.B Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany; audEERING GmbH, Gilching, Germany},
  comment          = {No music, discussed},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.21437/interspeech.2019-1658},
  hasabstract      = {Y},
  keywords         = {Emotion Recognition; Audio-Visual Speech Recognition; Speech Emotion; Emotional Responses; Affective Computing},
  la               = {en},
  modificationdate = {2024-05-16T12:02:36},
  priority         = {prio3},
  url              = {https://doi.org/10.21437/interspeech.2019-1658},
}

@Article{kshirsagar2022cr,
  author           = {Kshirsagar, Shruti and Falk, Tiago},
  journal          = {Sensors},
  title            = {Cross-Language Speech Emotion Recognition Using Bag-of-Word Representations, Domain Adaptation, and Data Augmentation},
  year             = {2022},
  month            = aug,
  number           = {17},
  pages            = {6445--6445},
  volume           = {22},
  abstract         = {To date, several methods have been explored for the challenging task of cross-language speech emotion recognition, including the bag-of-words (BoW) methodology for feature processing, domain adaptation for feature distribution "normalization", and data augmentation to make machine learning algorithms more robust across testing conditions. Their combined use, however, has yet to be explored. In this paper, we aim to fill this gap and compare the benefits achieved by combining different domain adaptation strategies with the BoW method, as well as with data augmentation. Moreover, while domain adaptation strategies, such as the correlation alignment (CORAL) method, require knowledge of the test data language, we propose a variant that we term N-CORAL, in which test languages (in our case, Chinese) are mapped to a common distribution in an unsupervised manner. Experiments with German, French, and Hungarian language datasets were performed, and the proposed N-CORAL method, combined with BoW and data augmentation, was shown to achieve the best arousal and valence prediction accuracy, highlighting the usefulness of the proposed method for "in the wild" speech emotion recognition. In fact, N-CORAL combined with BoW was shown to provide robustness across languages, whereas data augmentation provided additional robustness against cross-corpus nuance factors.},
  bdsk-url-1       = {https://doi.org/10.3390/s22176445},
  c1               = {Institut National de la Recherche Scientifique, University of Quebec, Montr{\'e}al, QC H3C 5J9, Canada; Institut National de la Recherche Scientifique, University of Quebec, Montr{\'e}al, QC H3C 5J9, Canada},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/s22176445},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Emotion Recognition; Speech Emotion; Affective Computing; End-to-End Speech Recognition; Statistical Language Modeling},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s22176445},
}

@Article{mohammad2023ab,
  author           = {Mohammad, Farah and Hussain, Muhammad and Aboalsamh, Hatim},
  journal          = {Diagnostics},
  title            = {A Bimodal Emotion Recognition Approach through the Fusion of Electroencephalography and Facial Sequences},
  year             = {2023},
  month            = mar,
  number           = {5},
  pages            = {977--977},
  volume           = {13},
  abstract         = {In recent years, human-computer interaction (HCI) systems have become increasingly popular. Some of these systems demand particular approaches for discriminating actual emotions through the use of better multimodal methods. In this work, a deep canonical correlation analysis (DCCA) based multimodal emotion recognition method is presented through the fusion of electroencephalography (EEG) and facial video clips. A two-stage framework is implemented, where the first stage extracts relevant features for emotion recognition using a single modality, while the second stage merges the highly correlated features from the two modalities and performs classification. Convolutional neural network (CNN) based Resnet50 and 1D-CNN (1-Dimensional CNN) have been utilized to extract features from facial video clips and EEG modalities, respectively. A DCCA-based approach was used to fuse highly correlated features, and three basic human emotion categories (happy, neutral, and sad) were classified using the SoftMax classifier. The proposed approach was investigated based on the publicly available datasets called MAHNOB-HCI and DEAP. Experimental results revealed an average accuracy of 93.86{\%} and 91.54{\%} on the MAHNOB-HCI and DEAP datasets, respectively. The competitiveness of the proposed framework and the justification for exclusivity in achieving this accuracy were evaluated by comparison with existing work.},
  bdsk-url-1       = {https://doi.org/10.3390/diagnostics13050977},
  c1               = {Department of Computer Science, College of Computer Science and Information, King Saud University, Riyadh 11451, Saudi Arabia; Department of Computer Science, College of Computer Science and Information, King Saud University, Riyadh 11451, Saudi Arabia; Department of Computer Science, College of Computer Science and Information, King Saud University, Riyadh 11451, Saudi Arabia},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/diagnostics13050977},
  hasabstract      = {Y},
  isbn             = {2075-4418},
  keywords         = {Emotion Recognition; Affective Computing; Head Gesture Recognition; Human-Computer Interaction; Multimodal Data},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/diagnostics13050977},
}

@Article{haratian2018on,
  author           = {Haratian, Roya and Timotijevic, Tijana},
  title            = {On-body Sensing and Signal Analysis for User Experience Recognition in Human-Machine Interaction},
  year             = {2018},
  month            = sep,
  abstract         = {In this paper, a new algorithm is proposed for recognition of user experience through emotion detection using physiological signals, for application in human-machine interaction. The algorithm recognizes user's emotion quality and intensity in a two dimensional emotion space continuously. The continuous recognition of the user's emotion during human-machine interaction will enable the machine to adapt its activity based on the user's emotion in a real-time manner, thus improving user experience. The emotion model underlying the proposed algorithm is one of the most recent emotion models, which models emotion's intensity and quality in a continuous two-dimensional space of valance and arousal axes. Using only two physiological signals, which are correlated to the valance and arousal axes of the emotion space, is among the contributions of this paper. Prediction of emotion through physiological signals has the advantage of elimination of social masking and making the prediction more reliable. The key advantage of the proposed algorithm over other algorithms presented to date is the use of the least number of modalities (only two physiological signals) to predict the quality and intensity of emotion continuously in time, and using the most recent widely accepted emotion model.},
  bdsk-url-1       = {https://doi.org/10.1109/icfsp.2018.8552058},
  c1               = {Faculty of Science and Technology, Bournemouth University, Poole, UK; School of Electronic Eng. and Computer Science, Queen Mary University of London, London, UK},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1109/icfsp.2018.8552058},
  hasabstract      = {Y},
  keywords         = {Emotion Recognition; Affective Computing; Human-Computer Interaction; Physiological Signals; Emotions},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.1109/icfsp.2018.8552058},
}

@Article{guo2018fe,
  author           = {Guo, Chenfeng and Wu, Dongrui},
  title            = {Feature Dimensionality Reduction for Video Affect Classification: A Comparative Study},
  year             = {2018},
  month            = may,
  abstract         = {Affective computing has become a very important research area in human-machine interaction. However, affects are subjective, subtle, and uncertain. So, it is very difficult to obtain a large number of labeled training samples, compared with the number of possible features we could extract. Thus, dimensionality reduction is critical in affective computing. This paper presents our preliminary study on dimensionality reduction for affect classification. Five popular dimensionality reduction approaches are introduced and compared. Experiments on the DEAP dataset showed that no approach can universally outperform others, and performing classification using the raw features directly may not always be a bad choice.},
  bdsk-url-1       = {https://doi.org/10.1109/aciiasia.2018.8470329},
  c1               = {School of Printing and Packaging, Wuhan University, Wuhan, China; School of Automation, Huazhong University of Science and Technology, Wuhan, China},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1109/aciiasia.2018.8470329},
  hasabstract      = {Y},
  keywords         = {Affective Computing; Affective Design; Emotion Recognition; Feature Extraction; Aspect-based Sentiment Analysis},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.1109/aciiasia.2018.8470329},
}

@Article{kurbalija2015cu,
  author           = {Kurbalija, Vladimir and Ivanovi{\'c}, Mirjana and Radovanovi{\'c}, Milo{\v s} and Geler, Zoltan and Mitrovi{\'c}, Darko and Dai, Weihui and Zhang, Weidong},
  title            = {Cultural Differences and Similarities in Emotion Recognition},
  year             = {2015},
  month            = sep,
  abstract         = {The electroencephalogram (EEG) is a powerful method for investigation of different cognitive processes. Recently, EEG analysis became very popular and important, where classification of these signals stands out as one of the mostly used methodologies. Emotion recognition is one of the most challenging tasks in EEG analysis since not much is known about representation of different emotion in EEG signals. In addition, inducing of desired emotion is by itself difficult, since various individuals react differently to external stimuli (audio, video, etc.) In this paper, we will examine the similarities in emotion perception of different individuals on the basis of audio stimuli. Since some of the participants in the experiment did not understand the language of the stimuli, we will also investigate the impact of language understanding on emotion perception. This study presents some preliminary results of more complex experiments in the area of affective computing that are planned.},
  bdsk-url-1       = {https://doi.org/10.1145/2801081.2801093},
  c1               = {Department of Mathematics and Informatics, Faculty of Science, University of Novi Sad, Trg Dositeja Obradovi{\'c}a 4, 21000 Novi Sad, Serbia, +381 21 4852877; Department of Mathematics and Informatics, Faculty of Science, University of Novi Sad, Trg Dositeja Obradovi{\'c}a 4, 21000 Novi Sad, Serbia, +381 21 4852855; Department of Mathematics and Informatics, Faculty of Science, University of Novi Sad, Trg Dositeja Obradovi{\'c}a 4, 21000 Novi Sad, Serbia, +381 21 4852875; Department of Media Studies, Faculty of Philosophy, University of Novi Sad, Dr Zorana Đinđi{\'c}a 2, 21000 Novi Sad, Serbia, +381 21 4853918; Department of Mathematics and Informatics, Faculty of Science, University of Novi Sad, Trg Dositeja Obradovi{\'c}a 4, 21000 Novi Sad, Serbia, +381 21 4852875; School of Management, Fudan University, Shanghai 200433, China; School of Software, Fudan University, Shanghai 200433, China},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1145/2801081.2801093},
  hasabstract      = {Y},
  keywords         = {Emotion Recognition; Speech Emotion; Audio Event Detection; Environmental Sound Recognition; Affective Computing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.1145/2801081.2801093},
}

@Article{mcintosh2021ex,
  author        = {Mcintosh, Tyler},
  title         = {Exploring the Relationship Between Music and Emotions with Machine Learning},
  year          = {2021},
  month         = jul,
  bdsk-url-1    = {https://doi.org/10.14236/ewic/eva2021.49},
  c1            = {University of Greenwich London, UK},
  date-added    = {2024-05-13 15:05:26 +0100},
  date-modified = {2024-05-13 15:05:26 +0100},
  doi           = {10.14236/ewic/eva2021.49},
  hasabstract   = {N},
  keywords      = {Emotion Recognition; Music Information Retrieval; Melody Extraction; Affective Computing; Audio Event Detection},
  la            = {en},
  priority      = {prio1},
  url           = {https://doi.org/10.14236/ewic/eva2021.49},
}

@Article{abayomi2021re,
  author           = {Abayomi, Abdultaofeek and Olugbara, Oludayo and Heukelman, Delene},
  journal          = {International journal of integrated engineering/International Journal of Integrated Engineering},
  title            = {Recognition of Human Emotion using Radial Basis Function Neural Networks with Inverse Fisher Transformed Physiological Signals},
  year             = {2021},
  month            = aug,
  number           = {6},
  volume           = {13},
  abstract         = {Emotion is a complex state of human mind influenced by body physiological changes and interdependent external events thus making an automatic recognition of emotional state a challenging task. A number of recognition methods have been applied in recent years to recognize human emotion. The motivation for this study is therefore to discover a combination of emotion features and recognition method that will produce the best result in building an efficient emotion recognizer in an affective system. We introduced a shifted tanh normalization scheme to realize the inverse Fisher transformation applied to the DEAP physiological dataset and consequently performed series of experiments using the Radial Basis Function Artificial Neural Networks (RBFANN). In our experiments, we have compared the performances of digital image based feature extraction techniques such as the Histogram of Oriented Gradient (HOG), Local Binary Pattern (LBP) and the Histogram of Images (HIM). These feature extraction techniques were utilized to extract discriminatory features from the multimodal DEAP dataset of physiological signals. Experimental results obtained indicate that the best recognition accuracy was achieved with the EEG modality data using the HIM features extraction technique and classification done along the dominance emotion dimension. The result is very remarkable when compared with existing results in the literature including deep learning studies that have utilized the DEAP corpus and also applicable to diverse fields of engineering studies.},
  bdsk-url-1       = {https://doi.org/10.30880/ijie.2021.13.06.001},
  c1               = {Durban University of Technology; Durban University of Technology, Durban, South Africa.; Durban University of Technology, Durban, South Africa.},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.30880/ijie.2021.13.06.001},
  hasabstract      = {Y},
  isbn             = {2229-838X},
  keywords         = {Emotion Recognition; Affective Computing; Deep Learning for EEG; Feature Extraction; Signal Processing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Penerbit UTHM},
  url              = {https://doi.org/10.30880/ijie.2021.13.06.001},
}

@Article{duan2023as,
  author           = {Duan, Danting and Zhong, Wei and Ran, Shuang and Ye, Long and Zhang, Qin},
  journal          = {PloS one},
  title            = {A standardized database of Chinese emotional short videos based on age and gender differences},
  year             = {2023},
  month            = mar,
  number           = {3},
  pages            = {e0283573--e0283573},
  volume           = {18},
  abstract         = {Most of the existing emotion elicitation databases use the film clips as stimuli and do not take into account the age and gender differences of participants. Considering the short videos have the advantages of short in time, easy to understand and strong emotional appeal, we choose them to construct a standardized database of Chinese emotional short videos by the joint analysis of age and gender differences. Two experiments are performed to establish and validate our database. In the Experiment 1, we selected 240 stimuli from 2700 short videos and analyzed the subjective evaluation results of 360 participants with different ages and genders. As a result, a total of 54 short videos with three categories of emotions were picked out for 6 groups of participants, including the male and female respectively aged in 20-24, 25-29 and 30-34. In the Experiment 2, we recorded the EEG signals and subjective experience scores of 81 participants while watching different video stimuli. Both the results of EEG emotion recognition and subjective evaluation indicate that our database of 54 short videos can achieve better emotion elicitation effects compared with film clips. Furthermore, the targeted delivery of specific short videos has also been verified to be effective, helping the researchers choose appropriate emotional elicitation stimuli for different participants and promoting the study of individual differences in emotion responses.},
  bdsk-url-1       = {https://doi.org/10.1371/journal.pone.0283573},
  c1               = {Key Laboratory of Media Audio \& Video, Communication University of China, Beijing, China; State Key Laboratory of Media Convergence and Communication, Communication University of China, Beijing, China; Key Laboratory of Media Audio \& Video, Communication University of China, Beijing, China; State Key Laboratory of Media Convergence and Communication, Communication University of China, Beijing, China; State Key Laboratory of Media Convergence and Communication, Communication University of China, Beijing, China},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1371/journal.pone.0283573},
  hasabstract      = {Y},
  isbn             = {1932-6203},
  keywords         = {Emotion Recognition; Affective Computing; Emotion Regulation},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Public Library of Science},
  url              = {https://doi.org/10.1371/journal.pone.0283573},
}

@Article{zawadzka2021gr,
  author        = {Zawadzka, Teresa and Wierci{\'n}ski, Tomasz and Meller, Grzegorz and Rock, Mateusz and Zwierzycki, Robert and Wr{\'o}bel, Micha{\l}},
  journal       = {Sensors},
  title         = {Graph Representation Integrating Signals for Emotion Recognition and Analysis},
  year          = {2021},
  month         = jun,
  number        = {12},
  pages         = {4035--4035},
  volume        = {21},
  abstract      = {Data reusability is an important feature of current research, just in every field of science. Modern research in Affective Computing, often rely on datasets containing experiments-originated data such as biosignals, video clips, or images. Moreover, conducting experiments with a vast number of participants to build datasets for Affective Computing research is time-consuming and expensive. Therefore, it is extremely important to provide solutions allowing one to (re)use data from a variety of sources, which usually demands data integration. This paper presents the Graph Representation Integrating Signals for Emotion Recognition and Analysis (GRISERA) framework, which provides a persistent model for storing integrated signals and methods for its creation. To the best of our knowledge, this is the first approach in Affective Computing field that addresses the problem of integrating data from multiple experiments, storing it in a consistent way, and providing query patterns for data retrieval. The proposed framework is based on the standardized graph model, which is known to be highly suitable for signal processing purposes. The validation proved that data from the well-known AMIGOS dataset can be stored in the GRISERA framework and later retrieved for training deep learning models. Furthermore, the second case study proved that it is possible to integrate signals from multiple sources (AMIGOS, ASCERTAIN, and DEAP) into GRISERA and retrieve them for further statistical analysis.},
  bdsk-url-1    = {https://doi.org/10.3390/s21124035},
  c1            = {Faculty of Electronics, Telecommunications and Informatics, Gda{\'n}sk University of Technology, 80-233 Gda{\'n}sk, Poland; Faculty of Electronics, Telecommunications and Informatics, Gda{\'n}sk University of Technology, 80-233 Gda{\'n}sk, Poland; Faculty of Electronics, Telecommunications and Informatics, Gda{\'n}sk University of Technology, 80-233 Gda{\'n}sk, Poland; Faculty of Electronics, Telecommunications and Informatics, Gda{\'n}sk University of Technology, 80-233 Gda{\'n}sk, Poland; Faculty of Electronics, Telecommunications and Informatics, Gda{\'n}sk University of Technology, 80-233 Gda{\'n}sk, Poland; Faculty of Electronics, Telecommunications and Informatics, Gda{\'n}sk University of Technology, 80-233 Gda{\'n}sk, Poland},
  comment       = {no music},
  date-added    = {2024-05-13 15:05:26 +0100},
  date-modified = {2024-05-13 15:05:26 +0100},
  doi           = {10.3390/s21124035},
  hasabstract   = {Y},
  isbn          = {1424-8220},
  keywords      = {Affective Computing; Emotion Recognition},
  la            = {en},
  priority      = {prio3},
  publisher     = {Multidisciplinary Digital Publishing Institute},
  url           = {https://doi.org/10.3390/s21124035},
}

@Article{liu2023em,
  author           = {Liu, Yucheng and Jia, Ziyang and Wang, H.},
  title            = {EmotionKD: A Cross-Modal Knowledge Distillation Framework for Emotion Recognition Based on Physiological Signals},
  year             = {2023},
  month            = oct,
  abstract         = {Emotion recognition using multi-modal physiological signals is an emerging field in affective computing that significantly improves performance compared to unimodal approaches. The combination of Electroencephalogram(EEG) and Galvanic Skin Response(GSR) signals are particularly effective for objective and complementary emotion recognition. However, the high cost and inconvenience of EEG signal acquisition severely hinder the popularity of multi-modal emotion recognition in real-world scenarios, while GSR signals are easier to obtain. To address this challenge, we propose EmotionKD, a framework for cross-modal knowledge distillation that simultaneously models the heterogeneity and interactivity of GSR and EEG signals under a unified framework. By using knowledge distillation, fully fused multi-modal features can be transferred to an unimodal GSR model to improve performance. Additionally, an adaptive feedback mechanism is proposed to enable the multi-modal model to dynamically adjust according to the performance of the unimodal model during knowledge distillation, which guides the unimodal model to enhance its performance in emotion recognition. Our experiment results demonstrate that the proposed model achieves state-of-the-art performance on two public datasets. Furthermore, our approach has the potential to reduce reliance on multi-modal data with lower sacrificed performance, making emotion recognition more applicable and feasible. The source code is available at https://github.com/YuchengLiu-Alex/EmotionKD},
  bdsk-url-1       = {https://doi.org/10.1145/3581783.3612277},
  c1               = {Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Tsinghua-Berkeley Shenzhen Institute, Shenzhen , China},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1145/3581783.3612277},
  hasabstract      = {Y},
  keywords         = {Emotion Recognition; Deep Learning for EEG; Affective Computing; Speech Emotion; EEG Analysis},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.1145/3581783.3612277},
}

@Article{smith2022th,
  author           = {Smith, Rebecca and Cross, Emily},
  journal          = {Psychological research},
  title            = {The McNorm library: creating and validating a new library of emotionally expressive whole body dance movements},
  year             = {2022},
  month            = apr,
  number           = {2},
  pages            = {484--508},
  volume           = {87},
  abstract         = {The ability to exchange affective cues with others plays a key role in our ability to create and maintain meaningful social relationships. We express our emotions through a variety of socially salient cues, including facial expressions, the voice, and body movement. While significant advances have been made in our understanding of verbal and facial communication, to date, understanding of the role played by human body movement in our social interactions remains incomplete. To this end, here we describe the creation and validation of a new set of emotionally expressive whole-body dance movement stimuli, named the Motion Capture Norming (McNorm) Library, which was designed to reconcile a number of limitations associated with previous movement stimuli. This library comprises a series of point-light representations of a dancer's movements, which were performed to communicate to observers neutrality, happiness, sadness, anger, and fear. Based on results from two validation experiments, participants could reliably discriminate the intended emotion expressed in the clips in this stimulus set, with accuracy rates up to 60{\%} (chance = 20{\%}). We further explored the impact of dance experience and trait empathy on emotion recognition and found that neither significantly impacted emotion discrimination. As all materials for presenting and analysing this movement library are openly available, we hope this resource will aid other researchers in further exploration of affective communication expressed by human bodily movement.},
  bdsk-url-1       = {https://doi.org/10.1007/s00426-022-01669-9},
  c1               = {Institute of Neuroscience and Psychology, University of Glasgow, Glasgow, Scotland; Institute of Neuroscience and Psychology, University of Glasgow, Glasgow, Scotland},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1007/s00426-022-01669-9},
  hasabstract      = {Y},
  isbn             = {0340-0727},
  keywords         = {Affective Computing; Emotion Recognition; Emotional Responses; Speech Emotion; Interpersonal Synchrony},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Springer Science+Business Media},
  url              = {https://doi.org/10.1007/s00426-022-01669-9},
}

@Article{yang2022ex,
  author           = {Yang, Kai and Li, Tong and Zeng, Ying and Lu, Runnan and Zhang, Rongkai and Gao, Yuanlong and Yan, Bin},
  journal          = {Frontiers in neuroscience},
  title            = {Exploration of effective electroencephalography features for the recognition of different valence emotions},
  year             = {2022},
  month            = oct,
  volume           = {16},
  abstract         = {Recent studies have shown that the recognition and monitoring of different valence emotions can effectively avoid the occurrence of human errors due to the decline in cognitive ability. The quality of features directly affects emotion recognition results, so this manuscript explores the effective electroencephalography (EEG) features for the recognition of different valence emotions. First, 110 EEG features were extracted from the time domain, frequency domain, time-frequency domain, spatial domain, and brain network, including all the current mainly used features. Then, the classification performance, computing time, and important electrodes of each feature were systematically compared and analyzed on the self-built dataset involving 40 subjects and the public dataset DEAP. The experimental results show that the first-order difference, second-order difference, high-frequency power, and high-frequency differential entropy features perform better in the recognition of different valence emotions. Also, the time-domain features, especially the first-order difference features and second-order difference features, have less computing time, so they are suitable for real-time emotion recognition applications. Besides, the features extracted from the frontal, temporal, and occipital lobes are more effective than others for the recognition of different valence emotions. Especially, when the number of electrodes is reduced by 3/4, the classification accuracy of using features from 16 electrodes located in these brain regions is 91.8{\%}, which is only about 2{\%} lower than that of using all electrodes. The study results can provide an important reference for feature extraction and selection in emotion recognition based on EEG.},
  bdsk-url-1       = {https://doi.org/10.3389/fnins.2022.1010951},
  c1               = {Henan Key Laboratory of Imaging and Intelligent Processing, People's Liberation Army (PLA), Strategy Support Force Information Engineering University, Zhengzhou, China; Henan Key Laboratory of Imaging and Intelligent Processing, People's Liberation Army (PLA), Strategy Support Force Information Engineering University, Zhengzhou, China; Henan Key Laboratory of Imaging and Intelligent Processing, People's Liberation Army (PLA), Strategy Support Force Information Engineering University, Zhengzhou, China; Henan Key Laboratory of Imaging and Intelligent Processing, People's Liberation Army (PLA), Strategy Support Force Information Engineering University, Zhengzhou, China; Henan Key Laboratory of Imaging and Intelligent Processing, People's Liberation Army (PLA), Strategy Support Force Information Engineering University, Zhengzhou, China; Henan Key Laboratory of Imaging and Intelligent Processing, People's Liberation Army (PLA), Strategy Support Force Information Engineering University, Zhengzhou, China; Henan Key Laboratory of Imaging and Intelligent Processing, People's Liberation Army (PLA), Strategy Support Force Information Engineering University, Zhengzhou, China},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3389/fnins.2022.1010951},
  hasabstract      = {Y},
  isbn             = {1662-453X},
  keywords         = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Epilepsy Detection; Affective Computing},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Frontiers Media},
  url              = {https://doi.org/10.3389/fnins.2022.1010951},
}

@Article{bhanumathi2022fe,
  author           = {Bhanumathi, K. and Jayadevappa, D. and Tunga, Satish},
  journal          = {International journal of telemedicine and applications},
  title            = {Feedback Artificial Shuffled Shepherd Optimization-Based Deep Maxout Network for Human Emotion Recognition Using EEG Signals},
  year             = {2022},
  month            = jan,
  pages            = {1--14},
  volume           = {2022},
  abstract         = {Emotion recognition is very important for the humans in order to enhance the self-awareness and react correctly to the actions around them. Based on the complication and series of emotions, EEG-enabled emotion recognition is still a difficult issue. Hence, an effective human recognition approach is designed using the proposed feedback artificial shuffled shepherd optimization- (FASSO-) based deep maxout network (DMN) for recognizing emotions using EEG signals. The proposed technique incorporates feedback artificial tree (FAT) algorithm and shuffled shepherd optimization algorithm (SSOA). Here, median filter is used for preprocessing to remove the noise present in the EEG signals. The features, like DWT, spectral flatness, logarithmic band power, fluctuation index, spectral decrease, spectral roll-off, and relative energy, are extracted to perform further processing. Based on the data augmented results, emotion recognition can be accomplished using the DMN, where the training process of the DMN is performed using the proposed FASSO method. Furthermore, the experimental results and performance analysis of the proposed algorithm provide efficient performance with respect to accuracy, specificity, and sensitivity with the maximal values of 0.889, 0.89, and 0.886, respectively.},
  bdsk-url-1       = {https://doi.org/10.1155/2022/3749413},
  c1               = {Department of Electronics and Instrumentation Engineering, JSS Academy of Technical Education, Bengaluru, VTU, India; Department of Electronics and Instrumentation Engineering, JSS Academy of Technical Education, Bengaluru, VTU, India; Department of Electronics \& Telecommunication Engineering, Ramaiah Institute of Technology, Bengaluru, India},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1155/2022/3749413},
  hasabstract      = {Y},
  isbn             = {1687-6415},
  keywords         = {Emotion Recognition; Deep Learning for EEG; Affective Computing; Speech Emotion; Signal Decomposition},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Hindawi Publishing Corporation},
  url              = {https://doi.org/10.1155/2022/3749413},
}

@Article{kim2022em,
  author           = {Kim, Jieun and Hwang, Dong‐Uk and Son, E. and Oh, Sang and Kim, Whansun and Kim, Youngkyung and Kwon, Gusang},
  journal          = {PloS one},
  title            = {Emotion recognition while applying cosmetic cream using deep learning from EEG data; cross-subject analysis},
  year             = {2022},
  month            = nov,
  number           = {11},
  pages            = {e0274203--e0274203},
  volume           = {17},
  abstract         = {We report a deep learning-based emotion recognition method using EEG data collected while applying cosmetic creams. Four creams with different textures were randomly applied, and they were divided into two classes, ``like (positive)''and ``dislike (negative)'', according to the preference score given by the subject. We extracted frequency features using well-known frequency bands, i.e., alpha, beta and low and high gamma bands, and then we created a matrix including frequency and spatial information of the EEG data. We developed seven CNN-based models: (1) inception-like CNN with four-band merged input, (2) stacked CNN with four-band merged input, (3) stacked CNN with four-band parallel input, and stacked CNN with single-band input of (4) alpha, (5) beta, (6) low gamma, and (7) high gamma. The models were evaluated by the Leave-One-Subject-Out Cross-Validation method. In like/dislike two-class classification, the average accuracies of all subjects were 73.2{\%}, 75.4{\%}, 73.9{\%}, 68.8{\%}, 68.0{\%}, 70.7{\%}, and 69.7{\%}, respectively. We found that the classification performance is higher when using multi-band features than when using single-band feature. This is the first study to apply a CNN-based deep learning method based on EEG data to evaluate preference for cosmetic creams.},
  bdsk-url-1       = {https://doi.org/10.1371/journal.pone.0274203},
  c1               = {AIRISS AI Team, Yuseong-gu, Deajeon, South Korea; Division of Fundamental Research on Public Agenda, National Institute for Mathematical Sciences, Daejeon, South Korea; Division of Fundamental Research on Public Agenda, National Institute for Mathematical Sciences, Daejeon, South Korea; Division of Fundamental Research on Public Agenda, National Institute for Mathematical Sciences, Daejeon, South Korea; Division of Fundamental Research on Public Agenda, National Institute for Mathematical Sciences, Daejeon, South Korea; Division of Fundamental Research on Public Agenda, National Institute for Mathematical Sciences, Daejeon, South Korea; AMOREPACIFIC R\&D Center, Yongin-si, Gyeonggi-do, South Korea; AMOREPACIFIC R\&D Center, Yongin-si, Gyeonggi-do, South Korea},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1371/journal.pone.0274203},
  hasabstract      = {Y},
  isbn             = {1932-6203},
  keywords         = {Emotion Recognition; Affective Computing; Deep Learning; Color Psychology; Emotions},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Public Library of Science},
  url              = {https://doi.org/10.1371/journal.pone.0274203},
}

@Article{zhang2023at,
  author           = {Zhang, Zhipeng and Zhang, Liyi},
  journal          = {Applied sciences},
  title            = {A Two-Step Framework to Recognize Emotion Using the Combinations of Adjacent Frequency Bands of EEG},
  year             = {2023},
  month            = feb,
  number           = {3},
  pages            = {1954--1954},
  volume           = {13},
  abstract         = {Electroencephalography (EEG)-based emotion recognition technologies can effectively help robots to perceive human behavior, which have attracted extensive attention in human--machine interaction (HMI). Due to the complexity of EEG data, current researchers tend to extract different types of hand-crafted features and connect all frequency bands for further study. However, this may result in the loss of some discriminative information of frequency band combinations and make the classification models unable to obtain the best results. In order to recognize emotions accurately, this paper designs a novel EEG-based emotion recognition framework using complementary information of frequency bands. First, after the features of the preprocessed EEG data are extracted, the combinations of all the adjacent frequency bands in different scales are obtained through permutation and reorganization. Subsequently, the improved classification method, homogeneous-collaboration-representation-based classification, is used to obtain the classification results of each combination. Finally, the circular multi-grained ensemble learning method is put forward to re-exact the characteristics of each result and merge the machine learning methods and simple majority voting for the decision fusion. In the experiment, the classification accuracies of our framework in arousal and valence on the DEAP database are 95.09{\%} and 94.38{\%} respectively, and that in the four classification problems on the SEED IV database is 96.37{\%}.},
  bdsk-url-1       = {https://doi.org/10.3390/app13031954},
  c1               = {School of Information and Management, Wuhan University, No. 16, Luojiashan Road, Wuchang District, Wuhan 430072, China; School of Information and Management, Wuhan University, No. 16, Luojiashan Road, Wuchang District, Wuhan 430072, China},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/app13031954},
  hasabstract      = {Y},
  isbn             = {2076-3417},
  keywords         = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Head Gesture Recognition},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/app13031954},
}

@Article{dacostaabreu2017fa,
  author           = {Da Costa‐Abreu, M{\'a}rjory and Bezerra, Giuliana},
  journal          = {Pattern analysis and applications/Pattern analysis \& applications},
  title            = {FAMOS: a framework for investigating the use of face features to identify spontaneous emotions},
  year             = {2017},
  month            = dec,
  number           = {2},
  pages            = {683--701},
  volume           = {22},
  abstract         = {Emotion-based analysis has raised a lot of interest, particularly in areas such as forensics, medicine, music, psychology, and human-machine interface. Following this trend, the use of facial analysis (either automatic or human-based) is the most common subject to be investigated once this type of data can easily be collected and is well accepted in the literature as a metric for inference of emotional states. Despite this popularity, due to several constraints found in real-world scenarios (e.g. lightning, complex backgrounds, facial hair and so on), automatically obtaining affective information from face accurately is a very challenging accomplishment. This work presents a framework which aims to analyse emotional experiences through spontaneous facial expressions. The method consists of a new four-dimensional model, called FAMOS, to describe emotional experiences in terms of appraisal, facial expressions, mood, and subjective experiences using a semi-automatic facial expression analyser as ground truth for describing the facial actions. In addition, we present an experiment using a new protocol proposed to obtain spontaneous emotional reactions. The results have suggested that the initial emotional state described by the participants of the experiment was different from that described after the exposure to the eliciting stimulus, thus showing that the used stimuli were capable of inducing the expected emotional states in most individuals. Moreover, our results pointed out that spontaneous facial reactions to emotions are very different from those in prototypic expressions, especially in terms of expressiveness.},
  bdsk-url-1       = {https://doi.org/10.1007/s10044-017-0675-y},
  c1               = {DIMAp/UFRN, Natal, RN, 59078-970, Brazil; DIMAp/UFRN, Natal, RN, 59078-970, Brazil},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1007/s10044-017-0675-y},
  hasabstract      = {Y},
  isbn             = {1433-7541},
  keywords         = {Emotion Recognition; Affective Computing; Face Perception; Facial Expression; Emotional Expressions},
  la               = {en},
  modificationdate = {2024-05-17T04:30:27},
  priority         = {prio3},
  publisher        = {Springer Science+Business Media},
  url              = {https://doi.org/10.1007/s10044-017-0675-y},
}

@Article{baradaran2023au,
  author           = {Baradaran, Farzad and Farzan, Ali and Daneshvar, Sabalan and Sheykhivand, Sobhan},
  journal          = {Electronics},
  title            = {Automatic Emotion Recognition from EEG Signals Using a Combination of Type-2 Fuzzy and Deep Convolutional Networks},
  year             = {2023},
  month            = may,
  number           = {10},
  pages            = {2216--2216},
  volume           = {12},
  abstract         = {Emotions are an inextricably linked component of human life. Automatic emotion recognition can be widely used in brain--computer interfaces. This study presents a new model for automatic emotion recognition from electroencephalography signals based on a combination of deep learning and fuzzy networks, which can recognize two different emotions: positive, and negative. To accomplish this, a standard database based on musical stimulation using EEG signals was compiled. Then, to deal with the phenomenon of overfitting, generative adversarial networks were used to augment the data. The generative adversarial network output is fed into the proposed model, which is based on improved deep convolutional networks with type-2 fuzzy activation functions. Finally, in two separate class, two positive and two negative emotions were classified. In the classification of the two classes, the proposed model achieved an accuracy of more than 98{\%}. In addition, when compared to previous studies, the proposed model performed well and can be used in future brain--computer interface applications.},
  bdsk-url-1       = {https://doi.org/10.3390/electronics12102216},
  c1               = {Department of Computer Engineering, Shabestar Branch, Islamic Azad University, Shabestar 5381637181, Iran; Department of Computer Engineering, Shabestar Branch, Islamic Azad University, Shabestar 5381637181, Iran; College of Engineering, Design and Physical Sciences, Brunel University London, Uxbridge UB8 3PH, UK; Department of Biomedical Engineering, University of Bonab, Bonab 5551395133, Iran},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/electronics12102216},
  hasabstract      = {Y},
  isbn             = {2079-9292},
  keywords         = {Emotion Recognition; Affective Computing; Deep Learning for EEG; EEG Analysis; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/electronics12102216},
}

@Article{bawa2021an,
  author           = {Bawa, Vivek and Sharma, Shailza and Usman, Mohammed and Gupta, Abhimat and Kumar, Vinay},
  journal          = {IEEE access},
  title            = {An Automatic Multimedia Likability Prediction System Based on Facial Expression of Observer},
  year             = {2021},
  month            = jan,
  pages            = {110421--110434},
  volume           = {9},
  abstract         = {Every individual's perception of multimedia content varies based on their interpretation.Therefore, it is quite challenging to predict likability of any multimedia just based on its content.This paper presents a novel system for analysis of facial expressions of subject against the multimedia content to be evaluated.First, we developed a dataset by recording facial expressions of subjects under uncontrolled environment.These subjects are volunteers recruited to watch the videos of different genre, and provide their feedback in terms of likability.Subject responses are divided into three categories: Like, Neutral and Dislike.A novel multimodal system is developed using the developed dataset.The model learns feature representation from data based on the three provided categories.The proposed system contains ensemble of time distributed convolutional neural network, 3D convolutional neural network, and long short term memory networks.All the modalities in proposed architecture are evaluated independently as well as in distinct combinations.The paper also provides detailed insight into learning behavior of the proposed system.},
  bdsk-url-1       = {https://doi.org/10.1109/access.2021.3102042},
  c1               = {Visual Artificial Intelligence Lab, Oxford Brookes University, Oxford, United Kingdom.; Department of Electronics \& Communication Engineering, Thapar Institute of Engineering \& Technology, Patiala, India.; Department of Electrical Engineering, King Khalid University, Abha, 61411, Saudi Arabia.; Department of Computer Science, Thapar Institute of Engineering \& Technology, Patiala, India.; Department of Electronics \& Communication Engineering, Thapar Institute of Engineering \& Technology, Patiala, India.},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1109/access.2021.3102042},
  hasabstract      = {Y},
  isbn             = {2169-3536},
  keywords         = {Emotion Recognition; Visual Attention; Affective Computing; Video Object Segmentation; Facial Expression},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Institute of Electrical and Electronics Engineers},
  url              = {https://doi.org/10.1109/access.2021.3102042},
}

@Article{goshvarpour2023em,
  author           = {Goshvarpour, Atefeh and Goshvarpour, Ateke},
  journal          = {Brain sciences},
  title            = {Emotion Recognition Using a Novel Granger Causality Quantifier and Combined Electrodes of EEG},
  year             = {2023},
  month            = may,
  number           = {5},
  pages            = {759--759},
  volume           = {13},
  abstract         = {Electroencephalogram (EEG) connectivity patterns can reflect neural correlates of emotion. However, the necessity of evaluating bulky data for multi-channel measurements increases the computational cost of the EEG network. To date, several approaches have been presented to pick the optimal cerebral channels, mainly depending on available data. Consequently, the risk of low data stability and reliability has increased by reducing the number of channels. Alternatively, this study suggests an electrode combination approach in which the brain is divided into six areas. After extracting EEG frequency bands, an innovative Granger causality-based measure was introduced to quantify brain connectivity patterns. The feature was subsequently subjected to a classification module to recognize valence-arousal dimensional emotions. A Database for Emotion Analysis Using Physiological Signals (DEAP) was used as a benchmark database to evaluate the scheme. The experimental results revealed a maximum accuracy of 89.55{\%}. Additionally, EEG-based connectivity in the beta-frequency band was able to effectively classify dimensional emotions. In sum, combined EEG electrodes can efficiently replicate 32-channel EEG information.},
  bdsk-url-1       = {https://doi.org/10.3390/brainsci13050759},
  c1               = {Department of Biomedical Engineering, Faculty of Electrical Engineering, Sahand University of Technology, Tabriz 51335-1996, Iran;; Department of Biomedical Engineering, Imam Reza International University, Mashhad 91388-3186, Iran},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/brainsci13050759},
  hasabstract      = {Y},
  isbn             = {2076-3425},
  keywords         = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Affective Computing; Epilepsy Detection},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/brainsci13050759},
}

@Article{ayesh2017cl,
  author           = {Ayesh, Aladdin and Arevalillo‐Herr{\'a}ez, Miguel and Arnau-Gonz{\'a}lez, Pablo},
  title            = {Class discovery from semi-structured EEG data for affective computing and personalisation},
  year             = {2017},
  month            = jul,
  abstract         = {Many approaches to recognising emotions from metrical data such as EEG signals rely on identifying a very small number of classes and to train a classifier. The interpretation of these classes varies from a single emotion such as stress {$[$}24{$]$} to features of emotional model such as valence-arousal {$[$}4{$]$}. There are two major issues here. First classification approach limits the analysis of the data within the selected classes and is also highly dependent on training data/cycles, all of which limits generalisation. Second issue is that it does not explore the inter-relationships between the data collected missing out on any correlations that could tell us interesting facts beyond emotional recognition. This second issue would be of particular interest to psychologists and medical professions. In this paper, we investigate the use of Self-Organizing Maps (SOM) in identifying clusters from EEG signals that could then be translated into classes. We start by training varying sizes of SOM with the EEG data provided in a public dataset (DEAP). The produced graphs showing Neighbour Distance, Sample Hits, Weight Position are analysed holistically to identify patterns in the structure. Following that, we have considered the ground-truth label provided in DEAP, in order to identify correlations between the label and the clustering produced by the SOM. The results show the potential of SOM for class discovery in this particular context. We conclude with a discussion on the implications of this work and the difficulties in evaluating the outcome.},
  bdsk-url-1       = {https://doi.org/10.1109/icci-cc.2017.8109736},
  c1               = {Faculty of Technology, De Montfort University, UK; Departament dInform{\`a}tica, Universitat de Val{\'e}ncia; University of the West of Scotland Scotland, United Kingdom},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1109/icci-cc.2017.8109736},
  hasabstract      = {Y},
  keywords         = {Emotion Recognition; Affective Computing; EEG Analysis; Deep Learning for EEG; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.1109/icci-cc.2017.8109736},
}

@Article{zong2023fc,
  author           = {Zong, Jing and Xiong, Xin and Zhou, Jingtao and Ji, Ying and Zhou, Deyi and Zhang, Qi},
  journal          = {Sensors},
  title            = {FCAN--XGBoost: A Novel Hybrid Model for EEG Emotion Recognition},
  year             = {2023},
  month            = jun,
  number           = {12},
  pages            = {5680--5680},
  volume           = {23},
  abstract         = {In recent years, artificial intelligence (AI) technology has promoted the development of electroencephalogram (EEG) emotion recognition. However, existing methods often overlook the computational cost of EEG emotion recognition, and there is still room for improvement in the accuracy of EEG emotion recognition. In this study, we propose a novel EEG emotion recognition algorithm called FCAN--XGBoost, which is a fusion of two algorithms, FCAN and XGBoost. The FCAN module is a feature attention network (FANet) that we have proposed for the first time, which processes the differential entropy (DE) and power spectral density (PSD) features extracted from the four frequency bands of the EEG signal and performs feature fusion and deep feature extraction. Finally, the deep features are fed into the eXtreme Gradient Boosting (XGBoost) algorithm to classify the four emotions. We evaluated the proposed method on the DEAP and DREAMER datasets and achieved a four-category emotion recognition accuracy of 95.26{\%} and 94.05{\%}, respectively. Additionally, our proposed method reduces the computational cost of EEG emotion recognition by at least 75.45{\%} for computation time and 67.51{\%} for memory occupation. The performance of FCAN--XGBoost outperforms the state-of-the-art four-category model and reduces computational costs without losing classification performance compared with other models.},
  bdsk-url-1       = {https://doi.org/10.3390/s23125680},
  c1               = {Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming 650500, China;; Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming 650500, China;; Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming 650500, China;; Graduate School, Kunming Medical University, Kunming 650500, China;; Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming 650500, China;; Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming 650500, China;},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/s23125680},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s23125680},
}

@Article{lammers2019in,
  author           = {Lammers, Sebastian and Bente, Gary and Tepest, Ralf and Jording, Mathis and Roth, Daniel and Vogeley, Kai},
  journal          = {Frontiers in robotics and AI},
  title            = {Introducing ACASS: An Annotated Character Animation Stimulus Set for Controlled (e)Motion Perception Studies},
  year             = {2019},
  month            = sep,
  volume           = {6},
  abstract         = {Others'movements inform us about their current activities as well as their intentions and emotions. Research on the distinct mechanisms underlying action recognition and emotion inferences has been limited due to a lack of suitable comparative stimulus material. Problematic confounds can derive from low-level physical features, (e.g. luminance), as well as from higher-level psychological features (e.g. stimulus difficulty). Here we present a standardized stimulus dataset, which allows to address both action and emotion recognition with identical stimuli. The stimulus set consists of 792 computer animations with a neutral avatar based on full body motion capture protocols. Motion capture was performed on 22 human volunteers, instructed to perform six everyday activities (mopping, sweeping, painting with a roller, painting with a brush, wiping, sanding) in three different moods (angry, happy, sad). Five-second clips of each motion protocol were rendered into AVI-files using two virtual camera perspectives for each clip. In contrast to video stimuli, the computer animations allowed to standardize the physical appearance of the avatarand to control lighting and coloring conditions, thus reducing the stimulus variation to mere movement. To control for low level optical features of the stimuli, we developed and applied a set of MATLAB routines extracting basic physical features of the stimuli, including average background-foreground proportion and frame-by-frame pixel change dynamics. This information was used to identify outliers and to homogenize the stimuli across action and emotion categories. This led to a smaller stimulus subset (n = 83 animations within the 792 clip database) which only contained two different actions (mopping, sweeping) and two different moods (angry, happy). To further homogenize this stimulus subset with regard to psychological criteria we conducted an online observer study (N = 112 participants) to assess the recognition rates for actions and moods, which led to a final sub-selection of 32 clips (8 per category) within the database. The ACASS database and its subsets provide unique opportunities for research applications in social psychology, social neuroscience and applied clinical studies on communication disorders. All 792 AVI-files, selected subsets, MATLAB code, annotations and motion capture data (FBX-files) are available online.},
  bdsk-url-1       = {https://doi.org/10.3389/frobt.2019.00094},
  c1               = {Cognitive Neuroscience (INM-3), Institute of Neuroscience and Medicine, Research Center J{\"u}lich, J{\"u}lich, Germany; Department of Psychiatry, Faculty of Medicine and University Hospital Cologne, University of Cologne, Cologne, Germany; Department of Communication, Michigan State University, East Lansing, MI, United States; Department of Psychiatry, Faculty of Medicine and University Hospital Cologne, University of Cologne, Cologne, Germany; Cognitive Neuroscience (INM-3), Institute of Neuroscience and Medicine, Research Center J{\"u}lich, J{\"u}lich, Germany; Human-Computer Interaction, Institute for Computer Science, University of W{\"u}rzburg, W{\"u}rzburg, Germany; Cognitive Neuroscience (INM-3), Institute of Neuroscience and Medicine, Research Center J{\"u}lich, J{\"u}lich, Germany; Department of Psychiatry, Faculty of Medicine and University Hospital Cologne, University of Cologne, Cologne, Germany},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3389/frobt.2019.00094},
  hasabstract      = {Y},
  isbn             = {2296-9144},
  keywords         = {Action Observation; Emotion Recognition; Affective Computing; Human-Computer Interaction},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Frontiers Media},
  url              = {https://doi.org/10.3389/frobt.2019.00094},
}

@Article{liang2022nu,
  author           = {Liang, Shuang and Yin, Mingbo and Huang, Yiyun and Dai, Xiubin and Wang, Qiong},
  journal          = {Frontiers in psychology},
  title            = {Nuclear Norm Regularized Deep Neural Network for EEG-Based Emotion Recognition},
  year             = {2022},
  month            = jun,
  volume           = {13},
  abstract         = {Electroencephalography (EEG) based emotion recognition enables machines to perceive users' affective states, which has attracted increasing attention. However, most of the current emotion recognition methods neglect the structural information among different brain regions, which can lead to the incorrect learning of high-level EEG feature representation. To mitigate possible performance degradation, we propose a novel nuclear norm regularized deep neural network framework (NRDNN) that can capture the structural information among different brain regions in EEG decoding. The proposed NRDNN first utilizes deep neural networks to learn high-level feature representations of multiple brain regions, respectively. Then, a set of weights indicating the contributions of each brain region can be automatically learned using a region-attention layer. Subsequently, the weighted feature representations of multiple brain regions are stacked into a feature matrix, and the nuclear norm regularization is adopted to learn the structural information within the feature matrix. The proposed NRDNN method can learn the high-level representations of EEG signals within multiple brain regions, and the contributions of them can be automatically adjusted by assigning a set of weights. Besides, the structural information among multiple brain regions can be captured in the learning procedure. Finally, the proposed NRDNN can perform in an efficient end-to-end manner. We conducted extensive experiments on publicly available emotion EEG dataset to evaluate the effectiveness of the proposed NRDNN. Experimental results demonstrated that the proposed NRDNN can achieve state-of-the-art performance by leveraging the structural information.},
  bdsk-url-1       = {https://doi.org/10.3389/fpsyg.2022.924793},
  c1               = {Smart Health Big Data Analysis and Location Services Engineering Lab of Jiangsu Province, Nanjing University of Posts and Telecommunications, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; School of Computer Science and Technology, Nanjing Tech University, Nanjing, China; School of Computer Science and Technology, Nanjing Tech University, Nanjing, China; Smart Health Big Data Analysis and Location Services Engineering Lab of Jiangsu Province, Nanjing University of Posts and Telecommunications, Nanjing, China; Guangdong Provincial Key Laboratory of Computer Vision and Virtual Reality Technology, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3389/fpsyg.2022.924793},
  hasabstract      = {Y},
  isbn             = {1664-1078},
  keywords         = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Neural Ensemble Physiology},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Frontiers Media},
  url              = {https://doi.org/10.3389/fpsyg.2022.924793},
}

@Article{melo2023so,
  author           = {de Melo, Celso and Gratch, Jonathan and Marsella, Stacy and P{\'e}lachaud, Catherine},
  journal          = {Proceedings of the IEEE},
  title            = {Social Functions of Machine Emotional Expressions},
  year             = {2023},
  month            = oct,
  number           = {10},
  pages            = {1382--1397},
  volume           = {111},
  abstract         = {Virtual humans and social robots frequently generate behaviors that human observers naturally see as expressing emotion. In this review article, we highlight that these expressions can have important benefits for human--machine interaction. We first summarize the psychological findings on how emotional expressions achieve important social functions in human relationships and highlight that artificial emotional expressions can serve analogous functions in human--machine interaction. We then review computational methods for determining what expressions make sense to generate within the context of interaction and how to realize those expressions across multiple modalities, such as facial expressions, voice, language, and touch. The use of synthetic expressions raises a number of ethical concerns, and we conclude with a discussion of principles to achieve the benefits of machine emotion in ethical ways.},
  bdsk-url-1       = {https://doi.org/10.1109/jproc.2023.3261137},
  c1               = {DEVCOM U.S. Army Research Laboratory (ARL), Adelphi, MD, USA; USC Institute for Creative Technologies, Los Angeles, CA, USA; Department of Computer Science and the Department of Psychology, Northeastern University, Boston, MA, USA; Centre National de Recherche Scientifique-Institut de Syst\&{\#}x00E8;mes Intelligents et de Robotique (CNRS-ISIR), Sorbonne University, Paris, France},
  comment          = {review},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1109/jproc.2023.3261137},
  hasabstract      = {Y},
  isbn             = {0018-9219},
  keywords         = {Emotional Expressions; Emotion Recognition; Affective Computing; Human Perception of Robots; Emotion Perception},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Institute of Electrical and Electronics Engineers},
  url              = {https://doi.org/10.1109/jproc.2023.3261137},
}

@Article{serranomamolar2021an,
  author           = {Serrano-Mamolar, Ana and Arevalillo‐Herr{\'a}ez, Miguel and Chicote-Huete, Guillermo and Boticario, Jes{\'u}s},
  journal          = {Sensors},
  title            = {An Intra-Subject Approach Based on the Application of HMM to Predict Concentration in Educational Contexts from Nonintrusive Physiological Signals in Real-World Situations},
  year             = {2021},
  month            = mar,
  number           = {5},
  pages            = {1777--1777},
  volume           = {21},
  abstract         = {Previous research has proven the strong influence of emotions on student engagement and motivation. Therefore, emotion recognition is becoming very relevant in educational scenarios, but there is no standard method for predicting students'affects. However, physiological signals have been widely used in educational contexts. Some physiological signals have shown a high accuracy in detecting emotions because they reflect spontaneous affect-related information, which is fresh and does not require additional control or interpretation. Most proposed works use measuring equipment for which applicability in real-world scenarios is limited because of its high cost and intrusiveness. To tackle this problem, in this work, we analyse the feasibility of developing low-cost and nonintrusive devices to obtain a high detection accuracy from easy-to-capture signals. By using both inter-subject and intra-subject models, we present an experimental study that aims to explore the potential application of Hidden Markov Models (HMM) to predict the concentration state from 4 commonly used physiological signals, namely heart rate, breath rate, skin conductance and skin temperature. We also study the effect of combining these four signals and analyse their potential use in an educational context in terms of intrusiveness, cost and accuracy. The results show that a high accuracy can be achieved with three of the signals when using HMM-based intra-subject models. However, inter-subject models, which are meant to obtain subject-independent approaches for affect detection, fail at the same task.},
  bdsk-url-1       = {https://doi.org/10.3390/s21051777},
  c1               = {aDeNu Research Group, Artificial Intelligence Department, Universidad Nacional de Educaci{\'o}n a Distancia, 28040 Madrid, Spain; Departament d'Inform{\`a}tica, Universitat de Val{\`e}ncia, 46100 Burjassot, Valencia, Spain; Departament d'Inform{\`a}tica, Universitat de Val{\`e}ncia, 46100 Burjassot, Valencia, Spain; aDeNu Research Group, Artificial Intelligence Department, Universidad Nacional de Educaci{\'o}n a Distancia, 28040 Madrid, Spain},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/s21051777},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Emotion Recognition; Emotion Regulation; Affective Computing; Physiological Signals; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s21051777},
}

@Article{mahfoudi2023em,
  author           = {Mahfoudi, Mehdi-Antoine and Meyer, Alexandre and Gaudin, Thibaut and Buendia, Axel and Bouakaz, Sa{\"\i}da},
  journal          = {IEEE transactions on affective computing},
  title            = {Emotion Expression in Human Body Posture and Movement: A Survey on Intelligible Motion Factors, Quantification and Validation},
  year             = {2023},
  month            = oct,
  number           = {4},
  pages            = {2697--2721},
  volume           = {14},
  abstract         = {Many areas in computer science are facing the need to analyze, quantify and reproduce movements expressing emotions.This paper presents a systematic review of the intelligible factors involved in the expression of emotions in human movement and posture.We have gathered the works that have studied and tried to identify these factors by sweeping many disciplinary fields such as psychology, biomechanics, choreography, robotics and computer vision.These researches have each used their own definitions, units and emotions, which prevents a global and coherent vision.We propose a meta-analysis approach that cross-references and aggregates these researches in order to have a unified list of expressive factors quantified for each emotion.A calculation method is then proposed for each of the expressive factors and we extract them from an emotionally annotated animation dataset: Emilya.The comparison between the results of the meta-analysis and the Emilya analysis reveals high correlation rates, which validates the relevance of the quantified values obtained by both methodologies.The analysis of the results raises interesting perspectives for future research in affective computing.},
  bdsk-url-1       = {https://doi.org/10.1109/taffc.2022.3226252},
  c1               = {Laboratoire d'InfoRmatique en Image et Syst{\`e}mes d'information; SPIR.OPS; Simulation, Analyse et Animation pour la R{\'e}alit{\'e}Augment{\'e}e; Universit{\'e}Claude Bernard Lyon 1; Laboratoire d'InfoRmatique en Image et Syst{\`e}mes d'information; Simulation, Analyse et Animation pour la R{\'e}alit{\'e}Augment{\'e}e; Universit{\'e}Claude Bernard Lyon 1; SPIR.OPS; CEDRIC - Interactivit{\'e}pour Lire et Jouer; SPIR.OPS; Laboratoire d'InfoRmatique en Image et Syst{\`e}mes d'information; Simulation, Analyse et Animation pour la R{\'e}alit{\'e}Augment{\'e}e; Universit{\'e}Claude Bernard Lyon 1},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1109/taffc.2022.3226252},
  hasabstract      = {Y},
  isbn             = {1949-3045},
  keywords         = {Emotion Recognition; Emotions; Affective Computing; Pose Estimation; Facial Expression},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Institute of Electrical and Electronics Engineers},
  url              = {https://doi.org/10.1109/taffc.2022.3226252},
}

@Article{daher2022em,
  author           = {Daher, Karl and Saad, Dahlia and Mugellini, Elena and Lalanne, Denis and Khaled, Omar},
  journal          = {Sensors},
  title            = {Empathic and Empathetic Systematic Review to Standardize the Development of Reliable and Sustainable Empathic Systems},
  year             = {2022},
  month            = apr,
  number           = {8},
  pages            = {3046--3046},
  volume           = {22},
  abstract         = {Empathy plays a crucial role in human life, and the evolution of technology is affecting the way humans interact with machines. The area of affective computing is attracting considerable interest within the human--computer interaction community. However, the area of empathic interactions has not been explored in depth. This systematic review explores the latest advances in empathic interactions and behaviour. We provide key insights into the exploration, design, implementation, and evaluation of empathic interactions. Data were collected from the CHI conference between 2011 and 2021 to provide an overview of all studies covering empathic and empathetic interactions. Two authors screened and extracted data from a total of 59 articles relevant to this review. The features extracted cover interaction modalities, context understanding, usage fields, goals, and evaluation. The results reported here can be used as a foundation for the future research and development of empathic systems and interfaces and as a starting point for the gaps found.},
  bdsk-url-1       = {https://doi.org/10.3390/s22083046},
  c1               = {HumanTech Institute, HEIA-FR, University of Applied Sciences Western Switzerland HES-SO, 2800 Del{\'e}mont, Switzerland; HumanTech Institute, HEIA-FR, University of Applied Sciences Western Switzerland HES-SO, 2800 Del{\'e}mont, Switzerland; HumanTech Institute, HEIA-FR, University of Applied Sciences Western Switzerland HES-SO, 2800 Del{\'e}mont, Switzerland; Human-IST, University of Fribourg, 1700 Fribourg, Switzerland; HumanTech Institute, HEIA-FR, University of Applied Sciences Western Switzerland HES-SO, 2800 Del{\'e}mont, Switzerland},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/s22083046},
  hasabstract      = {Y},
  isbn             = {1424-8220},
  keywords         = {Affective Computing; Human-Computer Interaction; Emotional Design; Emotion Recognition; Interaction Design},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/s22083046},
}

@Article{zhang2021hi,
  author           = {Zhang, Pengwei and Min, Chongdan and Zhang, Kangjia and Xue, Wen and Chen, Jingxia},
  journal          = {Frontiers in neuroscience},
  title            = {Hierarchical Spatiotemporal Electroencephalogram Feature Learning and Emotion Recognition With Attention-Based Antagonism Neural Network},
  year             = {2021},
  month            = dec,
  volume           = {15},
  abstract         = {Inspired by the neuroscience research results that the human brain can produce dynamic responses to different emotions, a new electroencephalogram (EEG)-based human emotion classification model was proposed, named R2G-ST-BiLSTM, which uses a hierarchical neural network model to learn more discriminative spatiotemporal EEG features from local to global brain regions. First, the bidirectional long- and short-term memory (BiLSTM) network is used to obtain the internal spatial relationship of EEG signals on different channels within and between regions of the brain. Considering the different effects of various cerebral regions on emotions, the regional attention mechanism is introduced in the R2G-ST-BiLSTM model to determine the weight of different brain regions, which could enhance or weaken the contribution of each brain area to emotion recognition. Then a hierarchical BiLSTM network is again used to learn the spatiotemporal EEG features from regional to global brain areas, which are then input into an emotion classifier. Especially, we introduce a domain discriminator to work together with the classifier to reduce the domain offset between the training and testing data. Finally, we make experiments on the EEG data of the DEAP and SEED datasets to test and compare the performance of the models. It is proven that our method achieves higher accuracy than those of the state-of-the-art methods. Our method provides a good way to develop affective brain-computer interface applications.},
  bdsk-url-1       = {https://doi.org/10.3389/fnins.2021.738167},
  c1               = {School of Electronic Information and Artificial Intelligence, Shaanxi University of Science and Technology, Xi'an, China; School of Electronic Information and Artificial Intelligence, Shaanxi University of Science and Technology, Xi'an, China; School of Electronic Information and Artificial Intelligence, Shaanxi University of Science and Technology, Xi'an, China; School of Electronic Information and Artificial Intelligence, Shaanxi University of Science and Technology, Xi'an, China; School of Electronic Information and Artificial Intelligence, Shaanxi University of Science and Technology, Xi'an, China},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3389/fnins.2021.738167},
  hasabstract      = {Y},
  isbn             = {1662-453X},
  keywords         = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Epilepsy Detection},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Frontiers Media},
  url              = {https://doi.org/10.3389/fnins.2021.738167},
}

@Article{gonzalez2021an,
  author           = {Gonz{\'a}lez, Miguel and Fern{\'a}ndez, Mario and Mart{\'\i}n, Dolores},
  journal          = {International journal of environmental research and public health/International journal of environmental research and public health},
  title            = {Analysis of Emotion and Recall in COVID-19 Advertisements: A Neuroscientific Study},
  year             = {2021},
  month            = aug,
  number           = {16},
  pages            = {8721--8721},
  volume           = {18},
  abstract         = {In this research, neuroscience techniques are applied to the field of marketing in the analysis of advertisements that include the COVID-19 pandemic in their stories. A study of emotion and memory in these audiovisual productions is carried out as two fundamental factors for the knowledge of consumer habits and decision making. By means of facial recognition biosensor systems (AFFDEX) and various tests, six informative and narrative, emotional and rational advertisements are presented to the subjects of the experiment to detect which emotions predominate; how they affect variables such as neuroticism, psychoticism or extroversion, among others; or what is remembered about the different works, brands and advertisers. Outstanding results are obtained in both emotional and cognitive analysis. Thus, in the field of public health, it is found that messages referring to COVID-19 included in advertisements are remembered more than other narratives or even the brands, products or services themselves. Likewise, joy is the predominant emotion, and its significance in such varied advertising stories stands out. Finally, it is clear that neuroscience research applied to marketing requires new methods and integrated applications to obtain satisfactory results in the advertising field.},
  bdsk-url-1       = {https://doi.org/10.3390/ijerph18168721},
  c1               = {Faculty of Communication Sciences, Rey Juan Carlos University, 28942 Fuenlabrada, Spain;; Faculty of Communication Sciences, Rey Juan Carlos University, 28942 Fuenlabrada, Spain;; Faculty of Law and Social Sciences, Rey Juan Carlos University, 28032 Vic{\'a}lvaro, Spain},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.3390/ijerph18168721},
  hasabstract      = {Y},
  isbn             = {1660-4601},
  keywords         = {Emotion Recognition; Affective Computing; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://doi.org/10.3390/ijerph18168721},
}

@Article{lykartsis2019pr,
  author        = {Lykartsis, Athanasios and Kotti, Margarita},
  title         = {Prediction of User Emotion and Dialogue Success Using Audio Spectrograms and Convolutional Neural Networks},
  year          = {2019},
  month         = jan,
  abstract      = {In this paper we aim to predict dialogue success and user satisfaction as well as emotion on a turn level.To achieve this, we investigate the use of spectrogram representations, extracted from audio files, in combination with several types of convolutional neural networks.The experiments were performed on the Let's Go V2 database, comprising 5065 audio files and having labels for subjective and objective dialogue turn success, as well as the emotional state of the user.Results show that by using only audio, it is possible to predict turn success with very high accuracy for all three labels (90{\%}).The best performing input representation were 1s long mel-spectrograms in combination with a CNN with a bottleneck architecture.The resulting system has the potential to be used real-time.Our results significantly surpass the state of the art for dialogue success prediction based only on audio.},
  bdsk-url-1    = {https://doi.org/10.18653/v1/w19-5939},
  c1            = {Audio Communication Group TU Berlin Germany; Speech Technology Group Toshiba Research Cambridge United Kingdom},
  comment       = {no music},
  date-added    = {2024-05-13 15:05:26 +0100},
  date-modified = {2024-05-13 15:05:26 +0100},
  doi           = {10.18653/v1/w19-5939},
  hasabstract   = {Y},
  keywords      = {Affective Computing; Emotion Recognition; Spoken Dialogue Systems; Speech Emotion; User Simulation},
  la            = {en},
  priority      = {prio3},
  url           = {https://doi.org/10.18653/v1/w19-5939},
}

@Article{du2023mm,
  author           = {Du, Xiaobing and Deng, Xiaoming and Qin, Haoran and Shu, Yezhi and Liu, Fang and Zhao, Guozhen and Lai, Yu‐Kun and Ma, Chenyue and Liu, Yong-Jin and Wang, Hongan},
  journal          = {IEEE transactions on affective computing},
  title            = {MMPosE: Movie-Induced Multi-Label Positive Emotion Classification Through EEG Signals},
  year             = {2023},
  month            = oct,
  number           = {4},
  pages            = {2925--2938},
  volume           = {14},
  abstract         = {Emotional information plays an important role in various multimedia applications. Movies, as a widely available form of multimedia content, can induce multiple positive emotions and stimulate people's pursuit of a better life. Different from negative emotions, positive emotions are highly correlated and difficult to distinguish in the emotional space. Since different positive emotions are often induced simultaneously by movies, traditional single-target or multi-class methods are not suitable for the classification of movie-induced positive emotions. In this paper, we propose <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" > TransEEG</i > , a model for multi-label positive emotion classification from a viewer's brain activities when watching emotional movies. The key features of <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" > TransEEG</i > include (1) explicitly modeling the spatial correlation and temporal dependencies of multi-channel EEG signals using the Transformer structure based model, which effectively addresses long-distance dependencies, (2) exploiting the label-label correlations to guide the discriminative EEG representation learning, for that we design an Inter-Emotion Mask for guiding the Multi-Head Attention to learn the inter-emotion correlations, and (3) constructing an attention score vector from the representation-label correlation matrix to refine emotion-relevant EEG features. To evaluate the ability of our model for multi-label positive emotion classification, we demonstrate our model on a state-of-the-art positive emotion database CPED. Extensive experimental results show that our proposed method achieves superior performance over the competitive approaches.},
  bdsk-url-1       = {https://doi.org/10.1109/taffc.2022.3221554},
  c1               = {Beijing Key Laboratory of Human Computer Interactions, Institute of Software, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Beijing Key Laboratory of Human Computer Interactions, Institute of Software, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Beijing Key Laboratory of Human Computer Interactions, Institute of Software, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Department of Computer Science and Technology, BNRist, MOE-Key Laboratory of Pervasive Computing, Tsinghua University, Beijing, China; Department of Computer Science and Technology, BNRist, MOE-Key Laboratory of Pervasive Computing, Tsinghua University, Beijing, China; CAS Key Laboratory of Behavioral Science, Institute of Psychology, Beijing, China; Department of Psychology, University of Chinese Academy of Sciences, Beijing, China; School of Computer Science and Informatics, Cardiff University, Cardiff, Wales, U.K.; Beijing Key Laboratory of Human Computer Interactions, International Joint Laboratory of Artificial Intelligence and Emotional Interaction, Beijing, China; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Department of Computer Science and Technology, BNRist, MOE-Key Laboratory of Pervasive Computing, Tsinghua University, Beijing, China; Beijing Key Laboratory of Human Computer Interactions, International Joint Laboratory of Artificial Intelligence and Emotional Interaction, Beijing, China; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1109/taffc.2022.3221554},
  hasabstract      = {Y},
  isbn             = {1949-3045},
  keywords         = {Emotion Recognition; Speech Emotion; Affective Computing; EEG Analysis; Deep Learning for EEG},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  publisher        = {Institute of Electrical and Electronics Engineers},
  url              = {https://doi.org/10.1109/taffc.2022.3221554},
}

@Article{valderrama2023ex,
  author        = {Valderrama, El{\'\i}as and Sarmiento, Auxiliadora and Dur{\'a}n-D{\'\i}az, Iv{\'a}n and Becerra, Juan and Garc{\'\i}a, Irene},
  title         = {Experimental Setup and Protocol for Creating an EEG-signal Database for Emotion Analysis Using Virtual Reality Scenarios},
  year          = {2023},
  month         = jan,
  bdsk-url-1    = {https://doi.org/10.5220/0011656600003417},
  c1            = {Departamento de Teor{\'\i}a de la Se{\~n}al y Comunicaciones, Escuela Superior de Ingenier{\'\i}a, Universidad de Sevilla, 41092 Seville, Spain, --- Select a Country ---; Departamento de Teor{\'\i}a de la Se{\~n}al y Comunicaciones, Escuela Superior de Ingenier{\'\i}a, Universidad de Sevilla, 41092 Seville, Spain, --- Select a Country ---; Departamento de Teor{\'\i}a de la Se{\~n}al y Comunicaciones, Escuela Superior de Ingenier{\'\i}a, Universidad de Sevilla, 41092 Seville, Spain, --- Select a Country ---; Departamento de Teor{\'\i}a de la Se{\~n}al y Comunicaciones, Escuela Superior de Ingenier{\'\i}a, Universidad de Sevilla, 41092 Seville, Spain, --- Select a Country ---; Departamento de Teor{\'\i}a de la Se{\~n}al y Comunicaciones, Escuela Superior de Ingenier{\'\i}a, Universidad de Sevilla, 41092 Seville, Spain, --- Select a Country ---},
  date-added    = {2024-05-13 15:05:26 +0100},
  date-modified = {2024-05-13 15:05:26 +0100},
  doi           = {10.5220/0011656600003417},
  hasabstract   = {N},
  keywords      = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Speech Emotion},
  la            = {en},
  priority      = {prio3},
  url           = {https://doi.org/10.5220/0011656600003417},
}

@Article{ahmad2023as,
  author           = {Ahmad, Zeeshan and Khan, Naimul},
  title            = {A Survey on Physiological Signal-Based Emotion Recognition},
  year             = {2023},
  month            = may,
  abstract         = {{$<$}p{$ > $}Physiological Signals are the most reliable form of signals for emotion recognition, as they cannot be controlled deliberately by the subject. Existing review papers on emotion recognition based on physiological signals surveyed only the regular steps involved in the workflow of emotion recognition such as preprocessing, feature extraction, and classification. While these are important steps, such steps are required for any signal processing application. Emotion recognition poses its own set of challenges that are very important to address for a robust system. Thus, to bridge the gap in the existing literature, in this paper, we review the effect of inter-subject data variance on emotion recognition, important data annotation techniques for emotion recognition and their comparison, data preprocessing techniques for each physiological signal, data splitting techniques for improving the generalization of emotion recognition models and different multimodal fusion techniques and their comparison. Finally we discuss key challenges and future directions in this field.{$<$}/p{$ > $}},
  bdsk-url-1       = {https://doi.org/10.32920/22734278.v1},
  c1               = {Department of Electrical, Computer and Biomedical Engineering, Toronto Metropolitan University, Toronto, ON M5B 2K3, Canada; Department of Electrical, Computer and Biomedical Engineering, Toronto Metropolitan University, Toronto, ON M5B 2K3, Canada},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.32920/22734278.v1},
  hasabstract      = {Y},
  keywords         = {Emotion Recognition; Physiological Signals; Affective Computing; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.32920/22734278.v1},
}

@Article{ahmad2023asa,
  author           = {Ahmad, Zeeshan and Khan, Naimul},
  title            = {A Survey on Physiological Signal-Based Emotion Recognition},
  year             = {2023},
  month            = may,
  abstract         = {{$<$}p{$ > $}Physiological Signals are the most reliable form of signals for emotion recognition, as they cannot be controlled deliberately by the subject. Existing review papers on emotion recognition based on physiological signals surveyed only the regular steps involved in the workflow of emotion recognition such as preprocessing, feature extraction, and classification. While these are important steps, such steps are required for any signal processing application. Emotion recognition poses its own set of challenges that are very important to address for a robust system. Thus, to bridge the gap in the existing literature, in this paper, we review the effect of inter-subject data variance on emotion recognition, important data annotation techniques for emotion recognition and their comparison, data preprocessing techniques for each physiological signal, data splitting techniques for improving the generalization of emotion recognition models and different multimodal fusion techniques and their comparison. Finally we discuss key challenges and future directions in this field.{$<$}/p{$ > $}},
  bdsk-url-1       = {https://doi.org/10.32920/22734278},
  c1               = {Department of Electrical, Computer and Biomedical Engineering, Toronto Metropolitan University, Toronto, ON M5B 2K3, Canada; Department of Electrical, Computer and Biomedical Engineering, Toronto Metropolitan University, Toronto, ON M5B 2K3, Canada},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.32920/22734278},
  hasabstract      = {Y},
  keywords         = {Emotion Recognition; Physiological Signals; Affective Computing; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.32920/22734278},
}

@Article{saravanan2018me,
  author           = {Saravanan, S. and Govindarajan, Suganya},
  journal          = {International journal of engineering \& technology},
  title            = {Mental health analysis on digital world with meditation using EEG},
  year             = {2018},
  month            = dec,
  number           = {4.36},
  pages            = {817--817},
  volume           = {7},
  abstract         = {Internet, e-mail and other social networks like Myspace, Facebook, twitter, LinkedIn are the indispensable components in today's world. These social networking makes the human to addict into the digital world. Digital world has become the integral part of our society. Addiction to the digital world slowly develops the negative symptoms in the area of physical, physiological, emotional and psychological. The most affected of all is the change in Emotional behaviour of the Humans. Emotions plays an important role in our day today life. The existing research work, based on subjective self-reports shows prolonged use of Digital Media induce negative emotions for Humans. There are several techniques are used to extract the human emotions from brain such as Electroencephalography (EEG), functional Magnetic Resonance Imaging (fMRI), or Positron Emission Tomography (PET). Many of the researchers are extensively used to extract the brain waves using EEG. The negative emotions are controlled by human through meditation. In this paper, the Mind Wave device has been used to extract the EEG signal using different range of age people during they use the Digital Medias and after they perform mediation. The proposed method identify the stress level of the human while they are using social media with meditation and without meditation. It evidently proved that the meditation reduces the stress level of human.},
  bdsk-url-1       = {https://doi.org/10.14419/ijet.v7i4.36.24539},
  c1               = {Department of Computer Science and Engineering, SRM Institute of Science and Technology, Chennai, India.; Department of EDP, SRM Institute of Medical Sciences, Chennai, India.},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.14419/ijet.v7i4.36.24539},
  hasabstract      = {Y},
  isbn             = {2227-524X},
  keywords         = {Emotion Recognition; EEG Analysis; Affective Computing; Speech Emotion},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.14419/ijet.v7i4.36.24539},
}

@Article{oliveira2023ex,
  author           = {de Oliveira, Catarina and Khanshan, Alireza and Van Gorp, Pieter},
  title            = {Exploring the Feasibility of Data-Driven Emotion Modeling for Human Digital Twins},
  year             = {2023},
  month            = jul,
  abstract         = {Human Digital Twins (HDTs) are specifically designed to represent humans virtually. Despite their recency, HDTs are considered to be powerful tools for enhancing personalized healthcare, early disease detection, improving patient outcomes, and optimizing lifestyle. Building HDTs is challenging especially due to the complexity of modeling human behavior. To unravel such complexity, in this paper we focus on specific aspects of human behavior and emotions (valence and arousal) to enable a more informed HTD behavior modeling. We assessed the feasibility and performance of our data collection infrastructure with N=112 participants. During a science festival, our participants wore a smartwatch to self-report their emotions while simultaneously their physiological data were collected through smartwatch sensors. We explored predictive modeling possibilities once with all the collected data and then with only the sensor-related data. The former performed best with 25 features, modeled with the Quadratic Discriminant Analysis classifier, resulting in 72.4{\%} accuracy, however, with only 4 features derived from the self-reports, the K-Nearest Neighbors Classifier was able to achieve an accuracy of 71.6{\%}. The latter used 17 sensor-related features and the Quadratic Discriminant Analysis classifier estimator reaching a 51.3{\%} accuracy.},
  bdsk-url-1       = {https://doi.org/10.1145/3594806.3596535},
  c1               = {Industrial Design Department of the Eindhoven University of Technology, Netherlands; Industrial Design Department of the Eindhoven University of Technology, Netherlands; Industrial Engineering Department of the Eindhoven University of Technology, Netherlands},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1145/3594806.3596535},
  hasabstract      = {Y},
  keywords         = {Affective Computing; Emotion Recognition; Emotion Dynamics; Personality Data},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.1145/3594806.3596535},
}

@Article{gong2023as,
  author           = {Gong, Peiliang and Jia, Ziyang and Wang, Pengpai and Zhou, Yanqing and Zhang, Daoqiang},
  title            = {ASTDF-Net: Attention-Based Spatial-Temporal Dual-Stream Fusion Network for EEG-Based Emotion Recognition},
  year             = {2023},
  month            = oct,
  abstract         = {Emotion recognition based on electroencephalography (EEG) has attracted significant attention and achieved considerable advances in the fields of affective computing and human-computer interaction. However, most existing studies ignore the coupling and complementarity of complex spatiotemporal patterns in EEG signals. Moreover, how to exploit and fuse crucial discriminative aspects in high redundancy and low signal-to-noise ratio EEG signals remains a great challenge for emotion recognition. In this paper, we propose a novel attention-based spatial-temporal dual-stream fusion network, named ASTDF-Net, for EEG-based emotion recognition. Specifically, ASTDF-Net comprises three main stages: first, the collaborative embedding module is designed to learn a joint latent subspace to capture the coupling of complicated spatiotemporal information in EEG signals. Second, stacked parallel spatial and temporal attention streams are employed to extract the most essential discriminative features and filter out redundant task-irrelevant factors. Finally, the hybrid attention-based feature fusion module is proposed to integrate significant features discovered from the dual-stream structure to take full advantage of the complementarity of the diverse characteristics. Extensive experiments on two publicly available emotion recognition datasets indicate that our proposed approach consistently outperforms state-of-the-art methods.},
  bdsk-url-1       = {https://doi.org/10.1145/3581783.3612208},
  c1               = {College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics \& Key Laboratory of Brain-Machine Intelligence Technology, Ministry of Education, Nanjing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics \& Key Laboratory of Brain-Machine Intelligence Technology, Ministry of Education, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics \& Key Laboratory of Brain-Machine Intelligence Technology, Ministry of Education, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics \& Key Laboratory of Brain-Machine Intelligence Technology, Ministry of Education, Nanjing, China},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1145/3581783.3612208},
  hasabstract      = {Y},
  keywords         = {Emotion Recognition; Affective Computing; Deep Learning for EEG; EEG Analysis; Emotion Regulation},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.1145/3581783.3612208},
}

@Article{khota2022mo,
  author           = {Khota, Ahmed and Cooper, Eric and Yu, Yanfang and Kovacs, Mate},
  title            = {Modelling emotional valence and arousal of non-linguistic utterances for sound design support},
  year             = {2022},
  month            = sep,
  abstract         = {Non-Linguistic Utterances (NLUs), produced for popular media, computers, robots, and public spaces, can quickly and wordlessly convey emotional characteristics of a message. They have been studied in terms of their ability to convey affect in robot communication. The objective of this research is to develop a model that correctly infers the emotional Valence and Arousal of an NLU. On a Likert scale, 17 subjects evaluated the relative Valence and Arousal of 560 sounds collected from popular movies, TV shows, and video games, including NLUs and other character utterances. Three audio feature sets were used to extract features including spectral energy, spectral spread, zero-crossing rate (ZCR), Mel Frequency Cepstral Coefficients (MFCCs), and audio chroma, as well as pitch, jitter, formant, shimmer, loudness, and Harmonics-to-Noise Ratio, among others. After feature reduction by Factor Analysis, the best-performing models inferred average Valence with a Mean Absolute Error (MAE) of 0.107 and Arousal with MAE of 0.097 on audio samples removed from the training stages. These results suggest the model infers Valence and Arousal of most NLUs to less than the difference between successive rating points on the 7-point Likert scale (0.14). This inference system is applicable to the development of novel NLUs to augment robot-human communication or to the design of sounds for other systems, machines, and settings.},
  bdsk-url-1       = {https://doi.org/10.5821/conference-9788419184849.52},
  c1               = {Ritsumeikan University, Japan,; Ritsumeikan University, Japan,; Ritsumeikan University, Japan,},
  comment          = {no music},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.5821/conference-9788419184849.52},
  hasabstract      = {Y},
  keywords         = {Affective Computing; Speech Emotion; Emotion Recognition; Auditory Processing; Speech Perception},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.5821/conference-9788419184849.52},
}

@Article{jiang2023mu,
  author           = {Jiang, Wei-Bang and Liu, X. and Zheng, Wei‐Long and Lu, Bao‐Liang},
  title            = {Multimodal Adaptive Emotion Transformer with Flexible Modality Inputs on A Novel Dataset with Continuous Labels},
  year             = {2023},
  month            = oct,
  abstract         = {Emotion recognition from physiological signals is a topic of widespread interest, and researchers continue to develop novel techniques for perceiving emotions. However, the emergence of deep learning has highlighted the need for high-quality emotional datasets to accurately decode human emotions. In this study, we present a novel multimodal emotion dataset that incorporates electroencephalography (EEG) and eye movement signals to systematically explore human emotions. Seven basic emotions (happy, sad, fear, disgust, surprise, anger, and neutral) are elicited by a large number of 80 videos and fully investigated with continuous labels that indicate the intensity of the corresponding emotions. Additionally, we propose a novel Multimodal Adaptive Emotion Transformer (MAET), that can flexibly process both unimodal and multimodal inputs. Adversarial training is utilized in MAET to mitigate subject discrepancy, which enhances domain generalization. Our extensive experiments, encompassing both subject-dependent and cross-subject conditions, demonstrate MAET's superior performance in handling various inputs. The filtering of data for high emotional evocation using continuous labels proved to be effective in the experiments. Furthermore, the complementary properties between EEG and eye movements are observed. Our code is available at https://github.com/935963004/MAET.},
  bdsk-url-1       = {https://doi.org/10.1145/3581783.3613797},
  c1               = {Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1145/3581783.3613797},
  hasabstract      = {Y},
  keywords         = {Emotion Recognition; Affective Computing; Speech Emotion; Deep Learning for EEG; Multimodal Data},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.1145/3581783.3613797},
}

@Article{ru2023se,
  author           = {Ru, Yiwei and Li, Peipei and Sun, Muyi and Wang, Yunlong and Zhang, Kunbo and Li, Qi and He, Z. and Sun, Zhenan},
  title            = {Sensing Micro-Motion Human Patterns using Multimodal mmRadar and Video Signal for Affective and Psychological Intelligence},
  year             = {2023},
  month            = oct,
  abstract         = {Affective and psychological perception are pivotal in human-machine interaction and essential domains within artificial intelligence. Existing physiological signal-based affective and psychological datasets primarily rely on contact-based sensors, potentially introducing extraneous affectives during the measurement process. Consequently, creating accurate non-contact affective and psychological perception datasets is crucial for overcoming these limitations and advancing affective intelligence. In this paper, we introduce the Remote Multimodal Affective and Psychological (ReMAP) dataset, for the first time, apply head micro-tremor (HMT) signals for affective and psychological perception. ReMAP features 68 participants and comprises two sub-datasets. The stimuli videos utilized for affective perception undergo rigorous screening to ensure the efficacy and universality of affective elicitation. Additionally, we propose a novel remote affective and psychological perception framework, leveraging multimodal complementarity and interrelationships to enhance affective and psychological perception capabilities. Extensive experiments demonstrate HMT as a "small yet powerful" physiological signal in psychological perception. Our method outperforms existing state-of-the-art approaches in remote affective recognition and psychological perception. The ReMAP dataset is publicly accessible at https://remap-dataset.github.io/ReMAP.},
  bdsk-url-1       = {https://doi.org/10.1145/3581783.3611754},
  c1               = {Beijing University of Posts and Telecommunications \& Institute of Automation, Chinese Academy of Sciences, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Beijing University of Post and Telecommunication, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China},
  date-added       = {2024-05-13 15:05:26 +0100},
  date-modified    = {2024-05-13 15:05:26 +0100},
  doi              = {10.1145/3581783.3611754},
  hasabstract      = {Y},
  keywords         = {Affective Computing; Emotion Recognition; Head Gesture Recognition; Multimodal Data; Human-Computer Interaction},
  la               = {en},
  modificationdate = {2024-05-16T06:35:35},
  priority         = {prio3},
  url              = {https://doi.org/10.1145/3581783.3611754},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 KeywordGroup:High Priority\;0\;priority\;prio1\;0\;0\;0\;\;\;\;;
2 KeywordGroup:With Abstract (High)\;1\;abstract\;\;0\;1\;1\;\;\;\;;
1 KeywordGroup:Medium Priority\;0\;priority\;prio2\;0\;0\;1\;\;\;\;;
2 KeywordGroup:With Abstract (Medium)\;1\;abstract\;\;0\;1\;1\;\;\;\;;
1 KeywordGroup:Low Priority\;0\;priority\;prio3\;0\;0\;1\;\;\;\;;
2 KeywordGroup:With Abstract (Low)\;1\;abstract\;\;0\;1\;1\;\;\;\;;
}
