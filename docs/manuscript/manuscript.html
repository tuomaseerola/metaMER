<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.353">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tuomas Eerola">
<meta name="author" content="Cameron J. Anderson">
<meta name="keywords" content="music, emotion, recognition, meta-analysis">

<title>metaMER - Meta-analysis of regression and classification success of emotion ratings from audio</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">metaMER</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html" rel="" target="">
 <span class="menu-text">README</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Meta-analysis of regression and classification success of emotion ratings from audio</h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Authors</div>
  <div class="quarto-title-meta-heading">Affiliations</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Tuomas Eerola </p>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            1
          </p>
      </div>
      <div class="quarto-title-meta-contents">
    <p class="author">Cameron J. Anderson </p>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            2
          </p>
      </div>
    </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="abstract-title">Abstract</div>
    <p>This is a meta-analysis of music emotion recognition. An analysis of the articles published between 2014-2024 containing models predicting either valence and arousal or emotion categories was carried out. A total of xx studies were included</p>
  </div>
</div>

</header>

<section id="introduction" class="level1">
<h1>Introduction</h1>
<ul>
<li><p>Emotional expression is one of the central reasons why people engage with music.</p></li>
<li><p>Great advances in music information retrieval have been made in recent years. The available features, modelling techniques and datasets have given scholars opportunities to refine the accuracy and reliability of predicting annotated emotions from audio.</p></li>
<li><p>Numerous studies over the last 25 years have established what emotions listeners perceive and recognise in music <span class="citation" data-cites="gomez2021">(<a href="#ref-gomez2021" role="doc-biblioref">Gómez-Cañón et al., 2021</a>)</span>. In the last 15 years, it has become possible to trace the recognised emotions to musical contents such as expressive features <span class="citation" data-cites="lindstrom2003expressivity">(<a href="#ref-lindstrom2003expressivity" role="doc-biblioref">Lindström et al., 2003</a>)</span>, structural aspects of music <span class="citation" data-cites="eerola_friberg_bresin_2013 anderson2022ex grimaud_eerola_2022">(<a href="#ref-anderson2022ex" role="doc-biblioref">Anderson &amp; Schutz, 2022</a>; <a href="#ref-eerola_friberg_bresin_2013" role="doc-biblioref">Eerola et al., 2013</a>; <a href="#ref-grimaud_eerola_2022" role="doc-biblioref">Grimaud &amp; Eerola, 2022</a>)</span>, or acoustic features <span class="citation" data-cites="yang2008 panda2013multi saari_et_al_2015 eerola2011c">(<a href="#ref-eerola2011c" role="doc-biblioref">Eerola, 2011</a>; <a href="#ref-panda2013multi" role="doc-biblioref">R. Panda et al., 2013</a>; <a href="#ref-saari_et_al_2015" role="doc-biblioref">Saari et al., 2015</a>; <a href="#ref-yang2008" role="doc-biblioref">Y. H. Yang et al., 2008</a>)</span> or emergent properties identified through deep learning <span class="citation" data-cites="er2019music sarkar2020recognition">(<a href="#ref-er2019music" role="doc-biblioref">Er &amp; Aydilek, 2019</a>; <a href="#ref-sarkar2020recognition" role="doc-biblioref">Sarkar et al., 2020</a>)</span>.</p></li>
</ul>
<p>However, there is no consensus on to what degree emotions can be recognised by computational models and the literature to date paints a diverse picture of success for concepts in affective circumplex – valence and arousal– <span class="citation" data-cites="Russell1980">(<a href="#ref-Russell1980" role="doc-biblioref">Russell, 1980</a>)</span> and classifying various emotion categories <span class="citation" data-cites="fu2010survey">(<a href="#ref-fu2010survey" role="doc-biblioref">Fu et al., 2010</a>)</span>.</p>
</section>
<section id="a-brief-history-of-mer" class="level1">
<h1>A brief history of MER</h1>
<p>Emotion has been widely discussed since the earliest artificial intelligence (AI) applications to music in the 1950s. Whereas early discourse largely focused on generative composition using computers <span class="citation" data-cites="zaripov1969">(<a href="#ref-zaripov1969" role="doc-biblioref">Zaripov &amp; Russell, 1969</a>)</span>, attention later shifted to creating methods to predict emotion using music’s structural cues. Novel techniques for information retrieval emerged in the 1950s and 1960s <span class="citation" data-cites="fairthorne1968">(<a href="#ref-fairthorne1968" role="doc-biblioref">Fairthorne, 1968</a>)</span>, inspiring analogous developments for automated music analysis (<span class="citation" data-cites="kassler1966toward">Kassler (<a href="#ref-kassler1966toward" role="doc-biblioref">1966</a>)</span>; <span class="citation" data-cites="mendel1969some">Mendel (<a href="#ref-mendel1969some" role="doc-biblioref">1969</a>)</span>). These developments would set the stage for early work in music emotion recognition (MER). Katayose et al.&nbsp;(1988) conducted the first study of this nature, creating an algorithm that associated emotions with analyzed chords to generate descriptions like “there is hopeful mood on chord from 69 to 97 [<em>sic</em>].” <span class="citation" data-cites="katayose_sentiment_1988">(<a href="#ref-katayose_sentiment_1988" role="doc-biblioref">Katayose et al., 1988, p. 1087</a>)</span>.</p>
<p>In the early 2000s, several research groups conducted studies using regression <span class="citation" data-cites="friberg_automatic_2002 liu_automatic_2003">(<a href="#ref-friberg_automatic_2002" role="doc-biblioref">Friberg et al., 2002</a>; <a href="#ref-liu_automatic_2003" role="doc-biblioref">Liu et al., 2003</a>)</span> and classification <span class="citation" data-cites="lu_automatic_2005 feng_popular_2003 mandel_support_2006">(<a href="#ref-feng_popular_2003" role="doc-biblioref">Feng et al., 2003</a>; <a href="#ref-lu_automatic_2005" role="doc-biblioref">Lu et al., 2005</a>; <a href="#ref-mandel_support_2006" role="doc-biblioref">Mandel et al., 2006</a>)</span> techniques to predict emotion in music audio or MIDI. Citing “MIR researchers’ growing interest in classifying music by moods” <span class="citation" data-cites="downie_music_2008">(<a href="#ref-downie_music_2008" role="doc-biblioref">Downie, 2008, p. 1</a>)</span>, the Music Information Retrieval EXchange (MIREX) introduced Audio Mood Classification (AMC) to their rotation of tasks in 2007. In the first year, nine systems classified mood labels in a common data set, reaching 52.65% accuracy (SD = 11.19%). These events, along with growing interest in the burgeoning field of affective computing, would lead to an explosion of interest in MER research.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>Researchers have assessed regression and classification techniques on diverse corpora with features drawn from music (e.g., audio, MIDI, metadata) and participants (e.g., demographic information, survey responses, physiological signals, etc.). In one widely-cited study, Yang (2008) approached MER as a regression task, predicting the valence (i.e., the negative—positive emotional quality) and arousal (i.e., the calm—exciting quality) of 195 Chinese pop songs, achieving 62% accuracy for arousal but only 28% for valence <span class="citation" data-cites="yang2008">(<a href="#ref-yang2008" role="doc-biblioref">Y. H. Yang et al., 2008</a>)</span>. This difference in prediction accuracy between dimensions has reappeared in several subsequent studies <span class="citation" data-cites="bai2016dimensional coutinho2013psychoacoustic">(e.g., <a href="#ref-bai2016dimensional" role="doc-biblioref">Bai et al., 2016</a>; <a href="#ref-coutinho2013psychoacoustic" role="doc-biblioref">Coutinho &amp; Dibben, 2013</a>)</span>, with some research suggesting this challenge reflects fewer well-established predictors and more individual differences for valence than arousal <span class="citation" data-cites="yang2007music">(<a href="#ref-yang2007music" role="doc-biblioref">Yi-Hsuan Yang et al., 2007</a>)</span>.</p>
</section>
<section id="the-semantic-gap-in-mer" class="level1">
<h1>The semantic gap in MER</h1>
<p>The difficulty in predicting valence reflects a broader challenge in information retrieval. Specifically, relations between low-level predictors from music and text and the perceptual phenomena they model remain poorly understood, reaching a ceiling in prediction accuracy <span class="citation" data-cites="celma_foafing_2006">(<a href="#ref-celma_foafing_2006" role="doc-biblioref">Celma, 2006</a>)</span>. To address this so-called <em>semantic gap</em>, researchers have attempted to identify new feature sets with greater relevance to emotion <span class="citation" data-cites="panda2020audio chowdhury2021perceived">(<a href="#ref-chowdhury2021perceived" role="doc-biblioref">Chowdhury &amp; Widmer, 2021</a>; <a href="#ref-panda2020audio" role="doc-biblioref">Renato Panda et al., 2020</a>)</span>, combine low-, mid-, and high-level features using multimodal data <span class="citation" data-cites="celma_foafing_2006">(<a href="#ref-celma_foafing_2006" role="doc-biblioref">Celma, 2006</a>)</span>, or train neural networks to automatically learn features from audio <span class="citation" data-cites="zhang_bridge_2016">(<a href="#ref-zhang_bridge_2016" role="doc-biblioref">J. Zhang et al., 2016</a>)</span>. Through these approaches, MER researchers attempt to shatter a so-called <em>glass ceiling</em> <span class="citation" data-cites="downie_music_2008">(<a href="#ref-downie_music_2008" role="doc-biblioref">Downie, 2008</a>)</span> by establishing strong emotion predictors. To date, however, no study has systematically compared results of these diverse approaches.</p>
<section id="aims" class="level2">
<h2 class="anchored" data-anchor-id="aims">Aims</h2>
<ul>
<li>Our aim is to establish the level of predictive accuracy for both models of emotional expression that can account for track-specific coordinates in affective circumple space (valence and arousal) and classification of emotion categories based on available and recent studies.</li>
<li>We seek to identify the types of issues (modelling techniques, features, and musical qualities used) that significantly influence the prediction rates.</li>
<li>To achieve these aims, we carry out a meta-analysis focused on journal articles published in the last 10 years.</li>
<li>We outline broad hypotheses such as arousal being predicted to a higher degree than valence, which is more challenging and more context dependent than arousal. For classification, simple utilitarian emotions (e.g., fear, anger) will be easier to predict than complex social emotions (e.g., sadness, nostalgia).</li>
</ul>
</section>
</section>
<section id="methods" class="level1">
<h1>Methods</h1>
<p>We preregistered the meta-analysis plan on 21 June 2024 at OSF, <a href="https://osf.io/c5wgd" class="uri">https://osf.io/c5wgd</a>, and the plan is also available at <a href="https://tuomaseerola.github.io/metaMER/preregistration/preregistration.html" class="uri">https://tuomaseerola.github.io/metaMER/preregistration/preregistration.html</a>).</p>
<p>In the search stage, we used three databases, <em>Web of Science</em>, <em>Scopus</em>, and <em>Open Alex</em> to identify journal articles published between 2014 and 2024 containing keywords/title <code>valence OR arousal OR classi* OR categor* OR algorithm AND music AND emotion AND recognition</code> (see specific search strings for each database in <a href="http://">SI</a>). All searches were done in May 2024.</p>
<p>The initial search yielded 553 potential studies after excluding duplicate entries. We interactively screened them for relevance in three stages, resulting in 46 studies that passed our inclusion criteria (music emotion studies using classification or regression methods to predict emotion ratings of music using symbolic or audio features, and containing sufficient detail to convert results to <span class="math inline">\(R^2\)</span> or <span class="math inline">\(MCC\)</span> values see <a href="http://">SI</a> for a breakdown). After the screening stage, we defined a set of entities to extract characterising (i) music (genre, stimulus number [N], duration), (ii) features extracted (number, type, source, defined by <span class="citation" data-cites="panda2020audio">(<a href="#ref-panda2020audio" role="doc-biblioref">Renato Panda et al., 2020</a>)</span>), (iii) model type (regression, neural network, SVM, etc.) and outcome measure (<span class="math inline">\(R^2\)</span>, <em>MSE</em>, <em>MCC</em>), (iv) model complexity (i.e., approximate number of features used to predict ratings), and (v) type of model cross-validation.</p>
<p>We converted all regression results into <span class="math inline">\(R^2\)</span> values for valence and arousal and classification results into Matthews correlation coefficient <span class="citation" data-cites="chicco2020advantages">(<em>MCC</em>, <a href="#ref-chicco2020advantages" role="doc-biblioref">Chicco &amp; Jurman, 2020</a>)</span>. To increase consistency in our analyses, we excluded studies using incompatible features (e.g., spectrograms of audio files <span class="citation" data-cites="nag2022">(<a href="#ref-nag2022" role="doc-biblioref">Nag et al., 2022</a>)</span>), or dependent variables (e.g., one regression study analyzed valence and arousal together, but not separately <span class="citation" data-cites="chin2018">(<a href="#ref-chin2018" role="doc-biblioref">Chin et al., 2018</a>)</span>).</p>
<p>[Figure 1: flowchart of the study inclusions/eliminations]</p>
<div class="cell">
<div class="cell-output-display">
<div>
<div>
<svg width="672" height="480" viewbox="0.00 0.00 602.00 488.00" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" style="; max-width: none; max-height: none">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 484)">
<title>
sub
</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-484 598,-484 598,4 -4,4"></polygon> <!-- A --> <g id="node1" class="node">
<title>
A
</title>
<polygon fill="none" stroke="black" points="288,-480.1 0,-480.1 0,-404.7 288,-404.7 288,-480.1"></polygon> <text text-anchor="middle" x="144" y="-463.4" font-family="Helvetica,sans-Serif" font-size="14.00">Records identified from:</text> <text text-anchor="middle" x="144" y="-446.6" font-family="Helvetica,sans-Serif" font-size="14.00"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Web of Science (n = 142)</text> <text text-anchor="middle" x="144" y="-429.8" font-family="Helvetica,sans-Serif" font-size="14.00"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Scopus (n = 227)</text> <text text-anchor="middle" x="144" y="-413" font-family="Helvetica,sans-Serif" font-size="14.00"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;OpenALEX (n = 278)</text> </g> <!-- B --> <g id="node2" class="node">
<title>
B
</title>
<polygon fill="none" stroke="black" points="594,-463 306,-463 306,-421.8 594,-421.8 594,-463"></polygon> <text text-anchor="middle" x="450" y="-446.6" font-family="Helvetica,sans-Serif" font-size="14.00">Records removed before screening: </text> <text text-anchor="middle" x="450" y="-429.8" font-family="Helvetica,sans-Serif" font-size="14.00"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Duplicates entries/errors (n = 94)</text> </g> <!-- A&#45;&gt;B --> <g id="edge1" class="edge">
<title>
A-&gt;B
</title>
<path fill="none" stroke="black" d="M288.04,-442.4C290.53,-442.4 293.02,-442.4 295.51,-442.4"></path> <polygon fill="black" stroke="black" points="295.75,-445.9 305.75,-442.4 295.75,-438.9 295.75,-445.9"></polygon> </g> <!-- C --> <g id="node3" class="node">
<title>
C
</title>
<polygon fill="none" stroke="black" points="288,-368.8 0,-368.8 0,-332.8 288,-332.8 288,-368.8"></polygon> <text text-anchor="middle" x="144" y="-346.6" font-family="Helvetica,sans-Serif" font-size="14.00">Records after duplicates removed (n = 553)</text> </g> <!-- A&#45;&gt;C --> <g id="edge2" class="edge">
<title>
A-&gt;C
</title>
<path fill="none" stroke="black" d="M144,-404.7C144,-396.1 144,-387.08 144,-378.98"></path> <polygon fill="black" stroke="black" points="147.5,-378.97 144,-368.97 140.5,-378.97 147.5,-378.97"></polygon> </g> <!-- D --> <g id="node4" class="node">
<title>
D
</title>
<polygon fill="none" stroke="black" points="288,-277.2 0,-277.2 0,-241.2 288,-241.2 288,-277.2"></polygon> <text text-anchor="middle" x="144" y="-255" font-family="Helvetica,sans-Serif" font-size="14.00">Records screened (n = 553)</text> </g> <!-- C&#45;&gt;D --> <g id="edge3" class="edge">
<title>
C-&gt;D
</title>
<path fill="none" stroke="black" d="M144,-332.52C144,-319.76 144,-302.12 144,-287.46"></path> <polygon fill="black" stroke="black" points="147.5,-287.42 144,-277.42 140.5,-287.42 147.5,-287.42"></polygon> </g> <!-- E --> <g id="node5" class="node">
<title>
E
</title>
<polygon fill="none" stroke="black" points="594,-296.9 306,-296.9 306,-221.5 594,-221.5 594,-296.9"></polygon> <text text-anchor="middle" x="450" y="-280.2" font-family="Helvetica,sans-Serif" font-size="14.00">Records removed: </text> <text text-anchor="middle" x="450" y="-263.4" font-family="Helvetica,sans-Serif" font-size="14.00"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Irrelevant titles/themes (n = 338)</text> <text text-anchor="middle" x="450" y="-246.6" font-family="Helvetica,sans-Serif" font-size="14.00"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Abstract screening (n = 95)</text> <text text-anchor="middle" x="450" y="-229.8" font-family="Helvetica,sans-Serif" font-size="14.00"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Discussion (n = 24)</text> </g> <!-- D&#45;&gt;E --> <g id="edge4" class="edge">
<title>
D-&gt;E
</title>
<path fill="none" stroke="black" d="M288.04,-259.2C290.53,-259.2 293.02,-259.2 295.51,-259.2"></path> <polygon fill="black" stroke="black" points="295.75,-262.7 305.75,-259.2 295.75,-255.7 295.75,-262.7"></polygon> </g> <!-- F --> <g id="node6" class="node">
<title>
F
</title>
<polygon fill="none" stroke="black" points="288,-182.8 0,-182.8 0,-146.8 288,-146.8 288,-182.8"></polygon> <text text-anchor="middle" x="144" y="-160.6" font-family="Helvetica,sans-Serif" font-size="14.00">Full text articles assessed (n = 96)</text> </g> <!-- D&#45;&gt;F --> <g id="edge5" class="edge">
<title>
D-&gt;F
</title>
<path fill="none" stroke="black" d="M144,-240.82C144,-227.4 144,-208.57 144,-193.14"></path> <polygon fill="black" stroke="black" points="147.5,-193.1 144,-183.1 140.5,-193.1 147.5,-193.1"></polygon> </g> <!-- G --> <g id="node7" class="node">
<title>
G
</title>
<polygon fill="none" stroke="black" points="288,-108 0,-108 0,-72 288,-72 288,-108"></polygon> <text text-anchor="middle" x="144" y="-85.8" font-family="Helvetica,sans-Serif" font-size="14.00">Studies included in final review (n = 46)</text> </g> <!-- F&#45;&gt;G --> <g id="edge7" class="edge">
<title>
F-&gt;G
</title>
<path fill="none" stroke="black" d="M144,-146.55C144,-138.12 144,-127.77 144,-118.3"></path> <polygon fill="black" stroke="black" points="147.5,-118.13 144,-108.13 140.5,-118.13 147.5,-118.13"></polygon> </g> <!-- H --> <g id="node8" class="node">
<title>
H
</title>
<polygon fill="none" stroke="black" points="594,-185.4 306,-185.4 306,-144.2 594,-144.2 594,-185.4"></polygon> <text text-anchor="middle" x="450" y="-169" font-family="Helvetica,sans-Serif" font-size="14.00">Full text articles excluded</text> <text text-anchor="middle" x="450" y="-152.2" font-family="Helvetica,sans-Serif" font-size="14.00"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(n = 50)</text> </g> <!-- F&#45;&gt;H --> <g id="edge6" class="edge">
<title>
F-&gt;H
</title>
<path fill="none" stroke="black" d="M288.04,-164.8C290.53,-164.8 293.02,-164.8 295.51,-164.8"></path> <polygon fill="black" stroke="black" points="295.75,-168.3 305.75,-164.8 295.75,-161.3 295.75,-168.3"></polygon> </g> <!-- I --> <g id="node9" class="node">
<title>
I
</title>
<polygon fill="none" stroke="black" points="594,-108 306,-108 306,-72 594,-72 594,-108"></polygon> <text text-anchor="middle" x="450" y="-85.8" font-family="Helvetica,sans-Serif" font-size="14.00">Studies excluded in final review (n = 10)</text> </g> <!-- G&#45;&gt;I --> <g id="edge8" class="edge">
<title>
G-&gt;I
</title>
<path fill="none" stroke="black" d="M288.04,-90C290.53,-90 293.02,-90 295.51,-90"></path> <polygon fill="black" stroke="black" points="295.75,-93.5 305.75,-90 295.75,-86.5 295.75,-93.5"></polygon> </g> <!-- J --> <g id="node10" class="node">
<title>
J
</title>
<polygon fill="none" stroke="black" points="288,-36 0,-36 0,0 288,0 288,-36"></polygon> <text text-anchor="middle" x="144" y="-13.8" font-family="Helvetica,sans-Serif" font-size="14.00">Final selection (n = 36)</text> </g> <!-- G&#45;&gt;J --> <g id="edge9" class="edge">
<title>
G-&gt;J
</title>
<path fill="none" stroke="black" d="M144,-71.7C144,-63.98 144,-54.71 144,-46.11"></path> <polygon fill="black" stroke="black" points="147.5,-46.1 144,-36.1 140.5,-46.1 147.5,-46.1"></polygon> </g> </g>
</svg>
</div>
</div>
</div>
</div>
<section id="quality-control" class="level2">
<h2 class="anchored" data-anchor-id="quality-control">Quality Control</h2>
<p>The search yielded studies of variable (and occasionally questionable) quality. To mitigate potentially spurious effects resulting from the inclusion of low-quality studies, <a href="include approx # for each? This would be good since the rest of this para already mentions counts. If this take a long time, we could save this for an additional analysis">we excluded studies lacking sufficient details about stimuli, analyzed features, or model architecture</a>. Finally, we excluded studies published in journals of questionable relevance/quality, (e.g., <em>Mathematical Problems in Engineering</em> ceased publication following 17 retractions published between July and September 2024).</p>
</section>
<section id="study-encoding" class="level2">
<h2 class="anchored" data-anchor-id="study-encoding">Study Encoding</h2>
<p>To capture key details of each study, we added additional fields to BibTeX entries for each study. Fields included information about the genre/type of stimuli employed, along with their duration and number; the number of analyzed features; and the model type, validation procedure and output measures. Additionally, we included study results using executable <em>R</em> code containing custom functions for meta-analysis. For complete details about our encoding procedure, see <code>studies/extraction_details.qmd</code> .</p>
</section>
</section>
<section id="results" class="level1">
<h1>Results</h1>
<p>First we describe the overall pattern of data (regression vs classification, modelling techniques, feature numbers, stimulus numbers, datasets, and other details).</p>
<p>TABLE 1: Summary of data (part of <code>analysis/preprocessing.qmd</code>)</p>
<table class="table">
<colgroup>
<col style="width: 16%">
<col style="width: 37%">
<col style="width: 36%">
<col style="width: 9%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">Regression</th>
<th style="text-align: left;">Classification</th>
<th style="text-align: left;">Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Study N</td>
<td style="text-align: left;">24</td>
<td style="text-align: left;">14</td>
<td style="text-align: left;">38</td>
</tr>
<tr class="even">
<td style="text-align: left;">Model N</td>
<td style="text-align: left;">258</td>
<td style="text-align: left;">108</td>
<td style="text-align: left;">366</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Techniques</td>
<td style="text-align: left;">Flexible Discriminants: 64</td>
<td style="text-align: left;">37</td>
<td style="text-align: left;">101</td>
</tr>
<tr class="even">
<td style="text-align: left;">Techniques</td>
<td style="text-align: left;">Neural Nets: 74</td>
<td style="text-align: left;">27</td>
<td style="text-align: left;">101</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Techniques</td>
<td style="text-align: left;">Linear Methods: 74</td>
<td style="text-align: left;">24</td>
<td style="text-align: left;">98</td>
</tr>
<tr class="even">
<td style="text-align: left;">Techniques</td>
<td style="text-align: left;">Random Forests: 22</td>
<td style="text-align: left;">12</td>
<td style="text-align: left;">34</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Techniques</td>
<td style="text-align: left;">KS, Add. &amp; KNN: 24</td>
<td style="text-align: left;">8</td>
<td style="text-align: left;">32</td>
</tr>
<tr class="even">
<td style="text-align: left;">Feature N</td>
<td style="text-align: left;">Min=3, Md=472.5, Max=654</td>
<td style="text-align: left;">Min=3, Md=231, Max=8904</td>
<td style="text-align: left;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Stimulus N</td>
<td style="text-align: left;">Min=40, Md=324, Max=2486</td>
<td style="text-align: left;">Min=124, Md=387, Max=5192</td>
<td style="text-align: left;">NA</td>
</tr>
</tbody>
</table>
<p>Although the total number of studies meeting the criteria described in the previous section is modest (38 in total), they encompass a large array of models (366 in total) with a relatively even distribution among the three most popular techniques: flexible discriminants, neural nets, and linear methods. The number of features and stimuli within these studies varies significantly, ranging from as few as three features <span class="citation" data-cites="battcock2021in">(<a href="#ref-battcock2021in" role="doc-biblioref">Battcock &amp; Schutz, 2021</a>)</span> to a maximum of almost 9000 features <span class="citation" data-cites="zhang2023mo">(<a href="#ref-zhang2023mo" role="doc-biblioref">M. Zhang et al., 2023</a>)</span>. The median number of features differs between regression (473) and classification (231) studies, primarily reflecting the nature of the datasets used in each approach. The number of stimuli is typically around 300-400 (with a median of 324 for regression and 387 for classification), though there is substantial variation, with the extremes from 40 stimuli in <span class="citation" data-cites="saizclar2022pr">Saiz-Clar et al. (<a href="#ref-saizclar2022pr" role="doc-biblioref">2022</a>)</span> to 5192 stimuli in <span class="citation" data-cites="alvarez2023ri">Álvarez et al. (<a href="#ref-alvarez2023ri" role="doc-biblioref">2023</a>)</span>. There are also additional dimensions to consider, such as the type of cross-validation used, the music genres analyzed (whether a single genre, multiple genres, or a mix), the type of journal in which the studies were published, and the source of the extracted features. However, these variables do not lend themselves to a simple summary, so we will revisit them during the interpretation and discussion stages. [COMMENT: IT MIGHT BE WORTWHILE TO SUMMARISE SOME OF THESE HERE].</p>
<p>We first report regression studies that predict valence and arousal.</p>
<section id="prediction-success-for-valence-and-arousal-or-affect-dimensions" class="level2">
<h2 class="anchored" data-anchor-id="prediction-success-for-valence-and-arousal-or-affect-dimensions">Prediction success for valence and arousal (or affect dimensions)</h2>
<p>See <code>analysis/analysis.qmd</code></p>
<p>Since there are many models contained within each of the studies, we will report the results in two parts; We first give an overview of the results for all models, and then we focus on the best performing models of each study. The best performing model is the model within each study with the highest correlation coefficient. This reduction is done to avoid the issue of multiple models from the same study deflating the results as majority of the models included are relative modest baseline or alternative models that do not represent the novelty or content of the article. Table 2 summarises the results for all models (All) as well as best performing models (Max) for each study. The summary includes the number of models and observations, the correlation coefficient and its 95% confidence interval, the t-value and p-value for the correlation, the heterogeneity statistics <span class="math inline">\(\tau^2\)</span> and <span class="math inline">\(I^2\)</span>, calculated through appropriate transformations (Fisher’s Z) for the correlation coefficient as part of a random-effects model using <code>meta</code> library <span class="citation" data-cites="balduzzi2019">(<a href="#ref-balduzzi2019" role="doc-biblioref">Balduzzi et al., 2019</a>)</span>.</p>
<p>Table 2. Meta-analytic diagnostic for all regression studies predicting valence from audio.</p>
<table class="table">
<colgroup>
<col style="width: 16%">
<col style="width: 15%">
<col style="width: 30%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 12%">
<col style="width: 8%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Concept</th>
<th style="text-align: left;">Models, obs</th>
<th style="text-align: left;"><span class="math inline">\(r\)</span> [95%-CI]</th>
<th style="text-align: left;"><span class="math inline">\(t\)</span></th>
<th style="text-align: left;"><span class="math inline">\(p\)</span></th>
<th style="text-align: left;"><span class="math inline">\(\tau^2\)</span></th>
<th style="text-align: left;"><span class="math inline">\(I^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Valence All</td>
<td style="text-align: left;">120,73685</td>
<td style="text-align: left;">0.567 [0.530; 0.603]</td>
<td style="text-align: left;">23.7</td>
<td style="text-align: left;">.0001</td>
<td style="text-align: left;">0.083</td>
<td style="text-align: left;">97.5%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Valence Max</td>
<td style="text-align: left;">24,15660</td>
<td style="text-align: left;">0.659 [0.557; 0.740]</td>
<td style="text-align: left;">10.14</td>
<td style="text-align: left;">.0001</td>
<td style="text-align: left;">0.138</td>
<td style="text-align: left;">98.2%</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><em>N Features</em></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">&lt;18 F</td>
<td style="text-align: left;">5,3036</td>
<td style="text-align: left;">0.811 [0.566; 0.775]</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.182</td>
<td style="text-align: left;">98.9%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">18-260 F</td>
<td style="text-align: left;">11,7318</td>
<td style="text-align: left;">0.548 [0.343; 0.703]</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.133</td>
<td style="text-align: left;">97.6%</td>
</tr>
<tr class="even">
<td style="text-align: left;">260+ F</td>
<td style="text-align: left;">7,4562</td>
<td style="text-align: left;">0.685 [0.566; 0.775]</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.044</td>
<td style="text-align: left;">97.2%</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><em>Techniques</em></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">KS</td>
<td style="text-align: left;">2,2582</td>
<td style="text-align: left;">0.466 [-0.634; 0.942]</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.0185</td>
<td style="text-align: left;">95.1%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">LM</td>
<td style="text-align: left;">8,1762</td>
<td style="text-align: left;">0.784 [0.625; 0.881]</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.1370</td>
<td style="text-align: left;">96.4%</td>
</tr>
<tr class="even">
<td style="text-align: left;">FD</td>
<td style="text-align: left;">6,4993</td>
<td style="text-align: left;">0.656 [0.484; 0.779]</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.0574</td>
<td style="text-align: left;">96.9%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">NN</td>
<td style="text-align: left;">4,2249</td>
<td style="text-align: left;">0.340 [-0.097; 0.668]</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.0761</td>
<td style="text-align: left;">97.1%</td>
</tr>
<tr class="even">
<td style="text-align: left;">RF</td>
<td style="text-align: left;">4,4074</td>
<td style="text-align: left;">0.702 [0.391; 0.869]</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.057</td>
<td style="text-align: left;">98.5%</td>
</tr>
</tbody>
</table>
<!-- | Val. MaxTrim | 13,7363     | 0.649 \[0.598; 0.695\] | 18.5 | .0001 | 0.008    | 88.5% | -->
<!-- | Valence Trim |             | 0.605 \[0.592; 0.617\] | 72.4 | .0001 | 0.0007   | 33.1% | -->
<!-- | Valence M    |             | 0.604 \[0.467; 0.712\] | 7.83 | .0001 | 0.100    | 97.8% | -->
<!-- | Valence Md   |             | 0.602 \[0.466; 0.710\] | 7.87 | .0001 | 0.098    | 97.7% | -->
<ul>
<li>OTHER GROUPINGS? STIMULUS MIXED/SINGLE GENRE, PREDICTION/EXPLANATION</li>
</ul>
<!-- see this to ignore I\^2 and rely on prediction interval: https://onlinelibrary.wiley.com/doi/full/10.1002/jrsm.1678 -->
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="manuscript_files/figure-html/fig1-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Forest plot of valence predictions from the MER models.</figcaption>
</figure>
</div>
</div>
</div>
<p>Summarise the results here briefly</p>
<p>Moving on the arousal, …</p>
<p>Table 3. Meta-analytic diagnostic for all regression studies predicting arousal from audio.</p>
<table class="table">
<colgroup>
<col style="width: 16%">
<col style="width: 15%">
<col style="width: 31%">
<col style="width: 7%">
<col style="width: 9%">
<col style="width: 11%">
<col style="width: 8%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Concept</th>
<th style="text-align: left;">Models, obs</th>
<th style="text-align: left;"><span class="math inline">\(r\)</span> [95%-CI]</th>
<th style="text-align: left;"><span class="math inline">\(t\)</span></th>
<th style="text-align: left;"><span class="math inline">\(p\)</span></th>
<th style="text-align: left;"><span class="math inline">\(\tau^2\)</span></th>
<th style="text-align: left;"><span class="math inline">\(I^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Arousal</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">0.7959 [0.7666; 0.8218]</td>
<td style="text-align: left;">29.0</td>
<td style="text-align: left;">0.0001</td>
<td style="text-align: left;">0.0676</td>
<td style="text-align: left;">95.6%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Arousal Max</td>
<td style="text-align: left;">24,15660</td>
<td style="text-align: left;">0.8070 [0.7453; 0.8550]</td>
<td style="text-align: left;">10.3</td>
<td style="text-align: left;">0.0001</td>
<td style="text-align: left;">0.155</td>
<td style="text-align: left;">96.8%</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><em>N Features</em></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">&lt;18 F</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">18-260 F</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">260+ F</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><em>Techniques</em></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">KS</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">LM</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">FD</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">NN</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">RF</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Figure 2. Forest plot of arousal prediction (using Max?)</p>
<!-- | Arousal Trim |             | 0.7932 \[0.7846; 0.8014\] | 96.5 | 0.0001 | 0.0030   | 73.3% | -->
<!-- | Arousal M    |             | 0.7567 \[0.6752; 0.8199\] | 12.7 | 0.0001 | 0.0739   | 95.6% | -->
<!-- | Arousal Md   |             | 0.7627 \[0.6819; 0.8252\] | 12.7 | 0.0001 | 0.0757   | 95.3% | -->
<!-- | Aro.Max.Trm  | 14,12061    | 0.8182 \[0.8021; 0.8331\] | 25.6 | 0.0001 | 0.0132   | 96.8% | -->
<p>Summarise here the pattern of results</p>
</section>
<section id="classification-studies" class="level2">
<h2 class="anchored" data-anchor-id="classification-studies">Classification studies</h2>
<p>Summary of details contained in Table 1, but summarise at least the categories predicted before moving onto the main findings.</p>
<p>Table 4. Meta-analytic diagnostic for all classification studies predicting emotion categories from audio.</p>
<table class="table">
<colgroup>
<col style="width: 8%">
<col style="width: 16%">
<col style="width: 34%">
<col style="width: 7%">
<col style="width: 10%">
<col style="width: 12%">
<col style="width: 8%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Models, obs</th>
<th style="text-align: left;"><span class="math inline">\(r\)</span> [95%-CI]</th>
<th style="text-align: left;"><span class="math inline">\(t\)</span></th>
<th style="text-align: left;"><span class="math inline">\(p\)</span></th>
<th style="text-align: left;"><span class="math inline">\(\tau^2\)</span></th>
<th style="text-align: left;"><span class="math inline">\(I^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">All</td>
<td style="text-align: left;">89,87347</td>
<td style="text-align: left;">0.8074 [0.7681; 0.8407]</td>
<td style="text-align: left;">21.4</td>
<td style="text-align: left;">0.0001</td>
<td style="text-align: left;">0.2415</td>
<td style="text-align: left;">99.7%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Max</td>
<td style="text-align: left;">14,17184</td>
<td style="text-align: left;">0.8564 [0.7386; 0.9234]</td>
<td style="text-align: left;">8.32</td>
<td style="text-align: left;">0.0001</td>
<td style="text-align: left;">0.329</td>
<td style="text-align: left;">99.8%</td>
</tr>
</tbody>
</table>
<!-- | All Trim | 29,6499     | 0.8185 \[0.8046; 0.8314\] | 58.3 | 0.0001 | 0.0066   | 60.4% | -->
<!-- | Max Trim | 6,3653      | 0.8689 \[0.7760; 0.9249\] | 11.6 | 0.0001 | 0.0749   | 97.5% | -->
<p>Heterogeneity issues</p>
<p>Figure 3. Forest plot of arousal prediction (Max?) (Unless we do some custom plotting)</p>
<ul>
<li>Figure Optional: Funnel plot (I haven’t seen this yet)</li>
</ul>
</section>
</section>
<section id="conclusion-and-discussion" class="level1">
<h1>Conclusion and Discussion</h1>
<section id="concise-summary-of-what-we-did-and-found" class="level2">
<h2 class="anchored" data-anchor-id="concise-summary-of-what-we-did-and-found">Concise summary of what we did and found</h2>
</section>
<section id="main-outcomes" class="level2">
<h2 class="anchored" data-anchor-id="main-outcomes">Main outcomes</h2>
<ul>
<li>Arousal is easier to predict (r = 0.7627) than valence (r = 0.6236), as we predicted. The glass ceiling seems to be at …</li>
<li>Classification …</li>
<li>Model accuracy is surprisingly little affected by the number of features (?) or modelling technique (?).</li>
<li>Some of the complex state-of-the-art techniques (e.g., NNs) do not deliver impressive improvements over older techniques (e.g., SVR, RF)</li>
<li>Variation in study/model/data quality is large and can be seen in heterogenuity and the amount of studies eliminated</li>
</ul>
</section>
<section id="calls-for-actionpoints-to-improve-in-such-studies" class="level2">
<h2 class="anchored" data-anchor-id="calls-for-actionpoints-to-improve-in-such-studies">Calls for action/points to improve in such studies</h2>
<ul>
<li><em>Documentation</em> the details in full (features, stimuli, model details, cross-validation)</li>
<li><em>Quality</em> of the underlying data (emotion ratings, classes, or even stimulus properties?</li>
<li><em>Generalisibility</em> of the models (some studies such as X and Y address this by applying the models across several datasets)</li>
<li>Diversity in the evaluative aspects of studies: <em>overfitting</em>, numerous ways of cross-validating, not sharing data or analysis scripts, not reporting in the same way</li>
<li>What proportion of stimuli are Western music, and what genres tend to dominate?</li>
</ul>
<section id="funding-statement" class="level3">
<h3 class="anchored" data-anchor-id="funding-statement">Funding statement</h3>
<p>CA was funded by Mitacs Globalink Research Award (Mitacs &amp; British High Commission - Ottawa, Canada).</p>
</section>
<section id="competing-interests-statement" class="level3">
<h3 class="anchored" data-anchor-id="competing-interests-statement">Competing interests statement</h3>
<p>There were no competing interests.</p>
</section>
<section id="open-practices-statement" class="level3">
<h3 class="anchored" data-anchor-id="open-practices-statement">Open practices statement</h3>
<p>Study preregistration, data, analysis scripts and supporting information is available at Github, <a href="https://tuomaseerola.github.io/metaMER" class="uri">https://tuomaseerola.github.io/metaMER</a>.</p>
</section>
<section id="acknowledgements" class="level3">
<h3 class="anchored" data-anchor-id="acknowledgements">Acknowledgements</h3>
<p>We thank Greggs food-on-the-go retailer for sustaining the work with affordable sandwiches and coffee.</p>
</section>
</section>
</section>
<section id="references" class="level1">




</section>


<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a><div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-line-spacing="2" role="list">
<div id="ref-alvarez2023ri" class="csl-entry" role="listitem">
Álvarez, P., Quirós, J. G. de, &amp; Baldassarri, S. (2023). RIADA: A machine-learning based infrastructure for recognising the emotions of spotify songs [Article]. <em>International Journal of Interactive Multimedia and Artificial Intelligence</em>, <em>8</em>(2), 168–181. <a href="https://doi.org/10.9781/ijimai.2022.04.002">https://doi.org/10.9781/ijimai.2022.04.002</a>
</div>
<div id="ref-anderson2022ex" class="csl-entry" role="listitem">
Anderson, C. J., &amp; Schutz, M. (2022). Exploring historic changes in musical communication: Deconstructing emotional cues in preludes by bach and chopin. <em>Psychology of Music</em>, <em>50</em>(5), 1424–1442.
</div>
<div id="ref-bai2016dimensional" class="csl-entry" role="listitem">
Bai, J., Peng, J., Shi, J., Tang, D., Wu, Y., Li, J., &amp; Luo, K. (2016). Dimensional music emotion recognition by valence-arousal regression. <em>2016 IEEE 15th International Conference on Cognitive Informatics &amp; Cognitive Computing (ICCI* CC)</em>, 42–49.
</div>
<div id="ref-balduzzi2019" class="csl-entry" role="listitem">
Balduzzi, S., Rücker, G., &amp; Schwarzer, G. (2019). How to perform a meta-analysis with <span>R</span>: A practical tutorial. <em>Evidence-Based Mental Health</em>, <em>22</em>, 153–160.
</div>
<div id="ref-battcock2021in" class="csl-entry" role="listitem">
Battcock, A., &amp; Schutz, M. (2021). Individualized interpretation: Exploring structural and interpretive effects on evaluations of emotional content in bach’s well tempered clavier. <em>JOURNAL OF NEW MUSIC RESEARCH</em>, <em>50</em>(5), 447–468. <a href="https://doi.org/10.1080/09298215.2021.1979050">https://doi.org/10.1080/09298215.2021.1979050</a>
</div>
<div id="ref-celma_foafing_2006" class="csl-entry" role="listitem">
Celma, O. (2006). Foafing the music: <span>Bridging</span> the semantic gap in music recommendation. <em>International Semantic Web Conference</em>, 927–934.
</div>
<div id="ref-chicco2020advantages" class="csl-entry" role="listitem">
Chicco, D., &amp; Jurman, G. (2020). The advantages of the matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation. <em>BMC Genomics</em>, <em>21</em>, 1–13.
</div>
<div id="ref-chin2018" class="csl-entry" role="listitem">
Chin, Y.-H., Wang, J.-C., Wang, J.-C., &amp; Yang, Y.-H. (2018). Predicting the probability density function of music emotion using emotion space mapping. <em>IEEE Transactions on Affective Computing</em>, <em>9</em>(4), 541 549. <a href="https://doi.org/10.1109/TAFFC.2016.2628794">https://doi.org/10.1109/TAFFC.2016.2628794</a>
</div>
<div id="ref-chowdhury2021perceived" class="csl-entry" role="listitem">
Chowdhury, S., &amp; Widmer, G. (2021). On perceived emotion in expressive piano performance: Further experimental evidence for the relevance of mid-level perceptual features. <em>International Society for Music Information Retrieval Conference (ISMIR 2023)</em>.
</div>
<div id="ref-coutinho2013psychoacoustic" class="csl-entry" role="listitem">
Coutinho, E., &amp; Dibben, N. (2013). Psychoacoustic cues to emotion in speech prosody and music. <em>Cognition &amp; Emotion</em>, <em>27</em>(4), 658–684.
</div>
<div id="ref-downie_music_2008" class="csl-entry" role="listitem">
Downie, J. S. (2008). The music information retrieval evaluation exchange (2005–2007): <span>A</span> window into music information retrieval research. <em>Acoustical Science and Technology</em>, <em>29</em>(4), 247–255.
</div>
<div id="ref-eerola2011c" class="csl-entry" role="listitem">
Eerola, T. (2011). Are the emotions expressed in music genre-specific? An audio-based evaluation of datasets spanning classical, film, pop and mixed genres. <em>Journal of New Music Research</em>, <em>40</em>(4), 349–366. <a href="https://doi.org/10.1080/09298215.2011.602195">https://doi.org/10.1080/09298215.2011.602195</a>
</div>
<div id="ref-eerola_friberg_bresin_2013" class="csl-entry" role="listitem">
Eerola, T., Friberg, A., &amp; Bresin, R. (2013). Emotional expression in music: Contribution, linearity, and additivity of primary musical cues. <em>Frontiers in Psychology</em>, <em>4</em>(487). <a href="https://doi.org/10.3389/fpsyg.2013.00487">https://doi.org/10.3389/fpsyg.2013.00487</a>
</div>
<div id="ref-er2019music" class="csl-entry" role="listitem">
Er, M. B., &amp; Aydilek, I. B. (2019). Music emotion recognition by using chroma spectrogram and deep visual features. <em>International Journal of Computational Intelligence Systems</em>, <em>12</em>(2), 1622–1634.
</div>
<div id="ref-fairthorne1968" class="csl-entry" role="listitem">
Fairthorne, R. A. (1968). <em>Towards information retrieval</em>.
</div>
<div id="ref-feng_popular_2003" class="csl-entry" role="listitem">
Feng, Y., Zhuang, Y., &amp; Pan, Y. (2003). Popular music retrieval by detecting mood. <em>Proceedings of the 26th Annual International <span>ACM</span> <span>SIGIR</span> Conference on <span>Research</span> and Development in Informaion Retrieval</em>, 375–376.
</div>
<div id="ref-friberg_automatic_2002" class="csl-entry" role="listitem">
Friberg, A., Schoonderwaldt, E., Juslin, P. N., &amp; Bresin, R. (2002). Automatic real-time extraction of musical expression. <em>International <span>Computer</span> <span>Music</span> <span>Conference</span>, <span>ICMC</span> 2002, <span>Gothenburg</span>, <span>Sweden</span></em>, 365–367.
</div>
<div id="ref-fu2010survey" class="csl-entry" role="listitem">
Fu, Z., Lu, G., Ting, K. M., &amp; Zhang, D. (2010). A survey of audio-based music classification and annotation. <em>IEEE Transactions on Multimedia</em>, <em>13</em>(2), 303–319.
</div>
<div id="ref-gomez2021" class="csl-entry" role="listitem">
Gómez-Cañón, J. S., Cano, E., Eerola, T., Herrera, P., Hu, X., Yang, Y.-H., &amp; Gómez, E. (2021). Music emotion recognition: Toward new, robust standards in personalized and context-sensitive applications. <em>IEEE Signal Processing Magazine</em>, <em>38</em>(6), 106–114. <a href="https://doi.org/10.1109/MSP.2021.3106232">https://doi.org/10.1109/MSP.2021.3106232</a>
</div>
<div id="ref-grimaud_eerola_2022" class="csl-entry" role="listitem">
Grimaud, A. M., &amp; Eerola, T. (2022). An interactive approach to emotional expression through musical cues. <em>Music &amp; Science</em>, <em>5</em>, 1–23. https://doi.org/<a href="https://doi.org/10.1177/20592043211061745">https://doi.org/10.1177/20592043211061745</a>
</div>
<div id="ref-kassler1966toward" class="csl-entry" role="listitem">
Kassler, M. (1966). Toward musical information retrieval. <em>Perspectives of New Music</em>, 59–67.
</div>
<div id="ref-katayose_sentiment_1988" class="csl-entry" role="listitem">
Katayose, H., Imai, M., &amp; Inokuchi, S. (1988). Sentiment extraction in music. <em>9th <span>International</span> <span>Conference</span> on <span>Pattern</span> <span>Recognition</span></em>, 1083–1084.
</div>
<div id="ref-lindstrom2003expressivity" class="csl-entry" role="listitem">
Lindström, E., Juslin, P. N., Bresin, R., &amp; Williamon, A. (2003). "Expressivity comes from within your soul”: A questionnaire study of music students’ perspectives on expressivity. <em>Research Studies in Music Education</em>, <em>20</em>(1), 23–47.
</div>
<div id="ref-liu_automatic_2003" class="csl-entry" role="listitem">
Liu, D., Lu, L., &amp; Zhang, H.-J. (2003, January). Automatic <span>Mood</span> <span>Detection</span> from <span>Acoustic</span> <span>Music</span> <span>Data</span>. <em>Proc. <span>ISMIR</span> 2003; 4th <span>Int</span>. <span>Symp</span>. <span>Music</span> <span>Information</span> <span>Retrieval</span></em>.
</div>
<div id="ref-lu_automatic_2005" class="csl-entry" role="listitem">
Lu, L., Liu, D., &amp; Zhang, H.-J. (2005). Automatic mood detection and tracking of music audio signals. <em>IEEE Transactions on Audio, Speech, and Language Processing</em>, <em>14</em>(1), 5–18.
</div>
<div id="ref-mandel_support_2006" class="csl-entry" role="listitem">
Mandel, M. I., Poliner, G. E., &amp; Ellis, D. P. (2006). Support vector machine active learning for music retrieval. <em>Multimedia Systems</em>, <em>12</em>(1), 3–13.
</div>
<div id="ref-mendel1969some" class="csl-entry" role="listitem">
Mendel, A. (1969). Some preliminary attempts at computer-assisted style analysis in music. <em>Computers and the Humanities</em>, 41–52.
</div>
<div id="ref-nag2022" class="csl-entry" role="listitem">
Nag, S., Basu, M., Sanyal, S., Banerjee, A., &amp; Ghosh, D. (2022). On the application of deep learning and multifractal techniques to classify emotions and instruments using indian classical music. <em>Physica A: Statistical Mechanics and Its Applications</em>, <em>597</em>. <a href="https://doi.org/10.1016/j.physa.2022.127261">https://doi.org/10.1016/j.physa.2022.127261</a>
</div>
<div id="ref-panda2020audio" class="csl-entry" role="listitem">
Panda, Renato, Malheiro, R., &amp; Paiva, R. P. (2020). Audio features for music emotion recognition: A survey. <em>IEEE Transactions on Affective Computing</em>, <em>14</em>(1), 68–88.
</div>
<div id="ref-panda2013multi" class="csl-entry" role="listitem">
Panda, R., Malheiro, R., Rocha, B., Oliveira, A. P., &amp; Paiva, R. P. (2013). Multi-modal music emotion recognition: A new dataset, methodology and comparative analysis. <em>10th International Symposium on Computer Music Multidisciplinary Research (CMMR 2013)</em>, 570–582.
</div>
<div id="ref-Russell1980" class="csl-entry" role="listitem">
Russell, J. A. (1980). A circumplex model of affect. <em>Journal of Personality and Social Psychology</em>, <em>39</em>(6), 1161–1178.
</div>
<div id="ref-saari_et_al_2015" class="csl-entry" role="listitem">
Saari, P., Eerola, T., Barthet, M., Fazekas, G., &amp; Lartillot, O. (2015). Genre-adaptive semantic computing and audio-based modelling for music mood annotation. <em>IEEE Transactions on Affective Computing</em>, <em>7</em>(2), 122–135.
</div>
<div id="ref-saizclar2022pr" class="csl-entry" role="listitem">
Saiz-Clar, E., Angel Serrano, M., &amp; Manuel Reales, J. (2022). Predicting emotions in music using the onset curve. <em>PSYCHOLOGY OF MUSIC</em>, <em>50</em>(4), 1107–1120. <a href="https://doi.org/10.1177/03057356211031658">https://doi.org/10.1177/03057356211031658</a>
</div>
<div id="ref-sarkar2020recognition" class="csl-entry" role="listitem">
Sarkar, R., Choudhury, S., Dutta, S., Roy, A., &amp; Saha, S. K. (2020). Recognition of emotion in music based on deep convolutional neural network. <em>Multimedia Tools and Applications</em>, <em>79</em>(1), 765–783.
</div>
<div id="ref-yang2008" class="csl-entry" role="listitem">
Yang, Y. H., Lin, Y. C., Su, Y. F., &amp; Chen, H. H. (2008). <span class="nocase">A regression approach to music emotion recognition</span>. <em>IEEE Transactions on Audio Speech and Language Processing</em>, <em>16</em>(2), 448–457.
</div>
<div id="ref-yang2007music" class="csl-entry" role="listitem">
Yang, Yi-Hsuan, Su, Y.-F., Lin, Y.-C., &amp; Chen, H. H. (2007). Music emotion recognition: The role of individuality. <em>Proceedings of the International Workshop on Human-Centered Multimedia</em>, 13–22.
</div>
<div id="ref-zaripov1969" class="csl-entry" role="listitem">
Zaripov, R. K., &amp; Russell, J. (1969). Cybernetics and music. <em>Perspectives of New Music</em>, 115154.
</div>
<div id="ref-zhang_bridge_2016" class="csl-entry" role="listitem">
Zhang, J., Huang, X., Yang, L., &amp; Nie, L. (2016). Bridge the semantic gap between pop music acoustic feature and emotion: <span>Build</span> an interpretable model. <em>Neurocomputing</em>, <em>208</em>, 333–341.
</div>
<div id="ref-zhang2023mo" class="csl-entry" role="listitem">
Zhang, M., Zhu, Y., Zhang, W., Zhu, Y., &amp; Feng, T. (2023). Modularized composite attention network for continuous music emotion recognition. <em>MULTIMEDIA TOOLS AND APPLICATIONS</em>, <em>82</em>(5), 7319–7341. <a href="https://doi.org/10.1007/s11042-022-13577-6">https://doi.org/10.1007/s11042-022-13577-6</a>
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>The best-performing model to date reached 69.83 % in the 2017 competition (Park et al., 2017)<a href="https://scholar.google.com/scholar?q=Representation%20learning%20using%20artist%20labels%20for%20audio%20classification%20tasks%2C%20Park">@park2017representation</a>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">Copyright 2024, Tuomas Eerola</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/tuomaseerola/">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/tuomas_ee">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>



</body></html>