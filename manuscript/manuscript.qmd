---
title             : "Meta-analysis of regression and classification success of emotion ratings from audio"
shorttitle        : "Meta-analysis of music emotion recognition"

author: 
  - name          : "Tuomas Eerola"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Palace Green, Department of Music, Durham University, DH1 3RL Durham, United Kingdom"
    email         : "tuomas.eerola@durham.ac.uk"
  - name          : "Cameron J. Anderson"
    affiliation   : "2"
    address       : "Department of Psychology, Neuroscience & Behaviour, McMaster University, Hamilton, ON, Canada"
    email         : "andersoc@mcmaster.ca"

affiliation:
  - id            : "1"
    institution   : "Department of Music, Durham University"
  - id            : "2"
    institution   : "Department of Psychology, Neuroscience & Behaviour, McMaster University"

author_note: |
  Tuomas Eerola is at the Department of Music, Durham University, UK.
  Cameron J. Anderson is at the Department of Psychology, Neuroscience & Behaviour, McMaster University, Canada.

abstract: |
 This is a meta-analysis of music emotion recognition. An analysis of the articles published between 2014-2024 containing models predicting either valence and arousal or emotion categories was carried out. A total of xx studies were included

keywords          : "music, emotion, recognition, meta-analysis"
wordcount         : "5555"

#bibliography      : ["references.bib"]

figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
mask              : no
csl: apa7.csl

class             : "man"
output            : papaja::apa6_pdf
bibliography: references.bib
---

```{r global options, include = FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```

```{r include = FALSE, echo=FALSE}
#source('load_libraries.R')
## set options ##
```

# Introduction

Emotional engagement is a key reason why people engage with music in their every day activities, and it is also why music is increasingly being used in various health applications [@juslin2022emotions;@agres2021music]. In recent years, significant advances have been made in music information retrieval, particularly in music emotion recognition (MER) tasks [@gomez2021;@panda2020audio]. Improvements in available features, modeling techniques, and datasets have provided the field with opportunities to enhance the accuracy and reliability of predicting annotated emotions from audio. Over the past 25 years, numerous studies have established the types of emotions that listeners perceive and recognize in music. In the last 15 years, research has increasingly focused on tracing these recognized emotions back to specific musical components, such as expressive features [@lindstrom2003expressivity], structural aspects of music [@eerola_friberg_bresin_2013; @anderson2022ex; @grimaud_eerola_2022], acoustic features [@yang2008; @panda2013multi; @saari_et_al_2015; @eerola2011c; @panda2020audio], or emergent properties revealed through deep learning techniques [@er2019music; @sarkar2020recognition].

However, there is no consensus on the extent to which emotions can be accurately recognized by computational models. The current literature presents a diverse and mixed picture regarding the success of models in predicting emotions within the affective circumplex -- valence and arousal-- [@Russell1980] and in classifying distinct emotion categories [@fu2010survey].

# A brief history of MER

Emotion has been widely discussed since the earliest artificial intelligence (AI) applications to music in the 1950s. Whereas early discourse largely focused on generative composition using computers [@zaripov1969], attention later shifted to creating methods to predict emotion using music's structural cues. Novel techniques for information retrieval emerged in the 1950s and 1960s [@fairthorne1968], inspiring analogous developments for automated music analysis (@kassler1966toward; @mendel1969some). These developments would set the stage for early work in music emotion recognition (MER). Katayose et al. (1988) conducted the first study of this nature, creating an algorithm that associated emotions with analyzed chords to generate descriptions like "there is hopeful mood on chord from 69 to 97 \[*sic*\]." [@katayose_sentiment_1988, p. 1087].

In the early 2000s, several research groups conducted studies using regression [@friberg_automatic_2002; @liu_automatic_2003] and classification [@lu_automatic_2005; @feng_popular_2003; @mandel_support_2006] techniques to predict emotion in music audio or MIDI. Citing "MIR researchers' growing interest in classifying music by moods" [@downie_music_2008, p. 1], the Music Information Retrieval EXchange (MIREX) introduced Audio Mood Classification (AMC) to their rotation of tasks in 2007. In the first year, nine systems classified mood labels in a common data set, reaching 52.65% accuracy (SD = 11.19%). These events, along with growing interest in the burgeoning field of affective computing, would lead to an explosion of interest in MER research.[^1]

[^1]: The best-performing model to date reached 69.83 % in the 2017 competition (Park et al., 2017)[\@park2017representation](https://scholar.google.com/scholar?q=Representation%20learning%20using%20artist%20labels%20for%20audio%20classification%20tasks%2C%20Park).

Researchers have assessed regression and classification techniques on diverse corpora with features drawn from music (e.g., audio, MIDI, metadata) and participants (e.g., demographic information, survey responses, physiological signals, etc.). In one widely-cited study, Yang (2008) approached MER as a regression task, predicting the valence (i.e., the negative---positive emotional quality) and arousal (i.e., the calm---exciting quality) of 195 Chinese pop songs, achieving 62% accuracy for arousal but only 28% for valence [@yang2008]. This difference in prediction accuracy between dimensions has reappeared in several subsequent studies [e.g., @bai2016dimensional; @coutinho2013psychoacoustic], with some research suggesting this challenge reflects fewer well-established predictors and more individual differences for valence than arousal [@yang2007music; @eerola2011c].

# The semantic gap in MER

The difficulty in predicting valence reflects a broader challenge in information retrieval. Specifically, relations between low-level predictors from music and text and the perceptual phenomena they model remain poorly understood, reaching a ceiling in prediction accuracy [@celma_foafing_2006]. To address this so-called *semantic gap*, researchers have attempted to identify new feature sets with greater relevance to emotion [@panda2020audio; @chowdhury2021perceived], combine low-, mid-, and high-level features using multimodal data [@celma_foafing_2006], or train neural networks to automatically learn features from audio [@zhang_bridge_2016]. Through these approaches, MER researchers attempt to shatter a so-called *glass ceiling* [@downie_music_2008] by establishing central predictors for emotions. To date, however, no study has systematically compared results of these diverse approaches.

## Aims

Our aim is to evaluate the predictive accuracy of two models of emotional expression in music: (a) models that predict track-specific coordinates in affective circumplex space (valence and arousal), and (b) models that classify discrete emotion categories. We focus on recent to identify key factors such as modeling techniques and features that significantly affect  prediction accuracy. To achieve this, we conduct a meta-analysis of journal articles published in the past 10 years. Based on existing literature, we hypothesize that arousal will be predicted with higher accuracy than valence, as valence tends to be more context-dependent and challenging to model. For emotion classification, we expect simple utilitarian emotions (e.g., fear, anger) will be easier to predict than more complex social emotions (e.g., sadness, nostalgia).

# Methods

We preregistered the meta-analysis plan on 21 June 2024 at OSF, <https://osf.io/c5wgd>, and the plan is also available at <https://tuomaseerola.github.io/metaMER/preregistration/preregistration.html>).

In the search stage, we used three databases, *Web of Science*, *Scopus*, and *Open Alex* to identify journal articles published between 2014 and 2024 containing keywords/title `valence OR arousal OR classi* OR categor* OR algorithm AND music AND emotion AND recognition` (see specific search strings for each database in [SI](http://)). All searches were done in May 2024.

The initial search yielded 553 potential studies after excluding duplicate entries. We interactively screened them for relevance in three stages, resulting in 46 studies that passed our inclusion criteria (music emotion studies using classification or regression methods to predict emotion ratings of music using symbolic or audio features, and containing sufficient detail to convert results to $R^2$ or $MCC$ values see [SI](http://) for a breakdown). After the screening stage, we defined a set of entities to extract characterising (i) music (genre, stimulus number \[N\], duration), (ii) features extracted (number, type, source, defined by [@panda2020audio]), (iii) model type (regression, neural network, SVM, etc.) and outcome measure ($R^2$, *MSE*, *MCC*), (iv) model complexity (i.e., approximate number of features used to predict ratings), and (v) type of model cross-validation.

We converted all regression results into $R^2$ values for valence and arousal and classification results into Matthews correlation coefficient [*MCC*, @chicco2020advantages]. To increase consistency in our analyses, we excluded studies using incompatible features (e.g., spectrograms of audio files [@nag2022]), or dependent variables (e.g., one regression study analyzed valence and arousal together, but not separately [@chin2018]).

\[Figure 1: flowchart of the study inclusions/eliminations\]

```{dot}

digraph sub {
  node [fontname = Helvetica, fontsize = 14, shape = box, width = 4]
  A [label = "Records identified from:
                Web of Science (n = 142)
                Scopus (n = 227)
                OpenALEX (n = 278)" ]
  B [label = "Records removed before screening: 
                Duplicates entries/errors (n = 94)" ]
  C [label = "Records after duplicates removed (n = 553)"]
  D [label = "Records screened (n = 553)"]
  E [label = "Records removed: 
                Irrelevant titles/themes (n = 338)
                Abstract screening (n = 95)
                Discussion (n = 24)"]
  F [label = "Full text articles assessed (n = 96)"]
  G [label = "Studies included in final review (n = 46)"]
  H [label = "Full text articles excluded
             (n = 50)"]
  I [label = "Studies excluded in final review (n = 10)"]
  J [label = "Final selection (n = 36)"]
  A->B
  A->C
  C->D
  D->E
  D->F
  F->H 
  F->G
  G->I
  G->J

  
  { rank = same; A B}
  { rank = same; D, E}
  { rank = same; F, H}
  { rank = same; G, I}
}

```

## Quality Control

The search yielded studies of variable (and occasionally questionable) quality. To mitigate potentially spurious effects resulting from the inclusion of low-quality studies, [we excluded studies lacking sufficient details about stimuli, analyzed features, or model architecture](include%20approx%20#%20for%20each?%20This%20would%20be%20good%20since%20the%20rest%20of%20this%20para%20already%20mentions%20counts.%20If%20this%20take%20a%20long%20time,%20we%20could%20save%20this%20for%20an%20additional%20analysis). Finally, we excluded studies published in journals of questionable relevance/quality, (e.g., *Mathematical Problems in Engineering* ceased publication following 17 retractions published between July and September 2024).

## Study Encoding

To capture key details of each study, we added additional fields to BibTeX entries for each study. Fields included information about the genre/type of stimuli employed, along with their duration and number; the number of analyzed features; and the model type, validation procedure and output measures. Additionally, we included study results using executable *R* code containing custom functions for meta-analysis. For complete details about our encoding procedure, see `studies/extraction_details.qmd` .

# Results

First we describe the overall pattern of data (regression vs classification, modelling techniques, feature numbers, stimulus numbers, datasets, and other details).

TABLE 1: Summary of data (part of `analysis/preprocessing.qmd`)

|            | Regression                      | Classification            | Total |
|:-----------|:--------------------------------|:--------------------------|:------|
| Study N    | 24                              | 14                        | 38    |
| Model N    | 258                             | 108                       | 366   |
| Techniques | Flexible Discriminants (FD): 64 | 37                        | 101   |
| Techniques | Neural Nets (NN): 74            | 27                        | 101   |
| Techniques | Linear Methods (LM): 74         | 24                        | 98    |
| Techniques | Random Forests (RF): 22         | 12                        | 34    |
| Techniques | KS, Add. & KNN (KS): 24         | 8                         | 32    |
| Feature N  | Min=3, Md=472.5, Max=654        | Min=3, Md=231, Max=8904   | NA    |
| Stimulus N | Min=40, Md=324, Max=2486        | Min=124, Md=387, Max=5192 | NA    |

Although the total number of studies meeting the criteria described in the previous section is modest (38 in total), they encompass a large array of models (366 in total) with a relatively even distribution among the three most popular techniques: flexible discriminants, neural nets, and linear methods. The number of features and stimuli within these studies varies significantly, ranging from as few as three features [@battcock2021in] to a maximum of almost 9000 features [@zhang2023mo]. The median number of features differs between regression (473) and classification (231) studies, primarily reflecting the nature of the datasets used in each approach. The number of stimuli is typically around 300-400 (with a median of 324 for regression and 387 for classification), though there is substantial variation, with the extremes from 40 stimuli in @saizclar2022pr to 5192 stimuli in @alvarez2023ri. There are also additional dimensions to consider, such as the type of cross-validation used, the music genres analyzed (whether a single genre, multiple genres, or a mix), the type of journal in which the studies were published, and the source of the extracted features. However, these variables do not lend themselves to a simple summary, so we will revisit them during the interpretation and discussion stages. \[COMMENT: IT MIGHT BE WORTWHILE TO SUMMARISE SOME OF THESE HERE\].

We first report regression studies that predict valence and arousal.

## Prediction success for valence and arousal (or affect dimensions)

See `analysis/analysis.qmd`

Since there are many models contained within each of the studies, we will report the results in two parts; We first give an overview of the results for all models, and then we focus on the best performing models of each study. The best performing model is the model within each study with the highest correlation coefficient. This reduction is done to avoid the issue of multiple models from the same study deflating the results as majority of the models included are relative modest baseline or alternative models that do not represent the novelty or content of the article. Table 2 summarises the results for all models (All) as well as best performing models (Max) for each study. The summary includes the number of models and observations, the correlation coefficient and its 95% confidence interval, the t-value and p-value for the correlation, the heterogeneity statistics $\tau^2$ and $I^2$, calculated through appropriate transformations (Fisher's Z) for the correlation coefficient as part of a random-effects model using `meta` library [@balduzzi2019]. We used Paule-Mandel estimator for between-study heterogeneity [@langan2019comparison] and Knapp-Hartung [@knapp2003improved] adjustments for confidence intervals. In this table we also report two subgroup analyses. One where we have divided the studies according to the number of features they contain (three categories based on quantiles to keep the group size comparable) and into five modelling techniques introduced earlier (Table 1).

Table 2. Meta-analytic diagnostic for all regression studies predicting valence from audio. See Table 1 for the acronyms of the modelling techniques.

| Concept      | Models, obs | $r$ \[95%-CI\]          | $t$   | $p$   | $\tau^2$ | $I^2$ |
|:-------------|:------------|:------------------------|:------|:------|:---------|:------|
| Valence All  | 120,73685   | 0.567 \[0.530; 0.603\]  | 23.7  | .0001 | 0.083    | 97.5% |
| Valence Max  | 24,15660    | 0.659 \[0.557; 0.740\]  | 10.14 | .0001 | 0.138    | 98.2% |
| *N Features* |             |                         |       |       |          |       |
| \<18 F       | 5,3036      | 0.811 \[0.566; 0.775\]  | \-    | \-    | 0.182    | 98.9% |
| 18-260 F     | 11,7318     | 0.548 \[0.343; 0.703\]  | \-    | \-    | 0.133    | 97.6% |
| 260+ F       | 7,4562      | 0.685 \[0.566; 0.775\]  | \-    | \-    | 0.044    | 97.2% |
| *Techniques* |             |                         |       |       |          |       |
| KS           | 2,2582      | 0.466 \[-0.634; 0.942\] | \-    | \-    | 0.0185   | 95.1% |
| LM           | 8,1762      | 0.784 \[0.625; 0.881\]  | \-    | \-    | 0.1370   | 96.4% |
| FD           | 6,4993      | 0.656 \[0.484; 0.779\]  | \-    | \-    | 0.0574   | 96.9% |
| NN           | 4,2249      | 0.340 \[-0.097; 0.668\] | \-    | \-    | 0.0761   | 97.1% |
| RF           | 4,4074      | 0.702 \[0.391; 0.869\]  | \-    | \-    | 0.057    | 98.5% |


-   OTHER GROUPINGS? STIMULUS MIXED/SINGLE GENRE, PREDICTION/EXPLANATION

<!-- see this to ignore I\^2 and rely on prediction interval: https://onlinelibrary.wiley.com/doi/full/10.1002/jrsm.1678 -->

{{< include _figure1.qmd >}}

The results indicate that valence can generally be predicted with moderately accuracy, with the best model from each of the 24 studies achieving an average correlation of _r_ = 0.658 (95% CI: 0.557-0.740), called "valence Max" in Table 2. However, when considering all models across these studies (n = 120), the overall prediction rate drops significantly to _r_ = 0.567. We argue that this lower correlation is likely due to the inclusion of baseline models reported in these studies, which may not reflect the true success of the task for the purposes of our analysis.

Further analysis of between-study heterogeneity, as indexed by the $\tau^2$ (0.138) and Higgins & Thompson's $I^2$ statistic [@higgins2002quantifying] at 98.2%, reveals substantial heterogeneity. Since $I^2$ is heavily influenced by study size (with larger N leading to lower sampling error), its value may be less insightful in this context. In contrast, $\tau^2$, which is less sentive to the number of studies and directly linked to the outcome metric (_r_), provides a more reliable measure of heterogeneity in this case.

To better understand the effects across studies and the nature of the observed heterogeneity, Figure 2 presents a forest plot of the random-effects model based on the best-performing models from all studies.

[TO BE CONTINUED]


Moving on the arousal, ...

Table 3. Meta-analytic diagnostic for all regression studies predicting arousal from audio.

| Concept      | Models, obs | $r$ \[95%-CI\]            | $t$  | $p$    | $\tau^2$ | $I^2$ |
|:-------------|:------------|:--------------------------|:-----|:-------|:---------|:------|
| Arousal      |             | 0.7959 \[0.7666; 0.8218\] | 29.0 | 0.0001 | 0.0676   | 95.6% |
| Arousal Max  | 24,15660    | 0.8070 \[0.7453; 0.8550\] | 10.3 | 0.0001 | 0.155    | 96.8% |
| *N Features* |             |                           |      |        |          |       |
| \<18 F       |             |                           |      |        |          |       |
| 18-260 F     |             |                           |      |        |          |       |
| 260+ F       |             |                           |      |        |          |       |
| *Techniques* |             |                           |      |        |          |       |
| KS           |             |                           |      |        |          |       |
| LM           |             |                           |      |        |          |       |
| FD           |             |                           |      |        |          |       |
| NN           |             |                           |      |        |          |       |
| RF           |             |                           |      |        |          |       |

Figure 2. Forest plot of arousal prediction (using Max?)

<!-- | Arousal Trim |             | 0.7932 \[0.7846; 0.8014\] | 96.5 | 0.0001 | 0.0030   | 73.3% | -->

<!-- | Arousal M    |             | 0.7567 \[0.6752; 0.8199\] | 12.7 | 0.0001 | 0.0739   | 95.6% | -->

<!-- | Arousal Md   |             | 0.7627 \[0.6819; 0.8252\] | 12.7 | 0.0001 | 0.0757   | 95.3% | -->

<!-- | Aro.Max.Trm  | 14,12061    | 0.8182 \[0.8021; 0.8331\] | 25.6 | 0.0001 | 0.0132   | 96.8% | -->

Summarise here the pattern of results

## Classification studies

Summary of details contained in Table 1, but summarise at least the categories predicted before moving onto the main findings.

Table 4. Meta-analytic diagnostic for all classification studies predicting emotion categories from audio.

| Model | Models, obs | $r$ \[95%-CI\]            | $t$  | $p$    | $\tau^2$ | $I^2$ |
|:------|:------------|:--------------------------|:-----|:-------|:---------|:------|
| All   | 89,87347    | 0.8074 \[0.7681; 0.8407\] | 21.4 | 0.0001 | 0.2415   | 99.7% |
| Max   | 14,17184    | 0.8564 \[0.7386; 0.9234\] | 8.32 | 0.0001 | 0.329    | 99.8% |

<!-- | All Trim | 29,6499     | 0.8185 \[0.8046; 0.8314\] | 58.3 | 0.0001 | 0.0066   | 60.4% | -->

<!-- | Max Trim | 6,3653      | 0.8689 \[0.7760; 0.9249\] | 11.6 | 0.0001 | 0.0749   | 97.5% | -->

Heterogeneity issues

Figure 3. Forest plot of arousal prediction (Max?) (Unless we do some custom plotting)

-   Figure Optional: Funnel plot (I haven't seen this yet)

# Conclusion and Discussion

## Concise summary of what we did and found

## Main outcomes

-   Arousal is easier to predict (r = 0.7627) than valence (r = 0.6236), as we predicted. The glass ceiling seems to be at ...
-   Classification ...
-   Model accuracy is surprisingly little affected by the number of features (?) or modelling technique (?).
-   Some of the complex state-of-the-art techniques (e.g., NNs) do not deliver impressive improvements over older techniques (e.g., SVR, RF)
-   Variation in study/model/data quality is large and can be seen in heterogenuity and the amount of studies eliminated

## Calls for action/points to improve in such studies

-   *Documentation* the details in full (features, stimuli, model details, cross-validation)
-   *Quality* of the underlying data (emotion ratings, classes, or even stimulus properties?
-   *Generalisibility* of the models (some studies such as X and Y address this by applying the models across several datasets)
-   Diversity in the evaluative aspects of studies: *overfitting*, numerous ways of cross-validating, not sharing data or analysis scripts, not reporting in the same way
-   What proportion of stimuli are Western music, and what genres tend to dominate?

### Funding statement

CA was funded by Mitacs Globalink Research Award (Mitacs & British High Commission - Ottawa, Canada).

### Competing interests statement

There were no competing interests.

### Open practices statement

Study preregistration, data, analysis scripts and supporting information is available at Github, <https://tuomaseerola.github.io/metaMER>.

### Acknowledgements

We thank Greggs food-on-the-go retailer for sustaining the work with affordable sandwiches and coffee.

# References
