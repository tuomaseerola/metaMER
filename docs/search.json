[
  {
    "objectID": "manuscript/manuscript.html",
    "href": "manuscript/manuscript.html",
    "title": "Meta-analysis of regression and classification success of emotion ratings from audio",
    "section": "",
    "text": "Emotional expression is one of the central reasons why people engage with music.\nGreat advances in music information retrieval have been made in recent years. The available features, modelling techniques and datasets have given scholars opportunities to refine the accuracy and reliability of predicting annotated emotions from audio.\nNumerous studies over the last 25 years have established what emotions listeners perceive and recognise in music (Gómez-Cañón et al., 2021). In the last 15 years, it has become possible to trace the recognised emotions to musical contents such as expressive features (Lindström et al., 2003), structural aspects of music (Anderson & Schutz, 2022; Eerola et al., 2013; Grimaud & Eerola, 2022), or acoustic features (Eerola, 2011; Panda et al., 2013; Saari et al., 2015; Yang et al., 2008) or emergent properties identified through deep learning (Er & Aydilek, 2019; Sarkar et al., 2020).\n\nHowever, there is no consensus on to what degree emotions can be recognised by computational models and the literature to date paints a diverse picture of success for concepts in affective circumplex – valence and arousal– (Russell, 1980) and classifying various emotion categories (Fu et al., 2010).\n\n\n\nOur aim is to establish the level of predictive accuracy for both models of emotional expression that can account for track-specific coordinates in affective circumple space (valence and arousal) and classification of emotion categories based on available and recent studies.\nWe seek to identify the types of issues (modelling techniques, features, and musical qualities used) that significantly influence the prediction rates.\nTo achieve these aims, we carry out a meta-analysis focused on journal articles published in the last 10 years.\nWe outline broad hypotheses such as arousal being predicted to a higher degree than valence, which is more challenging and more context dependent than arousal. For classification, simple utilitarian emotions (e.g., fear, anger) will be easier to predict than complex social emotions (e.g., sadness, nostalgia)."
  },
  {
    "objectID": "manuscript/manuscript.html#aims",
    "href": "manuscript/manuscript.html#aims",
    "title": "Meta-analysis of regression and classification success of emotion ratings from audio",
    "section": "",
    "text": "Our aim is to establish the level of predictive accuracy for both models of emotional expression that can account for track-specific coordinates in affective circumple space (valence and arousal) and classification of emotion categories based on available and recent studies.\nWe seek to identify the types of issues (modelling techniques, features, and musical qualities used) that significantly influence the prediction rates.\nTo achieve these aims, we carry out a meta-analysis focused on journal articles published in the last 10 years.\nWe outline broad hypotheses such as arousal being predicted to a higher degree than valence, which is more challenging and more context dependent than arousal. For classification, simple utilitarian emotions (e.g., fear, anger) will be easier to predict than complex social emotions (e.g., sadness, nostalgia)."
  },
  {
    "objectID": "manuscript/manuscript.html#prediction-success-for-valence-and-arousal",
    "href": "manuscript/manuscript.html#prediction-success-for-valence-and-arousal",
    "title": "Meta-analysis of regression and classification success of emotion ratings from audio",
    "section": "Prediction success for valence and arousal",
    "text": "Prediction success for valence and arousal\nOut of XX studies reporting this task, the majority (xx%) …\n\n\n\nConcept\nN\nMd \\(R^2\\)\n\n\n\n\nValence\n??\n??\n\n\nArousal\n??\n??\n\n\n\nTABLE: xxxx"
  },
  {
    "objectID": "studies/extraction_details.html",
    "href": "studies/extraction_details.html",
    "title": "Extraction Details",
    "section": "",
    "text": "To capture relevant information from studies, we expanded BiBTeX fields for each study with additional fields. For reproducibility, these instructions provide information on the process followed for each field."
  },
  {
    "objectID": "studies/extraction_details.html#identifier",
    "href": "studies/extraction_details.html#identifier",
    "title": "Extraction Details",
    "section": "IDENTIFIER",
    "text": "IDENTIFIER\nUnique identifier of article. Contains last name of lead author, year of publication and first two letters of article title. Hyphenated last names collapsed."
  },
  {
    "objectID": "studies/extraction_details.html#author",
    "href": "studies/extraction_details.html#author",
    "title": "Extraction Details",
    "section": "AUTHOR",
    "text": "AUTHOR\nNames of all authors. Last name precedes first name and separated by comma. For multiple authors “and” precedes each listed subsequent author. E.g., Sorussa, Kanawat and Choksuriwong, Anant and Karnjanadecha, Montri"
  },
  {
    "objectID": "studies/extraction_details.html#journal",
    "href": "studies/extraction_details.html#journal",
    "title": "Extraction Details",
    "section": "JOURNAL",
    "text": "JOURNAL\nTitle of journal containing article."
  },
  {
    "objectID": "studies/extraction_details.html#note",
    "href": "studies/extraction_details.html#note",
    "title": "Extraction Details",
    "section": "NOTE",
    "text": "NOTE\nIncludes number of citing articles and open access details. E.g., Cited by: 4; All Open Access, Gold Open Access, Green Open Access"
  },
  {
    "objectID": "studies/extraction_details.html#title",
    "href": "studies/extraction_details.html#title",
    "title": "Extraction Details",
    "section": "TITLE",
    "text": "TITLE\nTitle of article."
  },
  {
    "objectID": "studies/extraction_details.html#volume",
    "href": "studies/extraction_details.html#volume",
    "title": "Extraction Details",
    "section": "VOLUME",
    "text": "VOLUME\nVolume number of publication."
  },
  {
    "objectID": "studies/extraction_details.html#year",
    "href": "studies/extraction_details.html#year",
    "title": "Extraction Details",
    "section": "YEAR",
    "text": "YEAR\nPublication year."
  },
  {
    "objectID": "studies/extraction_details.html#doi",
    "href": "studies/extraction_details.html#doi",
    "title": "Extraction Details",
    "section": "DOI",
    "text": "DOI\nDigital object identifier of article."
  },
  {
    "objectID": "studies/extraction_details.html#abstract",
    "href": "studies/extraction_details.html#abstract",
    "title": "Extraction Details",
    "section": "ABSTRACT",
    "text": "ABSTRACT\nComplete text of article abstract."
  },
  {
    "objectID": "studies/extraction_details.html#source",
    "href": "studies/extraction_details.html#source",
    "title": "Extraction Details",
    "section": "SOURCE",
    "text": "SOURCE\nDatabase article was sourced from. Scopus, Web of Science (WoS) or OpenAlex."
  },
  {
    "objectID": "studies/extraction_details.html#author_keywords",
    "href": "studies/extraction_details.html#author_keywords",
    "title": "Extraction Details",
    "section": "AUTHOR_KEYWORDS",
    "text": "AUTHOR_KEYWORDS\nCorresponding keywords for article indicated by author."
  },
  {
    "objectID": "studies/extraction_details.html#notes_authorinitials",
    "href": "studies/extraction_details.html#notes_authorinitials",
    "title": "Extraction Details",
    "section": "NOTES_AUTHORINITIALS",
    "text": "NOTES_AUTHORINITIALS\nDecision and comments by respective author"
  },
  {
    "objectID": "studies/extraction_details.html#stimulus_type",
    "href": "studies/extraction_details.html#stimulus_type",
    "title": "Extraction Details",
    "section": "STIMULUS_TYPE",
    "text": "STIMULUS_TYPE\nMetadata pertaining to stimuli employed in paradigm. Can be listed as genres of music stimuli employed, or if stimuli come from a standard database, name of standard."
  },
  {
    "objectID": "studies/extraction_details.html#stimulus_duration",
    "href": "studies/extraction_details.html#stimulus_duration",
    "title": "Extraction Details",
    "section": "STIMULUS_DURATION",
    "text": "STIMULUS_DURATION\nDuration of stimuli, if applicable. Unit of measurement (seconds, measures) specified in STIMULUS_DURATION_UNIT"
  },
  {
    "objectID": "studies/extraction_details.html#stimulus_duration_unit",
    "href": "studies/extraction_details.html#stimulus_duration_unit",
    "title": "Extraction Details",
    "section": "STIMULUS_DURATION_UNIT",
    "text": "STIMULUS_DURATION_UNIT\nUnit of measurement pertaining to STIMULUS_DURATION. E.g., seconds, measures, etc."
  },
  {
    "objectID": "studies/extraction_details.html#stimulus_n",
    "href": "studies/extraction_details.html#stimulus_n",
    "title": "Extraction Details",
    "section": "STIMULUS_N",
    "text": "STIMULUS_N\nNumber of stimuli employed in experiment. If multiple experimental conditions reported, separate \\(n\\) by conditions where possible."
  },
  {
    "objectID": "studies/extraction_details.html#feature_n",
    "href": "studies/extraction_details.html#feature_n",
    "title": "Extraction Details",
    "section": "FEATURE_N",
    "text": "FEATURE_N\nNumber of features included in data modeling (if available)."
  },
  {
    "objectID": "studies/extraction_details.html#participant_n",
    "href": "studies/extraction_details.html#participant_n",
    "title": "Extraction Details",
    "section": "PARTICIPANT_N",
    "text": "PARTICIPANT_N\nTotal number of participants in experiment."
  },
  {
    "objectID": "studies/extraction_details.html#participant_expertise",
    "href": "studies/extraction_details.html#participant_expertise",
    "title": "Extraction Details",
    "section": "PARTICIPANT_EXPERTISE",
    "text": "PARTICIPANT_EXPERTISE\nExpertise of annotators. E.g., experts, non-experts, not specified."
  },
  {
    "objectID": "studies/extraction_details.html#participant_origin",
    "href": "studies/extraction_details.html#participant_origin",
    "title": "Extraction Details",
    "section": "PARTICIPANT_ORIGIN",
    "text": "PARTICIPANT_ORIGIN\nOrigin country of participants, or online platform participants were recruited from (e.g., MTurk)"
  },
  {
    "objectID": "studies/extraction_details.html#participant_sampling",
    "href": "studies/extraction_details.html#participant_sampling",
    "title": "Extraction Details",
    "section": "PARTICIPANT_SAMPLING",
    "text": "PARTICIPANT_SAMPLING\nHow participants were recruited (e.g., convenience, random sampling, crowdsourcing)"
  },
  {
    "objectID": "studies/extraction_details.html#participant_task",
    "href": "studies/extraction_details.html#participant_task",
    "title": "Extraction Details",
    "section": "PARTICIPANT_TASK",
    "text": "PARTICIPANT_TASK\nNature of rating/classification task undertaken by participants. E.g., rate, annotate."
  },
  {
    "objectID": "studies/extraction_details.html#feature_categories",
    "href": "studies/extraction_details.html#feature_categories",
    "title": "Extraction Details",
    "section": "FEATURE_CATEGORIES",
    "text": "FEATURE_CATEGORIES\nNames of categories analyzed features pertain to, based on names in Panda (2021). Includes names of all pertinent categories: Melody, Rhythm, Timbre, Pitch, Tonality, Expressivity, Texture, Form, Vocal, High-Level"
  },
  {
    "objectID": "studies/extraction_details.html#feature_source",
    "href": "studies/extraction_details.html#feature_source",
    "title": "Extraction Details",
    "section": "FEATURE_SOURCE",
    "text": "FEATURE_SOURCE\nName(s) of feature analysis toolbox(es)."
  },
  {
    "objectID": "studies/extraction_details.html#feature_reduction_method",
    "href": "studies/extraction_details.html#feature_reduction_method",
    "title": "Extraction Details",
    "section": "FEATURE_REDUCTION_METHOD",
    "text": "FEATURE_REDUCTION_METHOD\nName(s) of feature reduction or feature selection methods employed."
  },
  {
    "objectID": "studies/extraction_details.html#model_category",
    "href": "studies/extraction_details.html#model_category",
    "title": "Extraction Details",
    "section": "MODEL_CATEGORY",
    "text": "MODEL_CATEGORY\nName of model type (regression, classification, or both)."
  },
  {
    "objectID": "studies/extraction_details.html#model_detail",
    "href": "studies/extraction_details.html#model_detail",
    "title": "Extraction Details",
    "section": "MODEL_DETAIL",
    "text": "MODEL_DETAIL\nAdditional information pertaining to predictive model, such as the name of algorithm used and other pertinent parameters. E.g., Random Forest, Commonality Analysis, Multiple Regression, Neural Networks, LDSM."
  },
  {
    "objectID": "studies/extraction_details.html#model_measure",
    "href": "studies/extraction_details.html#model_measure",
    "title": "Extraction Details",
    "section": "MODEL_MEASURE",
    "text": "MODEL_MEASURE\nMetric used in model evaluation. E.g., \\(R^2\\), \\(MSE\\), \\(CCC\\), Classification accuracy, etc."
  },
  {
    "objectID": "studies/extraction_details.html#model_complexity_parameters",
    "href": "studies/extraction_details.html#model_complexity_parameters",
    "title": "Extraction Details",
    "section": "MODEL_COMPLEXITY_PARAMETERS",
    "text": "MODEL_COMPLEXITY_PARAMETERS\nAdditional information pertaining to predictive model. E.g., training epochs: 100; n layers: 1, 2; LSTM units: 124,248."
  },
  {
    "objectID": "studies/extraction_details.html#model_rate_emotion_names",
    "href": "studies/extraction_details.html#model_rate_emotion_names",
    "title": "Extraction Details",
    "section": "MODEL_RATE_EMOTION_NAMES",
    "text": "MODEL_RATE_EMOTION_NAMES\nNames of predicted emotions. E.g., valence, arousal, happy, sad, angry, fearful, etc."
  },
  {
    "objectID": "studies/extraction_details.html#model_rate_emotion_values",
    "href": "studies/extraction_details.html#model_rate_emotion_values",
    "title": "Extraction Details",
    "section": "MODEL_RATE_EMOTION_VALUES",
    "text": "MODEL_RATE_EMOTION_VALUES\nPertinent prediction of model summaries. Report as R named arrays, including summary statistics in variables. When reporting results of multiple models, concatenate multiple entries with mrbind. When reporting results for different toolboxes or feature subsets, assign each to a new BiBTeX field with relevant identifier following final underscore. See additional details below."
  },
  {
    "objectID": "studies/extraction_details.html#model_validation",
    "href": "studies/extraction_details.html#model_validation",
    "title": "Extraction Details",
    "section": "MODEL_VALIDATION",
    "text": "MODEL_VALIDATION\nValidation method used (if applicable). E.g., 10-fold cross validation, leave one out cross validation."
  },
  {
    "objectID": "studies/extraction_details.html#classification",
    "href": "studies/extraction_details.html#classification",
    "title": "Extraction Details",
    "section": "Classification",
    "text": "Classification\nConfusion matrices can be encoded similarly through a function that assigns relevant meta-parameters to the model_parameters attribute of the output matrix. The row names are then replaced with the column names for clearer output. This assumes row names and column names of the confusion matrix are listed in the same order.\nconfusion_matrix(\nlibrary.model.features.data.experiment =  c(class_1 = 0, class_2 = 0, ..., class_n = 0),\nclass_2 = c(0, 0, ..., 0),\n...\n)\nWhen confusion matrices are not available, encode available parameters (accuracy, precision, recall, \\(F\\) scores, etc.) using the standard nomenclature to distinguish relevant outcomes for each class:\nbind_field(\nlibrary.model.features.data.experiment = c(class_measure.summaryStat = 0, ...),\n...\n)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "README",
    "section": "",
    "text": "How well we can predict emotions in music? What is the evidence in the published literature for explaining what emotions the listeners can perceive in music when the source consists of audio examples. To what degree the results are dependent on the actual models, emotions, musical/acoustic features, or musical materials or participants?\nTo obtain answers to these questions, we have set out to record and analyse the current state of the art from the literature using a meta-analysis paradigm. We focus on Music Emotion Recognition and hence the acronym metaMER.\nThe public-facing version of the repository is available at https://tuomaseerola.github.io/metaMER/\n\n\nWe define the aims and methods in preregistration plan, which has beeb preregistered at OSF.\n\n\n\nSearch databases and criteria are documented in studies/search_syntax.qmd.\n\n\n\nData extraction is described in extraction details. See also pass3 comparison.\nThe data will be parsed to tabular format using a custom library_parser.qmd.\n\n\n\nData analysis is covered in analysis/analysis.qmd document.\n\n\n\nThe study report is available manuscript/manuscript.qmd document."
  },
  {
    "objectID": "index.html#plan",
    "href": "index.html#plan",
    "title": "README",
    "section": "",
    "text": "We define the aims and methods in preregistration plan, which has beeb preregistered at OSF."
  },
  {
    "objectID": "index.html#study-search-and-selection",
    "href": "index.html#study-search-and-selection",
    "title": "README",
    "section": "",
    "text": "Search databases and criteria are documented in studies/search_syntax.qmd."
  },
  {
    "objectID": "index.html#data-extraction-and-coding",
    "href": "index.html#data-extraction-and-coding",
    "title": "README",
    "section": "",
    "text": "Data extraction is described in extraction details. See also pass3 comparison.\nThe data will be parsed to tabular format using a custom library_parser.qmd."
  },
  {
    "objectID": "index.html#analysis",
    "href": "index.html#analysis",
    "title": "README",
    "section": "",
    "text": "Data analysis is covered in analysis/analysis.qmd document."
  },
  {
    "objectID": "index.html#manuscript",
    "href": "index.html#manuscript",
    "title": "README",
    "section": "",
    "text": "The study report is available manuscript/manuscript.qmd document."
  },
  {
    "objectID": "preregistration/preregistration.html",
    "href": "preregistration/preregistration.html",
    "title": "metaMER",
    "section": "",
    "text": "This preregistration is made with preregr package from https://preregr.opens.science/ that implements the BMJ published guidance for meta-analysis protocols (Shamseer et al., 2015).\nMeta-analysis Pre-registration: Music Emotion Recognition\n\nSection: Metadata\n\n\n\nTitle\n\n\ntitle\n\n\n\nMusic emotion recognition: Meta-analysis of regression and classification success of emotion ratings from audio\n\n\n\n\n\nContributors\n\n\nauthors\n\n\n\nEerola, T., Anderson, C. J.\n\n\n\n\n\nSubjects\n\n\ntarget_discipline\n\n\n\nmusic cognition, music information retrieval, music psychology\n\n\n\n\n\nTasks and roles\n\n\ntasks_and_roles\n\n\n\nequal contribution\n\n\n\n\nSection: Review methods\n\n\n\nType of review\n\n\ntype_of_review\n\n\n\nMeta-analysis\n\n\n\n\n\nReview stages\n\n\nreview_stages\n\n\n\nSearch, Screening, Extraction, Synthesis\n\n\n\n\n\nCurrent review stage\n\n\ncurrent_stage\n\n\n\nScreening\n\n\n\n\n\nStart date\n\n\nstart_date\n\n\n\n2024-05-15 2024-05-15\n\n\n\n\n\nEnd date\n\n\nend_date\n\n\n\n2024-06-30\n\n\n\n\n\nBackground\n\n\nbackground\n\n\n\nThe aim is to establish the current state of the model success in predicting emotions expressed by music from audio. We will focus on the last 10 years of research and especially the research that has predicted valence and arousal ratings from music audio. No such analysis exists and there are interesting challenges in predicting emotional content of music that relates to specificity of the music and the type of emotions and features used that would benefit from a systematic analysis.\n\n\n\n\n\nPrimary research question(s)\n\n\nprimary_research_question\n\n\n\nTo what degree can arousal and valence ratings of emotions expressed by music be predicted from audio? How are the prediction rates related to genres of music, the type of models used, the type of features, modelling design and cross-validation utilised, and the model complexity and parsimony?\n\n\n\n\n\nSecondary research question(s)\n\n\nsecondary_research_question\n\n\n\nWhat is the prediction rate related to classification of quadrants in the affective circumplex?\n\n\n\n\n\nExpectations / hypotheses\n\n\nexpectations_hypotheses\n\n\n\nPrediction of arousal ratings is generally high and robust, and in terms of the model outcome metrics (correlation), achieves at least r = 0.77 (R square of 0.60). Prediction of valence ratings from audio is more challenging and more context dependent and will achieve generally a lower prediction rate, r = 0.63 (R square 0.40)\n\n\n\n\n\nDependent variable(s) / outcome(s) / main variables\n\n\ndvs_outcomes_main_vars\n\n\n\nRegression model performance will be converted to Pearson correlation coefficients and classification model performance will be converted to Matthews correlation coefficient (MCC) when possible.\n\n\n\n\n\nIndependent variable(s) / intervention(s) / treatment(s)\n\n\nivs_intervention_treatment\n\n\n\nMusic genre, prediction type (linear or classification), feature type (based on prior work by Panda et al., 2020), model complexity (high, medium, low), model validation (exists or not)\n\n\n\n\n\nAdditional variable(s) / covariate(s)\n\n\nadditional_variables\n\n\n\nUnspecified\n\n\n\n\n\nSoftware\n\n\nsoftware\n\n\n\nR and Github repository\n\n\n\n\n\nFunding\n\n\nfunding\n\n\n\nMitacs Globalink Research Award (Mitacs & British High Commission - Ottawa, Canada)\n\n\n\n\n\nConflicts of interest\n\n\ncois\n\n\n\nThere are no identified conflicts of interests.\n\n\n\n\n\nOverlapping authorships\n\n\noverlapping_authorships\n\n\n\nNot applicable\n\n\n\n\nSection: Search strategy\n\n\n\nDatabases\n\n\ndatabases\n\n\n\nWeb of Science, Scopus, and Open Alex\n\n\n\n\n\nInterfaces\n\n\ninterfaces\n\n\n\nWeb of Science, Scopus, and Open Alex\n\n\n\n\n\nGrey literature\n\n\ngrey_literature\n\n\n\nNot included\n\n\n\n\n\nInclusion and exclusion criteria\n\n\ninclusions_exclusion_criteria\n\n\n\nSample, Phenomenon of Interest, Design, Evaluation, Research type\n\n\n\n\n\nQuery strings\n\n\nquery_strings\n\n\n\nScopus: TITLE-ABS-KEY ( valence OR arousal OR classi OR categor OR algorithm AND music  AND emotion AND recognition ) AND PUBYEAR &gt; 2013 AND PUBYEAR &lt; 2025 AND  ( LIMIT-TO ( DOCTYPE , “ar” ) )  Web of science:  (DT=(Article) AND PY=(2014-2025)) AND ALL=(music emotion recognition valence arousal)  Open Alex:  https://openalex.org/works?page=1&filter=default.search%3Amusic%20emotion%20recognition%20valence%20arousal, type%3Atypes%2Farticle,publication_year%3A2014-2024, keywords.id%3Akeywords%2Femotion-recognition, keywords.id%3Akeywords%2Faffective-computing, language%3Alanguages%2Fen, open_access.any_repository_has_fulltext%3Atrue \n\n\n\n\n\nSearch validation procedure\n\n\nsearch_validation_procedure\n\n\n\nManual checking, separate keywords searches\n\n\n\n\n\nOther search strategies\n\n\nother_search_strategies\n\n\n\nNot applied\n\n\n\n\n\nProcedures to contact authors\n\n\nprocedure_for_contacting_authors\n\n\n\nUnspecified\n\n\n\n\n\nResults of contacting authors\n\n\nresults_of_contacting_authors\n\n\n\nNot carried out\n\n\n\n\n\nSearch expiration and repetition\n\n\nsearch_expiration_and_repetition\n\n\n\nSearches were done during the active search period in late May early June 2024 and no repetition is planned.\n\n\n\n\n\nSearch strategy justification\n\n\nsearch_strategy_justification\n\n\n\nThe three major databases should be able yield a robust picture of the topic\n\n\n\n\n\nMiscellaneous search strategy details\n\n\nmisc_search_strategy_details\n\n\n\nNo alternative searches were articulated or envisaged.\n\n\n\n\nSection: Screening\n\n\n\nScreening stages\n\n\nscreening_stages\n\n\n\nWe completed screening using custom fields inserted to the bibtex file and managed with citation managers (jabref and bibdesk). To filter relevant studies, we followed a three-stage screening procedure.  In stage 1, we screened the 553 studies’ titles for relevance, removing irrelevant studies and recording exclusion criteria (see Used exclusion criteria). CA assigned 63 studies to the High Priority based on titles’ relevance, assigned 338 studies to Low Priority based on irrelevant titles, and 152 studies to Medium Priority for additional screening. In stage 2, CA assessed the 152 Medium Priority studies for relevance by screening abstracts. 95 studies’ status changed to Low Priority, whereas 30 studies’ status changed to High Priority. 27 studies remained in the Medium priority category. TE and CA evaluated the remaining 27 studies’, moving 15 to the High Priority Category and 12 to the Medium Priority Category. For studies moved to Low Priority, brief BiBTex comments summarized the rationale for exclusion. In stage 3, TE and CA independently screened Priority 1 studies for relevance, including an include, exclude, or unsure decision in a user-comment BiBTeX field.\n\n\n\n\n\nScreened fields / masking\n\n\nscreened_fields_masking\n\n\n\nWe left authors, titles, publication years, and journal names unmasked.\n\n\n\n\n\nUsed exclusion criteria\n\n\nused_exclusion_criteria\n\n\n\nWe excluded studies according to the following exclusion criteria: soundscapes/vocalisations, non-music audio, video clips, physiological markers, dance, video/movie, physiological/EEG/ECG/MEG/GSR/brain imaging/heart rate/neuroscience/brain studies, sensor data, multimodal, autism, ageing, review/systematic review/overview/survey, face emotion recognition, mental health, music therapy, schizophrenia, memory/emotion factors as IVs, recommender systems, or systems that identify the location of emotional excerpts. We included results from some studies meeting exclusion criteria (e.g., multimodal studies involving physiological measurements) if they reported separately on acoustic-only models.\n\n\n\n\n\nScreener instructions\n\n\nscreener_instructions\n\n\n\nAs described above.\n\n\n\n\n\nScreening reliability\n\n\nscreening_reliability\n\n\n\nIn the pass 1 and 2, we included a quality control check after the pass to discuss the identified categories. In the third pass, we double-coded decisions, resolving discrepancies through discussion.\n\n\n\n\n\nScreening reconciliation procedure\n\n\nscreening_reconciliation_procedure\n\n\n\nWe reconcile discrepancies through discussion, resolving “unsure” votes first, followed by discrepancies in include/exclude decisions between authors Results of this updating procedure are available in the Pass 3 comparison document.\n\n\n\n\n\nSampling and sample size\n\n\nsampling_and_sample_size\n\n\n\nWe identified and retained 553 articles from Scopus, Web of Science, and Open Alex based on the search strategy outlined above. See table at the end that details the cumulative exclusions.\n\n\n\n\n\nScreening procedure justification\n\n\nscreening_procedure_justification\n\n\n\nTo offer a broad summary of music emotion recognition tasks, we attempted to include all studies involving prediction with acoustic features. We performed screening unblinded and determined inclusion/exclusion criteria based on studies’ relevance to the task explored.\n\n\n\n\n\nData management and sharing\n\n\nscreening_data_management_and_sharing\n\n\n\nSources will be shared as (a) BibTeX library(ies) including reviewer notes.\n\n\n\n\n\nMiscellaneous screening details\n\n\nmisc_screening_details\n\n\n\nUnspecified\n\n\n\n\nSection: Extraction\n\n\n\nEntities to extract\n\n\nentities_to_extract\n\n\n\nThese are listed and defined in extraction details.\n\n\n\n\n\nExtraction stages\n\n\nextraction_stages\n\n\n\nThe data extraction will be completed in stages. In the first stage, CA will complete a pass of the collection using our initial entities to extract document. The challenges are discussed and the entities are revised.\n\n\n\n\n\nExtractor instructions\n\n\nextractor_instructions\n\n\n\nSee extraction details.\n\n\n\n\n\nExtractor blinding\n\n\nextractor_blinding\n\n\n\nBlinding was not used.\n\n\n\n\n\nExtraction reliability\n\n\nextraction_reliability\n\n\n\nCA will perform extractions; TE will verify extractions for quality assurance.\n\n\n\n\n\nExtraction reconciliation procedure\n\n\nextraction_reconciliation_procedure\n\n\n\nDiscussion and joint decision for studies where extraction proves to be challenging and issues of interpretation arise.\n\n\n\n\n\nExtraction procedure justification\n\n\nextraction_procedure_justification\n\n\n\nThese are documented in the extraction details.\n\n\n\n\n\nData management and sharing\n\n\nextraction_data_management_and_sharing\n\n\n\nWe retain the information of the studies in shared bibtex files, extraction data will be stored in ascii data files (.bibtex), and the parser for reading the data from .bibtex files to R for the analysis will be available (as quarto/markdown/R files), and all these are managed, structured, shared and documented in Github repository according to FAIR principles.\n\n\n\n\n\nMiscellaneous extraction details\n\n\nmisc_extraction_details\n\n\n\nNA\n\n\n\n\nSection: Synthesis and Quality Assessment\n\n\n\nPlanned data transformations\n\n\nplanned_data_transformations\n\n\n\nFor regression studies, we convert all metrics to Pearson correlation coefficients. For classification studies, we convert the outcomes of classification to Matthews Correlation Coefficient (MCC) from the precision, accuracy, specificity, F1 scores. Alternatively, we use Cohen’s kappa for multiple classes.\n\n\n\n\n\nMissing data\n\n\nmissing_data\n\n\n\nIf no main outcome variables are available, we exclude the study.\n\n\n\n\n\nData validation\n\n\ndata_validation\n\n\n\nNone planned beyond the staged approached already documented in extraction process.\n\n\n\n\n\nQuality assessment\n\n\nquality_assessment\n\n\n\nNot all the bias assessment tools for clinical studies are relevant for our purposes, we adapt the overall approached advocated in [Higgins et al. (2011)] (https://doi.org/10.1136/bmj.d5928).\n\n\n\n\n\nSynthesis plan\n\n\nsynthesis_plan\n\n\n\nWe analyse regression and classification studies separately, and depending on the quantity of the studies forming suitable sub-groupings based on techniques, materials or music collections/genres, we may further synthesise the results across groupings that are formed along these subsets.\n\n\n\n\n\nCriteria for conclusions / inference criteria\n\n\ncriteria_for_conclusions\n\n\n\nNA\n\n\n\n\n\nSynthesist masking\n\n\nsynthesis_masking\n\n\n\nNA\n\n\n\n\n\nSynthesis reliability\n\n\nsynthesis_reliability\n\n\n\nNA\n\n\n\n\n\nSynthesis reconciliation procedure\n\n\nsynthesis_reconciliation_procedure\n\n\n\nNA\n\n\n\n\n\nPublication bias analyses\n\n\npublication_bias\n\n\n\nWe utilise Egger’s test to assess the publication bias and potentially correct the effect size bias by selecting 10% most precise effect sizes as recommended by Van Aert, Wicherts, & Van Assen (2019).\n\n\n\n\n\nSensitivity analyses / robustness checks\n\n\nsensitivity_analysis\n\n\n\nWithin regression and classificiation tasks, we will carry out sensitivity analysis using sub-groups of studied based on type of models, and the type of journal the studies were published in.\n\n\n\n\n\nSynthesis procedure justification\n\n\nsynthesis_procedure_justification\n\n\n\nWe share our justification of the synthesis and the subsetting carried out in the manuscript but we have not formulated these in advance except for synthesizing classiciation and regression approaches separately and creating subsets within these approaches according to techniques and datasets utilised.\n\n\n\n\n\nSynthesis data management and sharing\n\n\nsynthesis_data_management_and_sharing\n\n\n\nWe share the data, procedures, definitions, the analysis scripts with the outcomes as R code in Quarto notes at Github.\n\n\n\n\n\nMiscellaneous synthesis details\n\n\nmisc_synthesis_details\n\n\n\nUnspecified"
  },
  {
    "objectID": "preregistration/preregistration.html#preregr-prereg-spec-4fNbMqhK9n",
    "href": "preregistration/preregistration.html#preregr-prereg-spec-4fNbMqhK9n",
    "title": "metaMER",
    "section": "Meta-analysis Pre-registration: Music Emotion Recognition",
    "text": "Meta-analysis Pre-registration: Music Emotion Recognition\n\nSection: Metadata\n\n\n\nTitle\n\n\ntitle\n\n\n\nMusic emotion recognition: Meta-analysis of regression and classification success of emotion ratings from audio\n\n\n\n\n\nContributors\n\n\nauthors\n\n\n\nEerola, T., Anderson, C. J.\n\n\n\n\n\nSubjects\n\n\ntarget_discipline\n\n\n\nmusic cognition, music information retrieval, music psychology\n\n\n\n\n\nTasks and roles\n\n\ntasks_and_roles\n\n\n\nequal contribution\n\n\n\n\nSection: Review methods\n\n\n\nType of review\n\n\ntype_of_review\n\n\n\nMeta-analysis\n\n\n\n\n\nReview stages\n\n\nreview_stages\n\n\n\nSearch, Screening, Extraction, Synthesis\n\n\n\n\n\nCurrent review stage\n\n\ncurrent_stage\n\n\n\nScreening\n\n\n\n\n\nStart date\n\n\nstart_date\n\n\n\n2024-05-15 2024-05-15\n\n\n\n\n\nEnd date\n\n\nend_date\n\n\n\n2024-06-30\n\n\n\n\n\nBackground\n\n\nbackground\n\n\n\nThe aim is to establish the current state of the model success in predicting emotions expressed by music from audio. We will focus on the last 10 years of research and especially the research that has predicted valence and arousal ratings from music audio. No such analysis exists and there are interesting challenges in predicting emotional content of music that relates to specificity of the music and the type of emotions and features used that would benefit from a systematic analysis.\n\n\n\n\n\nPrimary research question(s)\n\n\nprimary_research_question\n\n\n\nTo what degree can arousal and valence ratings of emotions expressed by music be predicted from audio? How are the prediction rates related to genres of music, the type of models used, the type of features, modelling design and cross-validation utilised, and the model complexity and parsimony?\n\n\n\n\n\nSecondary research question(s)\n\n\nsecondary_research_question\n\n\n\nWhat is the prediction rate related to classification of quadrants in the affective circumplex?\n\n\n\n\n\nExpectations / hypotheses\n\n\nexpectations_hypotheses\n\n\n\nPrediction of arousal ratings is generally high and robust, and in terms of the model outcome metrics (correlation), achieves at least r = 0.77 (R square of 0.60). Prediction of valence ratings from audio is more challenging and more context dependent and will achieve generally a lower prediction rate, r = 0.63 (R square 0.40)\n\n\n\n\n\nDependent variable(s) / outcome(s) / main variables\n\n\ndvs_outcomes_main_vars\n\n\n\nRegression model performance will be converted to Pearson correlation coefficients and classification model performance will be converted to Matthews correlation coefficient (MCC) when possible.\n\n\n\n\n\nIndependent variable(s) / intervention(s) / treatment(s)\n\n\nivs_intervention_treatment\n\n\n\nMusic genre, prediction type (linear or classification), feature type (based on prior work by Panda et al., 2020), model complexity (high, medium, low), model validation (exists or not)\n\n\n\n\n\nAdditional variable(s) / covariate(s)\n\n\nadditional_variables\n\n\n\nUnspecified\n\n\n\n\n\nSoftware\n\n\nsoftware\n\n\n\nR and Github repository\n\n\n\n\n\nFunding\n\n\nfunding\n\n\n\nMitacs Globalink Research Award (Mitacs & British High Commission - Ottawa, Canada)\n\n\n\n\n\nConflicts of interest\n\n\ncois\n\n\n\nThere are no identified conflicts of interests.\n\n\n\n\n\nOverlapping authorships\n\n\noverlapping_authorships\n\n\n\nNot applicable\n\n\n\n\nSection: Search strategy\n\n\n\nDatabases\n\n\ndatabases\n\n\n\nWeb of Science, Scopus, and Open Alex\n\n\n\n\n\nInterfaces\n\n\ninterfaces\n\n\n\nWeb of Science, Scopus, and Open Alex\n\n\n\n\n\nGrey literature\n\n\ngrey_literature\n\n\n\nNot included\n\n\n\n\n\nInclusion and exclusion criteria\n\n\ninclusions_exclusion_criteria\n\n\n\nSample, Phenomenon of Interest, Design, Evaluation, Research type\n\n\n\n\n\nQuery strings\n\n\nquery_strings\n\n\n\nScopus: TITLE-ABS-KEY ( valence OR arousal OR classi OR categor OR algorithm AND music  AND emotion AND recognition ) AND PUBYEAR &gt; 2013 AND PUBYEAR &lt; 2025 AND  ( LIMIT-TO ( DOCTYPE , “ar” ) )  Web of science:  (DT=(Article) AND PY=(2014-2025)) AND ALL=(music emotion recognition valence arousal)  Open Alex:  https://openalex.org/works?page=1&filter=default.search%3Amusic%20emotion%20recognition%20valence%20arousal, type%3Atypes%2Farticle,publication_year%3A2014-2024, keywords.id%3Akeywords%2Femotion-recognition, keywords.id%3Akeywords%2Faffective-computing, language%3Alanguages%2Fen, open_access.any_repository_has_fulltext%3Atrue \n\n\n\n\n\nSearch validation procedure\n\n\nsearch_validation_procedure\n\n\n\nManual checking, separate keywords searches\n\n\n\n\n\nOther search strategies\n\n\nother_search_strategies\n\n\n\nNot applied\n\n\n\n\n\nProcedures to contact authors\n\n\nprocedure_for_contacting_authors\n\n\n\nUnspecified\n\n\n\n\n\nResults of contacting authors\n\n\nresults_of_contacting_authors\n\n\n\nNot carried out\n\n\n\n\n\nSearch expiration and repetition\n\n\nsearch_expiration_and_repetition\n\n\n\nSearches were done during the active search period in late May early June 2024 and no repetition is planned.\n\n\n\n\n\nSearch strategy justification\n\n\nsearch_strategy_justification\n\n\n\nThe three major databases should be able yield a robust picture of the topic\n\n\n\n\n\nMiscellaneous search strategy details\n\n\nmisc_search_strategy_details\n\n\n\nNo alternative searches were articulated or envisaged.\n\n\n\n\nSection: Screening\n\n\n\nScreening stages\n\n\nscreening_stages\n\n\n\nWe completed screening using custom fields inserted to the bibtex file and managed with citation managers (jabref and bibdesk). To filter relevant studies, we followed a three-stage screening procedure.  In stage 1, we screened the 553 studies’ titles for relevance, removing irrelevant studies and recording exclusion criteria (see Used exclusion criteria). CA assigned 63 studies to the High Priority based on titles’ relevance, assigned 338 studies to Low Priority based on irrelevant titles, and 152 studies to Medium Priority for additional screening. In stage 2, CA assessed the 152 Medium Priority studies for relevance by screening abstracts. 95 studies’ status changed to Low Priority, whereas 30 studies’ status changed to High Priority. 27 studies remained in the Medium priority category. TE and CA evaluated the remaining 27 studies’, moving 15 to the High Priority Category and 12 to the Medium Priority Category. For studies moved to Low Priority, brief BiBTex comments summarized the rationale for exclusion. In stage 3, TE and CA independently screened Priority 1 studies for relevance, including an include, exclude, or unsure decision in a user-comment BiBTeX field.\n\n\n\n\n\nScreened fields / masking\n\n\nscreened_fields_masking\n\n\n\nWe left authors, titles, publication years, and journal names unmasked.\n\n\n\n\n\nUsed exclusion criteria\n\n\nused_exclusion_criteria\n\n\n\nWe excluded studies according to the following exclusion criteria: soundscapes/vocalisations, non-music audio, video clips, physiological markers, dance, video/movie, physiological/EEG/ECG/MEG/GSR/brain imaging/heart rate/neuroscience/brain studies, sensor data, multimodal, autism, ageing, review/systematic review/overview/survey, face emotion recognition, mental health, music therapy, schizophrenia, memory/emotion factors as IVs, recommender systems, or systems that identify the location of emotional excerpts. We included results from some studies meeting exclusion criteria (e.g., multimodal studies involving physiological measurements) if they reported separately on acoustic-only models.\n\n\n\n\n\nScreener instructions\n\n\nscreener_instructions\n\n\n\nAs described above.\n\n\n\n\n\nScreening reliability\n\n\nscreening_reliability\n\n\n\nIn the pass 1 and 2, we included a quality control check after the pass to discuss the identified categories. In the third pass, we double-coded decisions, resolving discrepancies through discussion.\n\n\n\n\n\nScreening reconciliation procedure\n\n\nscreening_reconciliation_procedure\n\n\n\nWe reconcile discrepancies through discussion, resolving “unsure” votes first, followed by discrepancies in include/exclude decisions between authors Results of this updating procedure are available in the Pass 3 comparison document.\n\n\n\n\n\nSampling and sample size\n\n\nsampling_and_sample_size\n\n\n\nWe identified and retained 553 articles from Scopus, Web of Science, and Open Alex based on the search strategy outlined above. See table at the end that details the cumulative exclusions.\n\n\n\n\n\nScreening procedure justification\n\n\nscreening_procedure_justification\n\n\n\nTo offer a broad summary of music emotion recognition tasks, we attempted to include all studies involving prediction with acoustic features. We performed screening unblinded and determined inclusion/exclusion criteria based on studies’ relevance to the task explored.\n\n\n\n\n\nData management and sharing\n\n\nscreening_data_management_and_sharing\n\n\n\nSources will be shared as (a) BibTeX library(ies) including reviewer notes.\n\n\n\n\n\nMiscellaneous screening details\n\n\nmisc_screening_details\n\n\n\nUnspecified\n\n\n\n\nSection: Extraction\n\n\n\nEntities to extract\n\n\nentities_to_extract\n\n\n\nThese are listed and defined in extraction details.\n\n\n\n\n\nExtraction stages\n\n\nextraction_stages\n\n\n\nThe data extraction will be completed in stages. In the first stage, CA will complete a pass of the collection using our initial entities to extract document. The challenges are discussed and the entities are revised.\n\n\n\n\n\nExtractor instructions\n\n\nextractor_instructions\n\n\n\nSee extraction details.\n\n\n\n\n\nExtractor blinding\n\n\nextractor_blinding\n\n\n\nBlinding was not used.\n\n\n\n\n\nExtraction reliability\n\n\nextraction_reliability\n\n\n\nCA will perform extractions; TE will verify extractions for quality assurance.\n\n\n\n\n\nExtraction reconciliation procedure\n\n\nextraction_reconciliation_procedure\n\n\n\nDiscussion and joint decision for studies where extraction proves to be challenging and issues of interpretation arise.\n\n\n\n\n\nExtraction procedure justification\n\n\nextraction_procedure_justification\n\n\n\nThese are documented in the extraction details.\n\n\n\n\n\nData management and sharing\n\n\nextraction_data_management_and_sharing\n\n\n\nWe retain the information of the studies in shared bibtex files, extraction data will be stored in ascii data files (.bibtex), and the parser for reading the data from .bibtex files to R for the analysis will be available (as quarto/markdown/R files), and all these are managed, structured, shared and documented in Github repository according to FAIR principles.\n\n\n\n\n\nMiscellaneous extraction details\n\n\nmisc_extraction_details\n\n\n\nNA\n\n\n\n\nSection: Synthesis and Quality Assessment\n\n\n\nPlanned data transformations\n\n\nplanned_data_transformations\n\n\n\nFor regression studies, we convert all metrics to Pearson correlation coefficients. For classification studies, we convert the outcomes of classification to Matthews Correlation Coefficient (MCC) from the precision, accuracy, specificity, F1 scores. Alternatively, we use Cohen’s kappa for multiple classes.\n\n\n\n\n\nMissing data\n\n\nmissing_data\n\n\n\nIf no main outcome variables are available, we exclude the study.\n\n\n\n\n\nData validation\n\n\ndata_validation\n\n\n\nNone planned beyond the staged approached already documented in extraction process.\n\n\n\n\n\nQuality assessment\n\n\nquality_assessment\n\n\n\nNot all the bias assessment tools for clinical studies are relevant for our purposes, we adapt the overall approached advocated in [Higgins et al. (2011)] (https://doi.org/10.1136/bmj.d5928).\n\n\n\n\n\nSynthesis plan\n\n\nsynthesis_plan\n\n\n\nWe analyse regression and classification studies separately, and depending on the quantity of the studies forming suitable sub-groupings based on techniques, materials or music collections/genres, we may further synthesise the results across groupings that are formed along these subsets.\n\n\n\n\n\nCriteria for conclusions / inference criteria\n\n\ncriteria_for_conclusions\n\n\n\nNA\n\n\n\n\n\nSynthesist masking\n\n\nsynthesis_masking\n\n\n\nNA\n\n\n\n\n\nSynthesis reliability\n\n\nsynthesis_reliability\n\n\n\nNA\n\n\n\n\n\nSynthesis reconciliation procedure\n\n\nsynthesis_reconciliation_procedure\n\n\n\nNA\n\n\n\n\n\nPublication bias analyses\n\n\npublication_bias\n\n\n\nWe utilise Egger’s test to assess the publication bias and potentially correct the effect size bias by selecting 10% most precise effect sizes as recommended by Van Aert, Wicherts, & Van Assen (2019).\n\n\n\n\n\nSensitivity analyses / robustness checks\n\n\nsensitivity_analysis\n\n\n\nWithin regression and classificiation tasks, we will carry out sensitivity analysis using sub-groups of studied based on type of models, and the type of journal the studies were published in.\n\n\n\n\n\nSynthesis procedure justification\n\n\nsynthesis_procedure_justification\n\n\n\nWe share our justification of the synthesis and the subsetting carried out in the manuscript but we have not formulated these in advance except for synthesizing classiciation and regression approaches separately and creating subsets within these approaches according to techniques and datasets utilised.\n\n\n\n\n\nSynthesis data management and sharing\n\n\nsynthesis_data_management_and_sharing\n\n\n\nWe share the data, procedures, definitions, the analysis scripts with the outcomes as R code in Quarto notes at Github.\n\n\n\n\n\nMiscellaneous synthesis details\n\n\nmisc_synthesis_details\n\n\n\nUnspecified"
  },
  {
    "objectID": "preregistration/preregistration.html#references",
    "href": "preregistration/preregistration.html#references",
    "title": "metaMER",
    "section": "References",
    "text": "References\n\nHiggins, J. P. T., Altman, D. G., Gøtzsche, P. C., Jüni, P., Moher, D., Oxman, A. D., Savović, J., Schulz, K. F., Weeks, L., & Sterne, J. A. C. (2011). The Cochrane Collaboration tool for assessing risk of bias in randomised trials. BMJ, 343. https://www.bmj.com/content/343/bmj.d5928\nPanda, R., Malheiro, R., & Paiva, R. P. (2020). Audio features for music emotion recognition: a survey. IEEE Transactions on Affective Computing, 14(1), 68-88. https://doi.org/10.1109/TAFFC.2020.3032373\nShamseer, L., Moher, D., Clarke, M., Ghersi, D., Liberati, A., Petticrew, M., Shekelle, P., & Stewart, L. A. (2015). Preferred reporting items for systematic review and meta-analysis protocols (PRISMA-P) 2015: elaboration and explanation. BMJ, 349. https://www.bmj.com/content/349/bmj.g7647"
  },
  {
    "objectID": "analysis/analysis.html",
    "href": "analysis/analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "This assumes that the data has been parsed (parse-model-output.R, format-study-results.R) and preprocessed (processing.qmd).\n\n\n\n\n\nlibrary(dmetar)\nlibrary(tidyverse)\nlibrary(meta)\nlibrary(DescTools)\nlibrary(ggrepel)\n#R_studies &lt;- read.csv(\"R_studies.csv\")\nR_summary &lt;- read.csv(\"R_summary.csv\")\n\n# select regression studies with r2\ntmp &lt;- dplyr::filter(R_summary,dimension==\"valence\")\n#tmp &lt;- dplyr::filter(R_studies,dimension==\"valence\")\n#dim(tmp)\n\n#if all studies, remove two\n#tmp &lt;- tmp[!is.na(tmp$values),]\n#dim(tmp)\n\n#sqrt(tmp$values) # convert from R^2 to r\n#tmp$stimulus_n &lt;- 100 # ad-hoc for now\n\nm.cor &lt;- metacor(cor = valuesMax,     # values \n                 n = stimulus_n,\n                 studlab = citekey, # unique_id\n                 data = tmp,\n                 fixed = FALSE,\n                 random = TRUE,\n                 prediction = TRUE,\n                 backtransf = TRUE,\n                 sm = \"ZCOR\",\n                 method.tau = \"REML\",# could be PM (Paule-Mandel) as well\n                 method.random.ci = \"HK\", \n                 title = \"MER: Regression: Valence: Summary\")\n\n#print(m.cor)\n\nm.cor_backtransformed &lt;- m.cor\nm.cor_backtransformed$TE &lt;- FisherZInv(m.cor_backtransformed$TE)\nprint(funnel(m.cor_backtransformed, common = FALSE, studlab=TRUE,backtransf=TRUE))\n\n\n\n\n\n\n\n\n$xlim\n[1] 0.1281951 1.1857650\n\n$ylim\n[1] 0.164399 0.000000\n\n\n\n\n\ntmpdata &lt;- data.frame(SE = FisherZInv(m.cor$seTE), Zr = FisherZInv(m.cor$TE),studies=m.cor$studlab)\n\ntmpdata$citekey&lt;-str_replace_all(tmpdata$studies,'-.*$','')\ntmpdata$citekey&lt;-factor(tmpdata$citekey)\n\ng &lt;- ggplot(tmpdata,aes(y = SE, x = Zr,label=studies,color=citekey,fill=citekey)) +\n  geom_point(show.legend = FALSE) +\n  geom_label_repel(size=1.5,max.overlaps = 39,color=\"black\",show.legend = FALSE)+\n  ylab('Standard Error') + \n  xlab('r')+\n  scale_y_reverse()+\n  scale_x_continuous(breaks=seq(0.0,1.0,0.25),limits=c(0,1.0))+\n#  coord_flip()+\n  theme_bw()\nprint(g)\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_label_repel()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nforest(m.cor,\n       sortvar = TE,\n       prediction = FALSE, \n             print.tau2 = FALSE,\n             leftlabs = c(\"Author\", \"g\", \"SE\"),studlab = citekey)\n\n\n\n\n\n\n\nfunnel(m.cor, common = FALSE, studlab=TRUE,backtransf=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\nO &lt;- find.outliers(m.cor)\n\nWarning in find.outliers(m.cor): Studies with NAs not considered in outlier\nanalysis.\n\n# 6 datasets identified as outliers, without them the r drops to 0.5781\ninfan &lt;- InfluenceAnalysis(m.cor)\n\n[===========================================================================] DONE \n\nprint(eggers.test(m.cor))\n\nWarning in metabias.meta(x, k.min = 3, method = \"linreg\"): 3 observation(s)\ndropped due to missing values\n\n\nEggers' test of the intercept \n============================= \n\n intercept        95% CI     t          p\n     5.368 -0.09 - 10.83 1.926 0.06917371\n\nEggers' test does not indicate the presence of funnel plot asymmetry. \n\n\n\n\n\n\noutliers &lt;- c(\"chen2017co-various-GMM-mfcc-AMG1608-1\", \"chen2017co-various-GMM-tonal-AMG1608-1\", \"chen2017co-various-GMM-spectral-AMG1608-1\", \"chen2017co-various-GMM-temporal-AMG1608-1\", \"chen2017co-various-GMM-mfcctonalspectraltemporal-AMG1608-1\", \"griffiths2021am-various-mlr-mixed-new-validation\", \"griffiths2021am-various-mlr-mixed-new-validation-nooutliers\", \"hu2017cr-various-SVR-mixed-all-1\", \"hu2017cr-various-SVR-mixed-all-1\", \"hu2017cr-various-SVR-mixed-all-1\", \"hu2017cr-various-SVR-mixed-all-1\", \"koh2023me-openSMILE-fullyconnectedNN-FreeMusicArchive-audioOnly-1\", \"koh2023me-openSMILE-longshorttermmemoryNN-FreeMusicArchive-audioOnly-1\", \"wang2022co-various-extremelyrandomizedtreeregression-mixed-western-1\", \"wang2022co-various-extremelyrandomizedtreeregression-mixed-western-1\", \"wang2022co-various-extremelyrandomizedtreeregression-mixed-chinese-1\", \"wang2022co-various-extremelyrandomizedtreeregression-mixed-chinese-1\", \"wang2022cr-various-pls-mixed-chineseClassicalEnsembles-1\", \"wang2022cr-various-pls-mixed-chineseClassicalSolo-1\", \"wang2022cr-various-pls-mixed-westernClassicalEnsembles-1\", \"wang2022cr-various-pls-mixed-westernClassicalSolo-1\", \"zhang2019us-marsyas-NuSVR-music-xing2014-1\")\n\ntmp2&lt;-tmp[!tmp$unique_id %in% outliers,]\nm.cor &lt;- metacor(cor = values, \n                 n = stimulus_n,\n                 studlab = unique_id, # unique_id\n                 data = tmp2,\n                 fixed = FALSE,\n                 random = TRUE,\n                 sm = \"ZCOR\",\n                 prediction = TRUE,\n                 backtransf = TRUE,\n                 method.tau = \"REML\",# could be PM (Paule-Mandel) as well\n                 method.random.ci = \"HK\", \n                 title = \"MER: Regression: Valence: Summary\")\n\nprint(m.cor)\n\nfp &lt;- funnel(m.cor, common = TRUE,studlab=FALSE)\n\n\n\n\nTo show the quality differences between core and eliminated studies (in progress).\n\ntmpdata &lt;- data.frame(SE = m.cor$seTE, Zr = m.cor$TE)\nestimate = m.cor$TE.common\nse = m.cor$seTE.common\nse.seq=seq(0, max(m.cor$cor), 0.001)\nll95 = estimate-(1.96*se.seq)\nul95 = estimate+(1.96*se.seq)\nll99 = estimate-(3.29*se.seq)\nul99 = estimate+(3.29*se.seq)\nmeanll95 = estimate-(1.96*se)\nmeanul95 = estimate+(1.96*se)\ndfCI = data.frame(ll95, ul95, ll99, ul99, se.seq, estimate, meanll95, meanul95)\n\nfp = ggplot(aes(x = SE, y = Zr), data = tmpdata) +\n  geom_point(shape = 1) +\n  xlab('Standard Error') + ylab('Zr')+\n  geom_line(aes(x = se.seq, y = ll95), linetype = 'dotted', data = dfCI) +\n  geom_line(aes(x = se.seq, y = ul95), linetype = 'dotted', data = dfCI) +\n   geom_segment(aes(x = min(se.seq), y = meanll95, xend = max(se.seq), yend = meanll95), linetype='dotted', data=dfCI) +\n  geom_segment(aes(x = min(se.seq), y = meanul95, xend = max(se.seq), yend = meanul95), linetype='dotted', data=dfCI) +\n#Reverse the x-axis ordering (se) so that the tip of the funnel will appear\n#at the top of the figure once we swap the x- and y-axes...\n  scale_x_reverse(breaks=seq(0,0.2,0.05),limits=c(0.2,0))+\n#Specify the range and interval for the tick-marks of the y-axis (Zr);\n#Choose values that work for you based on your data\n  scale_y_continuous(breaks=seq(0.3,1.25,0.25),limits=c(0.3,1.25))+\n#  scale_x_continuous(breaks=seq(0.2,0,0.05))+\n#And now we flip the axes so that SE is on y- and Zr is on x-\n  coord_flip()+\n#Finally, apply my APA-format theme (see code at end of post).\n#You could, alternatively, specify theme_bw() instead.\n  theme_bw()\nprint(fp)\n\nWarning: Removed 7 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 721 rows containing missing values or values outside the scale range\n(`geom_line()`).\nRemoved 721 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\nWarning: Removed 922 rows containing missing values or values outside the scale range\n(`geom_segment()`).\nRemoved 922 rows containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\n\n\n\n\n\n\n\n\n\n\nadd journal_type and stimulus_genre_mixed as a grouping option\n\nm.cor_subgroups &lt;- update(m.cor, \n       subgroup = model_class_id, \n#       subgroup = journal_type, \n#       subgroup = stimulus_genre_mixed, \n       tau.common = FALSE,\n       prediction = TRUE)\n\nprint(m.cor_subgroups)\n\nReview:     MER: Regression: Valence: Summary\n\nNumber of studies: k = 21\nNumber of observations: o = 13876\n\n                        COR           95%-CI     t  p-value\nRandom effects model 0.6830 [0.5852; 0.7611] 10.60 &lt; 0.0001\nPrediction interval         [0.0873; 0.9189]               \n\nQuantifying heterogeneity:\n tau^2 = 0.1213 [0.0691; 0.2690]; tau = 0.3483 [0.2628; 0.5186]\n I^2 = 97.8% [97.3%; 98.2%]; H = 6.79 [6.13; 7.53]\n\nTest of heterogeneity:\n      Q d.f.  p-value\n 923.13   20 &lt; 0.0001\n\nResults for subgroups (random effects model):\n                                                      k    COR\nmodel_class_id = Kernel Smoothing, Additive and KNN   2 0.4662\nmodel_class_id = Random Forests                       3 0.6846\nmodel_class_id = Linear Methods                       8 0.7840\nmodel_class_id = Neural Nets                          2 0.4273\nmodel_class_id = Flexible Discriminants               6 0.6555\n                                                               95%-CI  tau^2\nmodel_class_id = Kernel Smoothing, Additive and KNN [-0.6342; 0.9424] 0.0185\nmodel_class_id = Random Forests                     [ 0.0489; 0.9256] 0.0997\nmodel_class_id = Linear Methods                     [ 0.6249; 0.8806] 0.1370\nmodel_class_id = Neural Nets                        [-0.9981; 0.9997] 0.1818\nmodel_class_id = Flexible Discriminants             [ 0.4838; 0.7786] 0.0574\n                                                       tau      Q   I^2\nmodel_class_id = Kernel Smoothing, Additive and KNN 0.1361  20.56 95.1%\nmodel_class_id = Random Forests                     0.3158 197.77 99.0%\nmodel_class_id = Linear Methods                     0.3701 195.78 96.4%\nmodel_class_id = Neural Nets                        0.4264  17.57 94.3%\nmodel_class_id = Flexible Discriminants             0.2396 161.22 96.9%\n\nTest for subgroup differences (random effects model):\n                   Q d.f. p-value\nBetween groups 12.31    4  0.0152\n\nDetails on meta-analytical method:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Hartung-Knapp adjustment for random effects model (df = 20)\n- Prediction interval based on t-distribution (df = 19)\n- Fisher's z transformation of correlations\n\n#forest(m.cor_subgroups,subgroup=TRUE)\n\n\nIdea: visualise the distributions of the model successes within studies (done in preprocessing)\n\n\n\n\n\n\n# select regression studies with r2\ntmp &lt;- dplyr::filter(R_summary,dimension==\"arousal\")\n#tmp &lt;- dplyr::filter(R_studies,dimension==\"arousal\")\ndim(tmp)\n\n[1] 24 11\n\n#tmp &lt;- tmp[!is.na(tmp$values),]\ndim(tmp)\n\n[1] 24 11\n\n#tmp&lt;-drop_na(tmp)\n\nm.cor &lt;- metacor(cor = valuesMax, \n                 n = stimulus_n,\n                 studlab = citekey,\n                 data = tmp,\n                 fixed = FALSE,\n                 random = TRUE,\n                 prediction = TRUE,\n                 backtransf = TRUE,\n                 sm = \"ZCOR\",\n                 method.tau = \"REML\",# could be PM (Paule-Mandel) as well\n                 method.random.ci = \"HK\", \n                 title = \"MER: Regression: Arousal: Summary\")\n\nprint(m.cor)\n\nReview:     MER: Regression: Arousal: Summary\n\nNumber of studies: k = 21\nNumber of observations: o = 13876\n\n                        COR           95%-CI     t  p-value\nRandom effects model 0.8221 [0.7646; 0.8666] 15.55 &lt; 0.0001\nPrediction interval         [0.4301; 0.9532]               \n\nQuantifying heterogeneity:\n tau^2 = 0.1074 [0.0612; 0.2437]; tau = 0.3278 [0.2475; 0.4937]\n I^2 = 95.9% [94.7%; 96.8%]; H = 4.93 [4.35; 5.59]\n\nTest of heterogeneity:\n      Q d.f.  p-value\n 486.07   20 &lt; 0.0001\n\nDetails on meta-analytical method:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Hartung-Knapp adjustment for random effects model (df = 20)\n- Prediction interval based on t-distribution (df = 19)\n- Fisher's z transformation of correlations\n\n\n\n\n\n\nO &lt;- find.outliers(m.cor)\n\nWarning in find.outliers(m.cor): Studies with NAs not considered in outlier\nanalysis.\n\n\n\n\n\nforest(m.cor,\n       sortvar = TE,\n       prediction = FALSE, \n             print.tau2 = FALSE,\n             leftlabs = c(\"Author\", \"g\", \"SE\"))\n\n\n\n\n\n\n\nplot(eggers.test(m.cor))\n\nWarning in metabias.meta(x, k.min = 3, method = \"linreg\"): 3 observation(s)\ndropped due to missing values\n\n\n\n\n\n\n\n\n\n\n\n\n\nmeta &lt;- metagen(valuesMax, sqrt(valuesMax), \n                data = tmp, \n                studlab = tmp$citekey, \n                comb.fixed = FALSE, \n                method.tau = \"PM\")\nfind.outliers(meta)\ninfan &lt;- InfluenceAnalysis(meta)\nprint(eggers.test(meta))\n\n\n\n\n\nupdate(m.cor, \n       subgroup = model_class_id, \n       tau.common = FALSE)\n\nReview:     MER: Regression: Arousal: Summary\n\nNumber of studies: k = 21\nNumber of observations: o = 13876\n\n                        COR           95%-CI     t  p-value\nRandom effects model 0.8221 [0.7646; 0.8666] 15.55 &lt; 0.0001\nPrediction interval         [0.4301; 0.9532]               \n\nQuantifying heterogeneity:\n tau^2 = 0.1074 [0.0612; 0.2437]; tau = 0.3278 [0.2475; 0.4937]\n I^2 = 95.9% [94.7%; 96.8%]; H = 4.93 [4.35; 5.59]\n\nTest of heterogeneity:\n      Q d.f.  p-value\n 486.07   20 &lt; 0.0001\n\nResults for subgroups (random effects model):\n                                                      k    COR\nmodel_class_id = Kernel Smoothing, Additive and KNN   2 0.8068\nmodel_class_id = Random Forests                       3 0.8011\nmodel_class_id = Linear Methods                       8 0.8812\nmodel_class_id = Neural Nets                          2 0.5369\nmodel_class_id = Flexible Discriminants               6 0.8077\n                                                               95%-CI  tau^2\nmodel_class_id = Kernel Smoothing, Additive and KNN [ 0.5493; 0.9244] 0.0022\nmodel_class_id = Random Forests                     [ 0.7566; 0.8381] 0.0011\nmodel_class_id = Linear Methods                     [ 0.8079; 0.9277] 0.0846\nmodel_class_id = Neural Nets                        [-0.9703; 0.9973] 0.0800\nmodel_class_id = Flexible Discriminants             [ 0.6437; 0.9008] 0.1125\n                                                       tau      Q   I^2\nmodel_class_id = Kernel Smoothing, Additive and KNN 0.0470   3.33 70.0%\nmodel_class_id = Random Forests                     0.0339   4.61 56.6%\nmodel_class_id = Linear Methods                     0.2908 104.87 93.3%\nmodel_class_id = Neural Nets                        0.2829   8.29 87.9%\nmodel_class_id = Flexible Discriminants             0.3354 246.31 98.0%\n\nTest for subgroup differences (random effects model):\n                   Q d.f. p-value\nBetween groups 11.98    4  0.0175\n\nDetails on meta-analytical method:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Hartung-Knapp adjustment for random effects model (df = 20)\n- Prediction interval based on t-distribution (df = 19)\n- Fisher's z transformation of correlations\n\nm.cor_subgroups &lt;- update(m.cor, \n       subgroup = model_class_id, \n       tau.common = FALSE)\n\nforest(m.cor_subgroups,subgroup=TRUE)\n\n\n\n\n\n\n\n\n\nIdea: visualise the distributions of the model successes within studies (done in preprocessing)\n\n\ng1 &lt;- ggplot(tmp,aes(y=valuesMax,fill=model_class_id))+\n   geom_histogram(show.legend = T)+\n  facet_wrap(.~studyREF)+\n   coord_flip()+\n   theme_bw()\ng1\n\n# S &lt;- summarise(group_by(tmp,citekey),maxvalue=max(values))\n# g&lt;-ggplot(S,aes(y=maxvalue))+\n#   geom_histogram(bins = 14)+\n#   #facet_wrap(.~studyREF)+\n#   coord_flip()+\n#   theme_bw()\n# g\n\n\n\n\n\n\n\nC_studies &lt;- read.csv(\"C_studies.csv\")\nC_studies &lt;- C_studies[!is.na(C_studies$values),]\n\nC_summary &lt;- read.csv(\"C_summary.csv\")\n\nm.cor &lt;- metacor(cor = valuesMax,     # values \n                 n = stimulus_n,\n                 studlab = citekey, # unique_id\n                 data = C_summary,\n                 fixed = FALSE,\n                 random = TRUE,\n                 prediction = TRUE,\n                 backtransf = TRUE,\n                 sm = \"ZCOR\",\n                 method.tau = \"REML\",# could be PM (Paule-Mandel) as well\n                 method.random.ci = \"HK\", \n                 title = \"MER: Classification: Summary\")\n\n#print(m.cor)\n\n#m.cor_backtransformed &lt;- m.cor\n#m.cor_backtransformed$TE &lt;- FisherZInv(m.cor_backtransformed$TE)\nprint(funnel(m.cor, common = FALSE, studlab=TRUE,backtransf=TRUE))\n\n\n\n\n\n\n\n\n$xlim\n[1] 0.598722 2.439471\n\n$ylim\n[1] 0.09090909 0.00000000\n\n\n\n\n\nlibrary(ggrepel)\ntmpdata &lt;- data.frame(SE = FisherZInv(m.cor$seTE), Zr = FisherZInv(m.cor$TE),studies=m.cor$studlab)\n\ntmpdata$citekey&lt;-str_replace_all(tmpdata$studies,'-.*$','')\ntmpdata$citekey&lt;-factor(tmpdata$citekey)\n\ng &lt;- ggplot(tmpdata,aes(y = SE, x = Zr,label=studies,color=citekey,fill=citekey)) +\n  geom_point(show.legend = FALSE) +\n  geom_label_repel(size=1.5,max.overlaps = 39,color=\"black\",show.legend = FALSE)+\n  ylab('Standard Error') + \n  xlab('r')+\n  scale_y_reverse()+\n#  scale_x_continuous(breaks=seq(0.0,1.0,0.25),limits=c(0,1.0))+\n#  coord_flip()+\n  theme_bw()\nprint(g)\n\n\n\n\n\n\n\n\n\n\n\n\nforest(m.cor,\n       sortvar = TE,\n       prediction = FALSE, \n             print.tau2 = FALSE,\n             leftlabs = c(\"Author\", \"g\", \"SE\"),studlab = citekey)\n\n\n\n\n\n\n\nfunnel(m.cor, common = FALSE, studlab=TRUE,backtransf=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\nO &lt;- find.outliers(m.cor)\n# 6 datasets identified as outliers, without them the r drops to 0.5781\n#infan &lt;- InfluenceAnalysis(m.cor)\n#print(eggers.test(m.cor))\n\n\n\n\n\noutliers &lt;- c(\"\")\n\ntmp2&lt;-tmp[!tmp$unique_id %in% outliers,]\nm.cor &lt;- metacor(cor = values, \n                 n = stimulus_n,\n                 studlab = unique_id, # unique_id\n                 data = tmp2,\n                 fixed = FALSE,\n                 random = TRUE,\n                 sm = \"ZCOR\",\n                 prediction = TRUE,\n                 backtransf = TRUE,\n                 method.tau = \"REML\",# could be PM (Paule-Mandel) as well\n                 method.random.ci = \"HK\", \n                 title = \"MER: Regression: Valence: Summary\")\n\nprint(m.cor)\n\nfp &lt;- funnel(m.cor, common = TRUE,studlab=FALSE)\n\n\n\n\nTo show the quality differences between core and eliminated studies (in progress).\n\ntmpdata &lt;- data.frame(SE = m.cor$seTE, Zr = m.cor$TE)\nestimate = m.cor$TE.common\nse = m.cor$seTE.common\nse.seq=seq(0, max(m.cor$cor), 0.001)\nll95 = estimate-(1.96*se.seq)\nul95 = estimate+(1.96*se.seq)\nll99 = estimate-(3.29*se.seq)\nul99 = estimate+(3.29*se.seq)\nmeanll95 = estimate-(1.96*se)\nmeanul95 = estimate+(1.96*se)\ndfCI = data.frame(ll95, ul95, ll99, ul99, se.seq, estimate, meanll95, meanul95)\n\nfp = ggplot(aes(x = SE, y = Zr), data = tmpdata) +\n  geom_point(shape = 1) +\n  xlab('Standard Error') + ylab('Zr')+\n  geom_line(aes(x = se.seq, y = ll95), linetype = 'dotted', data = dfCI) +\n  geom_line(aes(x = se.seq, y = ul95), linetype = 'dotted', data = dfCI) +\n   geom_segment(aes(x = min(se.seq), y = meanll95, xend = max(se.seq), yend = meanll95), linetype='dotted', data=dfCI) +\n  geom_segment(aes(x = min(se.seq), y = meanul95, xend = max(se.seq), yend = meanul95), linetype='dotted', data=dfCI) +\n#Reverse the x-axis ordering (se) so that the tip of the funnel will appear\n#at the top of the figure once we swap the x- and y-axes...\n  scale_x_reverse(breaks=seq(0,0.2,0.05),limits=c(0.2,0))+\n#Specify the range and interval for the tick-marks of the y-axis (Zr);\n#Choose values that work for you based on your data\n  scale_y_continuous(breaks=seq(0.3,1.25,0.25),limits=c(0.3,1.25))+\n#  scale_x_continuous(breaks=seq(0.2,0,0.05))+\n#And now we flip the axes so that SE is on y- and Zr is on x-\n  coord_flip()+\n#Finally, apply my APA-format theme (see code at end of post).\n#You could, alternatively, specify theme_bw() instead.\n  theme_bw()\nprint(fp)\n\n\n\n\nadd journal_type and stimulus_genre_mixed as a grouping option\n\nm.cor_subgroups &lt;- update(m.cor, \n       subgroup = model_class_id, \n#       subgroup = journal_type, \n#       subgroup = stimulus_genre_mixed, \n       tau.common = FALSE,\n       prediction = TRUE)\n\nprint(m.cor_subgroups)\n#forest(m.cor_subgroups,subgroup=TRUE)\n\n\nIdea: visualise the distributions of the model successes within studies (done in preprocessing)"
  },
  {
    "objectID": "analysis/analysis.html#read-annotated-data",
    "href": "analysis/analysis.html#read-annotated-data",
    "title": "Analysis",
    "section": "",
    "text": "Update 2024-06-19\nCA TODO later (if at all): find better way to read in functions\n\n\nrequire(here)\n\nLoading required package: here\n\n\nhere() starts at /Users/lqbn73/Documents/computational/R/metaMER\n\nsource(here::here('R/build-df.R'))\nsource(here::here('R/format-study-results.R'))\nsource(here::here('R/parse-model-output.R'))\n\n\nCA ToDo: Check the warning reported here\n\n\n# get metaMER df:\nmeta_df &lt;- get_metaMER_df(path_2_studies = here::here('studies'))\n\n# get included studies\nincluded_studies &lt;- meta_df[which(\n  !stringr::str_detect(meta_df$final_notes, '!EXCL!')),] |&gt; \n  dplyr::tibble()\n\n\n\n\n# get studies re-coded (currently identifiable by presence of bind_field.)\nrecoded_studies &lt;- included_studies[which(stringr::str_detect(\n  included_studies$model_rate_emotion_values,\n                                     'bind_field')),] \n\nMaybe limit the verbosity of get_study_results below\n\nmetaMER_results &lt;-\ndo.call(\n  rbind,\n    lapply(1:nrow(recoded_studies),\n       function(x) get_study_results(recoded_studies[x,])\n       )\n) \n\nMismatch in input lengths.\nMismatch in input lengths.\nMismatch in input lengths.\nMismatch in input lengths.\nMismatch in input lengths.\nMismatch in input lengths.\n\n# add unique identifiers\n\nunique_id &lt;- apply(metaMER_results[,c('citekey',\n                        'library_id',\n                         'model_id',\n                         'feature_id',\n                         'data_id',\n                         'experiment_id')],\n                      1, \n                      paste0, \n                      collapse = '-'\n) \nmetaMER_results$unique_id &lt;- stringr::str_remove_all(unique_id, \n                                                     ' ')\n\nmetaMER_results &lt;- metaMER_results |&gt; dplyr::select(unique_id, \n                                 dplyr::everything()) \n\nmetaMER_results |&gt; dplyr::tibble()\n\n# A tibble: 626 × 14\n   unique_id      citekey stimulus_n feature_n participant_n library_id model_id\n   &lt;chr&gt;          &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;         &lt;chr&gt;      &lt;chr&gt;   \n 1 agarwal2021an… agarwa… \" ISMIR20… \" 'eight… \" not specif… not speci… SVR KMB…\n 2 agarwal2021an… agarwa… \" ISMIR20… \" 'eight… \" not specif… not speci… SVR KMB…\n 3 agarwal2021an… agarwa… \" ISMIR20… \" 'eight… \" not specif… not speci… SVR KMB…\n 4 agarwal2021an… agarwa… \" ISMIR20… \" 'eight… \" not specif… not speci… SVR KMB…\n 5 agarwal2021an… agarwa… \" ISMIR20… \" 'eight… \" not specif… not speci… SVR KMB…\n 6 agarwal2021an… agarwa… \" ISMIR20… \" 'eight… \" not specif… not speci… SVR KMB…\n 7 agarwal2021an… agarwa… \" ISMIR20… \" 'eight… \" not specif… not speci… SVR KMB…\n 8 agarwal2021an… agarwa… \" ISMIR20… \" 'eight… \" not specif… not speci… SVR KMB…\n 9 agarwal2021an… agarwa… \" ISMIR20… \" 'eight… \" not specif… not speci… SVR KMB…\n10 agarwal2021an… agarwa… \" ISMIR20… \" 'eight… \" not specif… not speci… SVR KMB…\n# ℹ 616 more rows\n# ℹ 7 more variables: feature_id &lt;chr&gt;, data_id &lt;chr&gt;, experiment_id &lt;chr&gt;,\n#   dimension &lt;chr&gt;, measure &lt;chr&gt;, statistic &lt;chr&gt;, values &lt;dbl&gt;"
  },
  {
    "objectID": "analysis/analysis.html#read-annotated-data-1",
    "href": "analysis/analysis.html#read-annotated-data-1",
    "title": "Analysis",
    "section": "Read annotated data",
    "text": "Read annotated data\nlength(unique(metaMER_results$citekey))\n[1] 6\nlength(unique(metaMER_results$library_id))\n[1] 5\nlength(unique(metaMER_results$model_id))\n[1] 6\nprint(knitr::kable(table(metaMER_results$citekey,metaMER_results$dimension)))\n\n\n\n\n\n\n\n\n\n\n\n\n\narousal\nav\nclassification\nenergy arousal\ntension arousal\nvalence\n\n\n\n\nagarwal2021an\n0\n0\n63\n0\n0\n0\n\n\nalvarez2023ri\n0\n0\n8\n0\n0\n0\n\n\nbai2017mu\n6\n6\n0\n0\n0\n6\n\n\nwang2022cr\n0\n0\n0\n16\n16\n16\n\n\nxie2020mu\n18\n0\n0\n0\n0\n18\n\n\nxu2021us\n2\n0\n0\n0\n0\n2\n\n\n\nprint(knitr::kable(table(metaMER_results$model_id)))\n\n\n\nVar1\nFreq\n\n\n\n\nff\n12\n\n\npcr\n12\n\n\npls\n60\n\n\nrandom forest\n8\n\n\nrandom forest regression\n22\n\n\nSVR KMBSO\n63\n\n\n\nprint(knitr::kable(table(metaMER_results$feature_id)))\n\n\n\nVar1\nFreq\n\n\n\n\naudio\n4\n\n\nbaseline features\n12\n\n\nbaseline features and stc\n12\n\n\nkaraoke\n21\n\n\nlyrics\n21\n\n\nmixed\n48\n\n\nReliefF\n18\n\n\nsong\n21\n\n\nspotify\n8\n\n\nstc\n12\n\n\n\nprint(knitr::kable(table(metaMER_results$data_id)))\n\n\n\nVar1\nFreq\n\n\n\n\n100songs\n36\n\n\nacousticBrainz\n4\n\n\nchineseClassicalEnsembles\n12\n\n\nchineseClassicalSolo\n12\n\n\nhindi\n21\n\n\nismir2012\n21\n\n\nMediaEval\n18\n\n\nNJUV1\n21\n\n\npsic3839\n4\n\n\nspotify\n4\n\n\nwesternClassicalEnsembles\n12\n\n\nwesternClassicalSolo\n12\n\n\n\nWe need to identify the core metrics:\n\ncorrelation coefficient (obtained from \\(R^2\\)) for regression studies.\nMatthew’s correlation coefficient for classification studies.\nn (we use STIMULUS_N rather than participant n)\nSubsetting base on model and music qualities"
  },
  {
    "objectID": "analysis/analysis.html#analyse",
    "href": "analysis/analysis.html#analyse",
    "title": "Analysis",
    "section": "Analyse",
    "text": "Analyse\n\nIssue 1: We need unique identifiers for each input component (study + model + feature + data + experiment)\nIssue 2: We stimulus N\nIssue 3: measure vs statistic?\nIssue 4: We need to classify the modelling techniques into few\nIssue 5: We could polish citekey into ref (xu2021us to Xu 2021 et al., )\n\n\nRegression studies\n\nValence\n\nlibrary(dmetar)\nlibrary(tidyverse)\nlibrary(meta)\n\n# select regression studies with r2\ntmp &lt;- dplyr::filter(metaMER_results,dimension==\"valence\" & measure==\"r2\")\nsqrt(tmp$values) # convert from R^2 to r\n\ntmp$stimulus_n &lt;- 100 # ad-hoc for now\n\n\n\n\nm.cor &lt;- metacor(cor = sqrt(values), \n                 n = stimulus_n,\n                 studlab = citekey,\n                 data = tmp,\n                 fixed = FALSE,\n                 random = TRUE,\n                 method.tau = \"REML\",\n                 method.random.ci = \"HK\",\n                 title = \"MER: Regression: Valence: All\")\n\nprint(m.cor)\n\n## sub-group analysis (based on model type)\nmeta &lt;- metagen(values, values, \n                data = tmp, \n                studlab = tmp$citekey, \n                comb.fixed = FALSE, \n                method.tau = \"PM\")\n\nsubgroup.analysis.mixed.effects(x = meta, \n                                subgroups = tmp$model_id)\n\n## explore various qualities\nfind.outliers(meta)\ninfan &lt;- InfluenceAnalysis(meta)\nprint(eggers.test(meta))\n\nplot(m.cor)"
  },
  {
    "objectID": "analysis/analysis.html#visualise",
    "href": "analysis/analysis.html#visualise",
    "title": "Analysis",
    "section": "",
    "text": "plot(m.cor)\n\n\n\n\n\n\n\nplot(eggers.test(meta))"
  },
  {
    "objectID": "analysis/analysis.html#summarise-annotated-data",
    "href": "analysis/analysis.html#summarise-annotated-data",
    "title": "Analysis",
    "section": "",
    "text": "print(knitr::kable(table(metaMER_results$citekey,metaMER_results$dimension)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nactivation\narousal\nav\nclassification\nenergy arousal\npleasantness\ntension arousal\nvalence\n\n\n\n\nagarwal2021an\n0\n0\n0\n63\n0\n0\n0\n0\n\n\nalvarez2023ri\n0\n0\n0\n8\n0\n0\n0\n0\n\n\nbai2017mu\n0\n6\n6\n0\n0\n0\n0\n6\n\n\ngingras2014be\n0\n18\n0\n0\n0\n18\n0\n0\n\n\ngrekow2018au\n0\n22\n0\n0\n0\n0\n0\n22\n\n\ngrekow2021mu\n0\n52\n0\n0\n0\n0\n0\n52\n\n\ngriffiths2021am\n0\n10\n0\n0\n0\n0\n0\n10\n\n\nhu2017cr\n0\n48\n0\n0\n0\n0\n0\n48\n\n\nkoh2023me\n0\n4\n0\n0\n0\n0\n0\n4\n\n\nmarkov2014mu\n0\n16\n0\n0\n0\n0\n0\n16\n\n\norjesek2022en\n0\n8\n0\n0\n0\n0\n0\n8\n\n\nsaizclar2022pr\n2\n0\n0\n0\n0\n0\n0\n2\n\n\nwang2021ac\n0\n0\n0\n0\n8\n0\n8\n8\n\n\nwang2022co\n0\n8\n0\n0\n0\n0\n0\n8\n\n\nwang2022cr\n0\n0\n0\n0\n16\n0\n16\n16\n\n\nxie2020mu\n0\n18\n0\n0\n0\n0\n0\n18\n\n\nxu2021us\n0\n2\n0\n0\n0\n0\n0\n2\n\n\nzhang2016br\n0\n7\n0\n0\n0\n0\n0\n14\n\n\nzhang2019us\n0\n2\n0\n0\n0\n0\n0\n2\n\n\nzhang2023mo\n0\n12\n0\n0\n0\n0\n0\n12\n\n\n\nprint(knitr::kable(table(metaMER_results$citekey,metaMER_results$model_id)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCNN-Bidirectional Gated Recurrence Unit\nCNN-Iterative Reconstruction-CNN-Bidirectional Gated Recurrence Unit\nextremely randomized tree regression\nfeedforwardNN\nfully connected NN\ngaussian process regression\nl1 logistic regression\nl1 support vector regression\nl2 logistic regression\nlinear svm\nlogistic regression\nlong short term memory NN\nlr\nlr (stepwise forward)\nMCAN\nmlr\nMLR\nNuSVR\npcr\npls\nPLSR\npolynomial svm\nrandom forest\nrandom forest regression\nreguliarized discriminant analysis\nRFR\nrnn (1 layer x 124 LSTM units)\nrnn (1 layer x 248 LSTM units)\nrnn (1 layer x 529 LSTM units)\nrnn (2 layer x 124 LSTM units)\nrnn (2 layer x 248 LSTM units)\nrnn (2 layer x 529 LSTM units)\nsmoreg\nsvm\nsvr\nSVR\nSVR KMBSO\n\n\n\n\nagarwal2021an\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n63\n\n\nalvarez2023ri\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n8\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nbai2017mu\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n9\n0\n0\n0\n0\n0\n0\n0\n0\n0\n9\n0\n\n\ngingras2014be\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n12\n12\n0\n0\n0\n0\n0\n12\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\ngrekow2018au\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n44\n0\n0\n0\n0\n\n\ngrekow2021mu\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n12\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n16\n16\n8\n16\n16\n8\n0\n0\n12\n0\n0\n\n\ngriffiths2021am\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n20\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nhu2017cr\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n96\n0\n\n\nkoh2023me\n0\n0\n0\n0\n4\n0\n0\n0\n0\n0\n0\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nmarkov2014mu\n0\n0\n0\n0\n0\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n28\n0\n0\n0\n\n\norjesek2022en\n8\n8\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nsaizclar2022pr\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nwang2021ac\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n24\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nwang2022co\n0\n0\n16\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nwang2022cr\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n48\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nxie2020mu\n0\n0\n0\n12\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n12\n12\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nxu2021us\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nzhang2016br\n0\n0\n0\n0\n0\n0\n3\n3\n3\n3\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n3\n0\n0\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nzhang2019us\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nzhang2023mo\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n24\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\nprint(knitr::kable(table(metaMER_results$citekey,metaMER_results$feature_id)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhandcrafted and filter bank\naudio\nbaseline features\nbaseline features and stc\nchroma\nessentia\nessentia (attribute selection)\nessentia (no attribute selection)\nFree Music Archive\nhandcrafted and filter bank\nintensity\nkaraoke\nlowlevel\nlowlevel rhythm\nlowlevel rhythm tonal\nlowlevel tonal\nlyrics\nmarsyas (no attribute selection)\nmediaeval\nmfcc\nmixed\nmusic\nonset curves\npretrained model\nReliefF\nrhythm\nrhythm tonal\nsong\nspotify\nstc\ntimbre\ntonal\n\n\n\n\nagarwal2021an\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n21\n0\n0\n0\n0\n21\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n21\n0\n0\n0\n0\n\n\nalvarez2023ri\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n8\n0\n0\n0\n\n\nbai2017mu\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n18\n0\n0\n0\n0\n0\n0\n0\n\n\ngingras2014be\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n36\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\ngrekow2018au\n0\n0\n0\n0\n0\n0\n4\n8\n0\n0\n0\n0\n4\n4\n4\n4\n0\n4\n0\n0\n0\n0\n0\n0\n0\n4\n4\n0\n0\n0\n0\n4\n\n\ngrekow2021mu\n0\n0\n0\n0\n24\n32\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n24\n0\n0\n0\n24\n0\n0\n0\n0\n0\n0\n0\n0\n\n\ngriffiths2021am\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n20\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nhu2017cr\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n96\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nkoh2023me\n0\n0\n0\n0\n0\n0\n0\n0\n8\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nmarkov2014mu\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n32\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\norjesek2022en\n0\n16\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nsaizclar2022pr\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nwang2021ac\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n24\n0\n\n\nwang2022co\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n16\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nwang2022cr\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n48\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nxie2020mu\n0\n0\n12\n12\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n12\n0\n0\n\n\nxu2021us\n0\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nzhang2016br\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n21\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nzhang2019us\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nzhang2023mo\n12\n0\n0\n0\n0\n0\n0\n0\n0\n12\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\nprint(knitr::kable(table(metaMER_results$citekey,metaMER_results$data_id)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n100songs\nacousticBrainz\nall\namg_amg\namg_ch\namg_mer\naudioOnly\nch_amg\nch_ch\nch_mer\nchinese\nchineseClassicalEnsembles\nchineseClassicalSolo\ncross\ndeam\ngtzan (validated)\nhindi\nismir2012\nMediaEval\nmer_amg\nmer_ch\nmer_mer\nmfcc\nmfcc timbre\nmfcc timbre spectralCrest spectralFlatness\nmfcc timbre spectralCrest spectralFlatness chromagram lineSpectralPairs\nnew\nNJUV1\nPMEmo\npsic3839\nspotify\nwestern\nwesternClassicalEnsembles\nwesternClassicalSolo\nwithin\nxing2014\nzhang2015so\n\n\n\n\nagarwal2021an\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n21\n21\n0\n0\n0\n0\n0\n0\n0\n0\n0\n21\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nalvarez2023ri\n0\n0\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n4\n0\n0\n0\n0\n0\n0\n\n\nbai2017mu\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n18\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\ngingras2014be\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n36\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\ngrekow2018au\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n44\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\ngrekow2021mu\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n104\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\ngriffiths2021am\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n20\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nhu2017cr\n0\n0\n0\n8\n8\n8\n8\n0\n8\n8\n8\n0\n0\n0\n8\n0\n0\n0\n0\n0\n8\n8\n8\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n8\n0\n0\n\n\nkoh2023me\n0\n0\n0\n0\n0\n0\n0\n8\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nmarkov2014mu\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n8\n8\n8\n8\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\norjesek2022en\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n16\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nsaizclar2022pr\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nwang2021ac\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n12\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n12\n0\n0\n0\n0\n0\n\n\nwang2022co\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n8\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n8\n0\n0\n0\n0\n0\n\n\nwang2022cr\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n12\n12\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n12\n12\n0\n0\n0\n\n\nxie2020mu\n0\n36\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nxu2021us\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n4\n0\n0\n0\n0\n0\n0\n0\n\n\nzhang2016br\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n21\n\n\nzhang2019us\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n4\n0\n\n\nzhang2023mo\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n12\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n12\n0\n0\n0\n0\n0\n0\n0\n0"
  },
  {
    "objectID": "analysis/analysis.html#analysis-notes",
    "href": "analysis/analysis.html#analysis-notes",
    "title": "Analysis",
    "section": "",
    "text": "Issue 1: We need unique identifiers for each input component (combine: study + model + feature + data + experiment that would have be unique id)\nIssue 2: We stimulus_N\nIssue 3: measure vs statistic?\nIssue 4: We need to classify the modelling techniques into fewer number of techniques (example given, but no principles defined)\nIssue 5: We could polish citekey into ref (xu2021us to Xu 2021 et al., ) for nicer plotting output (can be done with str_replace)\nIf the number of stimuli varies, divide either to separate experiments or use the largest value (see below for ad-hoc solution).\n\n\n\n\n\n\nlibrary(dmetar)\nlibrary(tidyverse)\nlibrary(meta)\n\n# select regression studies with r2\ntmp &lt;- dplyr::filter(metaMER_results,dimension==\"valence\" & measure==\"r2\")\n\n# Temporary clarification of N\ntmp$stimulus_n[tmp$stimulus_n==\" emoMusic: 1000, soundtracks: 360, chinese: 500 \"]&lt;-1000\ntmp$stimulus_n[tmp$stimulus_n==\" 2372 (subset of PSIC3839, total n: 3839)    \"]&lt;-2372\ntmp$stimulus_n[tmp$stimulus_n==\" study 1: 20; study 2: 40) % three outliers  \"]&lt;-40\ntmp$stimulus_n &lt;- as.numeric(tmp$stimulus_n)\n\n#sqrt(tmp$values) # convert from R^2 to r\n#tmp$stimulus_n &lt;- 100 # ad-hoc for now\n\nm.cor &lt;- metacor(cor = sqrt(values), \n                 n = stimulus_n,\n                 studlab = citekey,\n                 data = tmp,\n                 fixed = FALSE,\n                 random = TRUE,\n                 sm = \"ZCOR\",\n                 method.tau = \"REML\",# could be PM (Paule-Mandel) as well\n                 method.random.ci = \"HK\", \n                 title = \"MER: Regression: Valence: All\")\n\nprint(m.cor)\n\nReview:     MER: Regression: Valence: All\n\nNumber of studies: k = 69\nNumber of observations: o = 36648\n\n                        COR           95%-CI     t  p-value\nRandom effects model 0.5673 [0.5100; 0.6197] 15.88 &lt; 0.0001\n\nQuantifying heterogeneity:\n tau^2 = 0.1078 [0.0788; 0.1605]; tau = 0.3283 [0.2807; 0.4006]\n I^2 = 97.9% [97.7%; 98.1%]; H = 6.91 [6.54; 7.30]\n\nTest of heterogeneity:\n       Q d.f. p-value\n 3246.86   68       0\n\nDetails on meta-analytical method:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Hartung-Knapp adjustment for random effects model (df = 68)\n- Fisher's z transformation of correlations"
  },
  {
    "objectID": "analysis/analysis.html#sub-group-analysis-based-on-model-type",
    "href": "analysis/analysis.html#sub-group-analysis-based-on-model-type",
    "title": "Analysis",
    "section": "",
    "text": "# divide models into random forests, SVM and MLR\ntmp$model_class_id &lt;- 'MLR/PLS'\ntmp$model_class_id[str_detect(tmp$model_id,'RFR|extremely randomized tree regression')]&lt;-'RF'\ntmp$model_class_id[str_detect(tmp$model_id,'svm|SVR')]&lt;-'SVM'\ntable(tmp$model_class_id)\n\n\nMLR/PLS      RF     SVM \n     49       5      63 \n\nmeta &lt;- metagen(values, sqrt(values), # Fix the TE.se \n                data = tmp, \n                studlab = tmp$citekey, \n                comb.fixed = FALSE, \n                method.tau = \"PM\")\n\nsubgroup.analysis.mixed.effects(x = meta, \n                                subgroups = tmp$model_class_id)\n\nSubgroup Results:\n--------------\n         k        TE       seTE   LLCI  ULCI           p         Q I2 I2.lower\nMLR/PLS 49 0.1113731 0.04767515  0.018 0.205 0.019486956 10.847720  0        0\nRF       5 0.5211998 0.32286214 -0.112 1.154 0.106460055  0.284001  0        0\nSVM     63 0.1084808 0.04149599  0.027 0.190 0.008942433  4.755712  0        0\n        I2.upper\nMLR/PLS     0.33\nRF          0.79\nSVM         0.30\n\nTest for subgroup differences (mixed/fixed-effects (plural) model):\n--------------\n                      Q df       p\nBetween groups 1.611199  2 0.44682\n\n- Total number of studies included in subgroup analysis:  117\n- Tau estimator used for within-group pooling:  PM"
  },
  {
    "objectID": "analysis/analysis.html#explore-various-qualities",
    "href": "analysis/analysis.html#explore-various-qualities",
    "title": "Analysis",
    "section": "explore various qualities",
    "text": "explore various qualities\nfind.outliers(meta) infan &lt;- InfluenceAnalysis(meta) print(eggers.test(meta))\nplot(m.cor)\n\n## Visualise\n\n::: {.cell}\n\n```{.r .cell-code}\n#\n:::"
  },
  {
    "objectID": "analysis/analysis.html#explore-qualities",
    "href": "analysis/analysis.html#explore-qualities",
    "title": "Analysis",
    "section": "",
    "text": "meta &lt;- metagen(values, sqrt(values), \n                data = tmp, \n                studlab = tmp$citekey, \n                comb.fixed = FALSE, \n                method.tau = \"PM\")\nfind.outliers(meta)\n\nNo outliers detected (random-effects model).\n\ninfan &lt;- InfluenceAnalysis(meta)\n\n[===========================================================================] DONE \n\nprint(eggers.test(meta))\n\nEggers' test of the intercept \n============================= \n\n intercept      95% CI      t            p\n     0.779 0.74 - 0.82 40.969 1.937557e-70\n\nEggers' test indicates the presence of funnel plot asymmetry."
  },
  {
    "objectID": "analysis/preprocessing.html",
    "href": "analysis/preprocessing.html",
    "title": "Preprocessing",
    "section": "",
    "text": "This assumes that the data has been parsed from the BibTeX files into table and exported as CSV file.\n\n\n\nrequire(here)\nsource(here::here('R/build-df.R'))\nsource(here::here('R/format-study-results.R'))\nsource(here::here('R/parse-model-output.R'))\n\n\nlibrary(stringr) \n# get metaMER df:\nmeta_df &lt;- get_metaMER_df(path_2_studies = here::here('studies'))\n\n# get included studies\nincluded_studies &lt;- meta_df[which(\n  !stringr::str_detect(meta_df$final_notes, '!EXCL!')),] |&gt; \n  dplyr::tibble()\n\n\n\n\n# get studies re-coded (currently identifiable by presence of bind_field.)\nrecoded_studies &lt;- included_studies[which(stringr::str_detect(\n  included_studies$model_rate_emotion_values,\n                                     'bind_field')),] \n\n\n\n\n\nmetaMER_results &lt;-\ndo.call(\n  rbind,\n    lapply(1:nrow(recoded_studies),\n       function(x) get_study_results(recoded_studies[x,])\n       )\n) \n\n# add unique identifiers\n\nunique_id &lt;- apply(metaMER_results[,c('citekey',\n                        'library_id',\n                         'model_id',\n                         'feature_id',\n                         'data_id',\n                         'experiment_id')],\n                      1, \n                      paste0, \n                      collapse = '-'\n) \nmetaMER_results$unique_id &lt;- stringr::str_remove_all(unique_id, \n                                                     ' ')\n\nmetaMER_results &lt;- metaMER_results |&gt; dplyr::select(unique_id, \n                                 dplyr::everything()) \n\nmetaMER_results |&gt; dplyr::tibble()\n\n# A tibble: 1,192 × 19\n   unique_id  citekey journal stimulus_genre model_category stimulus_n feature_n\n   &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;chr&gt;          &lt;chr&gt;      &lt;chr&gt;    \n 1 agarwal20… agarwa… \"   IE… \" popular, hi… classification \" ISMIR20… \" 'eight…\n 2 agarwal20… agarwa… \"   IE… \" popular, hi… classification \" ISMIR20… \" 'eight…\n 3 agarwal20… agarwa… \"   IE… \" popular, hi… classification \" ISMIR20… \" 'eight…\n 4 agarwal20… agarwa… \"   IE… \" popular, hi… classification \" ISMIR20… \" 'eight…\n 5 agarwal20… agarwa… \"   IE… \" popular, hi… classification \" ISMIR20… \" 'eight…\n 6 agarwal20… agarwa… \"   IE… \" popular, hi… classification \" ISMIR20… \" 'eight…\n 7 agarwal20… agarwa… \"   IE… \" popular, hi… classification \" ISMIR20… \" 'eight…\n 8 agarwal20… agarwa… \"   IE… \" popular, hi… classification \" ISMIR20… \" 'eight…\n 9 agarwal20… agarwa… \"   IE… \" popular, hi… classification \" ISMIR20… \" 'eight…\n10 agarwal20… agarwa… \"   IE… \" popular, hi… classification \" ISMIR20… \" 'eight…\n# ℹ 1,182 more rows\n# ℹ 12 more variables: participant_n &lt;chr&gt;, feature_source &lt;chr&gt;,\n#   feature_reduction_method &lt;chr&gt;, library_id &lt;chr&gt;, model_id &lt;chr&gt;,\n#   feature_id &lt;chr&gt;, data_id &lt;chr&gt;, experiment_id &lt;chr&gt;, dimension &lt;chr&gt;,\n#   measure &lt;chr&gt;, statistic &lt;chr&gt;, values &lt;dbl&gt;\n\n\n\n\n\nprint(knitr::kable(table(metaMER_results$citekey,metaMER_results$dimension)))\nprint(knitr::kable(table(metaMER_results$citekey,metaMER_results$model_id)))\nprint(knitr::kable(table(metaMER_results$citekey,metaMER_results$feature_id)))\nprint(knitr::kable(table(metaMER_results$citekey,metaMER_results$data_id)))\n\n\n\n\n\n\n\n# Classify techniques according Hastie, Tibshirani, Friedman (2008)\n# https://www.sas.upenn.edu/~fdiebold/NoHesitations/BookAdvanced.pdf\n#\nlibrary(stringr)\nmetaMER_results$model_id &lt;- tolower(metaMER_results$model_id)\nmetaMER_results$model_class_id &lt;- 'Unclassified'\nmetaMER_results$model_class_id[str_detect(metaMER_results$model_id,'lr|lm|pls|mlr|pcr|logistic regression|2d model full|pentagon|gaussian process regression|sparse bayesian regression|variational bayesian regression|logistic regression|lda|rda|regularized discriminant analysis|reguliarized discriminant analysis')] &lt;- 'Linear Methods' # Class name from Elements of Stat..\"\n#metaMER_results$model_class_id[str_detect(metaMER_results$model_id,'logistic regression|lda|rda|regularized discriminant analysis|reguliarized discriminant analysis')] &lt;- 'Linear Classification'\n#metaMER_results$model_class_id[str_detect(metaMER_results$model_id,'mars|gam')]&lt;-'Additive Trees and Related Methods' #\nmetaMER_results$model_class_id[str_detect(metaMER_results$model_id,'rbf|gmm|local|polynomial|polygonal|knn|mars|gam')]&lt;-'Kernel Smoothing, Additive and KNN'\n#metaMER_results$model_class_id[str_detect(metaMER_results$model_id,'adaboost|gradient')]&lt;-'Boosting'\nmetaMER_results$model_class_id[str_detect(metaMER_results$model_id,'nn|gru|lstm|ltsm|long short term memory|rprop|mcan')]&lt;-'Neural Nets'\nmetaMER_results$model_class_id[str_detect(metaMER_results$model_id,'svm|svr|support vector regression|smoreg|smo ')]&lt;-'Flexible Discriminants'\n#metaMER_results$model_class_id[str_detect(metaMER_results$model_id,'knn')]&lt;-'Prototype methods'\nmetaMER_results$model_class_id[str_detect(metaMER_results$model_id,'rf|extremely randomized tree regression|random forest|adaboost|gradient')]&lt;-'Random Forests'\n#metaMER_results$model_class_id[str_detect(metaMER_results$model_id,'rf')]&lt;-'Ensemble Learning'\n\n\n\n\n\nmetaMER_results$stimulus_genre_mixed &lt;- 'SingleGenre'\nmetaMER_results$stimulus_genre_mixed[str_detect(metaMER_results$stimulus_genre,',|multi')] &lt;- 'MultiGenre' # Class name from Elements of Stat..\"\n#table(metaMER_results$stimulus_genre_mixed)\n\n\n\n\n\nmetaMER_results$journal_type &lt;- \"Engineering\"\nmetaMER_results$journal_type[str_detect(metaMER_results$journal,'Quarterly Journal of Experimental Psychology|PSYCHOLOGY OF MUSIC|PLOS ONE|JOURNAL OF NEW MUSIC RESEARCH|IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING|FRONTIERS IN PSYCHOLOGY|Frontiers in Psychology')] &lt;- 'Psychology' # Class name from Elements of Stat..\"\n\n#table(metaMER_results$journal_type)\n\n\n\n\n\nprint(knitr::kable(table(metaMER_results$model_class_id)))\n\n\n\nVar1\nFreq\n\n\n\n\nFlexible Discriminants\n371\n\n\nKernel Smoothing, Additive and KNN\n106\n\n\nLinear Methods\n238\n\n\nNeural Nets\n361\n\n\nRandom Forests\n116\n\n\n\nprint(knitr::kable(table(metaMER_results$model_class_id,metaMER_results$model_category)))\n\n\n\n\nclassification\nregression\n\n\n\n\nFlexible Discriminants\n124\n247\n\n\nKernel Smoothing, Additive and KNN\n0\n106\n\n\nLinear Methods\n48\n190\n\n\nNeural Nets\n242\n119\n\n\nRandom Forests\n65\n51\n\n\n\nprint(paste(\"We have\", nrow(metaMER_results), \"observations\"))\n[1] “We have 1192 observations”\nprint(paste(\"We have\", length(unique(metaMER_results$citekey)), \"studies\"))\n[1] “We have 37 studies”\nprint(paste(\"Where\", length(unique(metaMER_results$citekey[metaMER_results$model_category=='regression'])), \"are regression studies\"))\n[1] “Where 25 are regression studies”\nprint(paste(\"Where\", length(unique(metaMER_results$citekey[metaMER_results$model_category=='classification'])), \"are classification studies\"))\n[1] “Where 12 are classification studies”\n# note that we have some classification studies that also do regression and vice versa?\n# THIS IS CORRECT:\n# [1] \"We have 1192 observations\"\n# [1] \"We have 37 studies\"\n# [1] \"Where 25 are regression studies\"\n# [1] \"Where 12 are classification studies\""
  },
  {
    "objectID": "analysis/preprocessing.html#read-annotated-data",
    "href": "analysis/preprocessing.html#read-annotated-data",
    "title": "Preprocessing",
    "section": "",
    "text": "require(here)\nsource(here::here('R/build-df.R'))\nsource(here::here('R/format-study-results.R'))\nsource(here::here('R/parse-model-output.R'))\n\n\nlibrary(stringr) \n# get metaMER df:\nmeta_df &lt;- get_metaMER_df(path_2_studies = here::here('studies'))\n\n# get included studies\nincluded_studies &lt;- meta_df[which(\n  !stringr::str_detect(meta_df$final_notes, '!EXCL!')),] |&gt; \n  dplyr::tibble()\n\n\n\n\n# get studies re-coded (currently identifiable by presence of bind_field.)\nrecoded_studies &lt;- included_studies[which(stringr::str_detect(\n  included_studies$model_rate_emotion_values,\n                                     'bind_field')),] \n\n\n\n\n\nmetaMER_results &lt;-\ndo.call(\n  rbind,\n    lapply(1:nrow(recoded_studies),\n       function(x) get_study_results(recoded_studies[x,])\n       )\n) \n\n# add unique identifiers\n\nunique_id &lt;- apply(metaMER_results[,c('citekey',\n                        'library_id',\n                         'model_id',\n                         'feature_id',\n                         'data_id',\n                         'experiment_id')],\n                      1, \n                      paste0, \n                      collapse = '-'\n) \nmetaMER_results$unique_id &lt;- stringr::str_remove_all(unique_id, \n                                                     ' ')\n\nmetaMER_results &lt;- metaMER_results |&gt; dplyr::select(unique_id, \n                                 dplyr::everything()) \n\nmetaMER_results |&gt; dplyr::tibble()\n\n# A tibble: 1,192 × 19\n   unique_id  citekey journal stimulus_genre model_category stimulus_n feature_n\n   &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;chr&gt;          &lt;chr&gt;      &lt;chr&gt;    \n 1 agarwal20… agarwa… \"   IE… \" popular, hi… classification \" ISMIR20… \" 'eight…\n 2 agarwal20… agarwa… \"   IE… \" popular, hi… classification \" ISMIR20… \" 'eight…\n 3 agarwal20… agarwa… \"   IE… \" popular, hi… classification \" ISMIR20… \" 'eight…\n 4 agarwal20… agarwa… \"   IE… \" popular, hi… classification \" ISMIR20… \" 'eight…\n 5 agarwal20… agarwa… \"   IE… \" popular, hi… classification \" ISMIR20… \" 'eight…\n 6 agarwal20… agarwa… \"   IE… \" popular, hi… classification \" ISMIR20… \" 'eight…\n 7 agarwal20… agarwa… \"   IE… \" popular, hi… classification \" ISMIR20… \" 'eight…\n 8 agarwal20… agarwa… \"   IE… \" popular, hi… classification \" ISMIR20… \" 'eight…\n 9 agarwal20… agarwa… \"   IE… \" popular, hi… classification \" ISMIR20… \" 'eight…\n10 agarwal20… agarwa… \"   IE… \" popular, hi… classification \" ISMIR20… \" 'eight…\n# ℹ 1,182 more rows\n# ℹ 12 more variables: participant_n &lt;chr&gt;, feature_source &lt;chr&gt;,\n#   feature_reduction_method &lt;chr&gt;, library_id &lt;chr&gt;, model_id &lt;chr&gt;,\n#   feature_id &lt;chr&gt;, data_id &lt;chr&gt;, experiment_id &lt;chr&gt;, dimension &lt;chr&gt;,\n#   measure &lt;chr&gt;, statistic &lt;chr&gt;, values &lt;dbl&gt;\n\n\n\n\n\nprint(knitr::kable(table(metaMER_results$citekey,metaMER_results$dimension)))\nprint(knitr::kable(table(metaMER_results$citekey,metaMER_results$model_id)))\nprint(knitr::kable(table(metaMER_results$citekey,metaMER_results$feature_id)))\nprint(knitr::kable(table(metaMER_results$citekey,metaMER_results$data_id)))\n\n\n\n\n\n\n\n# Classify techniques according Hastie, Tibshirani, Friedman (2008)\n# https://www.sas.upenn.edu/~fdiebold/NoHesitations/BookAdvanced.pdf\n#\nlibrary(stringr)\nmetaMER_results$model_id &lt;- tolower(metaMER_results$model_id)\nmetaMER_results$model_class_id &lt;- 'Unclassified'\nmetaMER_results$model_class_id[str_detect(metaMER_results$model_id,'lr|lm|pls|mlr|pcr|logistic regression|2d model full|pentagon|gaussian process regression|sparse bayesian regression|variational bayesian regression|logistic regression|lda|rda|regularized discriminant analysis|reguliarized discriminant analysis')] &lt;- 'Linear Methods' # Class name from Elements of Stat..\"\n#metaMER_results$model_class_id[str_detect(metaMER_results$model_id,'logistic regression|lda|rda|regularized discriminant analysis|reguliarized discriminant analysis')] &lt;- 'Linear Classification'\n#metaMER_results$model_class_id[str_detect(metaMER_results$model_id,'mars|gam')]&lt;-'Additive Trees and Related Methods' #\nmetaMER_results$model_class_id[str_detect(metaMER_results$model_id,'rbf|gmm|local|polynomial|polygonal|knn|mars|gam')]&lt;-'Kernel Smoothing, Additive and KNN'\n#metaMER_results$model_class_id[str_detect(metaMER_results$model_id,'adaboost|gradient')]&lt;-'Boosting'\nmetaMER_results$model_class_id[str_detect(metaMER_results$model_id,'nn|gru|lstm|ltsm|long short term memory|rprop|mcan')]&lt;-'Neural Nets'\nmetaMER_results$model_class_id[str_detect(metaMER_results$model_id,'svm|svr|support vector regression|smoreg|smo ')]&lt;-'Flexible Discriminants'\n#metaMER_results$model_class_id[str_detect(metaMER_results$model_id,'knn')]&lt;-'Prototype methods'\nmetaMER_results$model_class_id[str_detect(metaMER_results$model_id,'rf|extremely randomized tree regression|random forest|adaboost|gradient')]&lt;-'Random Forests'\n#metaMER_results$model_class_id[str_detect(metaMER_results$model_id,'rf')]&lt;-'Ensemble Learning'\n\n\n\n\n\nmetaMER_results$stimulus_genre_mixed &lt;- 'SingleGenre'\nmetaMER_results$stimulus_genre_mixed[str_detect(metaMER_results$stimulus_genre,',|multi')] &lt;- 'MultiGenre' # Class name from Elements of Stat..\"\n#table(metaMER_results$stimulus_genre_mixed)\n\n\n\n\n\nmetaMER_results$journal_type &lt;- \"Engineering\"\nmetaMER_results$journal_type[str_detect(metaMER_results$journal,'Quarterly Journal of Experimental Psychology|PSYCHOLOGY OF MUSIC|PLOS ONE|JOURNAL OF NEW MUSIC RESEARCH|IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING|FRONTIERS IN PSYCHOLOGY|Frontiers in Psychology')] &lt;- 'Psychology' # Class name from Elements of Stat..\"\n\n#table(metaMER_results$journal_type)"
  },
  {
    "objectID": "analysis/preprocessing.html#summarise",
    "href": "analysis/preprocessing.html#summarise",
    "title": "Preprocessing",
    "section": "",
    "text": "print(knitr::kable(table(metaMER_results$model_class_id)))\n\n\n\nVar1\nFreq\n\n\n\n\nFlexible Discriminants\n371\n\n\nKernel Smoothing, Additive and KNN\n106\n\n\nLinear Methods\n238\n\n\nNeural Nets\n365\n\n\nRandom Forests\n116\n\n\n\nprint(knitr::kable(table(metaMER_results$model_class_id,metaMER_results$model_category)))\n\n\n\n\nclassification\nregression\n\n\n\n\nFlexible Discriminants\n124\n247\n\n\nKernel Smoothing, Additive and KNN\n0\n106\n\n\nLinear Methods\n48\n190\n\n\nNeural Nets\n242\n123\n\n\nRandom Forests\n65\n51\n\n\n\nprint(paste(\"We have\", nrow(metaMER_results), \"observations\"))\n[1] “We have 1196 observations”\nprint(paste(\"We have\", length(unique(metaMER_results$citekey)), \"studies\"))\n[1] “We have 38 studies”\nprint(paste(\"Where\", length(unique(metaMER_results$citekey[metaMER_results$model_category=='regression'])), \"are regression studies\"))\n[1] “Where 26 are regression studies”\nprint(paste(\"Where\", length(unique(metaMER_results$citekey[metaMER_results$model_category=='classification'])), \"are classification studies\"))\n[1] “Where 12 are classification studies”\n# note that we have some classification studies that also do regression and vice versa?\n\n\n\nR_studies &lt;- dplyr::filter(metaMER_results,model_category=='regression' & str_detect(measure,'r|cc|r2|R2')) #  \ndim(R_studies)\n\n[1] 580  19\n\n# eliminate measures that we don't need now\nR_studies &lt;- dplyr::filter(R_studies,!str_detect(statistic,'r2-95l|r2-95u|pvalue|upper|lower|sd|null|rmse'))\nR_studies &lt;- dplyr::filter(R_studies,!str_detect(measure,'nFeatures|rmse|jensen shannon divergence|accuracy|ccc|vector distance'))\n\ntable(R_studies$measure)\n\n\n cc pcc   r  r2  R2 \n  2   4  22 168   5 \n\ntable(R_studies$measure, R_studies$statistic)\n\n     \n      adjusted  cc globalOptimal localOptimal mean   r  r2\n  cc         0   2             0            0    0   0   0\n  pcc        0   0             0            0    4   0   0\n  r          0   0             0            0    4  18   0\n  r2        12   0             4            4   28   0 120\n  R2         0   0             0            0    5   0   0\n\ndim(R_studies)\n\n[1] 201  19\n\nR_studies$values[R_studies$measure=='R2']&lt;-sqrt(R_studies$values[R_studies$measure=='R2']) # recode R2 into r\nR_studies$values[R_studies$measure=='r2']&lt;-sqrt(R_studies$values[R_studies$measure=='r2']) # recode R2 into r\n\n\n\n\n\nR_studies$dimension[str_detect(R_studies$dimension,'activation|energy arousal|tension arousal')]&lt;-'arousal'\nR_studies$dimension[str_detect(R_studies$dimension,'pleasantness')]&lt;-'valence'\nR_studies &lt;- dplyr::filter(R_studies,!str_detect(dimension,'av')) # relates to distances, can be omitted\n\ntable(R_studies$dimension)  \n\n\narousal valence \n    101      95 \n\ntable(R_studies$stimulus_n)  \n\n\n                                          1000   \n                                              16 \n                                            146  \n                                              12 \n                                          1838   \n                                              10 \n    2372 (subset of PSIC3839, total n: 3839)     \n                                               2 \n                                            40   \n                                               2 \n                                            420  \n                                               2 \n                                            431  \n                                               4 \n                                             48  \n                                               6 \n                                            54   \n                                               4 \n                                          744    \n                                              18 \n                                            84   \n                                              12 \n emoMusic: 1000, soundtracks: 360, chinese: 500  \n                                               8 \n          MER60: 60, CH818: 818, AMG1608: 1608   \n                                              96 \n    study 1: 20; study 2: 40) % three outliers   \n                                               4 \n\n# Deal with four studies involving multiple datasets: \nR_studies$stimulus_n[R_studies$stimulus_n==\" emoMusic: 1000, soundtracks: 360, chinese: 500 \"] &lt;- 938 #resolved from the paper\nR_studies$stimulus_n[R_studies$stimulus_n==\" 2372 (subset of PSIC3839, total n: 3839)    \"] &lt;- 2372 # resolved\nR_studies$stimulus_n[R_studies$stimulus_n==\" study 1: 20; study 2: 40) % three outliers  \"] &lt;- 40 # decided to take this from validation\n# REDO with a clearer function\neliminate &lt;- str_detect(R_studies$unique_id,\"hu2017cr\") & !str_detect(R_studies$unique_id,\"all\")\nR_studies &lt;- R_studies[!eliminate,]\nR_studies$stimulus_n[R_studies$stimulus_n==\" MER60: 60, CH818: 818, AMG1608: 1608  \"] &lt;- 60+818+1608 #\nR_studies$stimulus_n &lt;- as.numeric(R_studies$stimulus_n)\n\n\n\n\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.1\n\ng1&lt;-ggplot(R_studies,aes(x=values,fill=citekey,color=dimension))+\n  geom_histogram()+\n  facet_wrap(.~model_class_id)+\n  scale_color_manual(values = c('black','white'))+\n  theme_dark()+\n  scale_x_continuous(breaks = seq(0,1,by=.1))\n\ng1\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 4 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\n\nWarning: package 'tidyr' was built under R version 4.3.1\n\n\nWarning: package 'readr' was built under R version 4.3.1\n\n\nWarning: package 'dplyr' was built under R version 4.3.1\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nR_studies$citekey &lt;- factor(R_studies$citekey)\nR_studies$dimension &lt;- factor(R_studies$dimension)\n\nR_summary &lt;- summarise(group_by(R_studies,dimension,citekey),valuesMean=mean(values,na.rm=T),valuesMedian=median(values,na.rm=T),valuesMax=max(values,na.rm=T),stimulus_n=first(stimulus_n),studyREF=first(studyREF),model_class_id=first(model_class_id))\n\n`summarise()` has grouped output by 'dimension'. You can override using the\n`.groups` argument.\n\n\n\n\n\nAdd variation from within the studies (alternative models)\n\nR_summary_split &lt;- pivot_wider(R_summary,id_cols = citekey, names_from = c(dimension), values_from = valuesMax)\n\ng2 &lt;- ggplot(R_summary_split,aes(x=valence,y=arousal,label=citekey))+\n  geom_label()+\n  theme_bw()+\n  scale_x_continuous(breaks = seq(0,1,by=.1))+\n  scale_y_continuous(breaks = seq(0,1,by=.1))\n\ng2\n\n\n\n\n\n\n\n## could be more informative when done with the full data\nR_studies$citekey&lt;-factor(R_studies$citekey)\nR_studies$dimension&lt;-factor(R_studies$dimension)\n\nR_studies_split &lt;- pivot_wider(R_studies,id_cols = c(unique_id,citekey,model_class_id), names_from = c(dimension), values_from = c(values),values_fn = mean)\nR_studies_split&lt;-drop_na(R_studies_split)\n\nlibrary(ggrepel)\n\nWarning: package 'ggrepel' was built under R version 4.3.1\n\ng3 &lt;- ggplot(R_studies_split,aes(x=valence,y=arousal,label=citekey,color=model_class_id,fill=model_class_id))+\n  geom_point(size=4)+\n  geom_label_repel(size=3, max.overlaps=50,show.legend = T,color='white')+\n  scale_x_continuous(breaks = seq(0,1,by=.25),limits = c(0,1))+\n  scale_y_continuous(breaks = seq(0,1,by=.25),limits = c(0,1))+\n  theme_bw()\ng3\n\n\n\n\n\n\n\n\n\n\n\n\n# Add year!\nR_studies$year &lt;- as.numeric(str_match(R_studies$citekey,'[0-9]+'))\n\ng3 &lt;- ggplot(R_studies,aes(x=year,y=values,colour=model_class_id))+\n  geom_point(show.legend = T)+\n  facet_wrap(.~dimension)+\n  theme_bw()\ng3\n\nWarning: Removed 4 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\nRatio of obs./features or just a classification based on feature n (quintiles).\n\nR_studies$feature_n==\" 15; 23 in table, but 15 reported \"\n\n  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [97] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\nR_studies$feature_n[R_studies$feature_n==\" 15; 23 in table, but 15 reported  \"] &lt;- 15 #resolved from the paper\n\nR_studies$feature_n[R_studies$feature_n==\" 15; 23 in table, but 15 reported  \"] &lt;- 15 #resolved\nR_studies$feature_n[R_studies$feature_n==\" before_selection = 45, after_selection = 6  \"] &lt;- 45 #resolved\nR_studies$feature_n[R_studies$feature_n==\" model 1: 52, model 2 = 68, model 3 = 260, model 4 = 388 \"] &lt;- 388 #resolved\nR_studies$feature_n[R_studies$feature_n==\" variable, 557 before feature selection \"] &lt;- 557 #resolved\n\nR_studies$feature_n[R_studies$feature_n==\" 50 PCA features \"] &lt;- 50 #resolved\n#R_studies$feature_n[R_studies$feature_n==\"\"] &lt;- 0 #resolved\n#R_studies$feature_n[R_studies$feature_n==\"\"] &lt;- 0 #resolved\n\nR_studies$feature_n&lt;-as.numeric(R_studies$feature_n)\n\nWarning: NAs introduced by coercion\n\nprint(quantile(R_studies$feature_n,c(0.333,0.666),na.rm = T))\n\n33.3% 66.6% \n   18   260 \n\n#quantile(R_studies$feature_n,c(0.25,0.50,0.75),na.rm = T)\n\n# Assign\nR_studies$feature_n_complexity &lt;- cut(R_studies$feature_n,\n                                      breaks = c(0,\n                                                 as.numeric(quantile(R_studies$feature_n,c(0.333),na.rm = T)),\n                                                 as.numeric(quantile(R_studies$feature_n,c(0.666),na.rm = T)),\n                                                 1000),\n                                      labels = c(\"Feature n &lt; 18\",\"Feature n &gt; 18 & &lt; 260\",\"Feature n &gt; 260\"))\ntable(R_studies$feature_n_complexity)\n\n\n        Feature n &lt; 18 Feature n &gt; 18 & &lt; 260        Feature n &gt; 260 \n                    26                     24                     24 \n\n\n\n\n\nNeeds to be done from the unsummarised data (R_studies).\n\ntmp &lt;- drop_na(R_studies)\nlibrary(ggdist)\n\ntmp$dimension&lt;-str_to_title(tmp$dimension)\ntmp$model_class_id&lt;-factor(tmp$model_class_id,\n                           levels = c(\"Neural Nets\",\"Flexible Discriminants\", \"Kernel Smoothing, Additive and KNN\", \"Random Forests\",\"Linear Methods\"),\n                           labels = c(\"Neural\\nNets\",\"Flexible\\nDiscriminants\", \"KS\\n & KNN\", \"Random\\nForests\", \"Linear\\nMethods\"))\n\ng &lt;- ggplot(tmp,aes(x=model_class_id,y=values,color=citekey,label=citekey))+\n  stat_halfeye(aes(fill=citekey),point_interval=\"mean_qi\", trim=FALSE, expand=FALSE, show.legend = FALSE,adjust = 1.25, density=\"bounded\", point_size=3,scale = 1,alpha=0.5) + \n  geom_point(alpha=0.5,show.legend = F,position = position_jitter(width = .3))+\n  #geom_label_repel(size=2,max.overlaps = 50)+\n  facet_wrap(dimension~feature_n_complexity)+\n  ylab(\"Correlation Coefficient\")+\n  xlab(\"Model Technique\")+\n#  scale_y_continuous(limits = c(0,1),expand = c(0.01,0.01))+\n  geom_text_repel(aes(x = model_class_id, y = values, label = studyREF),\n             stat = \"summary\", fun = mean,show.legend = F)+\n  theme_bw()\ng\n\n\n\n\n\n\n\n#ggsave(filename = 'FeatureN_regression.pdf',g,height = 7,width = 11)"
  },
  {
    "objectID": "analysis/preprocessing.html#export-as-csv",
    "href": "analysis/preprocessing.html#export-as-csv",
    "title": "Preprocessing",
    "section": "Export as csv",
    "text": "Export as csv\n\nwrite.csv(x = R_studies,file = 'R_studies.csv')\nwrite.csv(x = R_summary,file = 'R_summary.csv')"
  },
  {
    "objectID": "analysis/analysis.html#regression-studies",
    "href": "analysis/analysis.html#regression-studies",
    "title": "Analysis",
    "section": "",
    "text": "library(dmetar)\nlibrary(tidyverse)\nlibrary(meta)\nlibrary(DescTools)\nlibrary(ggrepel)\n#R_studies &lt;- read.csv(\"R_studies.csv\")\nR_summary &lt;- read.csv(\"R_summary.csv\")\n\n# select regression studies with r2\ntmp &lt;- dplyr::filter(R_summary,dimension==\"valence\")\n#tmp &lt;- dplyr::filter(R_studies,dimension==\"valence\")\n#dim(tmp)\n\n#if all studies, remove two\n#tmp &lt;- tmp[!is.na(tmp$values),]\n#dim(tmp)\n\n#sqrt(tmp$values) # convert from R^2 to r\n#tmp$stimulus_n &lt;- 100 # ad-hoc for now\n\nm.cor &lt;- metacor(cor = valuesMax,     # values \n                 n = stimulus_n,\n                 studlab = citekey, # unique_id\n                 data = tmp,\n                 fixed = FALSE,\n                 random = TRUE,\n                 prediction = TRUE,\n                 backtransf = TRUE,\n                 sm = \"ZCOR\",\n                 method.tau = \"REML\",# could be PM (Paule-Mandel) as well\n                 method.random.ci = \"HK\", \n                 title = \"MER: Regression: Valence: Summary\")\n\n#print(m.cor)\n\nm.cor_backtransformed &lt;- m.cor\nm.cor_backtransformed$TE &lt;- FisherZInv(m.cor_backtransformed$TE)\nprint(funnel(m.cor_backtransformed, common = FALSE, studlab=TRUE,backtransf=TRUE))\n\n\n\n\n\n\n\n\n$xlim\n[1] 0.1281951 1.1857650\n\n$ylim\n[1] 0.164399 0.000000\n\n\n\n\n\ntmpdata &lt;- data.frame(SE = FisherZInv(m.cor$seTE), Zr = FisherZInv(m.cor$TE),studies=m.cor$studlab)\n\ntmpdata$citekey&lt;-str_replace_all(tmpdata$studies,'-.*$','')\ntmpdata$citekey&lt;-factor(tmpdata$citekey)\n\ng &lt;- ggplot(tmpdata,aes(y = SE, x = Zr,label=studies,color=citekey,fill=citekey)) +\n  geom_point(show.legend = FALSE) +\n  geom_label_repel(size=1.5,max.overlaps = 39,color=\"black\",show.legend = FALSE)+\n  ylab('Standard Error') + \n  xlab('r')+\n  scale_y_reverse()+\n  scale_x_continuous(breaks=seq(0.0,1.0,0.25),limits=c(0,1.0))+\n#  coord_flip()+\n  theme_bw()\nprint(g)\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_label_repel()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nforest(m.cor,\n       sortvar = TE,\n       prediction = FALSE, \n             print.tau2 = FALSE,\n             leftlabs = c(\"Author\", \"g\", \"SE\"),studlab = citekey)\n\n\n\n\n\n\n\nfunnel(m.cor, common = FALSE, studlab=TRUE,backtransf=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\nO &lt;- find.outliers(m.cor)\n\nWarning in find.outliers(m.cor): Studies with NAs not considered in outlier\nanalysis.\n\n# 6 datasets identified as outliers, without them the r drops to 0.5781\ninfan &lt;- InfluenceAnalysis(m.cor)\n\n[===========================================================================] DONE \n\nprint(eggers.test(m.cor))\n\nWarning in metabias.meta(x, k.min = 3, method = \"linreg\"): 3 observation(s)\ndropped due to missing values\n\n\nEggers' test of the intercept \n============================= \n\n intercept        95% CI     t          p\n     5.368 -0.09 - 10.83 1.926 0.06917371\n\nEggers' test does not indicate the presence of funnel plot asymmetry. \n\n\n\n\n\n\noutliers &lt;- c(\"chen2017co-various-GMM-mfcc-AMG1608-1\", \"chen2017co-various-GMM-tonal-AMG1608-1\", \"chen2017co-various-GMM-spectral-AMG1608-1\", \"chen2017co-various-GMM-temporal-AMG1608-1\", \"chen2017co-various-GMM-mfcctonalspectraltemporal-AMG1608-1\", \"griffiths2021am-various-mlr-mixed-new-validation\", \"griffiths2021am-various-mlr-mixed-new-validation-nooutliers\", \"hu2017cr-various-SVR-mixed-all-1\", \"hu2017cr-various-SVR-mixed-all-1\", \"hu2017cr-various-SVR-mixed-all-1\", \"hu2017cr-various-SVR-mixed-all-1\", \"koh2023me-openSMILE-fullyconnectedNN-FreeMusicArchive-audioOnly-1\", \"koh2023me-openSMILE-longshorttermmemoryNN-FreeMusicArchive-audioOnly-1\", \"wang2022co-various-extremelyrandomizedtreeregression-mixed-western-1\", \"wang2022co-various-extremelyrandomizedtreeregression-mixed-western-1\", \"wang2022co-various-extremelyrandomizedtreeregression-mixed-chinese-1\", \"wang2022co-various-extremelyrandomizedtreeregression-mixed-chinese-1\", \"wang2022cr-various-pls-mixed-chineseClassicalEnsembles-1\", \"wang2022cr-various-pls-mixed-chineseClassicalSolo-1\", \"wang2022cr-various-pls-mixed-westernClassicalEnsembles-1\", \"wang2022cr-various-pls-mixed-westernClassicalSolo-1\", \"zhang2019us-marsyas-NuSVR-music-xing2014-1\")\n\ntmp2&lt;-tmp[!tmp$unique_id %in% outliers,]\nm.cor &lt;- metacor(cor = values, \n                 n = stimulus_n,\n                 studlab = unique_id, # unique_id\n                 data = tmp2,\n                 fixed = FALSE,\n                 random = TRUE,\n                 sm = \"ZCOR\",\n                 prediction = TRUE,\n                 backtransf = TRUE,\n                 method.tau = \"REML\",# could be PM (Paule-Mandel) as well\n                 method.random.ci = \"HK\", \n                 title = \"MER: Regression: Valence: Summary\")\n\nprint(m.cor)\n\nfp &lt;- funnel(m.cor, common = TRUE,studlab=FALSE)\n\n\n\n\nTo show the quality differences between core and eliminated studies (in progress).\n\ntmpdata &lt;- data.frame(SE = m.cor$seTE, Zr = m.cor$TE)\nestimate = m.cor$TE.common\nse = m.cor$seTE.common\nse.seq=seq(0, max(m.cor$cor), 0.001)\nll95 = estimate-(1.96*se.seq)\nul95 = estimate+(1.96*se.seq)\nll99 = estimate-(3.29*se.seq)\nul99 = estimate+(3.29*se.seq)\nmeanll95 = estimate-(1.96*se)\nmeanul95 = estimate+(1.96*se)\ndfCI = data.frame(ll95, ul95, ll99, ul99, se.seq, estimate, meanll95, meanul95)\n\nfp = ggplot(aes(x = SE, y = Zr), data = tmpdata) +\n  geom_point(shape = 1) +\n  xlab('Standard Error') + ylab('Zr')+\n  geom_line(aes(x = se.seq, y = ll95), linetype = 'dotted', data = dfCI) +\n  geom_line(aes(x = se.seq, y = ul95), linetype = 'dotted', data = dfCI) +\n   geom_segment(aes(x = min(se.seq), y = meanll95, xend = max(se.seq), yend = meanll95), linetype='dotted', data=dfCI) +\n  geom_segment(aes(x = min(se.seq), y = meanul95, xend = max(se.seq), yend = meanul95), linetype='dotted', data=dfCI) +\n#Reverse the x-axis ordering (se) so that the tip of the funnel will appear\n#at the top of the figure once we swap the x- and y-axes...\n  scale_x_reverse(breaks=seq(0,0.2,0.05),limits=c(0.2,0))+\n#Specify the range and interval for the tick-marks of the y-axis (Zr);\n#Choose values that work for you based on your data\n  scale_y_continuous(breaks=seq(0.3,1.25,0.25),limits=c(0.3,1.25))+\n#  scale_x_continuous(breaks=seq(0.2,0,0.05))+\n#And now we flip the axes so that SE is on y- and Zr is on x-\n  coord_flip()+\n#Finally, apply my APA-format theme (see code at end of post).\n#You could, alternatively, specify theme_bw() instead.\n  theme_bw()\nprint(fp)\n\nWarning: Removed 7 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 721 rows containing missing values or values outside the scale range\n(`geom_line()`).\nRemoved 721 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\nWarning: Removed 922 rows containing missing values or values outside the scale range\n(`geom_segment()`).\nRemoved 922 rows containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\n\n\n\n\n\n\n\n\n\n\nadd journal_type and stimulus_genre_mixed as a grouping option\n\nm.cor_subgroups &lt;- update(m.cor, \n       subgroup = model_class_id, \n#       subgroup = journal_type, \n#       subgroup = stimulus_genre_mixed, \n       tau.common = FALSE,\n       prediction = TRUE)\n\nprint(m.cor_subgroups)\n\nReview:     MER: Regression: Valence: Summary\n\nNumber of studies: k = 21\nNumber of observations: o = 13876\n\n                        COR           95%-CI     t  p-value\nRandom effects model 0.6830 [0.5852; 0.7611] 10.60 &lt; 0.0001\nPrediction interval         [0.0873; 0.9189]               \n\nQuantifying heterogeneity:\n tau^2 = 0.1213 [0.0691; 0.2690]; tau = 0.3483 [0.2628; 0.5186]\n I^2 = 97.8% [97.3%; 98.2%]; H = 6.79 [6.13; 7.53]\n\nTest of heterogeneity:\n      Q d.f.  p-value\n 923.13   20 &lt; 0.0001\n\nResults for subgroups (random effects model):\n                                                      k    COR\nmodel_class_id = Kernel Smoothing, Additive and KNN   2 0.4662\nmodel_class_id = Random Forests                       3 0.6846\nmodel_class_id = Linear Methods                       8 0.7840\nmodel_class_id = Neural Nets                          2 0.4273\nmodel_class_id = Flexible Discriminants               6 0.6555\n                                                               95%-CI  tau^2\nmodel_class_id = Kernel Smoothing, Additive and KNN [-0.6342; 0.9424] 0.0185\nmodel_class_id = Random Forests                     [ 0.0489; 0.9256] 0.0997\nmodel_class_id = Linear Methods                     [ 0.6249; 0.8806] 0.1370\nmodel_class_id = Neural Nets                        [-0.9981; 0.9997] 0.1818\nmodel_class_id = Flexible Discriminants             [ 0.4838; 0.7786] 0.0574\n                                                       tau      Q   I^2\nmodel_class_id = Kernel Smoothing, Additive and KNN 0.1361  20.56 95.1%\nmodel_class_id = Random Forests                     0.3158 197.77 99.0%\nmodel_class_id = Linear Methods                     0.3701 195.78 96.4%\nmodel_class_id = Neural Nets                        0.4264  17.57 94.3%\nmodel_class_id = Flexible Discriminants             0.2396 161.22 96.9%\n\nTest for subgroup differences (random effects model):\n                   Q d.f. p-value\nBetween groups 12.31    4  0.0152\n\nDetails on meta-analytical method:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Hartung-Knapp adjustment for random effects model (df = 20)\n- Prediction interval based on t-distribution (df = 19)\n- Fisher's z transformation of correlations\n\n#forest(m.cor_subgroups,subgroup=TRUE)\n\n\nIdea: visualise the distributions of the model successes within studies (done in preprocessing)\n\n\n\n\n\n\n# select regression studies with r2\ntmp &lt;- dplyr::filter(R_summary,dimension==\"arousal\")\n#tmp &lt;- dplyr::filter(R_studies,dimension==\"arousal\")\ndim(tmp)\n\n[1] 24 11\n\n#tmp &lt;- tmp[!is.na(tmp$values),]\ndim(tmp)\n\n[1] 24 11\n\n#tmp&lt;-drop_na(tmp)\n\nm.cor &lt;- metacor(cor = valuesMax, \n                 n = stimulus_n,\n                 studlab = citekey,\n                 data = tmp,\n                 fixed = FALSE,\n                 random = TRUE,\n                 prediction = TRUE,\n                 backtransf = TRUE,\n                 sm = \"ZCOR\",\n                 method.tau = \"REML\",# could be PM (Paule-Mandel) as well\n                 method.random.ci = \"HK\", \n                 title = \"MER: Regression: Arousal: Summary\")\n\nprint(m.cor)\n\nReview:     MER: Regression: Arousal: Summary\n\nNumber of studies: k = 21\nNumber of observations: o = 13876\n\n                        COR           95%-CI     t  p-value\nRandom effects model 0.8221 [0.7646; 0.8666] 15.55 &lt; 0.0001\nPrediction interval         [0.4301; 0.9532]               \n\nQuantifying heterogeneity:\n tau^2 = 0.1074 [0.0612; 0.2437]; tau = 0.3278 [0.2475; 0.4937]\n I^2 = 95.9% [94.7%; 96.8%]; H = 4.93 [4.35; 5.59]\n\nTest of heterogeneity:\n      Q d.f.  p-value\n 486.07   20 &lt; 0.0001\n\nDetails on meta-analytical method:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Hartung-Knapp adjustment for random effects model (df = 20)\n- Prediction interval based on t-distribution (df = 19)\n- Fisher's z transformation of correlations\n\n\n\n\n\n\nO &lt;- find.outliers(m.cor)\n\nWarning in find.outliers(m.cor): Studies with NAs not considered in outlier\nanalysis.\n\n\n\n\n\nforest(m.cor,\n       sortvar = TE,\n       prediction = FALSE, \n             print.tau2 = FALSE,\n             leftlabs = c(\"Author\", \"g\", \"SE\"))\n\n\n\n\n\n\n\nplot(eggers.test(m.cor))\n\nWarning in metabias.meta(x, k.min = 3, method = \"linreg\"): 3 observation(s)\ndropped due to missing values\n\n\n\n\n\n\n\n\n\n\n\n\n\nmeta &lt;- metagen(valuesMax, sqrt(valuesMax), \n                data = tmp, \n                studlab = tmp$citekey, \n                comb.fixed = FALSE, \n                method.tau = \"PM\")\nfind.outliers(meta)\ninfan &lt;- InfluenceAnalysis(meta)\nprint(eggers.test(meta))\n\n\n\n\n\nupdate(m.cor, \n       subgroup = model_class_id, \n       tau.common = FALSE)\n\nReview:     MER: Regression: Arousal: Summary\n\nNumber of studies: k = 21\nNumber of observations: o = 13876\n\n                        COR           95%-CI     t  p-value\nRandom effects model 0.8221 [0.7646; 0.8666] 15.55 &lt; 0.0001\nPrediction interval         [0.4301; 0.9532]               \n\nQuantifying heterogeneity:\n tau^2 = 0.1074 [0.0612; 0.2437]; tau = 0.3278 [0.2475; 0.4937]\n I^2 = 95.9% [94.7%; 96.8%]; H = 4.93 [4.35; 5.59]\n\nTest of heterogeneity:\n      Q d.f.  p-value\n 486.07   20 &lt; 0.0001\n\nResults for subgroups (random effects model):\n                                                      k    COR\nmodel_class_id = Kernel Smoothing, Additive and KNN   2 0.8068\nmodel_class_id = Random Forests                       3 0.8011\nmodel_class_id = Linear Methods                       8 0.8812\nmodel_class_id = Neural Nets                          2 0.5369\nmodel_class_id = Flexible Discriminants               6 0.8077\n                                                               95%-CI  tau^2\nmodel_class_id = Kernel Smoothing, Additive and KNN [ 0.5493; 0.9244] 0.0022\nmodel_class_id = Random Forests                     [ 0.7566; 0.8381] 0.0011\nmodel_class_id = Linear Methods                     [ 0.8079; 0.9277] 0.0846\nmodel_class_id = Neural Nets                        [-0.9703; 0.9973] 0.0800\nmodel_class_id = Flexible Discriminants             [ 0.6437; 0.9008] 0.1125\n                                                       tau      Q   I^2\nmodel_class_id = Kernel Smoothing, Additive and KNN 0.0470   3.33 70.0%\nmodel_class_id = Random Forests                     0.0339   4.61 56.6%\nmodel_class_id = Linear Methods                     0.2908 104.87 93.3%\nmodel_class_id = Neural Nets                        0.2829   8.29 87.9%\nmodel_class_id = Flexible Discriminants             0.3354 246.31 98.0%\n\nTest for subgroup differences (random effects model):\n                   Q d.f. p-value\nBetween groups 11.98    4  0.0175\n\nDetails on meta-analytical method:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Hartung-Knapp adjustment for random effects model (df = 20)\n- Prediction interval based on t-distribution (df = 19)\n- Fisher's z transformation of correlations\n\nm.cor_subgroups &lt;- update(m.cor, \n       subgroup = model_class_id, \n       tau.common = FALSE)\n\nforest(m.cor_subgroups,subgroup=TRUE)\n\n\n\n\n\n\n\n\n\nIdea: visualise the distributions of the model successes within studies (done in preprocessing)\n\n\ng1 &lt;- ggplot(tmp,aes(y=valuesMax,fill=model_class_id))+\n   geom_histogram(show.legend = T)+\n  facet_wrap(.~studyREF)+\n   coord_flip()+\n   theme_bw()\ng1\n\n# S &lt;- summarise(group_by(tmp,citekey),maxvalue=max(values))\n# g&lt;-ggplot(S,aes(y=maxvalue))+\n#   geom_histogram(bins = 14)+\n#   #facet_wrap(.~studyREF)+\n#   coord_flip()+\n#   theme_bw()\n# g"
  },
  {
    "objectID": "analysis/preprocessing.html#summarise-all",
    "href": "analysis/preprocessing.html#summarise-all",
    "title": "Preprocessing",
    "section": "",
    "text": "print(knitr::kable(table(metaMER_results$model_class_id)))\n\n\n\nVar1\nFreq\n\n\n\n\nFlexible Discriminants\n371\n\n\nKernel Smoothing, Additive and KNN\n106\n\n\nLinear Methods\n238\n\n\nNeural Nets\n361\n\n\nRandom Forests\n116\n\n\n\nprint(knitr::kable(table(metaMER_results$model_class_id,metaMER_results$model_category)))\n\n\n\n\nclassification\nregression\n\n\n\n\nFlexible Discriminants\n124\n247\n\n\nKernel Smoothing, Additive and KNN\n0\n106\n\n\nLinear Methods\n48\n190\n\n\nNeural Nets\n242\n119\n\n\nRandom Forests\n65\n51\n\n\n\nprint(paste(\"We have\", nrow(metaMER_results), \"observations\"))\n[1] “We have 1192 observations”\nprint(paste(\"We have\", length(unique(metaMER_results$citekey)), \"studies\"))\n[1] “We have 37 studies”\nprint(paste(\"Where\", length(unique(metaMER_results$citekey[metaMER_results$model_category=='regression'])), \"are regression studies\"))\n[1] “Where 25 are regression studies”\nprint(paste(\"Where\", length(unique(metaMER_results$citekey[metaMER_results$model_category=='classification'])), \"are classification studies\"))\n[1] “Where 12 are classification studies”\n# note that we have some classification studies that also do regression and vice versa?\n# THIS IS CORRECT:\n# [1] \"We have 1192 observations\"\n# [1] \"We have 37 studies\"\n# [1] \"Where 25 are regression studies\"\n# [1] \"Where 12 are classification studies\""
  },
  {
    "objectID": "analysis/preprocessing.html#pull-data-to-analyse-regression-studies",
    "href": "analysis/preprocessing.html#pull-data-to-analyse-regression-studies",
    "title": "Preprocessing",
    "section": "",
    "text": "R_studies &lt;- dplyr::filter(metaMER_results,model_category=='regression' & str_detect(measure,'r|cc|r2|R2')) #  \ndim(R_studies)\n\n[1] 580  19\n\n# eliminate measures that we don't need now\nR_studies &lt;- dplyr::filter(R_studies,!str_detect(statistic,'r2-95l|r2-95u|pvalue|upper|lower|sd|null|rmse'))\nR_studies &lt;- dplyr::filter(R_studies,!str_detect(measure,'nFeatures|rmse|jensen shannon divergence|accuracy|ccc|vector distance'))\n\ntable(R_studies$measure)\n\n\n cc pcc   r  r2  R2 \n  2   4  22 168   5 \n\ntable(R_studies$measure, R_studies$statistic)\n\n     \n      adjusted  cc globalOptimal localOptimal mean   r  r2\n  cc         0   2             0            0    0   0   0\n  pcc        0   0             0            0    4   0   0\n  r          0   0             0            0    4  18   0\n  r2        12   0             4            4   28   0 120\n  R2         0   0             0            0    5   0   0\n\ndim(R_studies)\n\n[1] 201  19\n\nR_studies$values[R_studies$measure=='R2']&lt;-sqrt(R_studies$values[R_studies$measure=='R2']) # recode R2 into r\nR_studies$values[R_studies$measure=='r2']&lt;-sqrt(R_studies$values[R_studies$measure=='r2']) # recode R2 into r\n\n\n\n\nR_studies$dimension[str_detect(R_studies$dimension,'activation|energy arousal|tension arousal')]&lt;-'arousal'\nR_studies$dimension[str_detect(R_studies$dimension,'pleasantness')]&lt;-'valence'\nR_studies &lt;- dplyr::filter(R_studies,!str_detect(dimension,'av')) # relates to distances, can be omitted\n\ntable(R_studies$dimension)  \n\n\narousal valence \n    101      95 \n\ntable(R_studies$stimulus_n)  \n\n\n                                          1000   \n                                              16 \n                                            146  \n                                              12 \n                                          1838   \n                                              10 \n    2372 (subset of PSIC3839, total n: 3839)     \n                                               2 \n                                            40   \n                                               2 \n                                            420  \n                                               2 \n                                            431  \n                                               4 \n                                             48  \n                                               6 \n                                            54   \n                                               4 \n                                          744    \n                                              18 \n                                            84   \n                                              12 \n emoMusic: 1000, soundtracks: 360, chinese: 500  \n                                               8 \n          MER60: 60, CH818: 818, AMG1608: 1608   \n                                              96 \n    study 1: 20; study 2: 40) % three outliers   \n                                               4 \n\n# Deal with four studies involving multiple datasets: \nR_studies$stimulus_n[R_studies$stimulus_n==\" emoMusic: 1000, soundtracks: 360, chinese: 500 \"] &lt;- 938 #resolved from the paper\nR_studies$stimulus_n[R_studies$stimulus_n==\" 2372 (subset of PSIC3839, total n: 3839)    \"] &lt;- 2372 # resolved\nR_studies$stimulus_n[R_studies$stimulus_n==\" study 1: 20; study 2: 40) % three outliers  \"] &lt;- 40 # decided to take this from validation\n# REDO with a clearer function\neliminate &lt;- str_detect(R_studies$unique_id,\"hu2017cr\") & !str_detect(R_studies$unique_id,\"all\")\nR_studies &lt;- R_studies[!eliminate,]\nR_studies$stimulus_n[R_studies$stimulus_n==\" MER60: 60, CH818: 818, AMG1608: 1608  \"] &lt;- 60+818+1608 #\nR_studies$stimulus_n &lt;- as.numeric(R_studies$stimulus_n)\n\n\n\n\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.1\n\ng1&lt;-ggplot(R_studies,aes(x=values,fill=citekey,color=dimension))+\n  geom_histogram()+\n  facet_wrap(.~model_class_id)+\n  scale_color_manual(values = c('black','white'))+\n  theme_dark()+\n  scale_x_continuous(breaks = seq(0,1,by=.1))\n\ng1\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 4 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\n\nWarning: package 'tidyr' was built under R version 4.3.1\n\n\nWarning: package 'readr' was built under R version 4.3.1\n\n\nWarning: package 'dplyr' was built under R version 4.3.1\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nR_studies$citekey &lt;- factor(R_studies$citekey)\nR_studies$dimension &lt;- factor(R_studies$dimension)\n\nR_summary &lt;- summarise(group_by(R_studies,dimension,citekey),valuesMean=mean(values,na.rm=T),valuesMedian=median(values,na.rm=T),valuesMax=max(values,na.rm=T),stimulus_n=first(stimulus_n),studyREF=first(studyREF),model_class_id=first(model_class_id))\n\n`summarise()` has grouped output by 'dimension'. You can override using the\n`.groups` argument.\n\n\n\n\n\nAdd variation from within the studies (alternative models)\n\nR_summary_split &lt;- pivot_wider(R_summary,id_cols = citekey, names_from = c(dimension), values_from = valuesMax)\n\ng2 &lt;- ggplot(R_summary_split,aes(x=valence,y=arousal,label=citekey))+\n  geom_label()+\n  theme_bw()+\n  scale_x_continuous(breaks = seq(0,1,by=.1))+\n  scale_y_continuous(breaks = seq(0,1,by=.1))\n\ng2\n\n\n\n\n\n\n\n## could be more informative when done with the full data\nR_studies$citekey&lt;-factor(R_studies$citekey)\nR_studies$dimension&lt;-factor(R_studies$dimension)\n\nR_studies_split &lt;- pivot_wider(R_studies,id_cols = c(unique_id,citekey,model_class_id), names_from = c(dimension), values_from = c(values),values_fn = mean)\nR_studies_split&lt;-drop_na(R_studies_split)\n\nlibrary(ggrepel)\n\nWarning: package 'ggrepel' was built under R version 4.3.1\n\ng3 &lt;- ggplot(R_studies_split,aes(x=valence,y=arousal,label=citekey,color=model_class_id,fill=model_class_id))+\n  geom_point(size=4)+\n  geom_label_repel(size=3, max.overlaps=50,show.legend = T,color='white')+\n  scale_x_continuous(breaks = seq(0,1,by=.25),limits = c(0,1))+\n  scale_y_continuous(breaks = seq(0,1,by=.25),limits = c(0,1))+\n  theme_bw()\ng3\n\n\n\n\n\n\n\n\n\n\n\n\n# Add year!\nR_studies$year &lt;- as.numeric(str_match(R_studies$citekey,'[0-9]+'))\n\ng3 &lt;- ggplot(R_studies,aes(x=year,y=values,colour=model_class_id))+\n  geom_point(show.legend = T)+\n  facet_wrap(.~dimension)+\n  theme_bw()\ng3\n\nWarning: Removed 4 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\nRatio of obs./features or just a classification based on feature n (quintiles).\n\nR_studies$feature_n==\" 15; 23 in table, but 15 reported \"\n\n  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [97] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\nR_studies$feature_n[R_studies$feature_n==\" 15; 23 in table, but 15 reported  \"] &lt;- 15 #resolved from the paper\n\nR_studies$feature_n[R_studies$feature_n==\" 15; 23 in table, but 15 reported  \"] &lt;- 15 #resolved\nR_studies$feature_n[R_studies$feature_n==\" before_selection = 45, after_selection = 6  \"] &lt;- 45 #resolved\nR_studies$feature_n[R_studies$feature_n==\" model 1: 52, model 2 = 68, model 3 = 260, model 4 = 388 \"] &lt;- 388 #resolved\nR_studies$feature_n[R_studies$feature_n==\" variable, 557 before feature selection \"] &lt;- 557 #resolved\n\nR_studies$feature_n[R_studies$feature_n==\" 50 PCA features \"] &lt;- 50 #resolved\n#R_studies$feature_n[R_studies$feature_n==\"\"] &lt;- 0 #resolved\n#R_studies$feature_n[R_studies$feature_n==\"\"] &lt;- 0 #resolved\n\nR_studies$feature_n&lt;-as.numeric(R_studies$feature_n)\n\nWarning: NAs introduced by coercion\n\nprint(quantile(R_studies$feature_n,c(0.333,0.666),na.rm = T))\n\n33.3% 66.6% \n   18   260 \n\n#quantile(R_studies$feature_n,c(0.25,0.50,0.75),na.rm = T)\n\n# Assign\nR_studies$feature_n_complexity &lt;- cut(R_studies$feature_n,\n                                      breaks = c(0,\n                                                 as.numeric(quantile(R_studies$feature_n,c(0.333),na.rm = T)),\n                                                 as.numeric(quantile(R_studies$feature_n,c(0.666),na.rm = T)),\n                                                 1000),\n                                      labels = c(\"Feature n &lt; 18\",\"Feature n &gt; 18 & &lt; 260\",\"Feature n &gt; 260\"))\ntable(R_studies$feature_n_complexity)\n\n\n        Feature n &lt; 18 Feature n &gt; 18 & &lt; 260        Feature n &gt; 260 \n                    26                     24                     24 \n\n\n\n\n\nNeeds to be done from the unsummarised data (R_studies).\n\ntmp &lt;- drop_na(R_studies)\nlibrary(ggdist)\n\ntmp$dimension&lt;-str_to_title(tmp$dimension)\ntmp$model_class_id&lt;-factor(tmp$model_class_id,\n                           levels = c(\"Neural Nets\",\"Flexible Discriminants\", \"Kernel Smoothing, Additive and KNN\", \"Random Forests\",\"Linear Methods\"),\n                           labels = c(\"Neural\\nNets\",\"Flexible\\nDiscriminants\", \"KS\\n & KNN\", \"Random\\nForests\", \"Linear\\nMethods\"))\n\ng &lt;- ggplot(tmp,aes(x=model_class_id,y=values,color=citekey,label=citekey))+\n  stat_halfeye(aes(fill=citekey),point_interval=\"mean_qi\", trim=FALSE, expand=FALSE, show.legend = FALSE,adjust = 1.25, density=\"bounded\", point_size=3,scale = 1,alpha=0.5) + \n  geom_point(alpha=0.5,show.legend = F,position = position_jitter(width = .3))+\n  #geom_label_repel(size=2,max.overlaps = 50)+\n  facet_wrap(dimension~feature_n_complexity)+\n  ylab(\"Correlation Coefficient\")+\n  xlab(\"Model Technique\")+\n#  scale_y_continuous(limits = c(0,1),expand = c(0.01,0.01))+\n  geom_text_repel(aes(x = model_class_id, y = values, label = studyREF),\n             stat = \"summary\", fun = mean,show.legend = F)+\n  theme_bw()\ng\n\n\n\n\n\n\n\n#ggsave(filename = 'FeatureN_regression.pdf',g,height = 7,width = 11)\n\n\n\n\n\nTR &lt;- NULL\nTR$study_n &lt;- length(unique(R_studies$citekey))\nTR$model_n &lt;- nrow(R_studies)\nt&lt;-table(R_studies$model_class_id)\nt2 &lt;- paste0(names(t),': ', as.numeric(t))\nTR$model_types_n &lt;- str_c(t2,collapse = \"\\n\")\nTR$feature_Desc &lt;- paste0('Min=',min(R_studies$feature_n,na.rm = TRUE),', Md=',median(R_studies$feature_n,na.rm = TRUE),', Max=', max(R_studies$feature_n,na.rm = TRUE))\nTR$stimulus_Desc &lt;- paste0('Min=',min(R_studies$stimulus_n,na.rm = TRUE),', Md=',median(R_studies$stimulus_n,na.rm = TRUE),', Max=', max(R_studies$stimulus_n,na.rm = TRUE))\nprint(TR)\n\n$study_n\n[1] 14\n\n$model_n\n[1] 108\n\n$model_types_n\n[1] \"Flexible Discriminants: 24\\nKernel Smoothing, Additive and KNN: 10\\nLinear Methods: 50\\nNeural Nets: 14\\nRandom Forests: 10\"\n\n$feature_Desc\n[1] \"Min=15, Md=72, Max=557\"\n\n$stimulus_Desc\n[1] \"Min=40, Md=744, Max=2486\"\n\n\n\n\n\n\nwrite.csv(x = R_studies,file = 'R_studies.csv')\nwrite.csv(x = R_summary,file = 'R_summary.csv')"
  },
  {
    "objectID": "analysis/preprocessing.html#pull-data-to-analyse-classification-studies",
    "href": "analysis/preprocessing.html#pull-data-to-analyse-classification-studies",
    "title": "Preprocessing",
    "section": "",
    "text": "C_studies &lt;- dplyr::filter(metaMER_results,model_category=='classification' & str_detect(measure,'accuracy|r2|mcc')) #  \ndim(C_studies)\n\n[1] 303  19\n\n# eliminate measures that we don't need now\nC_studies &lt;- dplyr::filter(C_studies,values&gt;0)\nC_studies$statistic[is.na(C_studies$statistic)]&lt;-'Not defined'\nC_studies &lt;- dplyr::filter(C_studies,!str_detect(statistic,'pvalue|upper|lower'))\nC_studies &lt;- dplyr::filter(C_studies,str_detect(measure,'accuracy|mcc|r2'))\ndim(C_studies)\n\n[1] 174  19\n\ntable(C_studies$measure,C_studies$statistic)\n\n          \n           mean median Not defined null\n  accuracy   21      2          40   23\n  mcc         0      0          32    0\n  r2          0      0          56    0\n\ndim(C_studies)\n\n[1] 174  19\n\n# Conversion into mcc (==r), do we need\n\n\n\n\n#C_studies$dimension[str_detect(R_studies$dimension,'activation|energy arousal|tension arousal')]&lt;-'arousal'\n#R_studies$dimension[str_detect(R_studies$dimension,'pleasantness')]&lt;-'valence'\n#R_studies &lt;- dplyr::filter(R_studies,!str_detect(dimension,'av')) # relates to distances, can be omitted\n\n\ntable(C_studies$stimulus_n)  \n\n\n                                                                                                          not specified   \n                                                                                                                       48 \n                                                                                                                    124   \n                                                                                                                        8 \n                                                                                                                    171   \n                                                                                                                        9 \n                                                                                                                    1802  \n                                                                                                                       12 \n                                                                                                                    300   \n                                                                                                                        6 \n                                                                                                                    324   \n                                                                                                                       52 \n                                             429; 350 popular songs + 79 songs from the Beatles (Mirex 2009 collection)   \n                                                                                                                        2 \n 5192; 12 per user in user validation (not included here due to little information),   AcousticBrainz validation: 60000   \n                                                                                                                        5 \n                                                                                                                    744   \n                                                                                                                        4 \n                                                                              ISMIR2012: 2886, NJU_V1: 777, Hindi: 1037   \n                                                                                                                       18 \n                                                                                                    Specified elsewhere   \n                                                                                                                       10 \n\n# Deal with four studies involving multiple datasets: \nC_studies$stimulus_n[C_studies$stimulus_n==\" 429; 350 popular songs + 79 songs from the Beatles (Mirex 2009 collection)  \"] &lt;- 429 # resolved from the paper\nC_studies$stimulus_n[C_studies$stimulus_n==\" 5192; 12 per user in user validation (not included here due to little information),   AcousticBrainz validation: 60000  \"] &lt;- 5192 # resolved\nC_studies$stimulus_n[C_studies$stimulus_n==\" ISMIR2012: 2886, NJU_V1: 777, Hindi: 1037  \"] &lt;- 2886+777+1037 # decided to take this from validation\nC_studies&lt;-C_studies[!str_detect(C_studies$stimulus_n,'not specified'),]\nC_studies&lt;-C_studies[!str_detect(C_studies$stimulus_n,'Specified elsewhere'),]\nC_studies$stimulus_n &lt;- as.numeric(C_studies$stimulus_n)\n\n\n\n\n\nlibrary(ggplot2)\n\ng1&lt;-ggplot(R_studies,aes(x=values,fill=citekey,color=dimension))+\n  geom_histogram()+\n  facet_wrap(.~model_class_id)+\n  scale_color_manual(values = c('black','white'))+\n  theme_dark()+\n  scale_x_continuous(breaks = seq(0,1,by=.1))\n\ng1\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 4 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\nR_studies$citekey &lt;- factor(R_studies$citekey)\nR_studies$dimension &lt;- factor(R_studies$dimension)\n\nR_summary &lt;- summarise(group_by(R_studies,dimension,citekey),valuesMean=mean(values,na.rm=T),valuesMedian=median(values,na.rm=T),valuesMax=max(values,na.rm=T),stimulus_n=first(stimulus_n),studyREF=first(studyREF),model_class_id=first(model_class_id))\n\n`summarise()` has grouped output by 'dimension'. You can override using the\n`.groups` argument.\n\n\n\n\n\nAdd variation from within the studies (alternative models)\n\nR_summary_split &lt;- pivot_wider(R_summary,id_cols = citekey, names_from = c(dimension), values_from = valuesMax)\n\ng2 &lt;- ggplot(R_summary_split,aes(x=valence,y=arousal,label=citekey))+\n  geom_label()+\n  theme_bw()+\n  scale_x_continuous(breaks = seq(0,1,by=.1))+\n  scale_y_continuous(breaks = seq(0,1,by=.1))\n\ng2\n\n\n\n\n\n\n\n## could be more informative when done with the full data\nR_studies$citekey&lt;-factor(R_studies$citekey)\nR_studies$dimension&lt;-factor(R_studies$dimension)\n\nR_studies_split &lt;- pivot_wider(R_studies,id_cols = c(unique_id,citekey,model_class_id), names_from = c(dimension), values_from = c(values),values_fn = mean)\nR_studies_split&lt;-drop_na(R_studies_split)\n\nlibrary(ggrepel)\n\ng3 &lt;- ggplot(R_studies_split,aes(x=valence,y=arousal,label=citekey,color=model_class_id,fill=model_class_id))+\n  geom_point(size=4)+\n  geom_label_repel(size=3, max.overlaps=50,show.legend = T,color='white')+\n  scale_x_continuous(breaks = seq(0,1,by=.25),limits = c(0,1))+\n  scale_y_continuous(breaks = seq(0,1,by=.25),limits = c(0,1))+\n  theme_bw()\ng3\n\n\n\n\n\n\n\n\n\n\n\n\n# Add year!\nR_studies$year &lt;- as.numeric(str_match(R_studies$citekey,'[0-9]+'))\n\ng3 &lt;- ggplot(R_studies,aes(x=year,y=values,colour=model_class_id))+\n  geom_point(show.legend = T)+\n  facet_wrap(.~dimension)+\n  theme_bw()\ng3\n\nWarning: Removed 4 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\nRatio of obs./features or just a classification based on feature n (quintiles).\n\nR_studies$feature_n==\" 15; 23 in table, but 15 reported \"\n\n  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE    NA    NA\n [13]    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA FALSE FALSE\n [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [49] FALSE FALSE FALSE FALSE FALSE FALSE    NA    NA    NA    NA FALSE FALSE\n [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [85] FALSE FALSE    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA\n [97]    NA    NA    NA    NA    NA    NA    NA    NA FALSE FALSE FALSE FALSE\n\nR_studies$feature_n[R_studies$feature_n==\" 15; 23 in table, but 15 reported  \"] &lt;- 15 #resolved from the paper\n\nR_studies$feature_n[R_studies$feature_n==\" 15; 23 in table, but 15 reported  \"] &lt;- 15 #resolved\nR_studies$feature_n[R_studies$feature_n==\" before_selection = 45, after_selection = 6  \"] &lt;- 45 #resolved\nR_studies$feature_n[R_studies$feature_n==\" model 1: 52, model 2 = 68, model 3 = 260, model 4 = 388 \"] &lt;- 388 #resolved\nR_studies$feature_n[R_studies$feature_n==\" variable, 557 before feature selection \"] &lt;- 557 #resolved\n\nR_studies$feature_n[R_studies$feature_n==\" 50 PCA features \"] &lt;- 50 #resolved\n#R_studies$feature_n[R_studies$feature_n==\"\"] &lt;- 0 #resolved\n#R_studies$feature_n[R_studies$feature_n==\"\"] &lt;- 0 #resolved\n\nR_studies$feature_n&lt;-as.numeric(R_studies$feature_n)\n\nprint(quantile(R_studies$feature_n,c(0.333,0.666),na.rm = T))\n\n33.3% 66.6% \n   18   260 \n\n#quantile(R_studies$feature_n,c(0.25,0.50,0.75),na.rm = T)\n\n# Assign\nR_studies$feature_n_complexity &lt;- cut(R_studies$feature_n,\n                                      breaks = c(0,\n                                                 as.numeric(quantile(R_studies$feature_n,c(0.333),na.rm = T)),\n                                                 as.numeric(quantile(R_studies$feature_n,c(0.666),na.rm = T)),\n                                                 1000),\n                                      labels = c(\"Feature n &lt; 18\",\"Feature n &gt; 18 & &lt; 260\",\"Feature n &gt; 260\"))\ntable(R_studies$feature_n_complexity)\n\n\n        Feature n &lt; 18 Feature n &gt; 18 & &lt; 260        Feature n &gt; 260 \n                    26                     24                     24 \n\n\n\n\n\nNeeds to be done from the unsummarised data (R_studies).\n\ntmp &lt;- drop_na(R_studies)\nlibrary(ggdist)\n\ntmp$dimension&lt;-str_to_title(tmp$dimension)\ntmp$model_class_id&lt;-factor(tmp$model_class_id,\n                           levels = c(\"Neural Nets\",\"Flexible Discriminants\", \"Kernel Smoothing, Additive and KNN\", \"Random Forests\",\"Linear Methods\"),\n                           labels = c(\"Neural\\nNets\",\"Flexible\\nDiscriminants\", \"KS\\n & KNN\", \"Random\\nForests\", \"Linear\\nMethods\"))\n\ng &lt;- ggplot(tmp,aes(x=model_class_id,y=values,color=citekey,label=citekey))+\n  stat_halfeye(aes(fill=citekey),point_interval=\"mean_qi\", trim=FALSE, expand=FALSE, show.legend = FALSE,adjust = 1.25, density=\"bounded\", point_size=3,scale = 1,alpha=0.5) + \n  geom_point(alpha=0.5,show.legend = F,position = position_jitter(width = .3))+\n  #geom_label_repel(size=2,max.overlaps = 50)+\n  facet_wrap(dimension~feature_n_complexity)+\n  ylab(\"Correlation Coefficient\")+\n  xlab(\"Model Technique\")+\n#  scale_y_continuous(limits = c(0,1),expand = c(0.01,0.01))+\n  geom_text_repel(aes(x = model_class_id, y = values, label = studyREF),\n             stat = \"summary\", fun = mean,show.legend = F)+\n  theme_bw()\ng\n\n\n\n\n\n\n\n#ggsave(filename = 'FeatureN_regression.pdf',g,height = 7,width = 11)\n\n\n\n\n\nTR &lt;- NULL\nTR$study_n &lt;- length(unique(R_studies$citekey))\nTR$model_n &lt;- nrow(R_studies)\nt&lt;-table(R_studies$model_class_id)\nt2 &lt;- paste0(names(t),': ', as.numeric(t))\nTR$model_types_n &lt;- str_c(t2,collapse = \"\\n\")\nTR$feature_Desc &lt;- paste0('Min=',min(R_studies$feature_n,na.rm = TRUE),', Md=',median(R_studies$feature_n,na.rm = TRUE),', Max=', max(R_studies$feature_n,na.rm = TRUE))\nTR$stimulus_Desc &lt;- paste0('Min=',min(R_studies$stimulus_n,na.rm = TRUE),', Md=',median(R_studies$stimulus_n,na.rm = TRUE),', Max=', max(R_studies$stimulus_n,na.rm = TRUE))\nprint(TR)\n\n$study_n\n[1] 14\n\n$model_n\n[1] 108\n\n$model_types_n\n[1] \"Flexible Discriminants: 24\\nKernel Smoothing, Additive and KNN: 10\\nLinear Methods: 50\\nNeural Nets: 14\\nRandom Forests: 10\"\n\n$feature_Desc\n[1] \"Min=15, Md=72, Max=557\"\n\n$stimulus_Desc\n[1] \"Min=40, Md=744, Max=2486\"\n\n\n\n\n\n\nwrite.csv(x = R_studies,file = 'R_studies.csv')\nwrite.csv(x = R_summary,file = 'R_summary.csv')"
  },
  {
    "objectID": "manuscript/manuscript.html#prediction-success-for-valence-and-arousal-or-affect-dimensions",
    "href": "manuscript/manuscript.html#prediction-success-for-valence-and-arousal-or-affect-dimensions",
    "title": "Meta-analysis of regression and classification success of emotion ratings from audio",
    "section": "Prediction success for valence and arousal (or affect dimensions)",
    "text": "Prediction success for valence and arousal (or affect dimensions)\nSee analysis/analysis.qmd\nSince there are many models contained within the studies, we will report the results in a two parts; We first give an overview of the results for all models, and then we focus on the best performing models of each study. The focus on best performing model is carried out by taking the model with the highest correlation coefficient and using this model for the study. This is done to avoid the issue of multiple models from the same study deflating the results as majority of the models included are relative modest baseline models and alternative techniques that do not represent the novelty and content of the article. Table 2 summarises the results for all models (All) as well as best performing models (Max) for each study. The summary includes the number of models and observations, the correlation coefficient and its 95% confidence interval, the t-value and p-value for the correlation, the heterogeneity statistics \\(\\tau^2\\) and \\(I^2\\), calculated through appropriate transformations (Fisher’s Z) for the correlation coefficient, and a random-effects model using meta library (Balduzzi et al., 2019).\nTable 2. Meta-analytic diagnostic for all regression studies predicting valence from audio.\n\n\n\n\n\n\n\n\n\n\n\n\nConcept\nModels, obs\n\\(r\\) [95%-CI]\n\\(t\\)\n\\(p\\)\n\\(\\tau^2\\)\n\\(I^2\\)\n\n\n\n\nValence All\n\n0.614 [0.554; 0.667]\n15.9\n.0001\n0.092\n98.1%\n\n\nValence Trim\n\n0.605 [0.592; 0.617]\n72.4\n.0001\n0.0007\n33.1%\n\n\nValence M\n\n0.604 [0.467; 0.712]\n7.83\n.0001\n0.100\n97.8%\n\n\nValence Md\n\n0.602 [0.466; 0.710]\n7.87\n.0001\n0.098\n97.7%\n\n\nValence Max\n24,15660\n0.659 [0.557; 0.740]\n7.44\n.0001\n0.142\n97.9%\n\n\nVal. MaxTrim\n13,7363\n0.649 [0.598; 0.695]\n18.5\n.0001\n0.008\n88.5%\n\n\nN Features\n\n\n\n\n\n\n\n\n&lt;18 F\n\n0.xxxx [0.xxxx; 0.xxxx]\nxxxx\n.xxxx\n0.xxx\nXXXX%\n\n\n18-260 F\n\n0.xxxx [0.xxxx; 0.xxxx]\nxxxx\n.xxxx\n0.xxx\nXXXX%\n\n\n260+ F\n\n0.xxxx [0.xxxx; 0.xxxx]\nxxxx\n.xxxx\n0.xxx\nXXXX%\n\n\nTechniques\n\n\n\n\n\n\n\n\nKS\n\n0.3606 [0.3201; 0.3997]\nxxxx\n.xxxx\n0.xxx\nXXXX%\n\n\nLM\n\n0.xxxx [0.xxxx; 0.xxxx]\nxxxx\n.xxxx\n0.xxx\nXXXX%\n\n\nFD\n\n0.xxxx [0.xxxx; 0.xxxx]\nxxxx\n.xxxx\n0.xxx\nXXXX%\n\n\nNN\n\n0.xxxx [0.xxxx; 0.xxxx]\nxxxx\n.xxxx\n0.xxx\nXXXX%\n\n\nRF\n\n0.xxxx [0.xxxx; 0.xxxx]\nxxxx\n.xxxx\n0.xxx\nXXXX%\n\n\n\nOTHER GROUPINGS? STIMULUS MIXED/SINGLE GENRE, PREDICTION/EXPLANATION,\nsee this to ignore I^2 and rely on prediction interval: https://onlinelibrary.wiley.com/doi/full/10.1002/jrsm.1678\nFigure 1. Forest plot of valence prediction (Max?) analysis/figure1.qmd\nSummarise the results here briefly\nMoving on the arousal, …\nTable 3. Meta-analytic diagnostic for all regression studies predicting arousal from audio.\n\n\n\n\n\n\n\n\n\n\n\n\nConcept\nModels, obs\n\\(r\\) [95%-CI]\n\\(t\\)\n\\(p\\)\n\\(\\tau^2\\)\n\\(I^2\\)\n\n\n\n\nArousal\n\n0.7959 [0.7666; 0.8218]\n29.0\n0.0001\n0.0676\n95.6%\n\n\nArousal Trim\n\n0.7932 [0.7846; 0.8014]\n96.5\n0.0001\n0.0030\n73.3%\n\n\nArousal M\n\n0.7567 [0.6752; 0.8199]\n12.7\n0.0001\n0.0739\n95.6%\n\n\nArousal Md\n\n0.7627 [0.6819; 0.8252]\n12.7\n0.0001\n0.0757\n95.3%\n\n\nArousal Max\n24,15660\n0.8070 [0.7453; 0.8550]\n10.3\n0.0001\n0.155\n96.8%\n\n\nAro.Max.Trm\n14,12061\n0.8182 [0.8021; 0.8331]\n25.6\n0.0001\n0.0132\n96.8%\n\n\nN Features\n\n\n\n\n\n\n\n\n&lt;18 F\n\n\n\n\n\n\n\n\n18-260 F\n\n\n\n\n\n\n\n\n260+ F\n\n\n\n\n\n\n\n\nTechniques\n\n\n\n\n\n\n\n\nKS\n\n\n\n\n\n\n\n\nLM\n\n\n\n\n\n\n\n\nFD\n\n\n\n\n\n\n\n\nNN\n\n\n\n\n\n\n\n\nRF\n\n\n\n\n\n\n\n\n\nFigure 2. Forest plot of arousal prediction (using Max?)\nSummarise here the pattern of results"
  },
  {
    "objectID": "manuscript/manuscript.html#classification-studies",
    "href": "manuscript/manuscript.html#classification-studies",
    "title": "Meta-analysis of regression and classification success of emotion ratings from audio",
    "section": "Classification studies",
    "text": "Classification studies\nSummary of details contained in Table 1, but summarise at least the categories predicted before moving onto the main findings.\nTable 4. Meta-analytic diagnostic for all classification studies predicting emotion categories from audio.\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nModels, obs\n\\(r\\) [95%-CI]\n\\(t\\)\n\\(p\\)\n\\(\\tau^2\\)\n\\(I^2\\)\n\n\n\n\nAll\n89,87347\n0.8074 [0.7681; 0.8407]\n21.4\n0.0001\n0.2415\n99.7%\n\n\nAll Trim\n29,6499\n0.8185 [0.8046; 0.8314]\n58.3\n0.0001\n0.0066\n60.4%\n\n\nMax\n14,17184\n0.8564 [0.7386; 0.9234]\n8.32\n0.0001\n0.329\n99.8%\n\n\nMax Trim\n6,3653\n0.8689 [0.7760; 0.9249]\n11.6\n0.0001\n0.0749\n97.5%\n\n\n\nHeterogeneity issues\nFigure 3. Forest plot of arousal prediction (Max?) (Unless we do some custom plotting)\n\nFigure Optional: Funnel plot (I haven’t seen this yet)"
  },
  {
    "objectID": "manuscript/manuscript.html#concise-summary-of-what-we-did-and-found",
    "href": "manuscript/manuscript.html#concise-summary-of-what-we-did-and-found",
    "title": "Meta-analysis of regression and classification success of emotion ratings from audio",
    "section": "Concise summary of what we did and found",
    "text": "Concise summary of what we did and found"
  },
  {
    "objectID": "manuscript/manuscript.html#main-outcomes",
    "href": "manuscript/manuscript.html#main-outcomes",
    "title": "Meta-analysis of regression and classification success of emotion ratings from audio",
    "section": "Main outcomes",
    "text": "Main outcomes\n\nArousal is easier to predict (r = 0.7627) than valence (r = 0.6236), as we predicted. The glass ceiling seems to be at …\nClassification …\nModel accuracy is surprisingly little affected by the number of features (?) or modelling technique (?).\nSome of the complex state-of-the-art techniques (e.g., NNs) do not deliver impressive improvements over older techniques (e.g., SVR, RF)\nVariation in study/model/data quality is large and can be seen in heterogenuity and the amount of studies eliminated"
  },
  {
    "objectID": "manuscript/manuscript.html#calls-for-actionpoints-to-improve-in-such-studies",
    "href": "manuscript/manuscript.html#calls-for-actionpoints-to-improve-in-such-studies",
    "title": "Meta-analysis of regression and classification success of emotion ratings from audio",
    "section": "Calls for action/points to improve in such studies",
    "text": "Calls for action/points to improve in such studies\n\nDocumentation the details in full (features, stimuli, model details, cross-validation)\nQuality of the underlying data (emotion ratings, classes, or even stimulus properties?\nGeneralisibility of the models (some studies such as X and Y address this by applying the models across several datasets)\nDiversity in the evaluative aspects of studies: overfitting, numerous ways of cross-validating, not sharing data or analysis scripts, not reporting in the same way\nWhat proportion of stimuli are Western music, and what genres tend to dominate?\n\n\nFunding statement\nCA was funded by Mitacs Globalink Research Award (Mitacs & British High Commission - Ottawa, Canada).\n\n\nCompeting interests statement\nThere were no competing interests.\n\n\nOpen practices statement\nStudy preregistration, data, analysis scripts and supporting information is available at Github, https://tuomaseerola.github.io/metaMER.\n\n\nAcknowledgements\nWe thank Greggs food-on-the-go retailer for sustaining the work with affordable sandwiches and coffee."
  },
  {
    "objectID": "analysis/preprocessing.html#homogenise-the-outcome-variable-names-to-valence-and-arousal",
    "href": "analysis/preprocessing.html#homogenise-the-outcome-variable-names-to-valence-and-arousal",
    "title": "Preprocessing",
    "section": "Homogenise the outcome variable names to valence and arousal",
    "text": "Homogenise the outcome variable names to valence and arousal\n\nR_studies$dimension[str_detect(R_studies$dimension,'activation|energy arousal|tension arousal')]&lt;-'arousal'\nR_studies$dimension[str_detect(R_studies$dimension,'pleasantness')]&lt;-'valence'\nR_studies &lt;- dplyr::filter(R_studies,!str_detect(dimension,'av|resonance')) # relates to distances, can be omitted\n\ntable(R_studies$dimension)  \n\n\narousal valence \n    176     170 \n\ntable(R_studies$stimulus_n)  \n\n\n 1020; MediaEval2014: 1000: music perception database 1: 6, music perception database 2: 9, music perception database 3: 8, music perception database 4: 7    \n                                                                                                                                                            4 \n                                                                                                                                                         146  \n                                                                                                                                                           12 \n                                                                                                                                                       1838   \n                                                                                                                                                           10 \n                                                                                                                 2372 (subset of PSIC3839, total n: 3839)     \n                                                                                                                                                            2 \n                                                                                                                                                        275   \n                                                                                                                                                            6 \n                                                                                                                                                        324   \n                                                                                                                                                           74 \n                                                                                                                                                         336  \n                                                                                                                                                            2 \n                                                                                                                                                         40   \n                                                                                                                                                            2 \n                                                                                                                                                         420  \n                                                                                                                                                            2 \n                                                                                                                                                         431  \n                                                                                                                                                            4 \n                                                                                                                                                          48  \n                                                                                                                                                            6 \n                                                                                                                                                         54   \n                                                                                                                                                            4 \n                                                                                                                                                         744  \n                                                                                                                                                           30 \n                                                                                                                                                        744   \n                                                                                                                                                           30 \n                                                                                                                                                       744    \n                                                                                                                                                           18 \n                                                                                                                                                         84   \n                                                                                                                                                           24 \n                                                                                                                                      DEAM: 744, PMEmo: 206   \n                                                                                                                                                            4 \n                                                                                                               emoMusic: 744, soundtracks: 360, chinese: 500  \n                                                                                                                                                            8 \n                                                                                                                       MER60: 60, CH818: 818, AMG1608: 1608   \n                                                                                                                                                           96 \n                                                                                                                                    study 1: 100; study 2: 20 \n                                                                                                                                                            4 \n                                                                                                                 study 1: 20; study 2: 40) % three outliers   \n                                                                                                                                                            4 \n\n# Deal with four studies involving multiple datasets: \nR_studies$stimulus_n[R_studies$stimulus_n==\" emoMusic: 744, soundtracks: 360, chinese: 500 \"] &lt;- 938 # resolved from the paper\nR_studies$stimulus_n[R_studies$stimulus_n==\" 2372 (subset of PSIC3839, total n: 3839)    \"] &lt;- 2372 # resolved\nR_studies$stimulus_n[R_studies$stimulus_n==\" study 1: 20; study 2: 40) % three outliers  \"] &lt;- 40 # decided to take this from validation\n# REDO with a clearer function\neliminate &lt;- str_detect(R_studies$unique_id,\"hu2017cr\") & !str_detect(R_studies$unique_id,\"all\")\nR_studies &lt;- R_studies[!eliminate,]\nR_studies$stimulus_n[R_studies$stimulus_n==\" MER60: 60, CH818: 818, AMG1608: 1608  \"] &lt;- 60+818+1608 #\nR_studies$stimulus_n &lt;- as.numeric(R_studies$stimulus_n)\n\nWarning: NAs introduced by coercion"
  },
  {
    "objectID": "analysis/preprocessing.html#diagnostics",
    "href": "analysis/preprocessing.html#diagnostics",
    "title": "Preprocessing",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.1\n\ng1&lt;-ggplot(R_studies,aes(x=values,fill=citekey,color=dimension))+\n  geom_histogram()+\n  facet_wrap(.~model_class_id)+\n  scale_color_manual(values = c('black','white'))+\n  theme_dark()+\n  scale_x_continuous(breaks = seq(0,1,by=.1))\n\ng1\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 12 rows containing non-finite outside the scale range\n(`stat_bin()`)."
  },
  {
    "objectID": "analysis/preprocessing.html#select-a-summary-measure-for-valence-and-arousal-separately",
    "href": "analysis/preprocessing.html#select-a-summary-measure-for-valence-and-arousal-separately",
    "title": "Preprocessing",
    "section": "Select a summary measure for valence and arousal separately",
    "text": "Select a summary measure for valence and arousal separately\n\nlibrary(tidyverse)\n\nWarning: package 'tidyr' was built under R version 4.3.1\n\n\nWarning: package 'readr' was built under R version 4.3.1\n\n\nWarning: package 'dplyr' was built under R version 4.3.1\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nR_studies$citekey &lt;- factor(R_studies$citekey)\nR_studies$dimension &lt;- factor(R_studies$dimension)\n\nR_summary &lt;- summarise(group_by(R_studies,dimension,citekey),valuesMean=mean(values,na.rm=TRUE),valuesMedian=median(values,na.rm=TRUE),valuesMax=max(values,na.rm=TRUE),stimulus_n=first(stimulus_n),studyREF=first(studyREF),model_class_id=first(model_class_id),journal_type=first(journal_type),stimulus_genre_mixed=first(stimulus_genre_mixed))\n\n`summarise()` has grouped output by 'dimension'. You can override using the\n`.groups` argument."
  },
  {
    "objectID": "analysis/preprocessing.html#visualise-summary-on-two-dimensions",
    "href": "analysis/preprocessing.html#visualise-summary-on-two-dimensions",
    "title": "Preprocessing",
    "section": "Visualise Summary on two dimensions",
    "text": "Visualise Summary on two dimensions\nAdd variation from within the studies (alternative models)\n\nR_summary_split &lt;- pivot_wider(R_summary,id_cols = citekey, names_from = c(dimension), values_from = valuesMax)\n\ng2 &lt;- ggplot(R_summary_split,aes(x=valence,y=arousal,label=citekey))+\n  geom_label()+\n  theme_bw()+\n  scale_x_continuous(breaks = seq(0,1,by=.1))+\n  scale_y_continuous(breaks = seq(0,1,by=.1))\n\ng2\n\n\n\n\n\n\n\n## could be more informative when done with the full data\nR_studies$citekey&lt;-factor(R_studies$citekey)\nR_studies$dimension&lt;-factor(R_studies$dimension)\n\nR_studies_split &lt;- pivot_wider(R_studies,id_cols = c(unique_id,citekey,model_class_id), names_from = c(dimension), values_from = c(values),values_fn = mean)\nR_studies_split&lt;-drop_na(R_studies_split)\n\nlibrary(ggrepel)\n\nWarning: package 'ggrepel' was built under R version 4.3.1\n\ng3 &lt;- ggplot(R_studies_split,aes(x=valence,y=arousal,label=citekey,color=model_class_id,fill=model_class_id))+\n  geom_point(size=4)+\n  geom_label_repel(size=3, max.overlaps=50,show.legend = T,color='white')+\n  scale_x_continuous(breaks = seq(0,1,by=.25),limits = c(0,1))+\n  scale_y_continuous(breaks = seq(0,1,by=.25),limits = c(0,1))+\n  theme_bw()\ng3\n\nWarning: ggrepel: 68 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps"
  },
  {
    "objectID": "analysis/preprocessing.html#plot-success-across-the-years",
    "href": "analysis/preprocessing.html#plot-success-across-the-years",
    "title": "Preprocessing",
    "section": "Plot success across the years",
    "text": "Plot success across the years\n\n# Add year!\nR_studies$year &lt;- as.numeric(str_match(R_studies$citekey,'[0-9]+'))\n\ng3 &lt;- ggplot(R_studies,aes(x=year,y=values,colour=model_class_id))+\n  geom_point(show.legend = T)+\n  facet_wrap(.~dimension)+\n  theme_bw()\ng3\n\nWarning: Removed 12 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "analysis/preprocessing.html#simple-model-complexity-metric",
    "href": "analysis/preprocessing.html#simple-model-complexity-metric",
    "title": "Preprocessing",
    "section": "Simple model complexity metric",
    "text": "Simple model complexity metric\nRatio of obs./features or just a classification based on feature n (quintiles).\n\nR_studies$feature_n==\" 15; 23 in table, but 15 reported \"\n\n  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [97] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[109] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[121] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[133] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[145] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[157] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[169] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[181] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[193] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[205] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[217] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[229] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[241] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[253] FALSE FALSE FALSE FALSE FALSE FALSE\n\nR_studies$feature_n[R_studies$feature_n==\" 15; 23 in table, but 15 reported  \"] &lt;- 15 #resolved from the paper\n\nR_studies$feature_n[R_studies$feature_n==\" 15; 23 in table, but 15 reported  \"] &lt;- 15 #resolved\nR_studies$feature_n[R_studies$feature_n==\" before_selection = 45, after_selection = 6  \"] &lt;- 45 #resolved\nR_studies$feature_n[R_studies$feature_n==\" model 1: 52, model 2 = 68, model 3 = 260, model 4 = 388 \"] &lt;- 388 #resolved\nR_studies$feature_n[R_studies$feature_n==\" variable, 557 before feature selection \"] &lt;- 557 #resolved\n\nR_studies$feature_n[R_studies$feature_n==\" 50 PCA features \"] &lt;- 50 #resolved\n#R_studies$feature_n[R_studies$feature_n==\"\"] &lt;- 0 #resolved\n#R_studies$feature_n[R_studies$feature_n==\"\"] &lt;- 0 #resolved\n\nR_studies$feature_n&lt;-as.numeric(R_studies$feature_n)\n\nWarning: NAs introduced by coercion\n\nprint(quantile(R_studies$feature_n,c(0.333,0.666),na.rm = T))\n\n33.3% 66.6% \n  130   653 \n\n#quantile(R_studies$feature_n,c(0.25,0.50,0.75),na.rm = T)\n\n# Assign\nR_studies$feature_n_complexity &lt;- cut(R_studies$feature_n,\n                                      breaks = c(0,\n                                                 as.numeric(quantile(R_studies$feature_n,c(0.333),na.rm = T)),\n                                                 as.numeric(quantile(R_studies$feature_n,c(0.666),na.rm = T)),\n                                                 1000),\n                                      labels = c(\"Feature n &lt; 18\",\"Feature n &gt; 18 & &lt; 260\",\"Feature n &gt; 260\"))\ntable(R_studies$feature_n_complexity)\n\n\n        Feature n &lt; 18 Feature n &gt; 18 & &lt; 260        Feature n &gt; 260 \n                    56                     86                     22"
  },
  {
    "objectID": "analysis/preprocessing.html#explore-feature_n_complexity-and-model-success",
    "href": "analysis/preprocessing.html#explore-feature_n_complexity-and-model-success",
    "title": "Preprocessing",
    "section": "Explore feature_n_complexity and model success",
    "text": "Explore feature_n_complexity and model success\nNeeds to be done from the unsummarised data (R_studies).\n\ntmp &lt;- drop_na(R_studies)\nlibrary(ggdist)\n\ntmp$dimension&lt;-str_to_title(tmp$dimension)\ntmp$model_class_id&lt;-factor(tmp$model_class_id,\n                           levels = c(\"Neural Nets\",\"Flexible Discriminants\", \"Kernel Smoothing, Additive and KNN\", \"Random Forests\",\"Linear Methods\"),\n                           labels = c(\"Neural\\nNets\",\"Flexible\\nDiscriminants\", \"KS\\n & KNN\", \"Random\\nForests\", \"Linear\\nMethods\"))\n\ng &lt;- ggplot(tmp,aes(x=model_class_id,y=values,color=citekey,label=citekey))+\n  stat_halfeye(aes(fill=citekey),point_interval=\"mean_qi\", trim=FALSE, expand=FALSE, show.legend = FALSE,adjust = 1.25, density=\"bounded\", point_size=3,scale = 1,alpha=0.5) + \n  geom_point(alpha=0.5,show.legend = F,position = position_jitter(width = .3))+\n  #geom_label_repel(size=2,max.overlaps = 50)+\n  facet_wrap(dimension~feature_n_complexity)+\n  ylab(\"Correlation Coefficient\")+\n  xlab(\"Model Technique\")+\n#  scale_y_continuous(limits = c(0,1),expand = c(0.01,0.01))+\n  geom_text_repel(aes(x = model_class_id, y = values, label = studyREF),\n             stat = \"summary\", fun = mean,show.legend = F)+\n  theme_bw()\ng\n\nWarning: ggrepel: 4 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\n\n\n\n#ggsave(filename = 'FeatureN_regression.pdf',g,height = 7,width = 11)"
  },
  {
    "objectID": "analysis/preprocessing.html#create-descriptive-table-for-the-manuscript",
    "href": "analysis/preprocessing.html#create-descriptive-table-for-the-manuscript",
    "title": "Preprocessing",
    "section": "Create descriptive table for the manuscript",
    "text": "Create descriptive table for the manuscript\n\nTR &lt;- NULL\nTR$study_n &lt;- length(unique(R_studies$citekey))\nTR$model_n &lt;- nrow(R_studies)\nt&lt;-table(R_studies$model_class_id)\nt2 &lt;- paste0(names(t),': ', as.numeric(t))\nTR$model_types_n &lt;- str_c(t2,collapse = \"\\n\")\nTR$feature_Desc &lt;- paste0('Min=',min(R_studies$feature_n,na.rm = TRUE),', Md=',median(R_studies$feature_n,na.rm = TRUE),', Max=', max(R_studies$feature_n,na.rm = TRUE))\nTR$stimulus_Desc &lt;- paste0('Min=',min(R_studies$stimulus_n,na.rm = TRUE),', Md=',median(R_studies$stimulus_n,na.rm = TRUE),', Max=', max(R_studies$stimulus_n,na.rm = TRUE))\nprint(TR)\n\n$study_n\n[1] 24\n\n$model_n\n[1] 258\n\n$model_types_n\n[1] \"Flexible Discriminants: 64\\nKernel Smoothing, Additive and KNN: 24\\nLinear Methods: 74\\nNeural Nets: 74\\nRandom Forests: 22\"\n\n$feature_Desc\n[1] \"Min=3, Md=472.5, Max=654\"\n\n$stimulus_Desc\n[1] \"Min=40, Md=324, Max=2486\""
  },
  {
    "objectID": "analysis/preprocessing.html#homogenise-the-outcome-variable-names-to-valence-and-arousal-1",
    "href": "analysis/preprocessing.html#homogenise-the-outcome-variable-names-to-valence-and-arousal-1",
    "title": "Preprocessing",
    "section": "Homogenise the outcome variable names to valence and arousal",
    "text": "Homogenise the outcome variable names to valence and arousal\n\ntable(C_studies$stimulus_n)  \n\n\n                                                                                                                    387   \n                                                                                                                       48 \n                                                                                                                    124   \n                                                                                                                        8 \n                                                                                                                    171   \n                                                                                                                       19 \n                                                                                                                    1802  \n                                                                                                                       12 \n                                                                                                                    300   \n                                                                                                                        6 \n                                                                                                                    324   \n                                                                                                                       52 \n                                             429; 350 popular songs + 79 songs from the Beatles (Mirex 2009 collection)   \n                                                                                                                        2 \n 5192; 12 per user in user validation (not included here due to little information),   AcousticBrainz validation: 60000   \n                                                                                                                        5 \n                                                                                                                    744   \n                                                                                                                        4 \n                                                                              ISMIR2012: 2886, NJU_V1: 777, Hindi: 1037   \n                                                                                                                       18 \n\n# Deal with four studies involving multiple datasets: \nC_studies$stimulus_n[C_studies$stimulus_n==\" 429; 350 popular songs + 79 songs from the Beatles (Mirex 2009 collection)  \"] &lt;- 429 # resolved from the paper\nC_studies$stimulus_n[C_studies$stimulus_n==\" 5192; 12 per user in user validation (not included here due to little information),   AcousticBrainz validation: 60000  \"] &lt;- 5192 # resolved\nC_studies$stimulus_n[C_studies$stimulus_n==\" ISMIR2012: 2886, NJU_V1: 777, Hindi: 1037  \"] &lt;- 2886+777+1037 # decided to take this from validation\nC_studies&lt;-C_studies[!str_detect(C_studies$stimulus_n,'not specified'),]\nC_studies&lt;-C_studies[!str_detect(C_studies$stimulus_n,'Specified elsewhere'),]\nC_studies$stimulus_n &lt;- as.numeric(C_studies$stimulus_n)"
  },
  {
    "objectID": "analysis/preprocessing.html#diagnostics-1",
    "href": "analysis/preprocessing.html#diagnostics-1",
    "title": "Preprocessing",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nlibrary(ggplot2)\n\ng1&lt;-ggplot(C_studies,aes(x=values,fill=citekey,color=dimension))+\n  geom_histogram()+\n  facet_wrap(.~model_class_id)+\n  theme_dark()+\n  scale_x_continuous(breaks = seq(0,1,by=.1))\n\ng1"
  },
  {
    "objectID": "analysis/preprocessing.html#select-a-summary-measure-for-valence-and-arousal-separately-1",
    "href": "analysis/preprocessing.html#select-a-summary-measure-for-valence-and-arousal-separately-1",
    "title": "Preprocessing",
    "section": "Select a summary measure for valence and arousal separately",
    "text": "Select a summary measure for valence and arousal separately\n\nAsk Cameron whether it is OK to treat dimension contents as they are all classification studies (the exceptions are av and valence and arousal).\n\n\nlibrary(tidyverse)\nC_studies$citekey &lt;- factor(C_studies$citekey)\n\nC_summary &lt;- summarise(group_by(C_studies,citekey),valuesMean=mean(values,na.rm=TRUE),valuesMedian=median(values,na.rm=TRUE),valuesMax=max(values,na.rm=TRUE),stimulus_n=first(stimulus_n),studyREF=first(studyREF),model_class_id=first(model_class_id), stimulus_genre_mixed=first(stimulus_genre_mixed),journal_type = first(journal_type))"
  },
  {
    "objectID": "analysis/preprocessing.html#visualise-summary-on-two-dimensions-1",
    "href": "analysis/preprocessing.html#visualise-summary-on-two-dimensions-1",
    "title": "Preprocessing",
    "section": "Visualise Summary on two dimensions",
    "text": "Visualise Summary on two dimensions\nAdd variation from within the studies (alternative models)\n\nR_summary_split &lt;- pivot_wider(R_summary,id_cols = citekey, names_from = c(dimension), values_from = valuesMax)\n\ng2 &lt;- ggplot(R_summary_split,aes(x=valence,y=arousal,label=citekey))+\n  geom_label()+\n  theme_bw()+\n  scale_x_continuous(breaks = seq(0,1,by=.1))+\n  scale_y_continuous(breaks = seq(0,1,by=.1))\n\ng2\n\n\n\n\n\n\n\n## could be more informative when done with the full data\nR_studies$citekey&lt;-factor(R_studies$citekey)\nR_studies$dimension&lt;-factor(R_studies$dimension)\n\nR_studies_split &lt;- pivot_wider(R_studies,id_cols = c(unique_id,citekey,model_class_id), names_from = c(dimension), values_from = c(values),values_fn = mean)\nR_studies_split&lt;-drop_na(R_studies_split)\n\nlibrary(ggrepel)\n\ng3 &lt;- ggplot(R_studies_split,aes(x=valence,y=arousal,label=citekey,color=model_class_id,fill=model_class_id))+\n  geom_point(size=4)+\n  geom_label_repel(size=3, max.overlaps=50,show.legend = T,color='white')+\n  scale_x_continuous(breaks = seq(0,1,by=.25),limits = c(0,1))+\n  scale_y_continuous(breaks = seq(0,1,by=.25),limits = c(0,1))+\n  theme_bw()\ng3"
  },
  {
    "objectID": "analysis/preprocessing.html#plot-success-across-the-years-1",
    "href": "analysis/preprocessing.html#plot-success-across-the-years-1",
    "title": "Preprocessing",
    "section": "Plot success across the years",
    "text": "Plot success across the years\n\n# Add year!\nR_studies$year &lt;- as.numeric(str_match(R_studies$citekey,'[0-9]+'))\n\ng3 &lt;- ggplot(R_studies,aes(x=year,y=values,colour=model_class_id))+\n  geom_point(show.legend = T)+\n  facet_wrap(.~dimension)+\n  theme_bw()\ng3\n\nWarning: Removed 4 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "analysis/preprocessing.html#simple-model-complexity-metric-1",
    "href": "analysis/preprocessing.html#simple-model-complexity-metric-1",
    "title": "Preprocessing",
    "section": "Simple model complexity metric",
    "text": "Simple model complexity metric\nRatio of obs./features or just a classification based on feature n (quintiles).\n\nR_studies$feature_n==\" 15; 23 in table, but 15 reported \"\n\n  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE    NA    NA\n [13]    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA FALSE FALSE\n [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [49] FALSE FALSE FALSE FALSE FALSE FALSE    NA    NA    NA    NA FALSE FALSE\n [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [85] FALSE FALSE    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA\n [97]    NA    NA    NA    NA    NA    NA    NA    NA FALSE FALSE FALSE FALSE\n\nR_studies$feature_n[R_studies$feature_n==\" 15; 23 in table, but 15 reported  \"] &lt;- 15 #resolved from the paper\n\nR_studies$feature_n[R_studies$feature_n==\" 15; 23 in table, but 15 reported  \"] &lt;- 15 #resolved\nR_studies$feature_n[R_studies$feature_n==\" before_selection = 45, after_selection = 6  \"] &lt;- 45 #resolved\nR_studies$feature_n[R_studies$feature_n==\" model 1: 52, model 2 = 68, model 3 = 260, model 4 = 388 \"] &lt;- 388 #resolved\nR_studies$feature_n[R_studies$feature_n==\" variable, 557 before feature selection \"] &lt;- 557 #resolved\n\nR_studies$feature_n[R_studies$feature_n==\" 50 PCA features \"] &lt;- 50 #resolved\n#R_studies$feature_n[R_studies$feature_n==\"\"] &lt;- 0 #resolved\n#R_studies$feature_n[R_studies$feature_n==\"\"] &lt;- 0 #resolved\n\nR_studies$feature_n&lt;-as.numeric(R_studies$feature_n)\n\nprint(quantile(R_studies$feature_n,c(0.333,0.666),na.rm = T))\n\n33.3% 66.6% \n   18   260 \n\n#quantile(R_studies$feature_n,c(0.25,0.50,0.75),na.rm = T)\n\n# Assign\nR_studies$feature_n_complexity &lt;- cut(R_studies$feature_n,\n                                      breaks = c(0,\n                                                 as.numeric(quantile(R_studies$feature_n,c(0.333),na.rm = T)),\n                                                 as.numeric(quantile(R_studies$feature_n,c(0.666),na.rm = T)),\n                                                 1000),\n                                      labels = c(\"Feature n &lt; 18\",\"Feature n &gt; 18 & &lt; 260\",\"Feature n &gt; 260\"))\ntable(R_studies$feature_n_complexity)\n\n\n        Feature n &lt; 18 Feature n &gt; 18 & &lt; 260        Feature n &gt; 260 \n                    26                     24                     24"
  },
  {
    "objectID": "analysis/preprocessing.html#explore-feature_n_complexity-and-model-success-1",
    "href": "analysis/preprocessing.html#explore-feature_n_complexity-and-model-success-1",
    "title": "Preprocessing",
    "section": "Explore feature_n_complexity and model success",
    "text": "Explore feature_n_complexity and model success\nNeeds to be done from the unsummarised data (C_studies).\n\ntmp &lt;- drop_na(C_studies)\nlibrary(ggdist)\n\ntmp$dimension&lt;-str_to_title(tmp$dimension)\ntmp$model_class_id&lt;-factor(tmp$model_class_id,\n                           levels = c(\"Neural Nets\",\"Flexible Discriminants\", \"Kernel Smoothing, Additive and KNN\", \"Random Forests\",\"Linear Methods\"),\n                           labels = c(\"Neural\\nNets\",\"Flexible\\nDiscriminants\", \"KS\\n & KNN\", \"Random\\nForests\", \"Linear\\nMethods\"))\n\ng &lt;- ggplot(tmp,aes(x=model_class_id,y=values,color=citekey,label=citekey))+\n  stat_halfeye(aes(fill=citekey),point_interval=\"mean_qi\", trim=FALSE, expand=FALSE, show.legend = FALSE,adjust = 1.25, density=\"bounded\", point_size=3,scale = 1,alpha=0.5) + \n  geom_point(alpha=0.5,show.legend = F,position = position_jitter(width = .3))+\n  #geom_label_repel(size=2,max.overlaps = 50)+\n  facet_wrap(.~feature_n_complexity)+\n  ylab(\"Correlation Coefficient\")+\n  xlab(\"Model Technique\")+\n#  scale_y_continuous(limits = c(0,1),expand = c(0.01,0.01))+\n  geom_text_repel(aes(x = model_class_id, y = values, label = studyREF),\n             stat = \"summary\", fun = mean,show.legend = F)+\n  theme_bw()\nprint(g)\n#ggsave(filename = 'FeatureN_regression.pdf',g,height = 7,width = 11)"
  },
  {
    "objectID": "analysis/preprocessing.html#create-descriptive-table-for-the-manuscript-1",
    "href": "analysis/preprocessing.html#create-descriptive-table-for-the-manuscript-1",
    "title": "Preprocessing",
    "section": "Create descriptive table for the manuscript",
    "text": "Create descriptive table for the manuscript\n\nTC &lt;- NULL\nTC$study_n &lt;- length(unique(C_studies$citekey))\nTC$model_n &lt;- nrow(C_studies)\nt&lt;-table(C_studies$model_class_id)\nt2 &lt;- paste0(names(t),': ', as.numeric(t))\nTC$model_types_n &lt;- str_c(t2,collapse = \"\\n\")\nTC$feature_Desc &lt;- paste0('Min=',min(C_studies$feature_n,na.rm = TRUE),', Md=',median(C_studies$feature_n,na.rm = TRUE),', Max=', max(C_studies$feature_n,na.rm = TRUE))\nTC$stimulus_Desc &lt;- paste0('Min=',min(C_studies$stimulus_n,na.rm = TRUE),', Md=',median(C_studies$stimulus_n,na.rm = TRUE),', Max=', max(C_studies$stimulus_n,na.rm = TRUE))\nprint(TC)\n\n$study_n\n[1] 14\n\n$model_n\n[1] 108\n\n$model_types_n\n[1] \"Flexible Discriminants: 37\\nKernel Smoothing, Additive and KNN: 8\\nLinear Methods: 24\\nNeural Nets: 27\\nRandom Forests: 12\"\n\n$feature_Desc\n[1] \"Min=3, Md=231, Max=8904\"\n\n$stimulus_Desc\n[1] \"Min=124, Md=387, Max=5192\""
  },
  {
    "objectID": "analysis/preprocessing.html#export-as-csv-1",
    "href": "analysis/preprocessing.html#export-as-csv-1",
    "title": "Preprocessing",
    "section": "Export as csv",
    "text": "Export as csv\n\nwrite.csv(x = C_studies,file = 'C_studies.csv')\nwrite.csv(x = C_summary,file = 'C_summary.csv')"
  },
  {
    "objectID": "analysis/preprocessing.html#check-studies-included",
    "href": "analysis/preprocessing.html#check-studies-included",
    "title": "Preprocessing",
    "section": "Check studies included",
    "text": "Check studies included\n\nincluded_in_summary &lt;- R_summary$citekey[R_summary$dimension=='arousal']\n\nincluded_originally &lt;- unique(metaMER_results$citekey[metaMER_results$model_category=='regression'])\n\nincluded_originally[!included_originally %in% included_in_summary]"
  },
  {
    "objectID": "analysis/preprocessing.html#homogenise-the-stimulus-n",
    "href": "analysis/preprocessing.html#homogenise-the-stimulus-n",
    "title": "Preprocessing",
    "section": "Homogenise the stimulus N",
    "text": "Homogenise the stimulus N\n\ntable(C_studies$stimulus_n)  \n\n\n                                                                                                                    387   \n                                                                                                                       16 \n                                                                                                                    124   \n                                                                                                                        8 \n                                                                                                                    171   \n                                                                                                                       37 \n                                                                                                                    1802  \n                                                                                                                        4 \n                                                                                                                    300   \n                                                                                                                        2 \n                                             429; 350 popular songs + 79 songs from the Beatles (Mirex 2009 collection)   \n                                                                                                                        2 \n 5192; 12 per user in user validation (not included here due to little information),   AcousticBrainz validation: 60000   \n                                                                                                                        2 \n                                                                                                                     744  \n                                                                                                                       15 \n                                                                                                                    744   \n                                                                                                                        2 \n                                                                                                                    900   \n                                                                                                                        1 \n                                                                                                                    956   \n                                                                                                                        4 \n                                                                              ISMIR2012: 2886, NJU_V1: 777, Hindi: 1037   \n                                                                                                                        9 \n                                                                   total: 564; unambiguous: 416, circular validation: 39  \n                                                                                                                        6 \n\n# Deal with four studies involving multiple datasets: \nC_studies$stimulus_n[C_studies$stimulus_n==\" 429; 350 popular songs + 79 songs from the Beatles (Mirex 2009 collection)  \"] &lt;- 429 # resolved from the paper\nC_studies$stimulus_n[C_studies$stimulus_n==\" 5192; 12 per user in user validation (not included here due to little information),   AcousticBrainz validation: 60000  \"] &lt;- 5192 # resolved\nC_studies$stimulus_n[C_studies$stimulus_n==\" ISMIR2012: 2886, NJU_V1: 777, Hindi: 1037  \"] &lt;- 2886+777+1037 # decided to take this from validation\nC_studies$stimulus_n[C_studies$stimulus_n==\" total: 564; unambiguous: 416, circular validation: 39 \"] &lt;- 564 # \nC_studies$stimulus_n &lt;- as.numeric(C_studies$stimulus_n)"
  },
  {
    "objectID": "analysis/preprocessing.html#visualise-summary",
    "href": "analysis/preprocessing.html#visualise-summary",
    "title": "Preprocessing",
    "section": "Visualise Summary",
    "text": "Visualise Summary\n\ng2 &lt;- ggplot(C_summary,aes(x=stimulus_n,y=valuesMax,label=citekey,color=stimulus_genre_mixed))+\n  geom_point()+\n  geom_label_repel(size=1.5)+\n#  coord_flip()+\n  theme_bw()\ng2\n\n\n\n\n\n\n\ng3 &lt;- ggplot(C_summary,aes(x=stimulus_n,y=valuesMax,label=citekey,color=model_class_id))+\n  geom_point()+\n  geom_label_repel(size=1.2)+\n#  coord_flip()+\n  theme_bw()\ng3\n\n\n\n\n\n\n\n## could be more informative when done with the full data\nC_studies$citekey&lt;-factor(C_studies$citekey)\n\nlibrary(ggrepel)\n\ng4 &lt;- ggplot(C_studies,aes(x=stimulus_n,y=values,label=citekey,color=model_class_id,fill=model_class_id))+\n  geom_point(size=4)+\n  geom_label_repel(size=1.2, max.overlaps=50,show.legend = T,color='white')+\n#  scale_x_continuous(breaks = seq(0,1,by=.25),limits = c(0,1))+\n#  scale_y_continuous(breaks = seq(0,1,by=.25),limits = c(0,1))+\n  theme_bw()\ng4\n\nWarning: Removed 19 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 19 rows containing missing values or values outside the scale range\n(`geom_label_repel()`)."
  },
  {
    "objectID": "analysis/preprocessing.html#simple-model-complexity-metric-based-on-feature_n",
    "href": "analysis/preprocessing.html#simple-model-complexity-metric-based-on-feature_n",
    "title": "Preprocessing",
    "section": "Simple model complexity metric based on feature_n",
    "text": "Simple model complexity metric based on feature_n\n\ntable(C_studies$feature_n)\n\n\n 'eight different non‐text‐dependent features are employed; they are rhythm, timbre,intensity, chromagram, MFCC, OSC, SSDs, and DWCH'  \n                                                                                                                                     9 \n                                                                                                                                 119   \n                                                                                                                                    21 \n                                                                                                                                 122   \n                                                                                                                                     4 \n                                                                                                                    126, retained 97   \n                                                                                                                                     6 \n                                                                                           1702; best model uses 100 after reduction   \n                                                                                                                                     1 \n                                                                                                                                  231  \n                                                                                                                                     4 \n                                                                                                                                    3  \n                                                                                                                                     2 \n                                                                                                                                 397   \n                                                                                                                                     2 \n                                                                              548; after reduction, 139 for PCA and 276 for ReliefF    \n                                                                                                                                    15 \n                                                                                       548. Post-reduction: 139 (PCA), 276 (ReliefF)   \n                                                                                                                                     2 \n                                                                                       8; 3 after shrinkage-method feature selection   \n                                                                                                                                    16 \n                                                                                                                                 8904  \n                                                                                                                                     8 \n                                                                                                                    between 9 and 10   \n                                                                                                                                     2 \n                                                                  summarize feature categories, but aren't explicit about which ones   \n                                                                                                                                    16 \n\nC_studies$feature_n[str_detect(C_studies$feature_n,'eight different ')]&lt;-600 # arbitrary!\nC_studies$feature_n[str_detect(C_studies$feature_n,'126, retained 97')]&lt;-126 \nC_studies$feature_n[str_detect(C_studies$feature_n,'1702; best model uses 100 after reduction')]&lt;-1702 #  arbitrary!\nC_studies$feature_n[str_detect(C_studies$feature_n,'548; after reduction, ')]&lt;-548 \nC_studies$feature_n[str_detect(C_studies$feature_n,'548. Post-reduction')]&lt;-548 \nC_studies$feature_n[str_detect(C_studies$feature_n,'8; 3 after')]&lt;-8 \nC_studies$feature_n[str_detect(C_studies$feature_n,'between 9 and 10')]&lt;-10 \nC_studies$feature_n[str_detect(C_studies$feature_n,'summarize feature')]&lt;-600 # arbitrary\ntable(C_studies$feature_n)\n\n\n 119    122     231      3   397    8904      10    126   1702    548    600 \n    21      4      4      2      2      8      2      6      1     17     25 \n     8 \n    16 \n\nC_studies$feature_n&lt;-as.numeric(C_studies$feature_n)\n\nprint(quantile(C_studies$feature_n,c(0.333,0.666),na.rm = TRUE))\n\n33.3% 66.6% \n  119   548 \n\n#quantile(R_studies$feature_n,c(0.25,0.50,0.75),na.rm = T)\n\n# Assign\nC_studies$feature_n_complexity &lt;- cut(C_studies$feature_n,\n                                      breaks = c(0,\n                                                 as.numeric(quantile(C_studies$feature_n,c(0.333),na.rm = TRUE)),\n                                                 as.numeric(quantile(C_studies$feature_n,c(0.666),na.rm = TRUE)),\n                                                 1000),\n                                      labels = c(\"Feature n &lt; 119\",\"Feature n &gt; 119 & &lt; 548\",\"Feature n &gt; 548\"))\ntable(C_studies$feature_n_complexity)\n\n\n        Feature n &lt; 119 Feature n &gt; 119 & &lt; 548         Feature n &gt; 548 \n                     41                      33                      25"
  },
  {
    "objectID": "analysis/analysis.html#classification-studies",
    "href": "analysis/analysis.html#classification-studies",
    "title": "Analysis",
    "section": "",
    "text": "C_studies &lt;- read.csv(\"C_studies.csv\")\nC_studies &lt;- C_studies[!is.na(C_studies$values),]\n\nC_summary &lt;- read.csv(\"C_summary.csv\")\n\nm.cor &lt;- metacor(cor = valuesMax,     # values \n                 n = stimulus_n,\n                 studlab = citekey, # unique_id\n                 data = C_summary,\n                 fixed = FALSE,\n                 random = TRUE,\n                 prediction = TRUE,\n                 backtransf = TRUE,\n                 sm = \"ZCOR\",\n                 method.tau = \"REML\",# could be PM (Paule-Mandel) as well\n                 method.random.ci = \"HK\", \n                 title = \"MER: Classification: Summary\")\n\n#print(m.cor)\n\n#m.cor_backtransformed &lt;- m.cor\n#m.cor_backtransformed$TE &lt;- FisherZInv(m.cor_backtransformed$TE)\nprint(funnel(m.cor, common = FALSE, studlab=TRUE,backtransf=TRUE))\n\n\n\n\n\n\n\n\n$xlim\n[1] 0.598722 2.439471\n\n$ylim\n[1] 0.09090909 0.00000000\n\n\n\n\n\nlibrary(ggrepel)\ntmpdata &lt;- data.frame(SE = FisherZInv(m.cor$seTE), Zr = FisherZInv(m.cor$TE),studies=m.cor$studlab)\n\ntmpdata$citekey&lt;-str_replace_all(tmpdata$studies,'-.*$','')\ntmpdata$citekey&lt;-factor(tmpdata$citekey)\n\ng &lt;- ggplot(tmpdata,aes(y = SE, x = Zr,label=studies,color=citekey,fill=citekey)) +\n  geom_point(show.legend = FALSE) +\n  geom_label_repel(size=1.5,max.overlaps = 39,color=\"black\",show.legend = FALSE)+\n  ylab('Standard Error') + \n  xlab('r')+\n  scale_y_reverse()+\n#  scale_x_continuous(breaks=seq(0.0,1.0,0.25),limits=c(0,1.0))+\n#  coord_flip()+\n  theme_bw()\nprint(g)\n\n\n\n\n\n\n\n\n\n\n\n\nforest(m.cor,\n       sortvar = TE,\n       prediction = FALSE, \n             print.tau2 = FALSE,\n             leftlabs = c(\"Author\", \"g\", \"SE\"),studlab = citekey)\n\n\n\n\n\n\n\nfunnel(m.cor, common = FALSE, studlab=TRUE,backtransf=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\nO &lt;- find.outliers(m.cor)\n# 6 datasets identified as outliers, without them the r drops to 0.5781\n#infan &lt;- InfluenceAnalysis(m.cor)\n#print(eggers.test(m.cor))\n\n\n\n\n\noutliers &lt;- c(\"\")\n\ntmp2&lt;-tmp[!tmp$unique_id %in% outliers,]\nm.cor &lt;- metacor(cor = values, \n                 n = stimulus_n,\n                 studlab = unique_id, # unique_id\n                 data = tmp2,\n                 fixed = FALSE,\n                 random = TRUE,\n                 sm = \"ZCOR\",\n                 prediction = TRUE,\n                 backtransf = TRUE,\n                 method.tau = \"REML\",# could be PM (Paule-Mandel) as well\n                 method.random.ci = \"HK\", \n                 title = \"MER: Regression: Valence: Summary\")\n\nprint(m.cor)\n\nfp &lt;- funnel(m.cor, common = TRUE,studlab=FALSE)\n\n\n\n\nTo show the quality differences between core and eliminated studies (in progress).\n\ntmpdata &lt;- data.frame(SE = m.cor$seTE, Zr = m.cor$TE)\nestimate = m.cor$TE.common\nse = m.cor$seTE.common\nse.seq=seq(0, max(m.cor$cor), 0.001)\nll95 = estimate-(1.96*se.seq)\nul95 = estimate+(1.96*se.seq)\nll99 = estimate-(3.29*se.seq)\nul99 = estimate+(3.29*se.seq)\nmeanll95 = estimate-(1.96*se)\nmeanul95 = estimate+(1.96*se)\ndfCI = data.frame(ll95, ul95, ll99, ul99, se.seq, estimate, meanll95, meanul95)\n\nfp = ggplot(aes(x = SE, y = Zr), data = tmpdata) +\n  geom_point(shape = 1) +\n  xlab('Standard Error') + ylab('Zr')+\n  geom_line(aes(x = se.seq, y = ll95), linetype = 'dotted', data = dfCI) +\n  geom_line(aes(x = se.seq, y = ul95), linetype = 'dotted', data = dfCI) +\n   geom_segment(aes(x = min(se.seq), y = meanll95, xend = max(se.seq), yend = meanll95), linetype='dotted', data=dfCI) +\n  geom_segment(aes(x = min(se.seq), y = meanul95, xend = max(se.seq), yend = meanul95), linetype='dotted', data=dfCI) +\n#Reverse the x-axis ordering (se) so that the tip of the funnel will appear\n#at the top of the figure once we swap the x- and y-axes...\n  scale_x_reverse(breaks=seq(0,0.2,0.05),limits=c(0.2,0))+\n#Specify the range and interval for the tick-marks of the y-axis (Zr);\n#Choose values that work for you based on your data\n  scale_y_continuous(breaks=seq(0.3,1.25,0.25),limits=c(0.3,1.25))+\n#  scale_x_continuous(breaks=seq(0.2,0,0.05))+\n#And now we flip the axes so that SE is on y- and Zr is on x-\n  coord_flip()+\n#Finally, apply my APA-format theme (see code at end of post).\n#You could, alternatively, specify theme_bw() instead.\n  theme_bw()\nprint(fp)\n\n\n\n\nadd journal_type and stimulus_genre_mixed as a grouping option\n\nm.cor_subgroups &lt;- update(m.cor, \n       subgroup = model_class_id, \n#       subgroup = journal_type, \n#       subgroup = stimulus_genre_mixed, \n       tau.common = FALSE,\n       prediction = TRUE)\n\nprint(m.cor_subgroups)\n#forest(m.cor_subgroups,subgroup=TRUE)\n\n\nIdea: visualise the distributions of the model successes within studies (done in preprocessing)"
  },
  {
    "objectID": "manuscript/manuscript.html#quality-control",
    "href": "manuscript/manuscript.html#quality-control",
    "title": "Meta-analysis of regression and classification success of emotion ratings from audio",
    "section": "Quality Control",
    "text": "Quality Control\nThe search yielded studies of variable (and occasionally questionable) quality. To mitigate potentially spurious effects resulting from the inclusion of low-quality studies, we excluded studies lacking sufficient details about stimuli, analyzed features, or model architecture. Additionally, we excluded results comprising algorithms trained on image features such as spectrograms (affecting two studies, one of which was eliminated).Finally, we excluded one study published in two journals of questionable relevance to MER. This includes Mathematical Problems in Engineering which ceased publication following a large number of retractions in September 2024."
  }
]