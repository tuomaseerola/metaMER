%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Tuomas Eerola at 2025-01-22 11:05:46 +0000 


%% Saved with string encoding Unicode (UTF-8) 



@article{wang2021ac,
	author = {Wang, Xin and Wei, Yujia and Heng, Lena and McAdams, Stephen},
	doi = {10.3389/fpsyg.2021.732865},
	issn = {1664-1078},
	journal = {FRONTIERS IN PSYCHOLOGY},
	month = {SEP 29},
	title = {A Cross-Cultural Analysis of the Influence of Timbre on Affect Perception in Western Classical Music and Chinese Music Traditions},
	volume = {12},
	year = {2021}}

@article{koh2023me,
	author = {Koh, En Yan and Cheuk, Kin Wai and Heung, Kwan Yee and Agres, Kat R. and Herremans, Dorien},
	doi = {10.3390/s23010382},
	eissn = {1424-8220},
	journal = {SENSORS},
	month = {JAN},
	number = {1},
	title = {MERP: A Music Dataset with Emotion Ratings and Raters' Profile Information},
	volume = {23},
	year = {2023}}

@article{eggersmith_1997,
	author = {Egger, Matthias and Smith, George Davey and Schneider, Martin and Minder, Christoph},
	date-added = {2024-10-20 18:39:33 +0100},
	date-modified = {2024-10-20 18:39:33 +0100},
	journal = {bmj},
	number = {7109},
	pages = {629--634},
	publisher = {British Medical Journal Publishing Group},
	title = {Bias in meta-analysis detected by a simple, graphical test},
	volume = {315},
	year = {1997}}

@article{van-aertwicherts_2016,
	author = {Van Aert, Robbie CM and Wicherts, Jelte M and van Assen, Marcel ALM},
	date-added = {2024-10-20 10:40:20 +0100},
	date-modified = {2024-10-20 10:40:20 +0100},
	journal = {Perspectives on Psychological Science},
	number = {5},
	pages = {713--729},
	publisher = {Sage Publications Sage CA: Los Angeles, CA},
	title = {Conducting meta-analyses based on p values: Reservations and recommendations for applying p-uniform and p-curve},
	volume = {11},
	year = {2016}}

@article{beveridge2018po,
	author = {Beveridge, Scott and Knox, Don},
	date-added = {2024-10-11 11:31:06 +0100},
	date-modified = {2024-10-11 11:31:06 +0100},
	doi = {10.1177/0305735617713834},
	eissn = {1741-3087},
	issn = {0305-7356},
	journal = {PSYCHOLOGY OF MUSIC},
	month = {MAY},
	number = {3},
	pages = {411-423},
	source = {web_of_science},
	title = {Popular music and the role of vocal melody in perceived emotion},
	volume = {46},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1177/0305735617713834}}

@article{griffiths2021am,
	author = {Griffiths, Darryl and Cunningham, Stuart and Weinel, Jonathan and Picking, Richard},
	date-added = {2024-10-11 11:30:37 +0100},
	date-modified = {2024-10-11 11:30:37 +0100},
	doi = {10.1080/09298215.2021.1977336},
	earlyaccessdate = {SEP 2021},
	eissn = {1744-5027},
	issn = {0929-8215},
	journal = {JOURNAL OF NEW MUSIC RESEARCH},
	month = {AUG 8},
	number = {4},
	pages = {355-372},
	source = {web_of_science},
	title = {A multi-genre model for music emotion recognition using linear regressors},
	volume = {50},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1080/09298215.2021.1977336}}

@article{higgins2002quantifying,
	author = {Higgins, Julian PT and Thompson, Simon G},
	date-added = {2024-10-10 11:48:25 +0100},
	date-modified = {2024-10-10 11:48:30 +0100},
	journal = {Statistics in Medicine},
	number = {11},
	pages = {1539--1558},
	publisher = {Wiley Online Library},
	title = {Quantifying heterogeneity in a meta-analysis},
	volume = {21},
	year = {2002}}

@article{knapp2003improved,
	author = {Knapp, Guido and Hartung, Joachim},
	date-added = {2024-10-10 11:42:03 +0100},
	date-modified = {2024-10-10 11:42:08 +0100},
	journal = {Statistics in Medicine},
	number = {17},
	pages = {2693--2710},
	publisher = {Wiley Online Library},
	title = {Improved tests for a random effects meta-regression with a single covariate},
	volume = {22},
	year = {2003}}

@article{langan2019comparison,
	author = {Langan, Dean and Higgins, Julian PT and Jackson, Dan and Bowden, Jack and Veroniki, Areti Angeliki and Kontopantelis, Evangelos and Viechtbauer, Wolfgang and Simmonds, Mark},
	date-added = {2024-10-10 11:40:09 +0100},
	date-modified = {2024-10-10 11:40:16 +0100},
	journal = {Research Synthesis Methods},
	number = {1},
	pages = {83--98},
	publisher = {Wiley Online Library},
	title = {A comparison of heterogeneity variance estimators in simulated random-effects meta-analyses},
	volume = {10},
	year = {2019}}

@article{agres2021music,
	author = {Agres, Kat R and Schaefer, Rebecca S and Volk, Anja and Van Hooren, Susan and Holzapfel, Andre and Dalla Bella, Simone and M{\"u}ller, Meinard and De Witte, Martina and Herremans, Dorien and Ramirez Melendez, Rafael and others},
	date-added = {2024-10-10 11:03:55 +0100},
	date-modified = {2024-10-10 11:03:55 +0100},
	journal = {Music \& Science},
	pages = {2059204321997709},
	publisher = {SAGE Publications Sage UK: London, England},
	title = {Music, computing, and health: a roadmap for the current and future roles of music technology for health care and well-being},
	volume = {4},
	year = {2021}}

@article{balduzzi2019,
	author = {Sara Balduzzi and Gerta R{\"u}cker and Guido Schwarzer},
	date-added = {2024-09-25 13:43:41 +0100},
	date-modified = {2024-09-25 13:43:41 +0100},
	journal = {Evidence-Based Mental Health},
	number = {22},
	pages = {153--160},
	title = {How to perform a meta-analysis with {R}: a practical tutorial},
	year = {2019}}

@article{zhang2023mo,
	author = {Zhang, Meixian and Zhu, Yonghua and Zhang, Wenjun and Zhu, Yunwen and Feng, Tianyu},
	bdsk = {https://doi.org/10.1007/s11042-022-13577-6},
	date-added = {2024-09-25 11:16:42 +0100},
	date-modified = {2024-09-25 11:16:42 +0100},
	doi = {10.1007/s11042-022-13577-6},
	earlyaccessdate = {AUG 2022},
	eissn = {1573-7721},
	emotion_locus = {perceived},
	emotions = {valence, arousal},
	feature_categories = {filter banks, handcrafted},
	feature_n = {60 handcrafted and filter bank features, extracted filter bank output size: 120 * 120},
	feature_reduction_method = {weighted attention module, feature augmentation},
	feature_source = {daubechie, MIRToolbox, Sound Description Toolbox, filter bank output (size = 120*120), 60 handcrafted features (size = 60*60)},
	issn = {1380-7501},
	journal = {MULTIMEDIA TOOLS AND APPLICATIONS},
	model_category = {regression},
	model_complexity_parameters = {Batch size: 15, mixup technique applied to spectrograms, first convolutional layer alpha: 0.2, learning rate: 0.001, decreasing by 1/10 every 10 epochs. Adam optimizer. Dropout ratio 0.3 and 0.2, alpha = 0.18, beta = 0.49},
	model_detail = {Modularized Composite Attention Network. Case 1: no sample reconstruction. Case 2: no handcrafted features (or feature augmentation). Case 3: No self-attention mechanism or BLSTM. Case 4: no style embedding module},
	model_measure = {RMSE, MAE, CCC},
	model_rate_emotion_names = {valence, arousal},
	model_rate_emotion_values = {bind_field( 'ns.MCAN.handcrafted and filter bank.deam.1' = c(valence_ccc.mean = 0.309, valence_rmse.mean = 0.112, valence_mae.mean = 0.811, valence_ccc.sd = 0.358, valence_rmse.sd = 0.010, valence_mae.sd = 0.009, arousal_ccc.mean = 0.502, arousal_rmse.mean = 0.109, arousal_mae.mean = 0.764, arousal_ccc.sd = 0.287, arousal_rmse.sd = 0.011, arousal_mae.sd = 0.023), 'ns.MCAN. handcrafted and filter bank.PMEmo.1' = c(.215,.144,.892,0.265,.102,.030,.401,.135,.901,.315,0.057,0.103) )},
	model_validation = {ablation, train/test},
	month = {FEB},
	notes_ca = {include},
	notes_te = {include, R2},
	number = {5},
	orcid = {Zhang, Meixian/0000-0002-6696-2814},
	pages = {7319-7341},
	paradigm = {regression},
	participant_expertise = {primarily nonexperts},
	participant_n = {DEAM: at least 10 per piece, PMEmo: 457},
	participant_origin = {DEAM: MTurk; PMEmo: lab},
	participant_sampling = {DEAM: crowdsource; PMEmo: convenience},
	participant_task = {rate},
	researcherid = {Zhu, Yonghua/HSI-1360-2023 Feng, Tian-Yu/AAX-3892-2020},
	source = {web_of_science},
	stimulus_duration = {DEAM: 45, PMEmo: variable},
	stimulus_duration_unit = {s},
	stimulus_genre = {pop, multi (pop of various genres)},
	stimulus_n = {DEAM: 744, PMEmo: 206},
	stimulus_type = {PMEmo, DEAM},
	title = {Modularized composite attention network for continuous music emotion recognition},
	volume = {82},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1007/s11042-022-13577-6}}

@article{battcock2021in,
	author = {Battcock, Aimee and Schutz, Michael},
	bdsk = {https://doi.org/10.1080/09298215.2021.1979050},
	date-added = {2024-09-25 11:16:03 +0100},
	date-modified = {2024-09-25 11:16:03 +0100},
	doi = {10.1080/09298215.2021.1979050},
	earlyaccessdate = {JAN 2022},
	eissn = {1744-5027},
	emotion_locus = {perceived},
	emotions = {valence, arousal},
	feature_categories = {melody, harmony, rhythm},
	feature_n = {3},
	feature_reduction_method = {none},
	feature_source = {manual},
	issn = {0929-8215},
	journal = {JOURNAL OF NEW MUSIC RESEARCH},
	model_category = {regression},
	model_complexity_parameters = {4},
	model_detail = {commonality analysis, multiple regression},
	model_measure = {R2},
	model_rate_emotion_names = {valence, arousal},
	model_rate_emotion_values = {bind_field( manual.lm.score.bachwtc.1 = c('valence_r2' = 0.808, 'arousal_r2' = 0.788) )},
	model_validation = {none},
	month = {OCT 20},
	notes_ca = {include},
	notes_te = {include, commonality analysis},
	number = {5},
	pages = {447-468},
	paradigm = {regression},
	participant_expertise = {nonmusicians},
	participant_n = {180},
	participant_origin = {canada},
	participant_sampling = {convenience},
	participant_task = {rate},
	source = {web_of_science},
	stimulus_duration = {8},
	stimulus_duration_unit = {measure},
	stimulus_genre = {classical},
	stimulus_n = {336},
	stimulus_type = {prelude},
	title = {Individualized interpretation: Exploring structural and interpretive effects on evaluations of emotional content in Bach's Well Tempered Clavier},
	volume = {50},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1080/09298215.2021.1979050}}

@article{saizclar2022pr,
	author = {Saiz-Clar, Elena and Angel Serrano, Miguel and Manuel Reales, Jose},
	bdsk = {https://doi.org/10.1177/03057356211031658},
	date-added = {2024-09-25 11:02:28 +0100},
	date-modified = {2024-09-25 11:02:28 +0100},
	doi = {10.1177/03057356211031658},
	earlyaccessdate = {AUG 2021},
	eissn = {1741-3087},
	emotion_locus = {not specified},
	emotions = {activation, valence},
	feature_categories = {rhythm},
	feature_n = {22},
	feature_reduction_method = {correlation significance, PCA},
	feature_source = {MIRToolbox},
	issn = {0305-7356},
	journal = {PSYCHOLOGY OF MUSIC},
	model_category = {regression},
	model_complexity_parameters = {23},
	model_detail = {MLR, five predictors following PCA},
	model_measure = {R2, eta2},
	model_rate_emotion_names = {valence, activation},
	model_rate_emotion_values = {bind_field('MIRToolbox.MLR.onset curves.1.1' = c(activation_r2.r2 = 0.537, activation_eta2.eta2 = 0.537, valence_r2.r2 = 0.218, valence_eta2.eta2 = 0.218))},
	model_validation = {cross-validation},
	month = {JUL},
	notes_ca = {exclude, no modeling task},
	notes_te = {include, R2},
	number = {4},
	orcid = {Serrano, Miguel-Angel/0000-0002-6574-4532 Serrano, Miguel-Angel/0000-0002-6574-4532 Saiz-Clar, Elena/0000-0002-8714-2305},
	pages = {1107-1120},
	paradigm = {regression},
	participant_expertise = {no formal training},
	participant_n = {16},
	participant_origin = {not specified},
	participant_sampling = {not specified},
	participant_task = {rate, categorize},
	researcherid = {Serrano, Miguel-Angel/ABE-7279-2021 Serrano, Miguel-Angel/AAZ-4393-2020},
	source = {web_of_science},
	stimulus_duration = {8-12},
	stimulus_duration_unit = {s},
	stimulus_genre = {classical},
	stimulus_n = {40},
	stimulus_type = {piano pieces, classical music},
	title = {Predicting emotions in music using the onset curve},
	volume = {50},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1177/03057356211031658}}

@article{alvarez2023ri,
	abstract = {The music emotions can help to improve the personalization of services and contents offered by music streaming providers. Many research works based on the use of machine learning techniques have addressed the problem of recognising the music emotions during the last years. Nevertheless, the results obtained are only applied on small-size music repositories and do not consider what the users feel when they listen to the songs. These issues prevent the existing proposals to be integrated into the personalization mechanisms of the online music providers. In this paper, we present the RIADA infrastructure which is composed by a set of systems able to annotate emotionally the catalog of songs offered by Spotify based on the users' perception. RIADA works with the Spotify playlist miner and data services to build emotion recognition models that can solve the open challenges previously mentioned. Machine learning algorithms, music information retrieval techniques, architectures for parallelization of applications and cloud computing have been combined to develop a complex result of engineering able to integrate the music emotions into the Spotify-based applications. {\copyright} 2023, Universidad Internacional de la Rioja. All rights reserved.},
	author = {{\'A}lvarez, P. and de Quir{\'o}s, J. Garc{\'\i}a and Baldassarri, S.},
	author_keywords = {Affective Annotation; Cloud Computing; Emotion Recognition; Machine Learning; Music; Spotify},
	date-added = {2024-09-25 11:01:59 +0100},
	date-modified = {2024-09-25 11:01:59 +0100},
	doi = {10.9781/ijimai.2022.04.002},
	emotion_locus = {perceived},
	emotions = {discrete},
	feature_categories = {timbre, dynamic, high-level, expressivity},
	feature_n = {between 9 and 10},
	feature_reduction_method = {expert validation},
	feature_source = {Spotify API},
	final_notes = {Did not report results from user validation due to differences in formatting.},
	journal = {International Journal of Interactive Multimedia and Artificial Intelligence},
	model_category = {classification},
	model_complexity_parameters = {not specified},
	model_detail = {SVR, KNN, RF},
	model_measure = {classification accuracy, f1, precision, recall},
	model_rate_emotion_names = {happy, sad, angry, relaxed; Experimental validation: excited, cheerful, tense, irritated, sad, bored, relaxed, calm, don't know},
	model_rate_emotion_values = {bind_field( 'spotify.random forest.spotify.spotify.model comparison' = c( unlist( lapply( list( unflatten( Happy=1582,Angry=40,Sad=29,Relaxed=22, 39,1201,8,3, 24,22,1536,50, 0,0,7,425 ) ), summarize_matrix ) ), 'classification_accuracy.mean' = mean(c(0.844,0.899,0.862,0.945)), 'classification_f1.mean' = mean(c(0.820,.860,0.839,0.801)), 'classification_precision.mean' = mean(c(.8308,.8828,.8488,.9299)), 'classification_recall.mean' = mean(c(.8083,.8446,.8353,.7392)) ), 'spotify.random forest.spotify.acousticBrainz.model validation' = c('classification_accuracy.mean' = mean(c(0.694,0.705,0.771,0.729)), 'classification_f1.mean' = mean(c(0.623,0.700,0.745,0.719)), 'classification_class.n' = 4) )},
	model_validation = {Repeated five-fold cross validation, AcousticBrainz, experimental validation with real users},
	note = {Cited by: 4; All Open Access, Gold Open Access, Green Open Access},
	notes_ca = {include, classification task},
	notes_te = {include, classification, spotify},
	number = {2},
	pages = {168 -- 181},
	paradigm = {classification},
	participant_expertise = {not specified},
	participant_n = {25},
	participant_origin = {online},
	participant_sampling = {not specified},
	participant_task = {classification},
	publication_stage = {Final},
	source = {scopus},
	stimulus_duration = {150 approx},
	stimulus_duration_unit = {s},
	stimulus_genre = {multi},
	stimulus_n = {5192; 12 per user in user validation (not included here due to little information), AcousticBrainz validation: 60000},
	stimulus_type = {Spotify},
	title = {RIADA: A Machine-Learning Based Infrastructure for Recognising the Emotions of Spotify Songs},
	type = {Article},
	url = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85147947463&doi = 10.9781},
	volume = {8},
	year = {2023},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid%20=%202-s2.0-85147947463&doi%20=%2010.9781},
	bdsk-url-2 = {https://doi.org/10.9781/ijimai.2022.04.002}}

@article{fu2010survey,
	author = {Fu, Zhouyu and Lu, Guojun and Ting, Kai Ming and Zhang, Dengsheng},
	date-added = {2024-06-18 08:51:34 +0100},
	date-modified = {2024-06-18 08:53:52 +0100},
	journal = {IEEE Transactions on Multimedia},
	number = {2},
	pages = {303--319},
	title = {A survey of audio-based music classification and annotation},
	volume = {13},
	year = {2010}}

@article{chicco2020advantages,
	author = {Chicco, Davide and Jurman, Giuseppe},
	date-added = {2024-06-18 08:37:28 +0100},
	date-modified = {2024-06-18 08:37:28 +0100},
	journal = {BMC genomics},
	pages = {1--13},
	publisher = {Springer},
	title = {The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation},
	volume = {21},
	year = {2020}}

@article{sarkar2020recognition,
	author = {Sarkar, Rajib and Choudhury, Sombuddha and Dutta, Saikat and Roy, Aneek and Saha, Sanjoy Kumar},
	date-added = {2024-06-18 08:28:19 +0100},
	date-modified = {2024-06-18 08:28:19 +0100},
	journal = {Multimedia Tools and Applications},
	number = {1},
	pages = {765--783},
	publisher = {Springer},
	title = {Recognition of emotion in music based on deep convolutional neural network},
	volume = {79},
	year = {2020}}

@article{er2019music,
	author = {Er, Mehmet Bilal and Aydilek, Ibrahim Berkan},
	date-added = {2024-06-18 08:27:33 +0100},
	date-modified = {2024-06-18 08:27:33 +0100},
	journal = {International Journal of Computational Intelligence Systems},
	number = {2},
	pages = {1622--1634},
	publisher = {Springer},
	title = {Music emotion recognition by using chroma spectrogram and deep visual features},
	volume = {12},
	year = {2019}}

@article{eerola2011c,
	author = {Eerola, T.},
	date-added = {2024-06-18 08:26:30 +0100},
	date-modified = {2024-06-18 08:26:30 +0100},
	journal = {Journal of New Music Research},
	keywords = {Genre, Emotion, Modelling, Acoustic feature},
	number = {4},
	pages = {349-366},
	title = {Are the emotions expressed in music genre-specific? An audio-based evaluation of datasets spanning classical, film, pop and mixed genres},
	url = {https://doi.org/10.1080/09298215.2011.602195},
	volume = {40},
	year = {2011},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfED8uLi8uLi8uLi8uLi9tZXJpdHMvUHVibGljYXRpb25zL2RhdGFiYXNlL1BhcGVycy9FZXJvbGEyMDExYy5wZGZPEQRYYm9va1gEAAAAAAQQMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAVAMAAAUAAAABAQAAVXNlcnMAAAAGAAAAAQEAAGxxYm43MwAABwAAAAEBAABEcm9wYm94AAYAAAABAQAAbWVyaXRzAAAMAAAAAQEAAFB1YmxpY2F0aW9ucwgAAAABAQAAZGF0YWJhc2UGAAAAAQEAAFBhcGVycwAADwAAAAEBAABFZXJvbGEyMDExYy5wZGYAIAAAAAEGAAAEAAAAFAAAACQAAAA0AAAARAAAAFgAAABoAAAAeAAAAAgAAAAEAwAAFV0AAAAAAAAIAAAABAMAAEAFBAAAAAAACAAAAAQDAAAlNssAAAAAAAgAAAAEAwAAkerMAAAAAAAIAAAABAMAACnrzAAAAAAACAAAAAQDAACw98wAAAAAAAgAAAAEAwAAwvfMAAAAAAAIAAAABAMAAOT6zAAAAAAAIAAAAAEGAAC4AAAAyAAAANgAAADoAAAA+AAAAAgBAAAYAQAAKAEAAAgAAAAABAAAQbSzdboAAAAYAAAAAQIAAAEAAAAAAAAADwAAAAAAAAAAAAAAAAAAAAgAAAAEAwAABgAAAAAAAAAEAAAAAwMAAPcBAAAIAAAAAQkAAGZpbGU6Ly8vDAAAAAEBAABNYWNpbnRvc2ggSEQIAAAABAMAAABQoRtzAAAACAAAAAAEAABBxfUErAAAACQAAAABAQAAMjhDOEI0RjktNzc1OC00NUFELUE1NTYtREU0MDJFNEU2MzcwGAAAAAECAACBAAAAAQAAAO8TAAABAAAAAAAAAAAAAAABAAAAAQEAAC8AAAAAAAAAAQUAAPsAAAABAgAAMDg2MjMwN2NjNzFjNTQ4MTgzZDVjNTEwNjk5ZTEyNmFjMDJiODgwYTkwYWZiM2E5OTM4MDYwMTFkYzM0NWE3NTswMDswMDAwMDAwMDswMDAwMDAwMDswMDAwMDAwMDswMDAwMDAwMDAwMDAwMDIwO2NvbS5hcHBsZS5hcHAtc2FuZGJveC5yZWFkLXdyaXRlOzAxOzAxMDAwMDBmOzAwMDAwMDAwMDBjY2ZhZTQ7MDE7L3VzZXJzL2xxYm43My9kcm9wYm94L21lcml0cy9wdWJsaWNhdGlvbnMvZGF0YWJhc2UvcGFwZXJzL2Vlcm9sYTIwMTFjLnBkZgAAzAAAAP7///8BAAAAAAAAABAAAAAEEAAAkAAAAAAAAAAFEAAAOAEAAAAAAAAQEAAAcAEAAAAAAABAEAAAYAEAAAAAAAACIAAAPAIAAAAAAAAFIAAArAEAAAAAAAAQIAAAvAEAAAAAAAARIAAA8AEAAAAAAAASIAAA0AEAAAAAAAATIAAA4AEAAAAAAAAgIAAAHAIAAAAAAAAwIAAASAIAAAAAAAABwAAAkAEAAAAAAAARwAAAFAAAAAAAAAASwAAAoAEAAAAAAACA8AAAUAIAAAAAAAAACAANABoAIwBlAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABME=},
	bdsk-url-1 = {https://doi.org/10.1080/09298215.2011.602195}}

@article{lindstrom2003expressivity,
	author = {Lindstr{\"o}m, Erik and Juslin, Patrik N and Bresin, Roberto and Williamon, Aaron},
	date-added = {2024-06-18 08:26:04 +0100},
	date-modified = {2024-06-18 08:26:04 +0100},
	journal = {Research Studies in Music Education},
	number = {1},
	pages = {23--47},
	publisher = {Sage Publications Sage UK: London, England},
	title = {"Expressivity comes from within your soul'': A questionnaire study of music students' perspectives on expressivity},
	volume = {20},
	year = {2003}}

@article{Russell1980,
	author = {J. A. Russell},
	date-added = {2024-06-18 08:20:59 +0100},
	date-modified = {2024-06-18 08:20:59 +0100},
	journal = {Journal of Personality and Social Psychology},
	number = {6},
	pages = {1161-1178},
	title = {A circumplex model of affect},
	volume = {39},
	year = {1980}}

@article{yang2008,
	author = {Yang, Y.H. and Lin, Y.C. and Su, Y.F. and Chen, H.H.},
	date-added = {2024-06-18 08:20:17 +0100},
	date-modified = {2024-06-18 08:20:17 +0100},
	emotion = {valence and arousal, dimensional, perceived},
	journal = {IEEE Transactions on Audio Speech and Language Processing},
	method = {self-report, computational, music analytic},
	number = {2},
	pages = {448-457},
	publisher = {IEEE},
	rating = {5},
	read = {Yes},
	sample = {N=253 students},
	stimuli = {N=195 popular songs, length 25 s, researcher chosen, real},
	title = {{A regression approach to music emotion recognition}},
	volume = {16},
	year = {2008}}

@inproceedings{barthet2013,
	abstract = {The striking ability of music to elicit emotions assures its prominent status in human culture and every day life. Music is often enjoyed and sought for its ability to induce or convey emotions, which may manifest in anything from a slight variation in mood, to changes in our physical condition and actions. Consequently, research on how we might associate musical pieces with emotions and, more generally, how music brings about an emotional response is attracting ever increasing attention. First, this paper provides a thorough review of studies on the relation of music and emotions from different disciplines. We then propose new insights to enhance automated music emotion recognition models using recent results from psychology, musicology, affective computing, semantic technologies and music information retrieval.},
	address = {Berlin, Heidelberg},
	author = {Barthet, Mathieu and Fazekas, Gy{\"o}rgy and Sandler, Mark},
	booktitle = {From Sounds to Music and Emotions},
	date-added = {2024-06-18 08:19:33 +0100},
	date-modified = {2024-06-18 08:19:33 +0100},
	editor = {Aramaki, Mitsuko and Barthet, Mathieu and Kronland-Martinet, Richard and Ystad, S{\o}lvi},
	isbn = {978-3-642-41248-6},
	pages = {228--252},
	publisher = {Springer Berlin Heidelberg},
	title = {Music Emotion Recognition: From Content- to Context-Based Models},
	year = {2013}}

@inproceedings{panda2013multi,
	author = {Panda, R. and Malheiro, Ricardo and Rocha, Bruno and Oliveira, Ant{\'o}nio Pedro and Paiva, Rui Pedro},
	booktitle = {10th International symposium on computer music multidisciplinary research (CMMR 2013)},
	date-added = {2024-06-18 08:19:27 +0100},
	date-modified = {2024-06-18 08:49:15 +0100},
	pages = {570--582},
	title = {Multi-modal music emotion recognition: A new dataset, methodology and comparative analysis},
	year = {2013}}

@article{eerola_friberg_bresin_2013,
	author = {Eerola, T. and Friberg, A. and Bresin, R.},
	date-added = {2024-06-18 08:19:19 +0100},
	date-modified = {2024-06-18 08:48:23 +0100},
	doi = {10.3389/fpsyg.2013.00487},
	issn = {1664-1078},
	journal = {Frontiers in Psychology},
	keywords = {Modelling, Emotion},
	number = {487},
	title = {Emotional Expression in Music: Contribution, Linearity, and Additivity of Primary Musical Cues},
	url = {http://www.frontiersin.org/emotion_science/10.3389/fpsyg.2013.00487/abstract},
	volume = {4},
	year = {2013},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEEAuLi8uLi8uLi8uLi9yZXNlYXJjaC9iaWJsaW9ncmFwaHkvYmliZGVzay9QYXBlcnMvZWVyb2xhMjAxM2EucGRmTxEEYGJvb2tgBAAAAAAEEDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFwDAAAFAAAAAQEAAFVzZXJzAAAABgAAAAEBAABscWJuNzMAAAkAAAABAQAARG9jdW1lbnRzAAAACAAAAAEBAAByZXNlYXJjaAwAAAABAQAAYmlibGlvZ3JhcGh5BwAAAAEBAABiaWJkZXNrAAYAAAABAQAAUGFwZXJzAAAPAAAAAQEAAGVlcm9sYTIwMTNhLnBkZgAgAAAAAQYAAAQAAAAUAAAAJAAAADgAAABIAAAAXAAAAGwAAAB8AAAACAAAAAQDAAAVXQAAAAAAAAgAAAAEAwAAQAUEAAAAAAAIAAAABAMAAKpe9AAAAAAACAAAAAQDAABpLNEAAAAAAAgAAAAEAwAAlSzRAAAAAAAIAAAABAMAAGMu0QAAAAAACAAAAAQDAACGLtEAAAAAAAgAAAAEAwAA9kDRAAAAAAAgAAAAAQYAALwAAADMAAAA3AAAAOwAAAD8AAAADAEAABwBAAAsAQAACAAAAAAEAABBt6hPpQAAABgAAAABAgAAAQAAAAAAAAAPAAAAAAAAAAAAAAAAAAAACAAAAAQDAAAGAAAAAAAAAAQAAAADAwAA9wEAAAgAAAABCQAAZmlsZTovLy8MAAAAAQEAAE1hY2ludG9zaCBIRAgAAAAEAwAAAFChG3MAAAAIAAAAAAQAAEHGeDvVAAAAJAAAAAEBAAAyOEM4QjRGOS03NzU4LTQ1QUQtQTU1Ni1ERTQwMkU0RTYzNzAYAAAAAQIAAIEAAAABAAAA7xMAAAEAAAAAAAAAAAAAAAEAAAABAQAALwAAAAAAAAABBQAA/gAAAAECAAA0OThkNDE1NDJmZWNiYzY2YjZjMDZkMDZmNzAwMzA2YTA1NTZkOGJhOWQ5ZGFmZDkyMGM3MzhkZDc4NjViYzZmOzAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwMDAwMDAwMjA7Y29tLmFwcGxlLmFwcC1zYW5kYm94LnJlYWQtd3JpdGU7MDE7MDEwMDAwMGY7MDAwMDAwMDAwMGQxNDBmNjs1OTsvdXNlcnMvbHFibjczL2RvY3VtZW50cy9yZXNlYXJjaC9iaWJsaW9ncmFwaHkvYmliZGVzay9wYXBlcnMvZWVyb2xhMjAxM2EucGRmAAAAzAAAAP7///8BAAAAAAAAABAAAAAEEAAAlAAAAAAAAAAFEAAAPAEAAAAAAAAQEAAAdAEAAAAAAABAEAAAZAEAAAAAAAACIAAAQAIAAAAAAAAFIAAAsAEAAAAAAAAQIAAAwAEAAAAAAAARIAAA9AEAAAAAAAASIAAA1AEAAAAAAAATIAAA5AEAAAAAAAAgIAAAIAIAAAAAAAAwIAAATAIAAAAAAAABwAAAlAEAAAAAAAARwAAAFAAAAAAAAAASwAAApAEAAAAAAACA8AAAVAIAAAAAAAAACAANABoAIwBmAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABMo=},
	bdsk-url-1 = {http://www.frontiersin.org/emotion_science/10.3389/fpsyg.2013.00487/abstract},
	bdsk-url-2 = {https://doi.org/10.3389/fpsyg.2013.00487}}

@inproceedings{soleymani2013,
	abstract = {Music is composed to be emotionally expressive, and emotional associations provide an especially natural domain for indexing and recommendation in today's vast digital music libraries. But such libraries require powerful automated tools, and the development of systems for automatic prediction of musical emotion presents a myriad challenges. The perceptual nature of musical emotion necessitates the collection of data from human subjects. The interpretation of emotion varies between listeners thus each clip needs to be annotated by a distribution of subjects. In addition, the sharing of large music content libraries for the development of such systems, even for academic research, presents complicated legal issues which vary by country. This work presents a new publicly available dataset for music emotion recognition research and a baseline system. In addressing the difficulties of emotion annotation we have turned to crowdsourcing, using Amazon Mechanical Turk, and have developed a two-stage procedure for filtering out poor quality workers. The dataset consists entirely of creative commons music from the Free Music Archive, which as the name suggests, can be shared freely without penalty. The final dataset contains 1000 songs, each annotated by a minimum of 10 subjects, which is larger than many currently available music emotion dataset.},
	address = {New York, NY, USA},
	author = {Soleymani, Mohammad and Caro, Micheal N. and Schmidt, Erik M. and Sha, Cheng-Ya and Yang, Yi-Hsuan},
	booktitle = {Proceedings of the 2nd ACM International Workshop on Crowdsourcing for Multimedia},
	date-added = {2024-06-18 08:19:10 +0100},
	date-modified = {2024-06-18 08:19:10 +0100},
	doi = {10.1145/2506364.2506365},
	isbn = {9781450323963},
	keywords = {emotion, crowdsourcing, music},
	location = {Barcelona, Spain},
	numpages = {6},
	pages = {1--6},
	publisher = {Association for Computing Machinery},
	series = {CrowdMM '13},
	title = {1000 Songs for Emotional Analysis of Music},
	url = {https://doi.org/10.1145/2506364.2506365},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1145/2506364.2506365}}

@article{saari_et_al_2015,
	author = {Saari, P. and Eerola, T. and Barthet, M. and Fazekas, G. and Lartillot, O.},
	date-added = {2024-06-18 08:19:00 +0100},
	date-modified = {2024-06-18 08:19:00 +0100},
	journal = {IEEE Transactions on Affective Computing},
	number = {2},
	pages = {122-135},
	title = {Genre-adaptive Semantic Computing and Audio-based Modelling for Music Mood Annotation},
	volume = {7},
	year = {2015},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEEUuLi8uLi8uLi8uLi9yZXNlYXJjaC9iaWJsaW9ncmFwaHkvYmliZGVzay9QYXBlcnMvU2FhcmlfZXRfYWxfMjAxNS5wZGZPEQRoYm9va2gEAAAAAAQQMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAZAMAAAUAAAABAQAAVXNlcnMAAAAGAAAAAQEAAGxxYm43MwAACQAAAAEBAABEb2N1bWVudHMAAAAIAAAAAQEAAHJlc2VhcmNoDAAAAAEBAABiaWJsaW9ncmFwaHkHAAAAAQEAAGJpYmRlc2sABgAAAAEBAABQYXBlcnMAABQAAAABAQAAU2FhcmlfZXRfYWxfMjAxNS5wZGYgAAAAAQYAAAQAAAAUAAAAJAAAADgAAABIAAAAXAAAAGwAAAB8AAAACAAAAAQDAAAVXQAAAAAAAAgAAAAEAwAAQAUEAAAAAAAIAAAABAMAAKpe9AAAAAAACAAAAAQDAABpLNEAAAAAAAgAAAAEAwAAlSzRAAAAAAAIAAAABAMAAGMu0QAAAAAACAAAAAQDAACGLtEAAAAAAAgAAAAEAwAAWTvRAAAAAAAgAAAAAQYAAMAAAADQAAAA4AAAAPAAAAAAAQAAEAEAACABAAAwAQAACAAAAAAEAABBu4sPewAAABgAAAABAgAAAQAAAAAAAAAPAAAAAAAAAAAAAAAAAAAACAAAAAQDAAAGAAAAAAAAAAQAAAADAwAA9wEAAAgAAAABCQAAZmlsZTovLy8MAAAAAQEAAE1hY2ludG9zaCBIRAgAAAAEAwAAAFChG3MAAAAIAAAAAAQAAEHGeDvVAAAAJAAAAAEBAAAyOEM4QjRGOS03NzU4LTQ1QUQtQTU1Ni1ERTQwMkU0RTYzNzAYAAAAAQIAAIEAAAABAAAA7xMAAAEAAAAAAAAAAAAAAAEAAAABAQAALwAAAAAAAAABBQAAAwEAAAECAAAyMzMwZDc3MGEyOWQ3MzFjODEyODNjOTRjNzU1ZTM4ZmIyOGI5MDFmZDY4YzBmOTRhMzgzYWRmZThlMmMyYjE1OzAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwMDAwMDAwMjA7Y29tLmFwcGxlLmFwcC1zYW5kYm94LnJlYWQtd3JpdGU7MDE7MDEwMDAwMGY7MDAwMDAwMDAwMGQxM2I1OTs1OTsvdXNlcnMvbHFibjczL2RvY3VtZW50cy9yZXNlYXJjaC9iaWJsaW9ncmFwaHkvYmliZGVzay9wYXBlcnMvc2FhcmlfZXRfYWxfMjAxNS5wZGYAAMwAAAD+////AQAAAAAAAAAQAAAABBAAAJgAAAAAAAAABRAAAEABAAAAAAAAEBAAAHgBAAAAAAAAQBAAAGgBAAAAAAAAAiAAAEQCAAAAAAAABSAAALQBAAAAAAAAECAAAMQBAAAAAAAAESAAAPgBAAAAAAAAEiAAANgBAAAAAAAAEyAAAOgBAAAAAAAAICAAACQCAAAAAAAAMCAAAFACAAAAAAAAAcAAAJgBAAAAAAAAEcAAABQAAAAAAAAAEsAAAKgBAAAAAAAAgPAAAFgCAAAAAAAAAAgADQAaACMAawAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAATX}}

@article{aljanaki2017developing,
	author = {Aljanaki, Anna and Yang, Yi-Hsuan and Soleymani, Mohammad},
	date-added = {2024-06-18 08:18:51 +0100},
	date-modified = {2024-06-18 08:18:51 +0100},
	journal = {PloS one},
	number = {3},
	pages = {e0173392},
	publisher = {Public Library of Science San Francisco, CA USA},
	title = {Developing a benchmark for emotional analysis of music},
	volume = {12},
	year = {2017}}

@article{cowen2020music,
	author = {Cowen, Alan S and Fang, Xia and Sauter, Disa and Keltner, Dacher},
	date-added = {2024-06-18 08:18:37 +0100},
	date-modified = {2024-06-18 08:18:37 +0100},
	journal = {Proceedings of the National Academy of Sciences},
	number = {4},
	pages = {1924--1934},
	publisher = {National Acad Sciences},
	title = {What music makes us feel: At least 13 dimensions organize subjective experiences associated with music across different cultures},
	volume = {117},
	year = {2020}}

@article{grimaud_eerola_2022,
	author = {Grimaud, Annaliese Micallef and Eerola, T.},
	date-added = {2024-06-18 08:18:16 +0100},
	date-modified = {2024-06-18 08:18:16 +0100},
	doi = {https://doi.org/10.1177/20592043211061745},
	journal = {Music \& Science},
	keywords = {RL},
	pages = {1-23},
	status = {published},
	title = {An Interactive Approach to Emotional Expression through Musical Cues},
	volume = {5},
	year = {2022},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEEYuLi8uLi8uLi8uLi9yZXNlYXJjaC9iaWJsaW9ncmFwaHkvYmliZGVzay9QYXBlcnMvMjA1OTIwNDMyMTEwNjE3NDUucGRmTxEEbGJvb2tsBAAAAAAEEDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGgDAAAFAAAAAQEAAFVzZXJzAAAABgAAAAEBAABscWJuNzMAAAkAAAABAQAARG9jdW1lbnRzAAAACAAAAAEBAAByZXNlYXJjaAwAAAABAQAAYmlibGlvZ3JhcGh5BwAAAAEBAABiaWJkZXNrAAYAAAABAQAAUGFwZXJzAAAVAAAAAQEAADIwNTkyMDQzMjExMDYxNzQ1LnBkZgAAACAAAAABBgAABAAAABQAAAAkAAAAOAAAAEgAAABcAAAAbAAAAHwAAAAIAAAABAMAABVdAAAAAAAACAAAAAQDAABABQQAAAAAAAgAAAAEAwAAql70AAAAAAAIAAAABAMAAGks0QAAAAAACAAAAAQDAACVLNEAAAAAAAgAAAAEAwAAYy7RAAAAAAAIAAAABAMAAIYu0QAAAAAACAAAAAQDAAC1L9EAAAAAACAAAAABBgAAxAAAANQAAADkAAAA9AAAAAQBAAAUAQAAJAEAADQBAAAIAAAAAAQAAEHDyLm9AAAAGAAAAAECAAABAAAAAAAAAA8AAAAAAAAAAAAAAAAAAAAIAAAABAMAAAYAAAAAAAAABAAAAAMDAAD3AQAACAAAAAEJAABmaWxlOi8vLwwAAAABAQAATWFjaW50b3NoIEhECAAAAAQDAAAAUKEbcwAAAAgAAAAABAAAQcZ4O9UAAAAkAAAAAQEAADI4QzhCNEY5LTc3NTgtNDVBRC1BNTU2LURFNDAyRTRFNjM3MBgAAAABAgAAgQAAAAEAAADvEwAAAQAAAAAAAAAAAAAAAQAAAAEBAAAvAAAAAAAAAAEFAAAEAQAAAQIAADUyYWM1Yzg1YTFkNGY3MWMxMWM3MjQzZGI2NzcxZWFjMzU5YmNhZGIyMGEyZGI4NDYzZjMyNTI0YWNkYTRjNTg7MDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDAwMDAwMDAyMDtjb20uYXBwbGUuYXBwLXNhbmRib3gucmVhZC13cml0ZTswMTswMTAwMDAwZjswMDAwMDAwMDAwZDEyZmI1OzU5Oy91c2Vycy9scWJuNzMvZG9jdW1lbnRzL3Jlc2VhcmNoL2JpYmxpb2dyYXBoeS9iaWJkZXNrL3BhcGVycy8yMDU5MjA0MzIxMTA2MTc0NS5wZGYAzAAAAP7///8BAAAAAAAAABAAAAAEEAAAnAAAAAAAAAAFEAAARAEAAAAAAAAQEAAAfAEAAAAAAABAEAAAbAEAAAAAAAACIAAASAIAAAAAAAAFIAAAuAEAAAAAAAAQIAAAyAEAAAAAAAARIAAA/AEAAAAAAAASIAAA3AEAAAAAAAATIAAA7AEAAAAAAAAgIAAAKAIAAAAAAAAwIAAAVAIAAAAAAAABwAAAnAEAAAAAAAARwAAAFAAAAAAAAAASwAAArAEAAAAAAACA8AAAXAIAAAAAAAAACAANABoAIwBsAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABNw=},
	bdsk-url-1 = {https://doi.org/10.1177/20592043211061745}}

@article{panda2020audio,
	author = {Panda, R. and Malheiro, Ricardo and Paiva, Rui Pedro},
	date-added = {2024-06-18 08:18:07 +0100},
	date-modified = {2024-06-18 08:49:04 +0100},
	doi = {10.1109/TAFFC.2020.3032373},
	journal = {IEEE Transactions on Affective Computing},
	number = {1},
	pages = {68-88},
	title = {Audio Features for Music Emotion Recognition: A Survey},
	volume = {14},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TAFFC.2020.3032373}}

@article{russell2003core,
	author = {Russell, James A},
	date-added = {2024-06-18 08:17:01 +0100},
	date-modified = {2024-06-18 08:17:01 +0100},
	journal = {Psychological Review},
	number = {1},
	pages = {145-172},
	publisher = {American Psychological Association},
	title = {Core affect and the psychological construction of emotion},
	volume = {110},
	year = {2003}}

@article{eerola_2012,
	author = {Eerola, T. and Vuoskoski, J. K.},
	date-added = {2024-06-18 08:15:55 +0100},
	date-modified = {2024-06-18 08:15:55 +0100},
	doi = {10.1525/mp.2012.30.3.307},
	journal = {Music Perception},
	keywords = {Emotion},
	number = {3},
	pages = {307-340},
	title = {A review of music and emotion studies: Approaches, emotion models and stimuli},
	url = {https://doi.org/10.1525/mp.2012.30.3.307},
	volume = {30},
	year = {2012},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEEAuLi8uLi8uLi8uLi9yZXNlYXJjaC9iaWJsaW9ncmFwaHkvYmliZGVzay9QYXBlcnMvRWVyb2xhMjAxMmMucGRmTxEEYGJvb2tgBAAAAAAEEDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFwDAAAFAAAAAQEAAFVzZXJzAAAABgAAAAEBAABscWJuNzMAAAkAAAABAQAARG9jdW1lbnRzAAAACAAAAAEBAAByZXNlYXJjaAwAAAABAQAAYmlibGlvZ3JhcGh5BwAAAAEBAABiaWJkZXNrAAYAAAABAQAAUGFwZXJzAAAPAAAAAQEAAEVlcm9sYTIwMTJjLnBkZgAgAAAAAQYAAAQAAAAUAAAAJAAAADgAAABIAAAAXAAAAGwAAAB8AAAACAAAAAQDAAAVXQAAAAAAAAgAAAAEAwAAQAUEAAAAAAAIAAAABAMAAKpe9AAAAAAACAAAAAQDAABpLNEAAAAAAAgAAAAEAwAAlSzRAAAAAAAIAAAABAMAAGMu0QAAAAAACAAAAAQDAACGLtEAAAAAAAgAAAAEAwAAWzPRAAAAAAAgAAAAAQYAALwAAADMAAAA3AAAAOwAAAD8AAAADAEAABwBAAAsAQAACAAAAAAEAABBtq8tbAAAABgAAAABAgAAAQAAAAAAAAAPAAAAAAAAAAAAAAAAAAAACAAAAAQDAAAGAAAAAAAAAAQAAAADAwAA9wEAAAgAAAABCQAAZmlsZTovLy8MAAAAAQEAAE1hY2ludG9zaCBIRAgAAAAEAwAAAFChG3MAAAAIAAAAAAQAAEHGeDvVAAAAJAAAAAEBAAAyOEM4QjRGOS03NzU4LTQ1QUQtQTU1Ni1ERTQwMkU0RTYzNzAYAAAAAQIAAIEAAAABAAAA7xMAAAEAAAAAAAAAAAAAAAEAAAABAQAALwAAAAAAAAABBQAA/gAAAAECAAA5ZjMwYmVkYjg2NmQ1ZDYyMTFiZTkxZWRkZjAyM2Y5MzFlZDlkZDk3OWE0ZTY3MTcxMjE5Zjk4NDViNTU0NzFhOzAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwMDAwMDAwMjA7Y29tLmFwcGxlLmFwcC1zYW5kYm94LnJlYWQtd3JpdGU7MDE7MDEwMDAwMGY7MDAwMDAwMDAwMGQxMzM1Yjs1OTsvdXNlcnMvbHFibjczL2RvY3VtZW50cy9yZXNlYXJjaC9iaWJsaW9ncmFwaHkvYmliZGVzay9wYXBlcnMvZWVyb2xhMjAxMmMucGRmAAAAzAAAAP7///8BAAAAAAAAABAAAAAEEAAAlAAAAAAAAAAFEAAAPAEAAAAAAAAQEAAAdAEAAAAAAABAEAAAZAEAAAAAAAACIAAAQAIAAAAAAAAFIAAAsAEAAAAAAAAQIAAAwAEAAAAAAAARIAAA9AEAAAAAAAASIAAA1AEAAAAAAAATIAAA5AEAAAAAAAAgIAAAIAIAAAAAAAAwIAAATAIAAAAAAAABwAAAlAEAAAAAAAARwAAAFAAAAAAAAAASwAAApAEAAAAAAACA8AAAVAIAAAAAAAAACAANABoAIwBmAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABMo=},
	bdsk-url-1 = {https://doi.org/10.1525/mp.2012.30.3.307}}

@article{juslin2014makes,
	author = {Juslin, P. N. and Harmat, L{\'a}szl{\'o} and Eerola, T.},
	date-added = {2024-06-18 08:15:38 +0100},
	date-modified = {2024-06-18 08:48:09 +0100},
	journal = {Psychology of Music},
	keywords = {affect, expectancy, mechanisms, BRECVEMAC, psychophysiology},
	number = {4},
	pages = {599--623},
	publisher = {Sage Publications Sage UK: London, England},
	title = {What makes music emotionally significant? Exploring the underlying mechanisms},
	volume = {42},
	year = {2014}}

@article{juslin_et_al_2014,
	abstract = {A common approach to studying emotional reactions to music is to attempt to obtain direct links between musical surface features such as tempo and a listener's responses. however, such an analysis ultimately fails to explain why emotions are aroused in the listener. in this article we explore an alternative approach, which aims to account for musical emotions in terms of a set of psychological mechanisms that are activated by different types of information in a musical event. this approach was tested in 4 experiments that manipulated 4 mechanisms (brain stem reflex, contagion, episodic memory, musical expectancy) by selecting existing musical pieces that featured information relevant for each mechanism. the excerpts were played to 60 listeners, who were asked to rate their felt emotions on 15 scales. skin conductance levels and facial expressions were measured, and listeners reported subjective impressions of relevance to specific mechanisms. results indicated that the target mechanism conditions evoked emotions largely as predicted by a multimechanism framework and that mostly similar effects occurred across the experiments that included different pieces of music. we conclude that a satisfactory account of musical emotions requires consideration of how musical features and responses are mediated by a range of underlying mechanisms.},
	author = {Juslin, P. N. and Barradas, G. and Eerola, T.},
	date-added = {2024-06-18 08:14:59 +0100},
	date-modified = {2024-06-18 08:14:59 +0100},
	doi = {10.5406/amerjpsyc.128.3.0281},
	journal = {American Journal of Psychology},
	keywords = {emotion},
	number = {3},
	pages = {281-304},
	title = {From Sound to Significance: Exploring the Mechanisms Underlying Emotional Reactions to Music},
	url = {https://doi.org/10.5406/amerjpsyc.128.3.0281},
	volume = {128},
	year = {2015},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEEYuLi8uLi8uLi8uLi9yZXNlYXJjaC9iaWJsaW9ncmFwaHkvYmliZGVzay9QYXBlcnMvSnVzbGluX2V0X2FsXzIwMTUucGRmTxEEbGJvb2tsBAAAAAAEEDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGgDAAAFAAAAAQEAAFVzZXJzAAAABgAAAAEBAABscWJuNzMAAAkAAAABAQAARG9jdW1lbnRzAAAACAAAAAEBAAByZXNlYXJjaAwAAAABAQAAYmlibGlvZ3JhcGh5BwAAAAEBAABiaWJkZXNrAAYAAAABAQAAUGFwZXJzAAAVAAAAAQEAAEp1c2xpbl9ldF9hbF8yMDE1LnBkZgAAACAAAAABBgAABAAAABQAAAAkAAAAOAAAAEgAAABcAAAAbAAAAHwAAAAIAAAABAMAABVdAAAAAAAACAAAAAQDAABABQQAAAAAAAgAAAAEAwAAql70AAAAAAAIAAAABAMAAGks0QAAAAAACAAAAAQDAACVLNEAAAAAAAgAAAAEAwAAYy7RAAAAAAAIAAAABAMAAIYu0QAAAAAACAAAAAQDAAC8ONEAAAAAACAAAAABBgAAxAAAANQAAADkAAAA9AAAAAQBAAAUAQAAJAEAADQBAAAIAAAAAAQAAEG7ixPNAAAAGAAAAAECAAABAAAAAAAAAA8AAAAAAAAAAAAAAAAAAAAIAAAABAMAAAYAAAAAAAAABAAAAAMDAAD3AQAACAAAAAEJAABmaWxlOi8vLwwAAAABAQAATWFjaW50b3NoIEhECAAAAAQDAAAAUKEbcwAAAAgAAAAABAAAQcZ4O9UAAAAkAAAAAQEAADI4QzhCNEY5LTc3NTgtNDVBRC1BNTU2LURFNDAyRTRFNjM3MBgAAAABAgAAgQAAAAEAAADvEwAAAQAAAAAAAAAAAAAAAQAAAAEBAAAvAAAAAAAAAAEFAAAEAQAAAQIAADhmNGZkMDBiMDYzODdlNGYxNGMzMjJiYzU5YmM2MzJjZTEwNmI1MzZhNGY5ZWI3NWRlYzZhNWMyMDM5ODI2OWM7MDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDAwMDAwMDAyMDtjb20uYXBwbGUuYXBwLXNhbmRib3gucmVhZC13cml0ZTswMTswMTAwMDAwZjswMDAwMDAwMDAwZDEzOGJjOzU5Oy91c2Vycy9scWJuNzMvZG9jdW1lbnRzL3Jlc2VhcmNoL2JpYmxpb2dyYXBoeS9iaWJkZXNrL3BhcGVycy9qdXNsaW5fZXRfYWxfMjAxNS5wZGYAzAAAAP7///8BAAAAAAAAABAAAAAEEAAAnAAAAAAAAAAFEAAARAEAAAAAAAAQEAAAfAEAAAAAAABAEAAAbAEAAAAAAAACIAAASAIAAAAAAAAFIAAAuAEAAAAAAAAQIAAAyAEAAAAAAAARIAAA/AEAAAAAAAASIAAA3AEAAAAAAAATIAAA7AEAAAAAAAAgIAAAKAIAAAAAAAAwIAAAVAIAAAAAAAABwAAAnAEAAAAAAAARwAAAFAAAAAAAAAASwAAArAEAAAAAAACA8AAAXAIAAAAAAAAACAANABoAIwBsAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABNw=},
	bdsk-url-1 = {https://doi.org/10.5406/amerjpsyc.128.3.0281}}

@article{Mehr2019universality,
	abstract = {It is unclear whether there are universal patterns to music across cultures. Mehr et al. examined ethnographic data and observed music in every society sampled (see the Perspective by Fitch and Popescu). For songs specifically, three dimensions characterize more than 25\% of the performances studied: formality of the performance, arousal level, and religiosity. There is more variation in musical behavior within societies than between societies, and societies show similar levels of within-society variation in musical behavior. At the same time, one-third of societies significantly differ from average for any given dimension, and half of all societies differ from average on at least one dimension, indicating variability across cultures.Science, this issue p. eaax0868; see also p. 944INTRODUCTIONMusic is often assumed to be a human universal, emerging from an evolutionary adaptation specific to music and/or a by-product of adaptations for affect, language, motor control, and auditory perception. But universality has never actually been systematically demonstrated, and it is challenged by the vast diversity of music across cultures. Hypotheses of the evolutionary function of music are also untestable without comprehensive and representative data on its forms and behavioral contexts across societies.RATIONALEWe conducted a natural history of song: a systematic analysis of the features of vocal music found worldwide. It consists of a corpus of ethnographic text on musical behavior from a representative sample of mostly small-scale societies, and a discography of audio recordings of the music itself. We then applied tools of computational social science, which minimize the influence of sampling error and other biases, to answer six questions. Does music appear universally? What kinds of behavior are associated with song, and how do they vary among societies? Are the musical features of a song indicative of its behavioral context (e.g., infant care)? Do the melodic and rhythmic patterns of songs vary systematically, like those patterns found in language? And how prevalent is tonality across musical idioms?RESULTSAnalysis of the ethnography corpus shows that music appears in every society observed; that variation in song events is well characterized by three dimensions (formality, arousal, religiosity); that musical behavior varies more within societies than across them on these dimensions; and that music is regularly associated with behavioral contexts such as infant care, healing, dance, and love. Analysis of the discography corpus shows that identifiable acoustic features of songs (accent, tempo, pitch range, etc.) predict their primary behavioral context (love, healing, etc.); that musical forms vary along two dimensions (melodic and rhythmic complexity); that melodic and rhythmic bigrams fall into power-law distributions; and that tonality is widespread, perhaps universal.CONCLUSIONMusic is in fact universal: It exists in every society (both with and without words), varies more within than between societies, regularly supports certain types of behavior, and has acoustic features that are systematically related to the goals and responses of singers and listeners. But music is not a fixed biological response with a single prototypical adaptive function: It is produced worldwide in diverse behavioral contexts that vary in formality, arousal, and religiosity. Music does appear to be tied to specific perceptual, cognitive, and affective faculties, including language (all societies put words to their songs), motor control (people in all societies dance), auditory analysis (all musical systems have signatures of tonality), and aesthetics (their melodies and rhythms are balanced between monotony and chaos). These analyses show how applying the tools of computational social science to rich bodies of humanistic data can reveal both universal features and patterns of variability in culture, addressing long-standing debates about each.Studying world music systematically.We used primary ethnographic text and field recordings of song performances to build two richly annotated cross-cultural datasets: NHS Ethnography and NHS Discography. The original material in each dataset was annotated by humans (both amateur and expert) and by automated algorithms.What is universal about music, and what varies? We built a corpus of ethnographic text on musical behavior from a representative sample of the world{\textquoteright}s societies, as well as a discography of audio recordings. The ethnographic corpus reveals that music (including songs with words) appears in every society observed; that music varies along three dimensions (formality, arousal, religiosity), more within societies than across them; and that music is associated with certain behavioral contexts such as infant care, healing, dance, and love. The discography{\textemdash}analyzed through machine summaries, amateur and expert listener ratings, and manual transcriptions{\textemdash}reveals that acoustic features of songs predict their primary behavioral context; that tonality is widespread, perhaps universal; that music varies in rhythmic and melodic complexity; and that elements of melodies and rhythms found worldwide follow power laws.},
	author = {Mehr, Samuel A. and Singh, Manvir and Knox, Dean and Ketter, Daniel M. and Pickens-Jones, Daniel and Atwood, S. and Lucas, Christopher and Jacoby, Nori and Egner, Alena A. and Hopkins, Erin J. and Howard, Rhea M. and Hartshorne, Joshua K. and Jennings, Mariela V. and Simson, Jan and Bainbridge, Constance M. and Pinker, Steven and O{\textquoteright}Donnell, Timothy J. and Krasnow, Max M. and Glowacki, Luke},
	date-added = {2024-06-18 08:13:40 +0100},
	date-modified = {2024-06-18 08:13:40 +0100},
	doi = {10.1126/science.aax0868},
	elocation-id = {eaax0868},
	eprint = {https://science.sciencemag.org/content/366/6468/eaax0868.full.pdf},
	issn = {0036-8075},
	journal = {Science},
	number = {6468},
	publisher = {American Association for the Advancement of Science},
	title = {Universality and diversity in human song},
	url = {https://science.sciencemag.org/content/366/6468/eaax0868},
	volume = {366},
	year = {2019},
	bdsk-url-1 = {https://science.sciencemag.org/content/366/6468/eaax0868},
	bdsk-url-2 = {https://doi.org/10.1126/science.aax0868}}

@article{gomez2021,
	author = {G{\'o}mez-Ca{\~n}{\'o}n, Juan Sebasti{\'a}n and Cano, Estefan{\'\i}a and Eerola, T. and Herrera, Perfecto and Hu, Xiao and Yang, Yi-Hsuan and G{\'o}mez, Emilia},
	date-added = {2024-06-18 08:12:32 +0100},
	date-modified = {2024-06-18 08:12:32 +0100},
	doi = {10.1109/MSP.2021.3106232},
	journal = {IEEE Signal Processing Magazine},
	keywords = {RL},
	number = {6},
	pages = {106-114},
	status = {accepted},
	title = {Music Emotion Recognition: Toward new, robust standards in personalized and context-sensitive applications},
	volume = {38},
	year = {2021},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEE4uLi8uLi8uLi8uLi9yZXNlYXJjaC9iaWJsaW9ncmFwaHkvYmliZGVzay9QYXBlcnMvU1BNLU9jdC0yMDIwLTE4Ni5SMV9Qcm9vZi5wZGZPEQR8Ym9va3wEAAAAAAQQMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAeAMAAAUAAAABAQAAVXNlcnMAAAAGAAAAAQEAAGxxYm43MwAACQAAAAEBAABEb2N1bWVudHMAAAAIAAAAAQEAAHJlc2VhcmNoDAAAAAEBAABiaWJsaW9ncmFwaHkHAAAAAQEAAGJpYmRlc2sABgAAAAEBAABQYXBlcnMAAB0AAAABAQAAU1BNLU9jdC0yMDIwLTE4Ni5SMV9Qcm9vZi5wZGYAAAAgAAAAAQYAAAQAAAAUAAAAJAAAADgAAABIAAAAXAAAAGwAAAB8AAAACAAAAAQDAAAVXQAAAAAAAAgAAAAEAwAAQAUEAAAAAAAIAAAABAMAAKpe9AAAAAAACAAAAAQDAABpLNEAAAAAAAgAAAAEAwAAlSzRAAAAAAAIAAAABAMAAGMu0QAAAAAACAAAAAQDAACGLtEAAAAAAAgAAAAEAwAAAzvRAAAAAAAgAAAAAQYAAMwAAADcAAAA7AAAAPwAAAAMAQAAHAEAACwBAAA8AQAACAAAAAAEAABBwtwuzIAAABgAAAABAgAAAQAAAAAAAAAPAAAAAAAAAAAAAAAAAAAACAAAAAQDAAAGAAAAAAAAAAQAAAADAwAA9wEAAAgAAAABCQAAZmlsZTovLy8MAAAAAQEAAE1hY2ludG9zaCBIRAgAAAAEAwAAAFChG3MAAAAIAAAAAAQAAEHGeDvVAAAAJAAAAAEBAAAyOEM4QjRGOS03NzU4LTQ1QUQtQTU1Ni1ERTQwMkU0RTYzNzAYAAAAAQIAAIEAAAABAAAA7xMAAAEAAAAAAAAAAAAAAAEAAAABAQAALwAAAAAAAAABBQAADAEAAAECAAA2ZTBkMzllYTgzNWFjOGJkYzMwN2ZiZGM5Yjg5YmNiNTI5MWYxNDNjMmFhOTBhNjRmMTliNWU5MzU3OWY3ODdkOzAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwMDAwMDAwMjA7Y29tLmFwcGxlLmFwcC1zYW5kYm94LnJlYWQtd3JpdGU7MDE7MDEwMDAwMGY7MDAwMDAwMDAwMGQxM2IwMzs1OTsvdXNlcnMvbHFibjczL2RvY3VtZW50cy9yZXNlYXJjaC9iaWJsaW9ncmFwaHkvYmliZGVzay9wYXBlcnMvc3BtLW9jdC0yMDIwLTE4Ni5yMV9wcm9vZi5wZGYAzAAAAP7///8BAAAAAAAAABAAAAAEEAAApAAAAAAAAAAFEAAATAEAAAAAAAAQEAAAhAEAAAAAAABAEAAAdAEAAAAAAAACIAAAUAIAAAAAAAAFIAAAwAEAAAAAAAAQIAAA0AEAAAAAAAARIAAABAIAAAAAAAASIAAA5AEAAAAAAAATIAAA9AEAAAAAAAAgIAAAMAIAAAAAAAAwIAAAXAIAAAAAAAABwAAApAEAAAAAAAARwAAAFAAAAAAAAAASwAAAtAEAAAAAAACA8AAAZAIAAAAAAAAACAANABoAIwB0AAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABPQ=},
	bdsk-file-2 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEEUuLi8uLi8uLi8uLi9yZXNlYXJjaC9iaWJsaW9ncmFwaHkvYmliZGVzay9QYXBlcnMvR29tZXpfZXRfYWxfMjAyMS5wZGZPEQRoYm9va2gEAAAAAAQQMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAZAMAAAUAAAABAQAAVXNlcnMAAAAGAAAAAQEAAGxxYm43MwAACQAAAAEBAABEb2N1bWVudHMAAAAIAAAAAQEAAHJlc2VhcmNoDAAAAAEBAABiaWJsaW9ncmFwaHkHAAAAAQEAAGJpYmRlc2sABgAAAAEBAABQYXBlcnMAABQAAAABAQAAR29tZXpfZXRfYWxfMjAyMS5wZGYgAAAAAQYAAAQAAAAUAAAAJAAAADgAAABIAAAAXAAAAGwAAAB8AAAACAAAAAQDAAAVXQAAAAAAAAgAAAAEAwAAQAUEAAAAAAAIAAAABAMAAKpe9AAAAAAACAAAAAQDAABpLNEAAAAAAAgAAAAEAwAAlSzRAAAAAAAIAAAABAMAAGMu0QAAAAAACAAAAAQDAACGLtEAAAAAAAgAAAAEAwAA2TfRAAAAAAAgAAAAAQYAAMAAAADQAAAA4AAAAPAAAAAAAQAAEAEAACABAAAwAQAACAAAAAAEAABBw5YDRIAAABgAAAABAgAAAQAAAAAAAAAPAAAAAAAAAAAAAAAAAAAACAAAAAQDAAAGAAAAAAAAAAQAAAADAwAA9wEAAAgAAAABCQAAZmlsZTovLy8MAAAAAQEAAE1hY2ludG9zaCBIRAgAAAAEAwAAAFChG3MAAAAIAAAAAAQAAEHGeDvVAAAAJAAAAAEBAAAyOEM4QjRGOS03NzU4LTQ1QUQtQTU1Ni1ERTQwMkU0RTYzNzAYAAAAAQIAAIEAAAABAAAA7xMAAAEAAAAAAAAAAAAAAAEAAAABAQAALwAAAAAAAAABBQAAAwEAAAECAAA0NmEzYThjNDRmYjAxNzU1MGU3NzgzOWMzNzcyMDY4ZmVkOWRjYzUzMmNhYmU4ZDU1ZTY1NjMzNzZkYzk5ZWQxOzAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwMDAwMDAwMjA7Y29tLmFwcGxlLmFwcC1zYW5kYm94LnJlYWQtd3JpdGU7MDE7MDEwMDAwMGY7MDAwMDAwMDAwMGQxMzdkOTs1OTsvdXNlcnMvbHFibjczL2RvY3VtZW50cy9yZXNlYXJjaC9iaWJsaW9ncmFwaHkvYmliZGVzay9wYXBlcnMvZ29tZXpfZXRfYWxfMjAyMS5wZGYAAMwAAAD+////AQAAAAAAAAAQAAAABBAAAJgAAAAAAAAABRAAAEABAAAAAAAAEBAAAHgBAAAAAAAAQBAAAGgBAAAAAAAAAiAAAEQCAAAAAAAABSAAALQBAAAAAAAAECAAAMQBAAAAAAAAESAAAPgBAAAAAAAAEiAAANgBAAAAAAAAEyAAAOgBAAAAAAAAICAAACQCAAAAAAAAMCAAAFACAAAAAAAAAcAAAJgBAAAAAAAAEcAAABQAAAAAAAAAEsAAAKgBAAAAAAAAgPAAAFgCAAAAAAAAAAgADQAaACMAawAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAATX},
	bdsk-url-1 = {https://doi.org/10.1109/MSP.2021.3106232}}

@article{juslin2022emotions,
	author = {Juslin, P. N. and Sakka, Laura S and Barradas, Gon{\c{c}}alo T and Lartillot, Olivier},
	date-added = {2024-06-18 08:12:06 +0100},
	date-modified = {2024-06-18 08:12:06 +0100},
	journal = {Music Perception: An Interdisciplinary Journal},
	number = {1},
	pages = {55--86},
	publisher = {University of California Press},
	title = {Emotions, mechanisms, and individual differences in music listening: A stratified random sampling approach},
	volume = {40},
	year = {2022}}

@manual{R-base,
	address = {Vienna, Austria},
	author = {{R Core Team}},
	date-added = {2024-06-18 08:11:51 +0100},
	date-modified = {2024-06-18 08:11:51 +0100},
	organization = {R Foundation for Statistical Computing},
	title = {R: A Language and Environment for Statistical Computing},
	url = {https://www.R-project.org/},
	year = {2023},
	bdsk-url-1 = {https://www.R-project.org/}}

@article{Higginsd5928,
	author = {Higgins, Julian P T and Altman, Douglas G and G{\o}tzsche, Peter C and J{\"u}ni, Peter and Moher, David and Oxman, Andrew D and Savovi{\'c}, Jelena and Schulz, Kenneth F and Weeks, Laura and Sterne, Jonathan A C},
	date-added = {2024-06-17 21:15:10 +0100},
	date-modified = {2024-06-17 21:15:10 +0100},
	doi = {10.1136/bmj.d5928},
	elocation-id = {d5928},
	eprint = {https://www.bmj.com/content/343/bmj.d5928.full.pdf},
	issn = {0959-8138},
	journal = {BMJ},
	publisher = {BMJ Publishing Group Ltd},
	title = {The Cochrane Collaboration{\textquoteright}s tool for assessing risk of bias in randomised trials},
	url = {https://www.bmj.com/content/343/bmj.d5928},
	volume = {343},
	year = {2011},
	bdsk-url-1 = {https://www.bmj.com/content/343/bmj.d5928},
	bdsk-url-2 = {https://doi.org/10.1136/bmj.d5928}}

@article{Shamseerg7647,
	abstract = {Protocols of systematic reviews and meta-analyses allow for planning and documentation of review methods, act as a guard against arbitrary decision making during review conduct, enable readers to assess for the presence of selective reporting against completed reviews, and, when made publicly available, reduce duplication of efforts and potentially prompt collaboration. Evidence documenting the existence of selective reporting and excessive duplication of reviews on the same or similar topics is accumulating and many calls have been made in support of the documentation and public availability of review protocols. Several efforts have emerged in recent years to rectify these problems, including development of an international register for prospective reviews (PROSPERO) and launch of the first open access journal dedicated to the exclusive publication of systematic review products, including protocols (BioMed Central{\textquoteright}s Systematic Reviews). Furthering these efforts and building on the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-analyses) guidelines, an international group of experts has created a guideline to improve the transparency, accuracy, completeness, and frequency of documented systematic review and meta-analysis protocols{\textemdash}PRISMA-P (for protocols) 2015. The PRISMA-P checklist contains 17 items considered to be essential and minimum components of a systematic review or meta-analysis protocol.This PRISMA-P 2015 Explanation and Elaboration paper provides readers with a full understanding of and evidence about the necessity of each item as well as a model example from an existing published protocol. This paper should be read together with the PRISMA-P 2015 statement. Systematic review authors and assessors are strongly encouraged to make use of PRISMA-P when drafting and appraising review protocols.},
	author = {Shamseer, Larissa and Moher, David and Clarke, Mike and Ghersi, Davina and Liberati, Alessandro and Petticrew, Mark and Shekelle, Paul and Stewart, Lesley A},
	date-added = {2024-06-17 21:14:56 +0100},
	date-modified = {2024-06-17 21:14:56 +0100},
	doi = {10.1136/bmj.g7647},
	elocation-id = {g7647},
	eprint = {https://www.bmj.com/content/349/bmj.g7647.full.pdf},
	journal = {BMJ},
	publisher = {BMJ Publishing Group Ltd},
	title = {Preferred reporting items for systematic review and meta-analysis protocols (PRISMA-P) 2015: elaboration and explanation},
	url = {https://www.bmj.com/content/349/bmj.g7647},
	volume = {349},
	year = {2015},
	bdsk-url-1 = {https://www.bmj.com/content/349/bmj.g7647},
	bdsk-url-2 = {https://doi.org/10.1136/bmj.g7647}}

@article{anderson2022ex,
	author = {Anderson, Cameron J and Schutz, Michael},
	date-added = {2024-05-05 20:24:25 +0100},
	date-modified = {2024-05-05 20:24:30 +0100},
	journal = {Psychology of Music},
	number = {5},
	pages = {1424--1442},
	publisher = {SAGE Publications Sage UK: London, England},
	title = {Exploring historic changes in musical communication: Deconstructing emotional cues in preludes by Bach and Chopin},
	volume = {50},
	year = {2022}}

@article{nag2022,
	author = {Nag, Sayan and Basu, Medha and Sanyal, Shankha and Banerjee, Archi and Ghosh, Dipak},
	date = {2022},
	doi = {10.1016/j.physa.2022.127261},
	journal = {Physica A: Statistical Mechanics and its Applications},
	note = {Type: Article},
	title = {On the application of deep learning and multifractal techniques to classify emotions and instruments using Indian Classical Music},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127732290&doi=10.1016%2fj.physa.2022.127261&partnerID=40&md5=634cc77ee93e9ac834ca0dad8b815119},
	volume = {597},
	year = {2022},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127732290&doi=10.1016%2fj.physa.2022.127261&partnerID=40&md5=634cc77ee93e9ac834ca0dad8b815119},
	bdsk-url-2 = {https://doi.org/10.1016/j.physa.2022.127261}}

@article{chin2018,
	author = {Chin, Yu-Hao and Wang, Jia-Ching and Wang, Ju-Chiang and Yang, Yi-Hsuan},
	date = {2018},
	doi = {10.1109/TAFFC.2016.2628794},
	journal = {IEEE Transactions on Affective Computing},
	note = {Type: Article},
	number = {4},
	pages = {541 {\textendash} 549},
	title = {Predicting the Probability Density Function of Music Emotion Using Emotion Space Mapping},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058048732&doi=10.1109%2fTAFFC.2016.2628794&partnerID=40&md5=a50a624a1dd2a6e53d5920e2499b1aca},
	volume = {9},
	year = {2018},
	bdsk-url-1 = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058048732&doi=10.1109%2fTAFFC.2016.2628794&partnerID=40&md5=a50a624a1dd2a6e53d5920e2499b1aca},
	bdsk-url-2 = {https://doi.org/10.1109/TAFFC.2016.2628794}}

@book{hiller_experimental_1979,
	author = {Hiller, Lejaren Arthur and Isaacson, Leonard M},
	publisher = {Greenwood Publishing Group Inc.},
	title = {Experimental {Music}; {Composition} with an electronic computer},
	year = {1979}}

@article{zaripov_cybernetics_1969,
	author = {Zaripov, R Kh and Russell, JGK},
	journal = {Perspectives of New Music},
	note = {Publisher: JSTOR},
	pages = {115--154},
	title = {Cybernetics and music},
	year = {1969}}

@book{mozart_musikalisches_1957,
	author = {Mozart, Wolfgang Amadeus},
	publisher = {B. Schott's Soehne},
	title = {Musikalisches {W{\"u}rfelspiel}},
	year = {1957}}

@article{deutsch_ink-pot_1952,
	author = {Deutsch, Otto Erich},
	journal = {The Musical Times},
	note = {Publisher: JSTOR},
	number = {1315},
	pages = {401--403},
	title = {Ink-{Pot} and {Squirt}-{Gun} {Or} '{The} {Art} of {Composing} {Music} in the {New}-{Style}'},
	volume = {93},
	year = {1952}}

@inproceedings{katayose_sentiment_1988,
	author = {Katayose, Haruhiro and Imai, Masakazu and Inokuchi, Seiji},
	booktitle = {9th {International} {Conference} on {Pattern} {Recognition}},
	pages = {1083--1084},
	publisher = {IEEE Computer Society},
	title = {Sentiment extraction in music},
	year = {1988}}

@inproceedings{fairthorne1968,
	author = {Fairthorne, Robert A.},
	date-modified = {2024-10-21 19:36:12 +0100},
	publisher = {Archon Books},
	title = {Towards information retrieval},
	year = {1968}}

@phdthesis{meyer_emotion_1954,
	author = {Meyer, Leonard B},
	school = {The University of Chicago},
	title = {Emotion and meaning in music},
	type = {{PhD} {Thesis}},
	year = {1954}}

@book{langer_philosophy_1948,
	author = {Langer, Susanne K},
	publisher = {New American Library},
	title = {Philosophy in a new key: {A} study in the symbolism of reason, rite, and art},
	year = {1948}}

@article{tischler_aesthetic_1956,
	author = {Tischler, Hans},
	journal = {The Music Review},
	pages = {189},
	title = {The {Aesthetic} {Experience}},
	volume = {17},
	year = {1956}}

@book{picard_affective_1997,
	author = {Picard, Rosalind},
	publisher = {MIT Press},
	title = {Affective {Computing}},
	year = {1997}}

@inproceedings{marrin_analysis_1998,
	author = {Marrin, Teresa and Picard, Rosalind},
	booktitle = {{XII} {Colloquium} for {Musical} {Informatics}, {Gorizia}, {Italy}},
	pages = {61--64},
	title = {Analysis of {Affective} {Musical} {Expression} with the {Conductor}'s {Jacket}},
	year = {1998}}

@inproceedings{dabek_new_1998,
	author = {Dabek, Frank and Healey, Jennifer and Picard, Rosalind},
	booktitle = {Proc. from the 1998 {Workshop} on {Perceptual} {User} {Interfaces}},
	title = {A new affect-perceiving interface and its application to personalized music selection},
	year = {1998}}

@inproceedings{friberg_automatic_2002,
	author = {Friberg, Anders and Schoonderwaldt, Erwin and Juslin, Patrik N and Bresin, Roberto},
	booktitle = {International {Computer} {Music} {Conference}, {ICMC} 2002, {Gothenburg}, {Sweden}},
	pages = {365--367},
	title = {Automatic real-time extraction of musical expression},
	year = {2002}}

@inproceedings{liu_automatic_2003,
	author = {Liu, Dan and Lu, Lie and Zhang, Hong-Jiang},
	booktitle = {Proc. {ISMIR} 2003; 4th {Int}. {Symp}. {Music} {Information} {Retrieval}},
	month = jan,
	title = {Automatic {Mood} {Detection} from {Acoustic} {Music} {Data}},
	year = {2003}}

@article{lu_automatic_2005,
	author = {Lu, Lie and Liu, Dan and Zhang, Hong-Jiang},
	journal = {IEEE Transactions on audio, speech, and language processing},
	note = {Publisher: IEEE},
	number = {1},
	pages = {5--18},
	title = {Automatic mood detection and tracking of music audio signals},
	volume = {14},
	year = {2005}}

@article{mandel_support_2006,
	author = {Mandel, Michael I and Poliner, Graham E and Ellis, Daniel PW},
	journal = {Multimedia systems},
	note = {Publisher: Springer},
	number = {1},
	pages = {3--13},
	title = {Support vector machine active learning for music retrieval},
	volume = {12},
	year = {2006}}

@inproceedings{feng_popular_2003,
	author = {Feng, Yazhong and Zhuang, Yueting and Pan, Yunhe},
	booktitle = {Proceedings of the 26th annual international {ACM} {SIGIR} conference on {Research} and development in informaion retrieval},
	pages = {375--376},
	title = {Popular music retrieval by detecting mood},
	year = {2003}}

@inproceedings{hu_2007_2008,
	author = {Hu, Xiao and Downie, John Stephen and Laurier, Cyril and Bay, Mert and Ehmann, Andreas F.},
	booktitle = {Proc. 9th {Int}. {Conf}. {Music} {Inf}. {Retrieval}},
	pages = {462--467},
	title = {The 2007 {MIREX} audio mood classification task: {Lessons} learned},
	year = {2008}}

@article{yang_regression_2008,
	author = {Yang, Yi-Hsuan and Lin, Yu-Ching and Su, Ya-Fan and Chen, Homer H},
	journal = {IEEE Transactions on audio, speech, and language processing},
	note = {Publisher: IEEE},
	number = {2},
	pages = {448--457},
	title = {A regression approach to music emotion recognition},
	volume = {16},
	year = {2008}}

@inproceedings{celma_foafing_2006,
	author = {Celma, Oscar},
	booktitle = {International semantic web conference},
	pages = {927--934},
	publisher = {Springer},
	title = {Foafing the music: {Bridging} the semantic gap in music recommendation},
	year = {2006}}

@inproceedings{wiggins_semantic_2009,
	author = {Wiggins, Geraint A},
	booktitle = {2009 11th {IEEE} {International} {Symposium} on {Multimedia}},
	pages = {477--482},
	publisher = {IEEE},
	title = {Semantic gap?? {Schemantic} schmap‼ {Methodological} considerations in the scientific study of music},
	year = {2009}}

@article{downie_music_2008,
	author = {Downie, John Stephen},
	journal = {Acoustical Science and Technology},
	note = {Publisher: Acoustical Society of Japan},
	number = {4},
	pages = {247--255},
	title = {The music information retrieval evaluation exchange (2005--2007): {A} window into music information retrieval research},
	volume = {29},
	year = {2008}}

@article{zhang_bridge_2016,
	author = {Zhang, JiangLong and Huang, XiangLin and Yang, Lifang and Nie, Liqiang},
	journal = {Neurocomputing},
	note = {Publisher: Elsevier},
	pages = {333--341},
	title = {Bridge the semantic gap between pop music acoustic feature and emotion: {Build} an interpretable model},
	volume = {208},
	year = {2016}}

@article{kassler1966toward,
	author = {Kassler, Michael},
	journal = {Perspectives of New Music},
	pages = {59--67},
	publisher = {JSTOR},
	title = {Toward musical information retrieval},
	year = {1966}}

@article{mendel1969some,
	author = {Mendel, Arthur},
	journal = {Computers and the Humanities},
	pages = {41--52},
	publisher = {JSTOR},
	title = {Some preliminary attempts at computer-assisted style analysis in music},
	year = {1969}}

@inproceedings{grekow2016music,
	author = {Grekow, Jacek},
	booktitle = {Computer Information Systems and Industrial Management: 15th IFIP TC8 International Conference, CISIM 2016, Vilnius, Lithuania, September 14-16, 2016, Proceedings 15},
	organization = {Springer},
	pages = {697--706},
	title = {Music emotion maps in arousal-valence space},
	year = {2016}}

@article{yang2018review,
	author = {Yang, Xinyu and Dong, Yizhuo and Li, Juan},
	journal = {Multimedia systems},
	pages = {365--389},
	publisher = {Springer},
	title = {Review of data features-based music emotion recognition methods},
	volume = {24},
	year = {2018}}

@inproceedings{bai2016dimensional,
	author = {Bai, Junjie and Peng, Jun and Shi, Jinliang and Tang, Dedong and Wu, Ying and Li, Jianqing and Luo, Kan},
	booktitle = {2016 IEEE 15th International Conference on Cognitive Informatics \& Cognitive Computing (ICCI* CC)},
	organization = {IEEE},
	pages = {42--49},
	title = {Dimensional music emotion recognition by valence-arousal regression},
	year = {2016}}

@article{coutinho2013psychoacoustic,
	author = {Coutinho, Eduardo and Dibben, Nicola},
	journal = {Cognition \& emotion},
	number = {4},
	pages = {658--684},
	publisher = {Taylor \& Francis},
	title = {Psychoacoustic cues to emotion in speech prosody and music},
	volume = {27},
	year = {2013}}

@inproceedings{eerola2009prediction,
	author = {Eerola, Tuomas and Lartillot, Olivier and Toiviainen, Petri},
	booktitle = {Ismir},
	pages = {621--626},
	title = {Prediction of Multidimensional Emotional Ratings in Music from Audio Using Multivariate Regression Models.},
	year = {2009}}

@article{grekow2018audio,
	author = {Grekow, Jacek},
	journal = {Journal of Information and Telecommunication},
	number = {3},
	pages = {322--333},
	publisher = {Taylor \& Francis},
	title = {Audio features dedicated to the detection and tracking of arousal and valence in musical compositions},
	volume = {2},
	year = {2018}}

@article{park2017representation,
	author = {Park, Jiyoung and Lee, Jongpil and Nam, Juhan and Park, Jangyeon and Ha, Jung-Woo},
	journal = {The 13th Music Information Retrieval Evaluation eXchange, MIREX},
	title = {Representation learning using artist labels for audio classification tasks},
	year = {2017}}

@inproceedings{yang2007music,
	author = {Yang, Yi-Hsuan and Su, Ya-Fan and Lin, Yu-Ching and Chen, Homer H},
	booktitle = {Proceedings of the international workshop on Human-centered multimedia},
	pages = {13--22},
	title = {Music emotion recognition: The role of individuality},
	year = {2007}}

@inproceedings{chowdhury2021perceived,
	author = {Chowdhury, Shreyan and Widmer, Gerhard},
	booktitle = {International Society for Music Information Retrieval Conference (ISMIR 2023)},
	title = {On perceived emotion in expressive piano performance: Further experimental evidence for the relevance of mid-level perceptual features},
	year = {2021}}

@article{zaripov1969,
	author = {Zaripov, R Kh and Russell, JGK},
	date = {1969},
	journal = {Perspectives of New Music},
	note = {Publisher: JSTOR},
	pages = {115{\textendash}154},
	title = {Cybernetics and music},
	year = {1969}}
