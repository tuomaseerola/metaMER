# Analysis

This assumes that the data has been parsed (`parse-model-output.R`, `format-study-results.R`) and preprocessed (`processing.qmd`).

Updated 9/4/2025

## Regression studies

### Valence

For creating Table 2

```{r}
#| warning: false
#| eval: true
library(dmetar,quietly = TRUE)
library(tidyverse,quietly = TRUE)
library(meta)
library(DescTools)
library(ggrepel)
library(forestplot)
```

#### Using all models

```{r}
R_studies <- read.csv("R_studies.csv")
#R_summary <- read.csv("R_summary.csv")

# select regression studies with r2
#tmp <- dplyr::filter(R_summary,dimension=="valence")
tmp <- dplyr::filter(R_studies,dimension=="valence")
#dim(tmp)

#if all studies, remove two with NA values
tmp <- tmp[!is.na(tmp$values),]
#dim(tmp)

# Take all models
m.cor <- metacor(cor = values,     # values 
                 n = stimulus_n,
                 studlab = unique_id,
                 data = tmp,
                 common = FALSE,
                 random = TRUE,
                 prediction = TRUE,
#                 backtransf = TRUE,
#                 sm = "ZCOR",
                 method.tau = "PM",# was REML, but we switch to Paule-Mandel because Langan et al., 2019
                 method.random.ci = "HK", 
                 title = "MER: Regression: Valence: Summary")

print(m.cor)

print(FisherZInv(m.cor$TE.random))
print(FisherZInv(m.cor$upper.random))
print(FisherZInv(m.cor$lower.random))
```

#### Using the best model out of each study

```{r}
R_summary <- read.csv("R_summary.csv")

# print a summary of features

R_summary |> 
  group_by(feature_n_complexity) |> 
  summarize(min = min(feature_n), 
            max = max(feature_n), 
            mean = mean(feature_n), 
            mdn = median(feature_n))

# select regression studies with r2
tmp <- dplyr::filter(R_summary,dimension=="valence")
#tmp <- dplyr::filter(R_studies,dimension=="valence")
#dim(tmp)

#if all studies, remove two
#tmp <- tmp[!is.na(tmp$values),]
#dim(tmp)

## Disambiguate the studies
tmp$studyREF[tmp$studyREF=="Wang et al 2022"] <- c("Wang et al. 2022a","Wang et al. 2022b")

# Max values
m.cor <- metacor(cor = valuesMax,     # values 
                 n = stimulus_n,
                 studlab = studyREF,#citekey, # unique_id
                 data = tmp,
                 common = FALSE,
                 random = TRUE,
                 prediction = TRUE,
#                 backtransf = TRUE,
#                 sm = "ZCOR",
                 method.tau = "REML",# could be PM (Paule-Mandel) as well
                 method.random.ci = "HK", 
                 title = "MER: Regression: Valence: Summary")

print(m.cor)

print(FisherZInv(m.cor$TE.random))
print(FisherZInv(m.cor$upper.random))
print(FisherZInv(m.cor$lower.random))
```

### Explore heterogeneity

```{r}
#| eval: true
#| echo: true
# Method 1: Give us the Egger's test about beta coefficient from funnel
print(eggers.test(m.cor))

# Method 2: Find the impact to the results when removing those outside the 95CI
O <- find.outliers(m.cor) # 13 remaining out of 24
print(O)
# Method 3: Leave-out-out analysis etc for individual influence (not useful here)
#infan <- InfluenceAnalysis(m.cor)

# Method 4: Focus on 10% of most precise studies (Stanley, Jarrel, Doucouliagos 2010)
thres<-quantile(m.cor$seTE,0.1)
ind<-m.cor$seTE<=as.numeric(thres)
m.cor10pct <- update(m.cor, subset = which(ind))

# Method 5: P curve analysis (Simonsohn, Simmons & Nelson, 2015)
pcurve(m.cor)
```

### Visualise (forest and funnel plots)

```{r}
#| fig-width: 12
#| fig-height: 6
fig2a <- meta::forest(m.cor,
       sortvar = TE,
       prediction = FALSE, 
             print.tau2 = FALSE,
             leftlabs = c("Author", "N"),studlab = studyREF)

fig2b <- funnel(m.cor, common = FALSE, studlab=TRUE,backtransf=TRUE)

data<-tibble::tibble(mean=m.cor$cor,lower=FisherZInv(m.cor$lower),upper=FisherZInv(m.cor$upper),study=m.cor$studlab,n=m.cor$n,cor=round(m.cor$cor,2))
data<-dplyr::arrange(data,mean)

fp1 <- grid.grabExpr(print(data |>
  forestplot(labeltext = c(study, n, cor),
             xlab = "Correlation",
             xticks = c(0, .25,.5,.75, 1),
             clip = c(0, 1))|>
    fp_add_header(study = "Study",n = "N",cor = expression(italic(r))) |>
    fp_append_row(mean  = m.cor$TE.common,
                lower = m.cor$lower.common,
                upper = m.cor$upper.common,
                study = "Summary",
                n = sum(m.cor$n),
                cor = round(m.cor$TE.common,2),
                is.summary = TRUE) |>
  fp_set_style(box = "grey50",
               line = "grey20",
               summary = "black",
                txt_gp = fpTxtGp(label = list(gpar(cex = 0.80)),
                                ticks = gpar(cex = 0.80),
                                xlab  = gpar(cex = 0.80)))|>
    fp_decorate_graph(grid = structure( m.cor$TE.common,gp = gpar(lty = 2, col = "grey30")))
)
)

source('../etc/custom_funnel_plot.R')
fp2 <- custom_funnel_plot(m.cor)

gridExtra::grid.arrange(fp1, fp2, ncol=2, widths=c(2,1),heights=c(2,1))
```


### Sub-divisions

#### Sub-division based on techniques

```{r,results='asis'}
S <- summarise(group_by(tmp,model_class_id),n=n(),obs=sum(stimulus_n))
print(knitr::kable(S))
```

```{r}
m.cor1 <- update(m.cor,
       subgroup = model_class_id)
print(m.cor1)
```

#### Sub-division based on journals (engineering vs psych)

```{r,results='asis'}
S <- summarise(group_by(tmp,journal_type),n=n(),obs=sum(stimulus_n))
print(knitr::kable(S))
```

```{r}
m.cor2 <- update(m.cor,
       subgroup = journal_type)
print(m.cor2)
```

#### Sub-division based on N features

```{r,results='asis'}
tmp <- dplyr::filter(R_summary,dimension=="valence")
tmp<-tmp[!is.na(tmp$feature_n_complexity),] # remove 
S <- summarise(group_by(tmp,feature_n_complexity),n=n(),obs=sum(stimulus_n))
print(knitr::kable(S))
```


```{r}
tmp <- dplyr::filter(R_summary,dimension=="valence")
tmp<-tmp[!is.na(tmp$feature_n_complexity),] # remove missing values
m.cor3 <- update(m.cor,
       subgroup = feature_n_complexity)
print(m.cor3)
S<-summarise(group_by(tmp,feature_n_complexity),n=n(),obs=sum(stimulus_n))
print(S)

```

#### Sub-division based on N features and genres

```{r, results='asis'}
tmp <- dplyr::filter(R_summary,dimension=="valence")
tmp<-tmp[!is.na(tmp$feature_n_complexity),] # removemissing values

S<-summarise(group_by(tmp,stimulus_genre_mixed),n=n(),obs=sum(stimulus_n))
print(knitr::kable(S))

m.cor4 <- update(m.cor,
       subgroup = stimulus_genre_mixed)

print(m.cor4)
```

### Arousal

#### Using all models

```{r}
R_studies <- read.csv("R_studies.csv")
tmp <- dplyr::filter(R_studies,dimension=="arousal")
#if all studies, remove two with NA values
tmp <- tmp[!is.na(tmp$values),]
#dim(tmp)

# Take all models
m.cor <- metacor(cor = values,     # values 
                 n = stimulus_n,
                 studlab = unique_id,
                 data = tmp,
                 common = FALSE,
                 random = TRUE,
                 prediction = TRUE,
#                 backtransf = TRUE,
#                 sm = "ZCOR",
                 method.tau = "PM",# was REML, but we switch to Paule-Mandel because Langan et al., 2019
                 method.random.ci = "HK", 
                 title = "MER: Regression: Arousal: Summary")

print(m.cor)

print(FisherZInv(m.cor$TE.random))
print(FisherZInv(m.cor$upper.random))
print(FisherZInv(m.cor$lower.random))
```

#### Using the best model out of each study

```{r}
R_summary <- read.csv("R_summary.csv")

# select regression studies with r2
tmp <- dplyr::filter(R_summary,dimension=="arousal")
#tmp <- dplyr::filter(R_studies,dimension=="valence")
#dim(tmp)

#if all studies, remove two
#tmp <- tmp[!is.na(tmp$values),]
#dim(tmp)

## Disambiguate the studies
tmp$studyREF[tmp$studyREF=="Wang et al 2022"] <- c("Wang et al. 2022a","Wang et al. 2022b")

# Max values
m.cor <- metacor(cor = valuesMax,     # values 
                 n = stimulus_n,
                 studlab = studyREF,#citekey, # unique_id
                 data = tmp,
                 common = FALSE,
                 random = TRUE,
                 prediction = TRUE,
#                 backtransf = TRUE,
#                 sm = "ZCOR",
                 method.tau = "REML",# could be PM (Paule-Mandel) as well
                 method.random.ci = "HK", 
                 title = "MER: Regression: Arousal: Summary")

print(m.cor)

print(FisherZInv(m.cor$TE.random))
print(FisherZInv(m.cor$upper.random))
print(FisherZInv(m.cor$lower.random))
```

### Explore heterogeneity

```{r}
#| eval: true
#| echo: true
# Method 1: Give us the Egger's test about beta coefficient from funnel
print(eggers.test(m.cor))

# Method 2: Find the impact to the results when removing those outside the 95CI
O <- find.outliers(m.cor) # 13 remaining out of 24
print(O)
# Method 3: Leave-out-out analysis etc for individual influence (not useful here)
#infan <- InfluenceAnalysis(m.cor)

# Method 4: Focus on 10% of most precise studies (Stanley, Jarrel, Doucouliagos 2010)
thres<-quantile(m.cor$seTE,0.1)
ind<-m.cor$seTE<=as.numeric(thres)
m.cor10pct <- update(m.cor, subset = which(ind))

# Method 5: P curve analysis (Simonsohn, Simmons & Nelson, 2015)
pcurve(m.cor)
```

### Visualise (forest and funnel plots)

```{r}
#| fig-width: 12
#| fig-height: 6
fig2a <- forest(m.cor,
       sortvar = TE,
       prediction = FALSE, 
             print.tau2 = FALSE,
             leftlabs = c("Author", "N"),studlab = studyREF)

fig2b<-funnel(m.cor, common = FALSE, studlab=TRUE,backtransf=TRUE)

data<-tibble::tibble(mean=m.cor$cor,lower=FisherZInv(m.cor$lower),upper=FisherZInv(m.cor$upper),study=m.cor$studlab,n=m.cor$n,cor=round(m.cor$cor,2))
data<-dplyr::arrange(data,mean)

fp1 <- grid.grabExpr(print(data |>
  forestplot(labeltext = c(study, n, cor),
             xlab = "Correlation",
             xticks = c(0, .25,.5,.75, 1),
             clip = c(0, 1))|>
    fp_add_header(study = "Study",n = "N",cor = expression(italic(r))) |>
    fp_append_row(mean  = m.cor$TE.common,
                lower = m.cor$lower.common,
                upper = m.cor$upper.common,
                study = "Summary",
                n = sum(m.cor$n),
                cor = round(m.cor$TE.common,2),
                is.summary = TRUE) |>
  fp_set_style(box = "grey50",
               line = "grey20",
               summary = "black",
                txt_gp = fpTxtGp(label = list(gpar(cex = 0.80)),
                                ticks = gpar(cex = 0.80),
                                xlab  = gpar(cex = 0.80)))|>
    fp_decorate_graph(grid = structure( m.cor$TE.common,gp = gpar(lty = 2, col = "grey30")))
)
)

source('../etc/custom_funnel_plot.R')
fp2 <- custom_funnel_plot(m.cor)

gridExtra::grid.arrange(fp1, fp2, ncol=2, widths=c(2,1),heights=c(2,1))
```


### Sub-divisions

#### Sub-division based on techniques

```{r,results='asis'}
S <- summarise(group_by(tmp,model_class_id),n=n(),obs=sum(stimulus_n))
print(knitr::kable(S))
```

```{r}
m.cor1 <- update(m.cor,
                 subgroup = model_class_id
)
print(m.cor1)
```

#### Sub-division based on journals (engineering vs psych)

```{r,results='asis'}
S <- summarise(group_by(tmp,journal_type),n=n(),obs=sum(stimulus_n))
print(knitr::kable(S))
```

```{r}
m.cor2 <- update(
  m.cor,
  subgroup = journal_type
)
print(m.cor2)
```

#### Sub-division based on N features

```{r,results='asis'}
tmp <- dplyr::filter(R_summary,dimension=="arousal")
tmp<-tmp[!is.na(tmp$feature_n_complexity),] # remove 
S <- summarise(group_by(tmp,feature_n_complexity),n=n(),obs=sum(stimulus_n))
print(knitr::kable(S))
```


```{r}
tmp <- dplyr::filter(R_summary,dimension=="arousal")
tmp<-tmp[!is.na(tmp$feature_n_complexity),] # remove missing values
m.cor3 <- update(
  m.cor,
  subgroup = feature_n_complexity
)
print(m.cor3)
S<-summarise(group_by(tmp,feature_n_complexity),n=n(),obs=sum(stimulus_n))
print(S)

```

#### Sub-division based on N features and genres

```{r, results='asis'}
tmp <- dplyr::filter(R_summary,dimension=="arousal")
tmp<-tmp[!is.na(tmp$feature_n_complexity),] # remove missing values

S<-summarise(group_by(tmp,stimulus_genre_mixed),n=n(),obs=sum(stimulus_n))
print(knitr::kable(S))


m.cor4 <- update(
  m.cor,
  subgroup = stimulus_genre_mixed
)

print(m.cor4)
```




## Classification studies: overall success

```{r}
#| warning: false
#| eval: true

C_studies <- read.csv("C_studies.csv")
C_studies <- C_studies[!is.na(C_studies$values),]

# yang2021an needs to be encoded for classification (currently just regression)
C_summary <- read.csv("C_summary.csv")
C_classes <- read.csv("C_study_class_n.csv")

C_summary <- merge(C_summary, C_classes)

# print a summary of features

C_summary |> 
  group_by(feature_n_complexity) |> 
  summarize(min = min(feature_n), 
            max = max(feature_n), 
            mean = mean(feature_n), 
            mdn = median(feature_n))

m.cor.c.all <- metacor(cor = values,     # values 
                 n = stimulus_n,
                 studlab = studyREF, # unique_id
                 data = C_studies,
                 common = FALSE,
                 random = TRUE,
                 prediction = TRUE,
                 backtransf = TRUE,
                 sm = "ZCOR",
                 method.tau = "REML",# could be PM (Paule-Mandel) as well
                 method.random.ci = "HK", 
                 title = "MER: Classification: Summary")

m.cor.c.all

#m.cor_backtransformed <- m.cor
#m.cor_backtransformed$TE <- FisherZInv(m.cor_backtransformed$TE)
#print(funnel(m.cor.c.all, common = FALSE, studlab=TRUE,backtransf=TRUE))

```
## Define splits for genre & stimulus n combinations

```{r}
# get stimulus and genre metadata for classification studies:
C_studies |>
  group_by(citekey) |>
  summarize(feature_n = unique(feature_n),
            feature_n_complexity = unique(feature_n_complexity),
            stimulus_genre_mixed = unique(stimulus_genre_mixed)) -> C_splits

# create feature_n_complexity_genre column:
C_splits$feature_n_complexity_genre <- ""

# define splits
C_splits[C_splits$feature_n_complexity %in% "Feature n > 30 & < 300" &
        C_splits$stimulus_genre_mixed == "MultiGenre",]$feature_n_complexity_genre <- "Medium multi-genre study"
C_splits[C_splits$stimulus_genre_mixed %in% "SingleGenre" &
                             C_splits$feature_n_complexity %in% "Feature n < 30" ,]$feature_n_complexity_genre <- "Small single-genre study"
C_splits[C_splits$feature_n_complexity == "Feature n > 300",]$feature_n_complexity_genre <- "Huge single or multigenre study"
C_splits[C_splits$citekey == "alvarez2023ri",]$feature_n_complexity_genre <- "Small, multi-genre study"
C_splits[C_splits$citekey == "zhang2016br",]$feature_n_complexity_genre <- "Medium, single-genre study"
# add splits to summary:
C_summary <- left_join(C_summary, C_splits)


```

### Define splits for binary vs. multiclass

```{r}
C_summary$classes <- 0
C_summary[C_summary$n_classes < 3,]$classes <- "Binary"
C_summary[C_summary$n_classes >= 3,]$classes <- "Multiclass"


```


## Classification models: best models

```{r}
m.cor.c <- metacor(cor = valuesMax,     # values 
                 n = stimulus_n,
                 studlab = studyREF, # unique_id
                 data = C_summary,
                 common = FALSE,
                 random = TRUE,
                 prediction = TRUE,
                 backtransf = TRUE,
                 sm = "ZCOR",
                 method.tau = "REML",# could be PM (Paule-Mandel) as well
                 method.random.ci = "HK", 
                 title = "MER: Classification: Summary")

print(m.cor.c)

```

### Custom funnel for classification studies

```{r}
#| fig-width: 12
#| fig-height: 6
fig3a <- forest(m.cor.c,
                sortvar = TE,
                prediction = FALSE, 
                print.tau2 = FALSE,
                leftlabs = c("Author", "N"),studlab = studyREF)

fig3b<-funnel(m.cor.c, common = FALSE, studlab=TRUE,backtransf=TRUE)

data<-tibble::tibble(mean=m.cor.c$cor,lower=FisherZInv(m.cor.c$lower),upper=FisherZInv(m.cor.c$upper),study=m.cor.c$studlab,n=m.cor.c$n,cor=round(m.cor.c$cor,2))
data<-dplyr::arrange(data,mean)

fp3 <- grid.grabExpr(print(data |>
                             forestplot(labeltext = c(study, n, cor),
                                        xlab = "Correlation",
                                        xticks = c(0, .25,.5,.75, 1),
                                        clip = c(0, 1))|>
                             fp_add_header(study = "Study",n = "N",cor = expression(italic(r))) |>
                             fp_append_row(mean  = m.cor.c$TE.common,
                                           lower = m.cor.c$lower.common,
                                           upper = m.cor.c$upper.common,
                                           study = "Summary",
                                           n = sum(m.cor.c$n),
                                           cor = round(m.cor.c$TE.common,2),
                                           is.summary = TRUE) |>
                             fp_set_style(box = "grey50",
                                          line = "grey20",
                                          summary = "black",
                                          txt_gp = fpTxtGp(label = list(gpar(cex = 0.80)),
                                                           ticks = gpar(cex = 0.80),
                                                           xlab  = gpar(cex = 0.80)))|>
                             fp_decorate_graph(grid = structure( m.cor.c$TE.common,gp = gpar(lty = 2, col = "grey30")))
)
)

source('../etc/custom_funnel_plot.R')
fp4 <- custom_funnel_plot(m.cor.c)

gridExtra::grid.arrange(fp3, fp4, ncol=2, widths=c(2,1),heights=c(2,1))

```

## Exploring heterogeneity

```{r}
#| eval: true
#| echo: true
# Method 1: Give us the Egger's test about beta coefficient from funnel
print(eggers.test(m.cor.c))

# Method 2: Find the impact to the results when removing those outside the 95CI
O <- find.outliers(m.cor.c) # 13 remaining out of 24
print(O)
# Method 3: Leave-out-out analysis etc for individual influence (not useful here)
#infan <- InfluenceAnalysis(m.cor)

# Method 4: Focus on 10% of most precise studies (Stanley, Jarrel, Doucouliagos 2010)
thres<-quantile(m.cor.c$seTE,0.1)
ind<-m.cor.c$seTE<=as.numeric(thres)
m.cor10pct <- update(m.cor.c, subset = which(ind))

# Method 5: P curve analysis (Simonsohn, Simmons & Nelson, 2015)
pcurve(m.cor.c)
```
### Re-run the analysis without outliers

```{r}
#| eval: true
#| echo: false
outliers <- O$out.study.random

tmp2<-C_summary[!C_summary$studyREF %in% outliers,]
m.cor.c.o <- metacor(cor = valuesMax, 
                 n = stimulus_n,
                 studlab = studyREF,
                 data = tmp2,
                 common = FALSE,
                 random = TRUE,
                 sm = "ZCOR",
                 prediction = TRUE,
                 backtransf = TRUE,
                 method.tau = "REML",# could be PM (Paule-Mandel) as well
                 method.random.ci = "HK", 
                 title = "MER: Classification: Summary")

```

#### Visualise (forest and funnel plots)

```{r}
forest(m.cor.c.o,
       sortvar = TE,
       prediction = FALSE, 
             print.tau2 = FALSE,
             leftlabs = c("Author", "N"),studlab = citekey)

funnel(m.cor.c.o, common = FALSE, studlab=TRUE,backtransf=TRUE)


```
## Subgroup analyses

### By binary vs. multi-class classification

```{r}
m.cor_classes <- update(
  m.cor.c, 
  subgroup = classes)

print(m.cor_classes)

forest(m.cor_classes,
       sortvar = TE,
       prediction = FALSE, 
             print.tau2 = FALSE,
             leftlabs = c("Author", "N"),
       studlab = citekey)

funnel(m.cor, common = FALSE, studlab=TRUE,backtransf=TRUE)

```

### By journal type

```{r}
m.cor_journal <- update(
  m.cor.c, 
  subgroup = journal_type
)

print(m.cor_journal)

```

### By single vs. multi-genre

```{r}
#| eval: false
m.cor_subgroups <- update(
  m.cor.c, 
  subgroup = stimulus_genre_mixed
)

print(m.cor_subgroups)
#forest(m.cor_subgroups,subgroup=TRUE)
```

### By feature complexity

```{r}
m.cor_complexity <- update(
  m.cor.c,
  subgroup = feature_n_complexity
)

print(m.cor_complexity)

```
### By model type

```{r}
#| eval: false
m.cor_subgroups <- update(
  m.cor.c, 
  subgroup = model_class_id, 
)

print(m.cor_subgroups)
#forest(m.cor_subgroups,subgroup=TRUE)
```


#### Custom funnel plot

To show the quality differences between core and eliminated studies (in progress).

```{r}
#| eval: true
tmpdata <- data.frame(SE = m.cor.c$seTE, Zr = m.cor.c$TE, studies=m.cor.c$studlab)

tmpdata$studyREF <- substr(tmpdata$studies,1,nchar(tmpdata$studies)-2)
tmpdata$studyREF <- str_replace_all(tmpdata$studyREF,'([0-9]+)',' et al \\1')
tmpdata$studyREF <- str_to_sentence(tmpdata$studyREF)
tmpdata$studyREF

estimate = m.cor.c$TE.common
se = m.cor.c$seTE.common
se.seq=seq(0, max(m.cor.c$cor), 0.001)
ll95 = estimate-(1.96*se.seq)
ul95 = estimate+(1.96*se.seq)
ll99 = estimate-(3.29*se.seq)
ul99 = estimate+(3.29*se.seq)
meanll95 = estimate-(1.96*se)
meanul95 = estimate+(1.96*se)
dfCI = data.frame(ll95, ul95, ll99, ul99, se.seq, estimate, meanll95, meanul95)

fp = ggplot(NULL) +
  geom_point(aes(x = SE, y = Zr), color='grey50',data=tmpdata) +
  geom_text_repel(aes(x = SE, y = Zr, label=studyREF), data=tmpdata,size=2.5,max.overlaps = 40)+
  xlab('Standard Error') + ylab('Fisher\'s z transformed correlation')+
  geom_line(aes(x = se.seq, y = ll95), linetype = 'dotted', data = dfCI) +
  geom_line(aes(x = se.seq, y = ul95), linetype = 'dotted', data = dfCI) +
  geom_segment(aes(x = min(se.seq), y = meanll95, xend = max(se.seq), yend = meanll95), linetype='dotted', data=dfCI) +
  geom_segment(aes(x = min(se.seq), y = meanul95, xend = max(se.seq), yend = meanul95), linetype='dotted', data=dfCI) +
#  scale_x_reverse()+
  scale_x_reverse(breaks=seq(0,0.2,0.05),limits=c(0.15,0),expand=c(0.00,0.00))+
 # scale_y_continuous(breaks=seq(0.3,1.25,0.20),limits=c(0.3,1.23))+
  coord_flip()+
  theme_bw()
print(fp)


```


- Idea: visualise the distributions of the model successes within studies (done in preprocessing)


