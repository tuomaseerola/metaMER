# Preprocessing

This assumes that the data has been parsed from the BibTeX files into table and exported as CSV file.

```{r}
#| warning: false
#| echo: false
#| eval: true
# Set parameters and libraries
export_decision <- FALSE
verbose <- FALSE
require(here)
library(stringr)
library(tidyverse)
library(ggrepel)
library(ez)
library(ggdist)
```

## Read annotated data

```{r}
#| echo: true
source(here::here('R/build-df.R'))
source(here::here('R/format-study-results.R'))
source(here::here('R/parse-model-output.R'))
```

```{r}
#| echo: true
# get metaMER df:
meta_df <- get_metaMER_df(path_2_studies = here::here('studies'))

# get included studies
included_studies <- meta_df[which(
  !stringr::str_detect(meta_df$final_notes, '!EXCL!')),] |> 
  dplyr::tibble()
```

### Recoded

```{r}
#| warning: false
# get studies re-coded (currently identifiable by presence of bind_field.)
recoded_studies <- included_studies[which(stringr::str_detect(
  included_studies$model_rate_emotion_values,
                                     'bind_field')),] 

```

### Add unique identifiers

```{r}
#| warning: false

metaMER_results <-
do.call(
  rbind,
    lapply(1:nrow(recoded_studies),
       function(x) get_study_results(recoded_studies[x,])
       )
) 

# add unique identifiers
unique_id <- apply(metaMER_results[,c('citekey',
                        'library_id',
                         'model_id',
                         'feature_id',
                         'data_id',
                         'experiment_id')],
                      1, 
                      paste0, 
                      collapse = '-'
) 
metaMER_results$unique_id <- stringr::str_remove_all(unique_id, 
                                                     ' ')

metaMER_results <- metaMER_results |> dplyr::select(unique_id, 
                                 dplyr::everything()) 

metaMER_results |> dplyr::tibble()
```


### Summarise annotated data (optional)

```{r}
#| eval: false
#| output: asis

print(knitr::kable(table(metaMER_results$citekey,metaMER_results$dimension)))
print(knitr::kable(table(metaMER_results$citekey,metaMER_results$model_id)))
print(knitr::kable(table(metaMER_results$citekey,metaMER_results$feature_id)))
print(knitr::kable(table(metaMER_results$citekey,metaMER_results$data_id)))

```


### Add human readable author+date field

```{r}
#| echo: false
#| eval: true
metaMER_results$studyREF <- substr(metaMER_results$citekey,1,nchar(metaMER_results$citekey)-2)
metaMER_results$studyREF <- str_replace_all(metaMER_results$studyREF,'([0-9]+)',' et al \\1')
metaMER_results$studyREF <- str_to_sentence(metaMER_results$studyREF)
```

### Classify modelling techniques used

```{r}
#| eval: true
#| echo: false
# Classify techniques according Hastie, Tibshirani, Friedman (2008)
# https://www.sas.upenn.edu/~fdiebold/NoHesitations/BookAdvanced.pdf
#
metaMER_results$model_id <- tolower(metaMER_results$model_id)
metaMER_results$model_class_id <- 'Unclassified'
metaMER_results$model_class_id[str_detect(metaMER_results$model_id,'lr|lm|pls|mlr|pcr|logistic regression|2d model full|pentagon|gaussian process regression|sparse bayesian regression|variational bayesian regression|logistic regression|lda|rda|regularized discriminant analysis|reguliarized discriminant analysis|linear discriminant|quadratic discriminant|polygonal')] <- 'Linear Methods' # Class name from Elements of Stat.."
#metaMER_results$model_class_id[str_detect(metaMER_results$model_id,'logistic regression|lda|rda|regularized discriminant analysis|reguliarized discriminant analysis')] <- 'Linear Classification'
#metaMER_results$model_class_id[str_detect(metaMER_results$model_id,'mars|gam')]<-'Additive Trees and Related Methods' 

# make sure nn gets classified before knn
metaMER_results$model_class_id[str_detect(metaMER_results$model_id,'nn|gru|lstm|ltsm|long short term memory|rprop|mcan')]<-'Neural Nets'
metaMER_results$model_class_id[str_detect(metaMER_results$model_id,'rbf|local|knn|mars|gam')]<-'Kernel Smoothing, Additive and KNN'
#metaMER_results$model_class_id[str_detect(metaMER_results$model_id,'adaboost|gradient')]<-'Boosting'
metaMER_results$model_class_id[str_detect(metaMER_results$model_id,'gmm|svm|svr|support vector regression|smoreg|smo ')]<-'Support Vector Machines'
#metaMER_results$model_class_id[str_detect(metaMER_results$model_id,'knn')]<-'Prototype methods'
metaMER_results$model_class_id[str_detect(metaMER_results$model_id,'rf|extremely randomized tree regression|random forest|adaboost|gradient|tree|bag|boost')]<-'Tree-based Methods'
#metaMER_results$model_class_id[str_detect(metaMER_results$model_id,'rf')]<-'Ensemble Learning'

```


### Classify stimulus genres

```{r}
#| eval: true
#| echo: false
metaMER_results$stimulus_genre_mixed <- 'SingleGenre'
metaMER_results$stimulus_genre_mixed[str_detect(metaMER_results$stimulus_genre,',|multi')] <- 'MultiGenre' # Class name from Elements of Stat.."
#table(metaMER_results$stimulus_genre_mixed)
```

### Classify journals

```{r}
#| echo: false
#| eval: true
metaMER_results$journal_type <- "Engineering"
metaMER_results$journal_type[str_detect(metaMER_results$journal,'Quarterly Journal of Experimental Psychology|PSYCHOLOGY OF MUSIC|PLOS ONE|JOURNAL OF NEW MUSIC RESEARCH|IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING|FRONTIERS IN PSYCHOLOGY|Frontiers in Psychology')] <- 'Psychology' # Class name from Elements of Stat.."

#table(metaMER_results$journal_type)
```

## Summarise all

```{r}
#| eval: true
#| output: asis
print(knitr::kable(table(metaMER_results$model_class_id)))
print(knitr::kable(table(metaMER_results$model_class_id,metaMER_results$model_category)))

cat(paste("We have", nrow(metaMER_results), "observations"))

cat(paste("\nWe have", length(unique(metaMER_results$citekey)), "studies"))

cat(paste("\nWhere", length(unique(metaMER_results$citekey[metaMER_results$model_category=='regression'])), "are regression studies"))

cat(paste("\nWhere", length(unique(metaMER_results$citekey[metaMER_results$model_category=='classification'])), "are classification studies"))
# note that we have some classification studies that also do regression and vice versa?
# THIS IS CORRECT (updated 17 January 2025):
# [1] "We have 1114 observations"
# [1] "We have 34 studies"
# [1] "Where 22 are regression studies"
# [1] "Where 12 are classification studies"

# Add a check for these properties ToDo

```
2024-10-21: Update studies with multiple unique_ids due to multiple stats reported

```{r}
#| echo: true
#| eval: true

metaMER_results <- metaMER_results |> 
  dplyr::filter(!(citekey == "zhang2017fe" & statistic == "mean"),
         !(citekey == "zhang2016br" & statistic == "mean"),
         !(citekey == "zhang2016br" & is.na(values)),
         !(citekey == "coutinho2017sh" & measure == "ccc"),
         # Include only results for energy arousal
         !(citekey == "wang2022cr" & dimension == "tension arousal"),
         !(citekey == "wang2021ac" & dimension == "tension arousal"),
         !(citekey == "deng2015em" & dimension == "resonance"))
```
Deal with repeated experiment IDs in `hu2017cr`
```{r}
#| echo: true
#| eval: true

# get citation keys (all currently say experiment 1)
hu2017models <- metaMER_results[str_detect(metaMER_results$citekey, 
                                           "hu2017cr"),]$unique_id

# replace the last digit with 1:4 (representing each experiment)
hu2017models_replacement <- paste0(
substr(hu2017models, 
       1, 
       nchar(hu2017models) - 1),
1:4
)


# overwrite original values
metaMER_results[metaMER_results$unique_id %in% 
                  hu2017models,]$unique_id <- hu2017models_replacement
```

# Pull data to analyse regression studies

```{r}
#| echo: false
#| eval: true

unique(metaMER_results$citekey[metaMER_results$model_category=="regression"])

length(unique(metaMER_results$citekey[metaMER_results$model_category=='regression']))

R_studies <- dplyr::filter(metaMER_results, measure %in% c('r','ccc','pcc','cc','r2','R2') & model_category=="regression") # brings some classification studies! I have added an additional filter for model category
# Cameron: Can you check that this is correct now?

# explicit NA coding
R_studies$statistic[is.na(R_studies$statistic)]<-'r2'

# include measures we need 
R_studies <- dplyr::filter(R_studies,str_detect(statistic,'adjusted|ccc|pcc|cc|globalOptimal|localOptimal|mean|r2|r'))
#dim(R_studies)
#table(R_studies$measure)

#table(R_studies$measure, R_studies$statistic)

R_studies$values[R_studies$measure=='R2']<-sqrt(R_studies$values[R_studies$measure=='R2']) # recode R2 into r
R_studies$values[R_studies$measure=='r2']<-sqrt(R_studies$values[R_studies$measure=='r2']) # recode R2 into r

```
## Remove NAs

```{r}
#| echo: true
#| eval: true
R_studies <- R_studies[!is.na(R_studies$values),]
```

## Check models aren't double-counted

```{r}
#| echo: true
#| eval: true

length(unique(paste0(R_studies$unique_id, R_studies$dimension)))

ez::ezDesign(data = R_studies, x = unique_id, y = citekey)+
  theme(axis.text.x = element_blank(),
        axis.title.x = element_blank())

```


## Homogenise the outcome variable names to valence and arousal

```{r}
#| echo: false
#| eval: true

R_studies$dimension[str_detect(R_studies$dimension,'activation|energy arousal|tension arousal')]<-'arousal' # note: there is no longer "tension arousal" based on decision to include only energy
R_studies$dimension[str_detect(R_studies$dimension,'pleasantness')]<-'valence'
R_studies <- dplyr::filter(R_studies,!str_detect(dimension,'av|resonance')) # relates to distances, can be omitted

#table(R_studies$dimension)  

#table(R_studies$stimulus_n)  

# Deal with four studies involving multiple datasets: 

R_studies$stimulus_n[R_studies$stimulus_n==" emoMusic: 744, soundtracks: 360, chinese: 500 "] <- 938 # resolved from the paper
R_studies$stimulus_n[R_studies$stimulus_n==" 2372 (subset of PSIC3839, total n: 3839)    "] <- 2372 # resolved
R_studies$stimulus_n[R_studies$stimulus_n==" study 1: 20; study 2: 40) % three outliers  "] <- 40 # decided to take this from validation
R_studies$stimulus_n[R_studies$stimulus_n==" study 1: 100; study 2: 20"] <- 20 #

R_studies$stimulus_n[R_studies$stimulus_n==" DEAM: 744, PMEmo: 206  "] <- 744 # we have encoded this based on stronger results on DEAM compared to PMEmo.

R_studies$stimulus_n[R_studies$stimulus_n==" NTUMIR: 60, MediaEval2013: 744  "] <- 60 # we have encoded this based on stronger results on NTUMIR compared to MediaEval

R_studies$stimulus_n[R_studies$stimulus_n==" 1020; MediaEval2014: 1000: music perception database 1: 6, music perception database 2: 9, music perception database 3: 8, music perception database 4: 7   "] <- 1020

# REDO with a clearer function
eliminate <- str_detect(R_studies$unique_id,"hu2017cr") & !str_detect(R_studies$unique_id,"all")
R_studies <- R_studies[!eliminate,]
R_studies$stimulus_n[R_studies$stimulus_n==" MER60: 60, CH818: 818, AMG1608: 1608  "] <- 60+818+1608 #

#table(R_studies$stimulus_n)

R_studies$stimulus_n <- as.numeric(R_studies$stimulus_n)

length(unique(metaMER_results$citekey[metaMER_results$model_category=='regression']))
  
```

#### Clean feature N field

```{r}
#| echo: false
#| eval: true
# this entry no longer appears due to exclusion:
# R_studies$feature_n[R_studies$feature_n==" 548; after reduction, 139 for PCA and 276 for ReliefF   "] <- 548
# R_studies$feature_n[str_detect(R_studies$feature_n,'548 dimensions. Pos')]<-548
R_studies$feature_n[str_detect(R_studies$feature_n,'pre_fitting')] <- 21
R_studies$feature_n[str_detect(R_studies$feature_n,'but 15 reported')]<-15 #resolved from the paper
R_studies$feature_n[str_detect(R_studies$feature_n,'150 PCA features')]<-3000 #resolved
R_studies$feature_n[str_detect(R_studies$feature_n,' 50 PCA features')]<-  499 #resolved
R_studies$feature_n[str_detect(R_studies$feature_n,'60 handcrafted and')]<-14400+60 # this should be 14400+60! resolved
R_studies$feature_n[str_detect(R_studies$feature_n,'before_selection = 45')]<-45 #resolved
R_studies$feature_n[str_detect(R_studies$feature_n,'model 4 = 388')]<-388 #resolved
# yang2021an appears there are nine unique features based on table 1
R_studies$feature_n[str_detect(R_studies$feature_n,'6669 in MediaEval')]<-6669 # resolved. 9 reported in table 1, but likely used full mediaeval feature set. 
R_studies$feature_n[str_detect(R_studies$feature_n,'557 before feature')]<-557 #resolved
# this seems to refer to yang2021an, which is handled above with "6669 in mediaeval"
# R_studies$feature_n[str_detect(R_studies$feature_n,'not specified')]<-NA
# 2025-01-15
R_studies$feature_n[str_detect(R_studies$feature_n,'study 1: 26, study 2: 28')] <- 54 # resolved used audio and midi features. study 2 was external validation
R_studies$feature_n<-as.numeric(R_studies$feature_n)




quantile(R_studies$feature_n,c(0.333,0.666),na.rm = TRUE)
quantile(R_studies$feature_n,c(0.25,0.500,0.750),na.rm = TRUE)
quantile(R_studies$feature_n,c(0.1,0.500,0.90),na.rm = TRUE)

# R_studies$feature_n_categories <-cut(R_studies$feature_n,
#                                       breaks = c(0,18,260,
#                                                  14460),
#                                       labels = c("Feature n < 18","Feature n > 18 & < 260","Feature n > 260"))

R_studies$feature_n_complexity <- cut(R_studies$feature_n,
                                      breaks = c(0,30,300,14460),
                                      labels = c("Feature n < 30","Feature n > 30 & < 300","Feature n > 300"))



```


## Diagnostics

```{r}
#| echo: false
#| eval: false
g1<-ggplot(R_studies,aes(x=values,fill=citekey,color=dimension))+
  geom_histogram()+
  facet_wrap(.~model_class_id)+
  theme_dark()+
  scale_x_continuous(breaks = seq(0,1,by=.1))
print(g1)
```

## Feature N and genre split

```{r}
#| echo: false
#| eval: true

quantile(subset(R_studies,stimulus_genre_mixed=='SingleGenre')$feature_n,probs = seq(0,1,0.333))
quantile(subset(R_studies,stimulus_genre_mixed=='MultiGenre')$feature_n,na.rm=TRUE,probs = seq(0,1,0.333))

# a combination of the two
R_studies$feature_n_complexity_genre <- cut(R_studies$feature_n,
                                      breaks = c(0,15,501,653,14460),
                                      labels = c("Small single genre study","Medium single genre/multigenre study","Medium-large single genre/multigenre study","Huge multigenre study"))
#table(R_studies$feature_n_complexity_genre)
```

## Select a summary measure for valence and arousal separately

Note: Before adding feature_n to the summary, they need to be cleaned!

```{r}
#| echo: true
#| eval: true
R_studies$citekey <- factor(R_studies$citekey) # 22 unique values
R_studies$dimension <- factor(R_studies$dimension)

# get best model class
R_studies |> 
  group_by(citekey, dimension, model_class_id) |> 
  summarize(best_model = max(values)) |> 
  filter(best_model == max(best_model)) -> best_model_class

best_model_class <- best_model_class[,c("citekey", "dimension", "model_class_id")] 

R_summary <- summarise(group_by(R_studies,dimension,citekey),valuesMean=mean(values,na.rm=TRUE),valuesMedian=median(values,na.rm=TRUE),valuesMax=max(values,na.rm=TRUE),stimulus_n=first(stimulus_n),studyREF=first(studyREF),feature_n=first(feature_n),journal_type=first(journal_type),feature_n=first(feature_n),feature_n_complexity=first(feature_n_complexity),stimulus_genre_mixed=first(stimulus_genre_mixed),feature_n_complexity_genre=first(feature_n_complexity_genre))

R_summary <- left_join(R_summary, best_model_class)


print(dim(R_summary))
```

## Visualise Summary on two dimensions

Add variation from within the studies (alternative models)

```{r}
#| echo: false
#| eval: false

R_summary_split <- pivot_wider(R_summary,id_cols = c(citekey, model_class_id), names_from = c(dimension), values_from = valuesMax)

g2 <- ggplot(R_summary_split,aes(x=valence,y=arousal,label=citekey))+
  geom_label()+
  theme_bw()+
  scale_x_continuous(breaks = seq(0,1,by=.1))+
  scale_y_continuous(breaks = seq(0,1,by=.1))
print(g2)

## could be more informative when done with the full data
R_studies$citekey<-factor(R_studies$citekey)
R_studies$dimension<-factor(R_studies$dimension)

R_studies_split <- pivot_wider(R_studies,id_cols = c(unique_id,citekey,model_class_id), names_from = c(dimension), values_from = c(values),values_fn = mean)
R_studies_split<-drop_na(R_studies_split)

g3 <- ggplot(R_studies_split,aes(x=valence,y=arousal,label=citekey,color=model_class_id,fill=model_class_id))+
  geom_point(size=4)+
  geom_label_repel(size=3, max.overlaps=50,show.legend = T,color='white')+
  scale_x_continuous(breaks = seq(0,1,by=.25),limits = c(0,1))+
  scale_y_continuous(breaks = seq(0,1,by=.25),limits = c(0,1))+
  theme_bw()
print(g3)
```

## Plot success across the years

```{r}
#| eval: false
#| echo: false
# Add year!
R_studies$year <- as.numeric(str_match(R_studies$citekey,'[0-9]+'))

g3 <- ggplot(R_studies,aes(x=year,y=values,colour=model_class_id))+
  geom_point(show.legend = T)+
  facet_wrap(.~dimension)+
  theme_bw()
print(g3)
```

## Simple model complexity metric

Ratio of obs./features or just a classification based on feature n (quantiles).

```{r}
#| echo: false
#| eval: true
print(quantile(R_studies$feature_n,c(0.333,0.666),na.rm = T))
#quantile(R_studies$feature_n,c(0.25,0.50,0.75),na.rm = T)

# Assign
# R_studies$feature_n_complexity <- cut(R_studies$feature_n,
#                                       breaks = c(0,
#                                                  as.numeric(quantile(R_studies$feature_n,c(0.333),na.rm = T)),
#                                                  as.numeric(quantile(R_studies$feature_n,c(0.666),na.rm = T)),
#                                                  14460),
#                                       labels = c("Feature n < 236","Feature n > 236 & < 653","Feature n > 653"))

#table(R_studies$feature_n_complexity)
```

## Feature N and genre split

```{r}
#| echo: false
#| eval: true
quantile(subset(R_studies,stimulus_genre_mixed=='SingleGenre')$feature_n,probs = seq(0,1,0.333))
quantile(subset(R_studies,stimulus_genre_mixed=='MultiGenre')$feature_n,na.rm=TRUE,probs = seq(0,1,0.333))

# a combination of the two
R_studies$feature_n_complexity_genre <- cut(R_studies$feature_n,
                                      breaks = c(0,15,501,653,14460),
                                      labels = c("Small single genre study","Medium single genre/multigenre study","Medium-large single genre/multigenre study","Huge multigenre study"))
#table(R_studies$feature_n_complexity_genre)
```

## Explore feature_n_complexity and model success

Needs to be done from the unsummarised data (`R_studies`).

```{r}
#| echo: false
#| eval: false

#tmp <- drop_na(R_studies)
tmp <- R_studies[!is.na(R_studies$values),]

tmp$dimension<-str_to_title(tmp$dimension)
tmp$model_class_id<-factor(tmp$model_class_id,
                           levels = c("Neural Nets","Support Vector Machines", "Kernel Smoothing, Additive and KNN", "Tree-based Methods","Linear Methods"),
                           labels = c("Neural\nNets","Flexible\nDiscriminants", "KS\n & KNN", "Random\nForests", "Linear\nMethods"))

g <- ggplot(tmp,aes(x=model_class_id,y=values,color=citekey,label=citekey,shape=stimulus_genre_mixed))+
  stat_halfeye(aes(fill=citekey),point_interval="mean_qi", trim=FALSE, expand=FALSE, show.legend = FALSE,adjust = 1.25, density="bounded", point_size=3,scale = 1,alpha=0.5) + 
  geom_point(alpha=0.5,show.legend = F,position = position_jitter(width = .3))+
  #geom_label_repel(size=2,max.overlaps = 50)+
  facet_wrap(dimension~feature_n_complexity)+
  ylab("Correlation Coefficient")+
  xlab("Model Technique")+
  scale_y_continuous(limits = c(0,1),expand = c(0.01,0.01))+
  geom_text_repel(aes(x = model_class_id, y = values, label = studyREF),
             stat = "summary", fun = mean,show.legend = F)+
  theme_bw()
print(g)
#ggsave(filename = 'FeatureN_regression.pdf',g,height = 7,width = 11)
```

## Create descriptive table for the manuscript  (Table 1, column 1)

```{r}
#| echo: true
#| eval: true

TR <- NULL
TR$study_n <- length(unique(R_studies$citekey))
TR$model_n <- nrow(R_studies)
t<-table(R_studies$model_class_id)
t2 <- paste0(names(t),': ', as.numeric(t))
TR$model_types_n <- str_c(t2,collapse = "\n")
TR$feature_Desc <- paste0('Min=',min(R_studies$feature_n,na.rm = TRUE),', Md=',median(R_studies$feature_n,na.rm = TRUE),', Max=', max(R_studies$feature_n,na.rm = TRUE))
TR$stimulus_Desc <- paste0('Min=',min(R_studies$stimulus_n,na.rm = TRUE),', Md=',median(R_studies$stimulus_n,na.rm = TRUE),', Max=', max(R_studies$stimulus_n,na.rm = TRUE))
print(TR)

```

## Export as csv 

```{r}
#| echo: true
#| eval: true
if (export_decision == TRUE){
  write.csv(x = R_studies,file = 'R_studies.csv')
  write.csv(x = R_summary,file = 'R_summary.csv')
}
```


# Pull data to analyse classification studies

```{r}
#| echo: false
#| eval: true

C_studies<-metaMER_results
sum(is.na(C_studies$measure)) 
sum(is.na(C_studies$statistic)) 

C_studies$statistic[is.na(C_studies$statistic)]<-'acc'

C_studies <- dplyr::filter(C_studies, str_detect(measure,'mcc|accuracy|auc|classification|f1|fscore|fvalue|kappa|precision|recall')) #
table(C_studies$measure)
dim(C_studies)
table(C_studies$statistic)

C_studies <- dplyr::filter(C_studies, str_detect(statistic,'mean|median|acc')) #  
#dim(C_studies)
length(unique(C_studies$citekey))

#table(C_studies$measure,C_studies$statistic)
#table(C_studies$measure,C_studies$citekey)
#table(C_studies$statistic,C_studies$citekey)

# ToDo
# how to convert accuracy (precision, recall, f) into kappa or mcc
# There IS NO solution unfortunately, accuracy and mcc can vary independently, but
# see Chicco and Jurman, 2020, https://doi.org/10.1186/s12864-019-6413-7
# which shows that MCC is the best. The paer compares accuracy and F1 to MCC 
# and shows high correlations between accuracy and MCC (r=0.90-0.92). 
# It is only the exceptions (unbalanced datasets with binary classification) that deliver major discrepancies. Our dataset does not seem to be having these, so
# perhaps we can equate accuracy with MCC in a minority of cases.

# Conversion
# convert one 'classification' into accuracy (bai2017mu)
# convert one 'f1' into accuracy (hu2022de)
C_studies$measure[C_studies$citekey=='bai2017mu'] <- 'accuracy'
C_studies$measure[C_studies$citekey=='hu2022de' & C_studies$measure=='f1']<-'accuracy'
#dim(C_studies)
C_studies <- dplyr::filter(C_studies, str_detect(measure,'mcc|accuracy')) #  
#dim(C_studies)
#table(C_studies$measure,C_studies$citekey)

# take accuracy as MCC in those studies that have not specified mcc
C_studies$accuracy_converted_to_mcc <- 0
C_studies$accuracy_converted_to_mcc[
                  C_studies$unique_id==
'alvarez2023ri-spotify-randomforest-spotify-acousticBrainz-modelvalidation'|
                  C_studies$citekey=='bai2016di' | 
                  C_studies$citekey=='bai2017mu' | 
                  C_studies$citekey=='hizlisoy2021mu' |
                  C_studies$citekey=='hu2022de' |    
                  C_studies$citekey=='yeh2014po' |
                  C_studies$citekey=='zhang2016br' |
                  C_studies$citekey=='zhang2017fe' 
                  & C_studies$measure=='accuracy']<-1

C_studies$measure[C_studies$unique_id==
'alvarez2023ri-spotify-randomforest-spotify-acousticBrainz-modelvalidation'|
                  C_studies$citekey=='bai2016di' | 
                  C_studies$citekey=='bai2017mu' | 
                  C_studies$citekey=='hizlisoy2021mu' |
                  C_studies$citekey=='hu2022de' |    
                  C_studies$citekey=='yeh2014po' |
                  C_studies$citekey=='zhang2016br' |
                  C_studies$citekey=='zhang2017fe' 
                  & C_studies$measure=='accuracy']<-'mcc'
#table(C_studies$measure,C_studies$citekey)
C_studies <- dplyr::filter(C_studies, str_detect(measure,'mcc')) #  
#dim(C_studies)

C_studies <- C_studies[!is.na(C_studies$values),]

#table(C_studies$measure,C_studies$accuracy_converted_to_mcc)
#t.test(C_studies$values,y = C_studies$accuracy_converted_to_mcc) # There is a difference

length(unique(C_studies$citekey))

```

## Average across valence and arousal for VA classification

```{r}
#| echo: true
#| eval: true

#length(unique(paste0(C_studies$unique_id, C_studies$dimension)))

ez::ezDesign(data = C_studies, x = unique_id, y = citekey)+
  theme(axis.text.x = element_blank(),
        axis.title.x = element_blank())

```



```{r}
#| echo: true
#| eval: true

va_studies <- c("zhang2016br", "hu2022de")
  
va_acc <- C_studies |> 
  filter(citekey %in% va_studies) |> 
  group_by(unique_id, measure, statistic) |> 
  mutate(values = mean(values), 
         accuracy_converted_to_mcc = mean(accuracy_converted_to_mcc))

va_acc$dimension <- "classification"

va_acc <- va_acc |> distinct()

C_studies <- C_studies[!C_studies$citekey %in% va_studies,]
C_studies <- rbind(C_studies, va_acc)


# These are encoded slightly differently, so need to use a different approach

# 2025-01-24: averaging across quadrants for bhuvanakumar study
# subset study
bhuvanakumar <- subset(
  C_studies, 
  stringr::str_detect(
    C_studies$citekey,
    "bhuvanakumar"
    )
)
# group by algorithm to average across the four quadrants
bhuvanakumar$model_algorithm <- substr(
  bhuvanakumar$model_id, 
  1, 
  4
)
# take the average for each algorithm
bhuvanakumar |> 
  group_by(model_algorithm) |>
  mutate(values = mean(values)) -> bhuvanakumar

C_studies <-C_studies |> filter(citekey != "bhuvanakumar2023em")
bhuvanakumar$model_algorithm <- NULL
C_studies <- rbind(C_studies, bhuvanakumar)

remove(bhuvanakumar)

# 2025-01-24: averaging valence and arousal for this study, which makes the max equivalent to both the mean and median (two models: one valence, one arousal)
C_studies[C_studies$citekey == "nguyen2017an",]$values <- mean(C_studies[C_studies$citekey == "nguyen2017an",]$values)



```

### Revisualize studies

```{r}
#| echo: true
#| eval: true

length(unique(paste0(C_studies$unique_id, C_studies$dimension)))

ez::ezDesign(data = C_studies, x = unique_id, y = citekey)+
  theme(axis.text.x = element_blank(),
        axis.title.x = element_blank())
```



#### Get the number of categories in the classification task

```{r}
#| echo: true
#| eval: true

tmp <- metaMER_results
tmp2 <- tmp[which(tmp$statistic=='n'),]
no_of_classes <- summarise(group_by(tmp2,citekey),first(values))
print(no_of_classes)
hist(no_of_classes$`first(values)`)

rm(tmp,tmp2)
```


## Homogenise the stimulus N

```{r}
#| echo: false
#| eval: true

#table(C_studies$stimulus_n)  

# Deal with four studies involving multiple datasets: 
C_studies$stimulus_n[C_studies$stimulus_n==" 429; 350 popular songs + 79 songs from the Beatles (Mirex 2009 collection)  "] <- 429 # resolved from the paper
C_studies$stimulus_n[C_studies$stimulus_n==" 5192; 12 per user in user validation (not included here due to little information),   AcousticBrainz validation: 60000  "] <- 5192 # resolved
C_studies$stimulus_n[C_studies$stimulus_n==" ISMIR2012: 2886, NJU_V1: 777, Hindi: 1037  "] <- 2886+777+1037 # decided to take this from validation
C_studies$stimulus_n[C_studies$stimulus_n==" total: 564; unambiguous: 416, circular validation: 39 "] <- 564 # 
#table(C_studies$stimulus_n)
C_studies$stimulus_n <- as.numeric(C_studies$stimulus_n)
  
```

## Diagnostics

```{r}
#| echo: false
#| eval: false

g1<-ggplot(C_studies,aes(x=values,fill=citekey))+
  geom_histogram()+
  facet_wrap(.~model_class_id)+
  theme_dark()+
  scale_x_continuous(breaks = seq(0,1,by=.1))
print(g1)
```

## Simple model complexity metric based on feature_n

```{r}
#| echo: false
#| eval: true

#table(C_studies$feature_n)
C_studies$feature_n[str_detect(C_studies$feature_n,'eight different ')]<-21+12+4+14+39+1+1+6 # estimated from feature subsection 4.1.2 (encoded in reverse order from h to a)
C_studies$feature_n[str_detect(C_studies$feature_n,'126, retained 97')]<-126 
C_studies$feature_n[str_detect(C_studies$feature_n,'1702; best model uses 100 after reduction')]<-1702 #  resolved
# these seem to refer to the excluded bai studies
# C_studies$feature_n[str_detect(C_studies$feature_n,'548; after reduction, ')] <- 548 
#C_studies$feature_n[str_detect(C_studies$feature_n,'548 dimensions')] <- 548 
# C_studies$feature_n[str_detect(C_studies$feature_n,'548. Post-reduction')] <- 548 
C_studies$feature_n[str_detect(C_studies$feature_n,'8; 3 after')]<-8 
C_studies$feature_n[str_detect(C_studies$feature_n,'between 9 and 10')]<-10 
C_studies$feature_n[str_detect(C_studies$feature_n,'summarize feature')]<-2+2+2 # estimated from 3.2.1 to 3.2.3
#table(C_studies$feature_n)

C_studies$feature_n <- as.numeric(C_studies$feature_n)

#ggplot(C_studies,aes(x=feature_n))+geom_histogram()

#print(quantile(C_studies$feature_n,c(0.333,0.666),na.rm = TRUE))
#quantile(R_studies$feature_n,c(0.25,0.50,0.75),na.rm = T)

# Assign
C_studies$feature_n_complexity <- cut(C_studies$feature_n,
                                      breaks = c(0,30,300,
                                                 10000),
                                      labels = c("Feature n < 30","Feature n > 30 & < 300","Feature n > 300"))
#table(C_studies$feature_n_complexity)
```

## Explore feature_n_complexity and model success

Needs to be done from the unsummarised data (`C_studies`).

```{r}
#| echo: false
#| eval: false
C_studies$citekey <- factor(C_studies$citekey)
tmp <- drop_na(C_studies)
tmp$dimension<-str_to_title(tmp$dimension)
tmp$model_class_id<-factor(tmp$model_class_id,
                           levels = c("Neural Nets","Support Vector Machines", "Kernel Smoothing, Additive and KNN", "Tree-based Methods","Linear Methods"),
                           labels = c("Neural\nNets","Flexible\nDiscriminants", "KS\n & KNN", "Random\nForests", "Linear\nMethods"))

g <- ggplot(tmp,aes(x=model_class_id,y=values,color=citekey,label=citekey,shape=stimulus_genre_mixed))+
  stat_halfeye(aes(fill=citekey),point_interval="mean_qi", trim=FALSE, expand=FALSE, show.legend = FALSE,adjust = 1.25, density="bounded", point_size=3,scale = 1,alpha=0.5) + 
  geom_point(alpha=0.5,show.legend = F,position = position_jitter(width = .3))+
  #geom_label_repel(size=2,max.overlaps = 50)+
  facet_wrap(.~feature_n_complexity)+
  ylab("Correlation Coefficient")+
  xlab("Model Technique")+
  scale_y_continuous(limits = c(0,1),expand = c(0.01,0.01))+
  geom_text_repel(aes(x = model_class_id, y = values, label = studyREF),
             stat = "summary", fun = mean,show.legend = F)+
  theme_bw()
print(g)
#ggsave(filename = 'FeatureN_regression.pdf',g,height = 7,width = 11)
```

## Aggregate valence and arousal results for nguygen2017, bhuvanakumar2023

```{r}
# get best model class
C_studies |> 
  group_by(citekey, dimension, model_class_id) |> 
  summarize(best_model = max(values)) |> 
  filter(best_model == max(best_model)) -> best_model_class_C

best_model_class_C <- best_model_class_C[,c("citekey", "dimension", "model_class_id")] 




```

```{r}
C_summary <- summarise(group_by(C_studies,dimension,citekey),valuesMean=mean(values,na.rm=TRUE),valuesMedian=median(values,na.rm=TRUE),valuesMax=max(values,na.rm=TRUE),stimulus_n=first(stimulus_n),studyREF=first(studyREF),feature_n=first(feature_n),journal_type=first(journal_type),feature_n=first(feature_n),feature_n_complexity=first(feature_n_complexity),stimulus_genre_mixed=first(stimulus_genre_mixed)#,
                        #feature_n_complexity_genre=first(feature_n_complexity_genre)
                        )

C_summary <- left_join(C_summary, best_model_class_C)



```

## Visualise Summary

```{r}
#| echo: false
#| eval: false

g2 <- ggplot(C_summary,aes(x=stimulus_n,y=valuesMax,label=citekey,color=stimulus_genre_mixed))+
  geom_point()+
  geom_label_repel(size=1.5)+
#  coord_flip()+
  theme_bw()
g2

g3 <- ggplot(C_summary,aes(x=stimulus_n,y=valuesMax,label=citekey,color=model_class_id))+
  geom_point()+
  geom_label_repel(size=1.2)+
#  coord_flip()+
  theme_bw()
g3

## could be more informative when done with the full data
C_studies$citekey<-factor(C_studies$citekey)


g4 <- ggplot(C_studies,aes(x=stimulus_n,y=values,label=citekey,color=model_class_id,fill=model_class_id))+
  geom_point(size=4)+
  geom_label_repel(size=1.2, max.overlaps=50,show.legend = T,color='white')+
#  scale_x_continuous(breaks = seq(0,1,by=.25),limits = c(0,1))+
#  scale_y_continuous(breaks = seq(0,1,by=.25),limits = c(0,1))+
  theme_bw()
g4

```




## Create descriptive table for the manuscript (Table 1, column 2)

CA: I think this should be correct, though may need to update after averaging across VA for some classification studies (zhang2016, hu2022)

```{r}
#| echo: true
#| eval: true

TC <- NULL
TC$study_n <- length(unique(C_studies$citekey))
TC$model_n <- nrow(C_studies)
t<-table(C_studies$model_class_id)
t2 <- paste0(names(t),': ', as.numeric(t))
TC$model_types_n <- str_c(t2,collapse = "\n")
TC$feature_Desc <- paste0('Min=',min(C_studies$feature_n,na.rm = TRUE),', Md=',median(C_studies$feature_n,na.rm = TRUE),', Max=', max(C_studies$feature_n,na.rm = TRUE))
TC$stimulus_Desc <- paste0('Min=',min(C_studies$stimulus_n,na.rm = TRUE),', Md=',median(C_studies$stimulus_n,na.rm = TRUE),', Max=', max(C_studies$stimulus_n,na.rm = TRUE))
print(TC)

```

## Export as csv 

```{r}
#| echo: true
#| eval: true
if (export_decision == TRUE){
  write.csv(x = C_studies,file = 'C_studies.csv')
  write.csv(x = C_summary,file = 'C_summary.csv')
}
```
