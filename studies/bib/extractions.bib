@Article{agarwal2021an,
  AUTHOR = {Agarwal, Gaurav and Om, Hari},
  JOURNAL = {IET Signal Processing},
  NOTE = {Cited by: 23},
  NUMBER = {2},
  PAGES = {98 – 121},
  TITLE = {An efficient supervised framework for music mood recognition using autoencoder-based optimised support vector regression model},
  TYPE = {Article},
  VOLUME = {15},
  YEAR = {2021},
  DOI = {10.1049/sil2.12015},
  ABSTRACT = {Music is the art of ‘language of emotions’. Recently, music mood recognition is an emerging task. An efficient supervised framework for music mood recognition using autoencoder-based optimised support vector regression (SVR) model is developed for the music emotion recognition. Our main intention is to increase the accuracy of emotion classification of music by considering text-dependent and non-text-dependent features. For the high level feature representation, stacked autoencoder is used with two hidden layers. Modified K-Medoid-based brain storm optimisation-based support vector regression (SVR_KMBSO) model is utilised for the emotion classification. Using the K-Medoid-based brain storm algorithm, the optimal parameters of the SVR are selected. The proposed framework utilises ISMIR2012 dataset and NJU_V1 dataset for English and for Hindi; online songs are also gathered and used for the music mood recognition. All the three datasets include songs based on four emotions like happy, angry, relax and sad. The experimental results are evaluated and compared with the existing classifiers including SVR, deep belief network (DBN) and Recurrent neural network (RNN). The proposed method SVR_KMBSO achieved high accuracy using three different datasets. © 2021 The Authors. IET Signal Processing published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.},
  KEYWORDS = {Character recognition; Emotion Recognition; Learning systems; Recurrent neural networks; Regression analysis; Storms; Auto encoders; Emotion classification; Emotion recognition; Feature representation; Hidden layers; High-level features; K-medoid; Music emotions; Support vector regression models; Support vector regressions; Music},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85116929889&doi = 10.1049},
  PARADIGM = {classification},
  NOTES_CA = {include, classification},
  NOTES_TE = {include, classification},
  EMOTIONS = {discrete},
  EMOTION_LOCUS = {not specified},
  STIMULUS_TYPE = {Hindi music, English music (from ISMIR2012, NJU_V1, and Hindi datasets)},
  STIMULUS_DURATION = {not specified},
  STIMULUS_DURATION_UNIT = {not specified},
  STIMULUS_N = {ISMIR2012: 2886, NJU_V1: 777, Hindi: 1037},
  FEATURE_N = {'eight different non‐text‐dependent features are employed; they are rhythm, timbre,intensity, chromagram, MFCC, OSC, SSDs, and DWCH'}, %% unclear from paper... may be worth reviewing
  PARTICIPANT_N = {not specified},
  PARTICIPANT_EXPERTISE = {not specified},
  PARTICIPANT_ORIGIN = {not specified},
  PARTICIPANT_SAMPLING = {not specified},
  PARTICIPANT_TASK = {rating},
  FEATURE_CATEGORIES = {Timbre, Rhythm, Dynamic, Harmony, Lyric},
  FEATURE_SOURCE = {not specified},
  FEATURE_REDUCTION_METHOD = {KMBSO},
  MODEL_CATEGORY = {classification}, %% task is classification though use SVR
  MODEL_DETAIL = {SVR, KMBSO}, %% compare to SVR, RNN, DBN (I didn't report these)
  MODEL_MEASURE = {classification accuracy, precision, sensitivity, specificity, false positive rate, F1, error},
  MODEL_COMPLEXITY_PARAMETERS = {SVR: 512 hidden layers, L2 normalization: 1x10^-4, 0.14 dropout rate, Relu activation function, batch size: 128, learning rate: 1, 1000 iterations (max), variance: 1 mean: 0 population size: 200, c: 1.0},
  MODEL_RATE_EMOTION_NAMES = {happy, sad, angry, relax},
  MODEL_RATE_EMOTION_VALUES = {
  bind_field(
  'not specified.SVR KMBSO.song.ismir2012.1' = c('classification_accuracy.mean' = 0.9879,  
                                                 'classification_precision.mean' = 0.9877,
                                                 'classification_sensitivity.mean' = .9878,
                                                 'classification_f1.mean' = 0.9877,
                                                 'classification_specificity.mean' = .9960,
                                                 'classification_fpr.mean' = 0.40,
                                                 'classification_error.mean' = 0.0121),
  'not specified.SVR KMBSO.karaoke.ismir2012.1' = c(0.974,0.9741,0.9738,0.974,0.9913,0.87,0.026),
  'not specified.SVR KMBSO.lyrics.ismir2012.1' = c(0.9723,0.9722,0.9721,0.9725,0.9908,9.23,0.0277),
  'not specified.SVR KMBSO.song.NJUV1.1' = c(0.9788,0.9787,0.9793,0.9789,0.9929,0.71,0.0212),
  'not specified.SVR KMBSO.karaoke.NJUV1.1' = c(0.9682,0.9664,0.9674,0.9669,0.9895,1.05,0.0318),
  'not specified.SVR KMBSO.lyrics.NJUV1.1' = c(0.9576,0.9566,0.9575,0.9570,0.9858,1.45,0.0424),
  'not specified.SVR KMBSO.song.hindi.1' = c(0.9757,0.9741,0.9760,0.9750,0.992,0.92,0.0243),
  'not specified.SVR KMBSO.karaoke.hindi.1' = c(0.9563,0.9536,0.9557,0.9546,0.9855,1.42,0.0437),
  'not specified.SVR KMBSO.lyrics.hindi.1' = c(0.9417,0.9392,0.9399,0.9395,0.9805,1.95,0.0583)
)
  }, 
  MODEL_VALIDATION = {five-fold cross-validation},
  FINAL_NOTES = {reporting proposed model. Values converted to proportions (no conversion to FPR). Assuming mean predictions reported.}
}


@Article{alvarez2023ri,
  AUTHOR = {Álvarez, P. and de Quirós, J. García and Baldassarri, S.},
  JOURNAL = {International Journal of Interactive Multimedia and Artificial Intelligence},
  NOTE = {Cited by: 4; All Open Access, Gold Open Access, Green Open Access},
  NUMBER = {2},
  PAGES = {168 – 181},
  TITLE = {RIADA: A Machine-Learning Based Infrastructure for Recognising the Emotions of Spotify Songs},
  TYPE = {Article},
  VOLUME = {8},
  YEAR = {2023},
  DOI = {10.9781/ijimai.2022.04.002},
  ABSTRACT = {The music emotions can help to improve the personalization of services and contents offered by music streaming providers. Many research works based on the use of machine learning techniques have addressed the problem of recognising the music emotions during the last years. Nevertheless, the results obtained are only applied on small-size music repositories and do not consider what the users feel when they listen to the songs. These issues prevent the existing proposals to be integrated into the personalization mechanisms of the online music providers. In this paper, we present the RIADA infrastructure which is composed by a set of systems able to annotate emotionally the catalog of songs offered by Spotify based on the users’ perception. RIADA works with the Spotify playlist miner and data services to build emotion recognition models that can solve the open challenges previously mentioned. Machine learning algorithms, music information retrieval techniques, architectures for parallelization of applications and cloud computing have been combined to develop a complex result of engineering able to integrate the music emotions into the Spotify-based applications. © 2023, Universidad Internacional de la Rioja. All rights reserved.},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85147947463&doi = 10.9781},
  AUTHOR_KEYWORDS = {Affective Annotation; Cloud Computing; Emotion Recognition; Machine Learning; Music; Spotify},
  PARADIGM = {classification},
  NOTES_CA = {include, classification task},
  NOTES_TE = {include, classification, spotify},
  EMOTIONS = {discrete},
  EMOTION_LOCUS = {not specified},
  STIMULUS_TYPE = {Spotify},
  STIMULUS_DURATION = {150 approx},
  STIMULUS_DURATION_UNIT = {s}, 
  STIMULUS_N = {5192; 12 per user in user validation (not included here due to little information),
  AcousticBrainz validation: 60000},
  FEATURE_N = {9 to 10},
  PARTICIPANT_N = {25},
  PARTICIPANT_EXPERTISE = {'not specified'},
  PARTICIPANT_ORIGIN = {'not specified'},
  PARTICIPANT_SAMPLING = {'not specified'},
  PARTICIPANT_TASK = {'classification'},
  FEATURE_CATEGORIES = {timbre,  dynamic, high-level, expressivity},
  FEATURE_SOURCE = {Spotify API},
  FEATURE_REDUCTION_METHOD = {expert validation},
  MODEL_CATEGORY = {classification},
  MODEL_DETAIL = {SVR, KNN, RF},
  MODEL_MEASURE = {classification accuracy, f1, precision, recall},
  MODEL_COMPLEXITY_PARAMETERS = {unable to find},
  MODEL_RATE_EMOTION_NAMES = {happy, sad, angry, relaxed; Experimental validation: excited, cheerful, tense, irritated, sad, bored, relaxed, calm, don't know}, 
  MODEL_RATE_EMOTION_VALUES = {bind_field('spotify.random forest.spotify.spotify.model comparison' = c('classification_accuracy.mean' = mean(c(0.844,0.899,0.862,0.945)),
                                                                'classification_f1.mean' = mean(c(0.820,.860,0.839,0.801)),
                                                                'classification_precision.mean' = mean(c(.8308,.8828,.8488,.9299)),
                                                                'classification_recall.mean' = mean(c(.8083,.8446,.8353,.7392))),
      'spotify.random forest.spotify.acousticBrainz.model validation' = c('classification_accuracy.mean' = mean(c(0.694,0.705,0.771,0.729)),
                                                                          'classification_f1.mean' = mean(c(0.623,0.700,0.745,0.719)))
      )
}, %% don't know how to report user-validation study
  MODEL_VALIDATION = {Repeated five-fold cross validation, AcousticBrainz, experimental validation with real users},
  FINAL_NOTES = {Did not report results from user validation due to differences in formatting.}
}


@Article{bai2017mu,
  AUTHOR = {Bai, Junjie and Luo, Kan and Peng, Jun and Shi, Jinliang and Wu, Ying and Feng, Lixiao and Li, Jianqing and Wang, Yingxu},
  JOURNAL = {International Journal of Cognitive Informatics and Natural Intelligence},
  NOTE = {Cited by: 6},
  NUMBER = {4},
  PAGES = {80 – 92},
  TITLE = {Music emotions recognition by machine learning with cognitive classification methodologies},
  TYPE = {Article},
  VOLUME = {11},
  YEAR = {2017},
  DOI = {10.4018/IJCINI.2017100105},
  ABSTRACT = {Music emotions recognition (MER) is a challenging field of studies addressed in multiple disciplines such as musicology, cognitive science, physiology, psychology, arts and affective computing. In this article, music emotions are classified into four types known as those of pleasing, angry, sad and relaxing. MER is formulated as a classification problem in cognitive computing where 548 dimensions of music features are extracted and modeled. A set of classifications and machine learning algorithms are explored and comparatively studied for MER, which includes Support Vector Machine (SVM), k-Nearest Neighbors (KNN), Neuro-Fuzzy Networks Classification (NFNC), Fuzzy KNN (FKNN), Bayes classifier and Linear Discriminant Analysis (LDA). Experimental results show that the SVM, FKNN and LDA algorithms are the most effective methodologies that obtain more than 80% accuracy for MER. © 2017 IGI Global.},
  KEYWORDS = {Artificial intelligence; Arts computing; Barium compounds; Discriminant analysis; Feature extraction; Fuzzy neural networks; Image retrieval; Learning systems; Nearest neighbor search; Pattern recognition; Psychophysiology; Support vector machines; Affective Computing; Classification methodologies; Emotion classification; K nearest neighbor (KNN); Linear discriminant analysis; Multiple disciplines; Music emotions; Neuro-fuzzy network; Learning algorithms},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85038434665&doi = 10.4018},
  AUTHOR_KEYWORDS = {Emotion Classification; Feature Extraction; Machine Learning; Music Emotion Recognition; Pattern Recognition},
  PARADIGM = {classification},
  NOTES_CA = {include, classification},
  NOTES_TE = {include, classification, quality issues?},
  EMOTIONS = {valence, arousal},
  EMOTION_LOCUS = {perceived},
  STIMULUS_TYPE = {MediaEval},
  STIMULUS_DURATION = {45},
  STIMULUS_DURATION_UNIT = {s},
  STIMULUS_N = {744},
  FEATURE_N = {548. Post-reduction: 139 (PCA), 276 (ReliefF)},
  PARTICIPANT_N = {specified elsewhere},
  PARTICIPANT_EXPERTISE = {specified elsewhere},
  PARTICIPANT_ORIGIN = {specified elsewhere},
  PARTICIPANT_SAMPLING = {specified elsewhere},
  PARTICIPANT_TASK = {specified elsewhere},
  FEATURE_CATEGORIES = {rhythm, tonality, timbre, pitch, melody},
  FEATURE_SOURCE = {Daubechies, Sound Description Toolbox, MIRToolbox},
  FEATURE_REDUCTION_METHOD = {PCA, ReliefF},
  MODEL_CATEGORY = {classification},
  MODEL_DETAIL = {SVM, KNN, Neuro-fuzzy networks, fuzzy KNN, Bayes classifier, LDA},
  MODEL_MEASURE = {accuracy (average)},
  MODEL_COMPLEXITY_PARAMETERS = {SVM: Kernel function = radial basis function, 
  KNN: K = 8, FKNN: fuzzy function = Gaussian, NFNC: back propogation, gradient descent},
  MODEL_RATE_EMOTION_NAMES = {valence, arousal},
  MODEL_RATE_EMOTION_VALUES = {
  bind_field(
'various.random forest regression.ReliefF.MediaEval.1' = c('valence_r2.mean' = 0.293,
                                                           'arousal_r2.mean' = 0.538,
                                                           'valence_vector distance.mean' = 0.08,
                                                           'valence_vector distance.sd' = 0.06,
                                                           'arousal_vector distance.mean' = 0.08,
                                                           'arousal_vector distance.sd' = 0.05,
                                                           'av_vector distance.mean' = 0.12,
                                                           'av_vector distance.sd' = 0.06,
                                                           'av_classification.mean' = 0.584),
'various.random forest regression.ReliefF.MediaEval.1' = c(0.26,0.625,0.08,0.06,0.07,0.05,0.12,0.06,0.592)




)
  },
  MODEL_VALIDATION = {"The experiments adopted 619 training samples and 125 test sets provided in the annotation file of the music database"},
  FINAL_NOTES = {}
}


@Article{bhuvanakumar2023em,
  AUTHOR = {Bhuvana Kumar, V. and Kathiravan, M.},
  JOURNAL = {Frontiers in Computer Science},
  NOTE = {Cited by: 0; All Open Access, Gold Open Access},
  TITLE = {Emotion recognition from MIDI musical file using Enhanced Residual Gated Recurrent Unit architecture},
  TYPE = {Article},
  VOLUME = {5},
  YEAR = {2023},
  DOI = {10.3389/fcomp.2023.1305413},
  ABSTRACT = {The complex synthesis of emotions seen in music is meticulously composed using a wide range of aural components. Given the expanding soundscape and abundance of online music resources, creating a music recommendation system is significant. The area of music file emotion recognition is particularly fascinating. The RGRU (Enhanced Residual Gated Recurrent Unit), a complex architecture, is used in our study to look at MIDI (Musical Instrument and Digital Interface) compositions for detecting emotions. This involves extracting diverse features from the MIDI dataset, encompassing harmony, rhythm, dynamics, and statistical attributes. These extracted features subsequently serve as input to our emotion recognition model for emotion detection. We use an improved RGRU version to identify emotions and the Adaptive Red Fox Algorithm (ARFA) to optimize the RGRU hyperparameters. Our suggested model offers a sophisticated classification framework that effectively divides emotional content into four separate quadrants: positive-high, positive-low, negative-high, and negative-low. The Python programming environment is used to implement our suggested approach. We use the EMOPIA dataset to compare its performance to the traditional approach and assess its effectiveness experimentally. The trial results show better performance compared to traditional methods, with higher accuracy, recall, F-measure, and precision. Copyright © 2023 Kumar and Kathiravan.},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85181443074&doi = 10.3389},
  AUTHOR_KEYWORDS = {adaptive Red Fox algorithm; EMOPIA; emotion recognition; Enhanced Residual Gated Recurrent Unit; Musical Instrument Digital Interface},
  PARADIGM = {classification},
  NOTES_CA = {include, quadrant classification},
  NOTES_TE = {include, midi, classification},
  EMOTIONS = {quadrants},
  EMOTION_LOCUS = {not specified},
  STIMULUS_TYPE = {EMOPIA; pop piano recordings},
  STIMULUS_DURATION = {not specified},
  STIMULUS_DURATION_UNIT = {not specified},
  STIMULUS_N =  {not specified},
  FEATURE_N = {summarize feature categories, but aren't explicit about which ones},
  PARTICIPANT_N = {not specified},
  PARTICIPANT_EXPERTISE = {not specified},
  PARTICIPANT_ORIGIN = {not specified},
  PARTICIPANT_SAMPLING = {not specified},
  PARTICIPANT_TASK = {not specified},
  FEATURE_CATEGORIES = {harmony, dynamic, rhythm}, %% include counts??
  FEATURE_SOURCE = {PrettyMIDI},
  FEATURE_REDUCTION_METHOD = {NA},
  MODEL_CATEGORY = {classification},
  MODEL_DETAIL = {enhanced residual gated recurrent unit},
  MODEL_MEASURE = {accuracy, precision, recall, F score},
  MODEL_COMPLEXITY_PARAMETERS = {Red fox algorithm with Levy flight model; iteration parameter 1-500. Optimization value: 0.18.},
  MODEL_RATE_EMOTION_NAMES = {positive-high, positive-low, negative-high, negative-low},
  MODEL_RATE_EMOTION_VALUES = { rbind(
  'rgru_positive_high_proposed' = c('H' = 157, 'M' = 2, 'FP' = 1, 'CR' = 120),
  'gru_positive_high' = c(143, 5, 3, 99),
  'ltsm_positive_high' = c(132, 10, 8, 110),
  'dnn_positive_high' = c(117,13,10,110),
  'rgru_positive_low_proposed' = c(130,2,1,120),
  'gru_positive_low' = c(124,7,3,119),
  'ltsm_positive_low' = c(120,10,8,115),
  'dnn_positive_low' = c(120,13,10,110),
  'rgru_negative_high_proposed' = c(184,3,3,120),
  'gru_negative_high' = c(187,8,5,110),
  'ltsm_negative_high' = c(162,18,10,120),
  'dnn_negative_high' = c(145,25,10,130),
  'rgru_negative_low_proposed' = c(169,3,3,90),
  'gru_negative_low' = c(162,8,5,90),
  'ltsm_negative_low' = c(136,19,10,100),
  'dnn_negative_low' = c(121,19,15,110)
)},
  MODEL_VALIDATION = {not reported},
  FINAL_NOTES = {Unsure how to standardize confusion matrices for individual quadrants.}
}


@Article{dufour2021us,
  AUTHOR = {Dufour, Isabelle and Tzanetakis, George},
  JOURNAL = {IEEE TRANSACTIONS ON AFFECTIVE COMPUTING},
  MONTH = {JUL-SEP},
  NUMBER = {3},
  PAGES = {666-681},
  TITLE = {Using Circular Models to Improve Music Emotion Recognition},
  VOLUME = {12},
  YEAR = {2021},
  DOI = {10.1109/TAFFC.2018.2885744},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1109/TAFFC.2018.2885744},
  ISSN = {1949-3045},
  ORCID = {Tzanetakis, George/0000-0002-6844-7912},
  RESEARCHERID = {Tzanetakis, George/I-6593-2013},
  PARADIGM = {classification},
  NOTES_CA = {include, classification},
  NOTES_TE = {include, classification},
  EMOTIONS = { c1 (happy): rousing, rowdy, boisterous, thrilling, epic, exhilerated, exalted, ecstatic, spirited;
  c2 (happy): rollicking, fun, cheerful, sweet, sprightly, summery, playful, flirty, NA;
  c3 (sad): autumnal, bittersweet, wistful, nostalgic, sentimental, tender, pensive, regretful, NA;
  c4 (sad): poignant, brooding, melancholic, mournful, tragic, gloomy, dark, creepy, paranoid;
  c5 (mad) = agressive, volatile, fiery, threatening, hostile, belligerent, arrogant, angry, NA
},
  EMOTION_LOCUS = {not specified},
  STIMULUS_TYPE = {pop, disco, soul, rock, jazz, electronic, hip-hop, classical, dance, heavy metal, contemporary, reggae, country, latino, traditional etc.},
  STIMULUS_DURATION = {30},
  STIMULUS_DURATION_UNIT = {seconds},
  STIMULUS_N = {total: 564; unambiguous: 416, circular validation: 39}, %% full set
  FEATURE_N = {126, retained 97},
  PARTICIPANT_N = {groundtruth: 12, circular validation: 8}, %% distinct from the 12 participants who created ground truth; 8 annotators per piece
  PARTICIPANT_EXPERTISE = {not specified},
  PARTICIPANT_ORIGIN = {not specified},
  PARTICIPANT_SAMPLING = {convenience},
  PARTICIPANT_TASK = {rate, label},
  FEATURE_CATEGORIES = {intensity, timbre, rhythm, pitch}, %% they call pitch "register"
  FEATURE_SOURCE = {marsyas},
  FEATURE_REDUCTION_METHOD = {observation from previous studies},
  MODEL_CATEGORY = {categorical},
  MODEL_DETAIL = {categorical: SVM, SMO; pentagon regression: full vs. reduced; 2D regression: full vs. reduced},
  MODEL_MEASURE = {classification accuracy},
  MODEL_COMPLEXITY_PARAMETERS = {categorical: polynomial kernel exponent = -1, epsilon - 12E-12},
  MODEL_RATE_EMOTION_NAMES = {C1,C2,C3,C4,C5},
  MODEL_RATE_EMOTION_VALUES_UNAMBIGUOUS_SMO = {rbind(
    C1 = c('C1' = 52.6, 'C2' = 17.1, 'C3' = 0.0, 'C4' = 9.2, 'C5' = 21.1),
    C2 = c(12, 65.2, 7.6, 4.3, 10.9),
    C3 = c(1.1,9.9,73.6,14.3,1.1),
    C4 = c(3.6,10.8,16.9,61.5,7.2),
    C5 = c(21.9,19.2,0,6.8,52.1)
 )
}
MODEL_RATE_EMOTION_VALUES_FULL.SMO = {rbind(
    C1 = c('C1' = 56.0, 'C2' = 19, 'C3' = 0.0, 'C4' = 7, 'C5' = 18),
    C2 = c(11.3,58.9,13.7,4,12.1),
    C3 = c(0,13.2,68.6,18.2,0),
    C4 = c(4.9,11.7,22.5,54.2,6.7),
    C5 = c(18.5,12.2,1,12.2,56.1)
  )
},
MODEL_RATE_EMOTION_VALUES_FULL_FULLPOLYGONAL = { rbind(
  C1 = c('C1' = 65, 'C2' = 11, 'C3' = 12, 'C4' = 0, 'C5' = 12),
  C2 = c(14.5, 62.1, 22.6, 0.8, 0),
  C3 = c(0, 13.2, 68.6, 18.2, 0),
  C4 = c(0.8, 0, 20.8, 54.2, 24.2),
  C5 = c(38.8, 1, 0, 8.2, 52)
)
},
MODEL_RATE_EMOTION_VALUES_FULL_REDUCEDPOLYGONAL = { rbind(
  C1 = c('C1' = 67, 'C2' = 16, 'C3' = 4, 'C4' = 0, 'C5' = 13),
  C2 = c(21.8,55.6,22.6,0,0),
  C3 = c(0,9.1,77.7,13.2,0),
  C4 = c(0,0,23.3,55,21.6),
  C5 = c(27.5,0,0,10.2,62.3)
)
},
MODEL_RATE_EMOTION_VALUES_FULL_FULL2D =  { rbind(
  C1 = c('C1' = 61, 'C2' = 9, 'C3' = 3, 'C4' = 7, 'C5' = 20),
  C2 = c(23.4,46,18.5,4,8.1),
  C3 = c(2.5,14,74.4,9.1,0),
  C4 = c(7.5,7.5,35.8,39.2,10),
  C5 = c(34.7,16.3,1,12.2,35.7)
)
},
MODEL_RATE_EMOTION_VALUES_FULL_REDUCED2D = { rbind(
  C1 = c('C1' = 67, 'C2' = 10, 'C3' = 3, 'C4' = 0, 'C5' = 20),
  C2 = c(25.8,62.1,12.1,0,0),
  C3 = c(0,11.6,81,7.4,0),
  C4 = c(3.3,2.5,29.2,51.7,13.3),
  C5 = c(40.8,3.1,2,14.3,39.8)
)
},
  MODEL_VALIDATION =  {10-fold cross validation},
  FINAL_NOTES = {}
}


@Article{feng2024ex,
  AUTHOR = {Feng, Qiyue},
  JOURNAL = {Applied Mathematics and Nonlinear Sciences},
  NOTE = {Cited by: 0},
  NUMBER = {1},
  TITLE = {Exploration of the Integration of Traditional Music Cultural Elements in Music Informatization Teaching},
  TYPE = {Article},
  VOLUME = {9},
  YEAR = {2024},
  DOI = {10.2478/amns-2024-0973},
  ABSTRACT = {With the development of information technology, music teaching methods are getting more affluent and more prosperous. This paper proposes a model for emotion classification and assessment that integrates traditional music culture elements with information technology in music teaching. The research first combines TextCNN and BiLSTM algorithms to establish the emotion classification model of conventional music. Then it combines PYIN and DTW algorithms to establish the evaluation model of traditional music, which completes the auxiliary efficacy of music informationized teaching. In the emotion classification test of the model, the classification accuracy and F1 value of emotions of different music samples are 82.98% and 75.22%, respectively. The model’s recognition accuracy of the four voices is 86.76%, and the overall effective scoring percentage is 81.98% under different playing abnormalities. This study has had an impact on traditional music evaluation. The model in this paper can be used to classify and evaluate emotions in conventional music, providing more intelligent and high-quality technical services for integrating traditional music into music teaching. © 2023 Qiyue Feng, published by Sciendo.},
  KEYWORDS = {Quality control; BiLSTM; Classification models; DTW; Emotion classification; Informatization; Music evaluation; PYIN; Teaching methods; TextCNN; Traditional music; Classification (of information)},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85192244090&doi = 10.2478},
  AUTHOR_KEYWORDS = {BiLSTM; DTW; Music evaluation; PYIN; TextCNN; Traditional music},
  PARADIGM = {classification},
  NOTES_CA = {include, classification task},
  NOTES_TE = {include, classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_NOTES = {non-specialized journal? !EXCL!}
}


@Article{hizlisoy2021mu,
  AUTHOR = {Hizlisoy, Serhat and Yildirim, Serdar and Tufekci, Zekeriya},
  JOURNAL = {Engineering Science and Technology, an International Journal},
  NOTE = {Cited by: 85; All Open Access, Gold Open Access},
  NUMBER = {3},
  PAGES = {760 – 767},
  TITLE = {Music emotion recognition using convolutional long short term memory deep neural networks},
  TYPE = {Article},
  VOLUME = {24},
  YEAR = {2021},
  DOI = {10.1016/j.jestch.2020.10.009},
  ABSTRACT = {In this paper, we propose an approach for music emotion recognition based on convolutional long short term memory deep neural network (CLDNN) architecture. In addition, we construct a new Turkish emotional music database composed of 124 Turkish traditional music excerpts with a duration of 30 s each and the performance of the proposed approach is evaluated on the constructed database. We utilize features obtained by feeding convolutional neural network (CNN) layers with log-mel filterbank energies and mel frequency cepstral coefficients (MFCCs) in addition to standard acoustic features. Classification results show that the best performance is obtained when the new feature set is combined with the standard features using the long short term memory (LSTM) + deep neural network (DNN) classi fier. The overall accuracy of 99.19% is obtained using the proposed system with 10 fold cross-validation. Specifically, 6.45 points improvement is achieved. Additionally, the results also show that the LSTM + DNN classifier yields 1.61, 1.61 and 3.23 points improvements in music emotion recognition accuracies compared to k-nearest neighbor (k-NN), support vector machine (SVM), and Random Forest classifiers, respectively. © 2020 Karabuk University},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85096857185&doi = 10.1016},
  AUTHOR_KEYWORDS = {Convolutional long short term memory deep neural networks; Music emotion recognition; Turkish emotional music database},
  PARADIGM = {classification},
  NOTES_CA = {include, classification},
  NOTES_TE = {include, classification},
  EMOTIONS = {valence, arousal},
  EMOTION_LOCUS = {perceived},
  STIMULUS_TYPE = {Turkish traditional music},
  STIMULUS_DURATION = {30},
  STIMULUS_DURATION_UNIT = {s},
  STIMULUS_N = {124},
  FEATURE_N = {8904}, %% multiple feature selections applied
  PARTICIPANT_N = {21},
  PARTICIPANT_EXPERTISE = {university students highly knowledgable about Turkish music},
  PARTICIPANT_ORIGIN = {Turkey},
  PARTICIPANT_SAMPLING = {convenience}, %% presumably
  PARTICIPANT_TASK = {rate},
  FEATURE_CATEGORIES = {standard features: 7368, MFCC: 768, log-mel filterbank: 768}, %% Double-check
  FEATURE_SOURCE = {OpenSMILE, MIRToolbox, MFCC, log-mel filterbank energies},
  FEATURE_REDUCTION_METHOD = {Correlation-based Feature Selection. Adaptive-moment estimation optimizer},
  MODEL_CATEGORY = {classification},
  MODEL_DETAIL = {support vector machine, k-nearest neighbours, randomforest, Long Short-Term Memory-Deep Neural Network},
  MODEL_MEASURE = {accuracy, f_measure, precision, recall},
  MODEL_COMPLEXITY_PARAMETERS = {layer n: 4, filter n: 64, 128, 128, 128,
  ADAM learning rate: 1e-04, training batch n: 10, training epochs: 100, loss function: categorical cross-entropy},
  MODEL_RATE_EMOTION_NAMES = {Quadrant 1, Quadrant 2, Quadrant 3},
  MODEL_RATE_EMOTION_VALUES_KNN = {
  rbind(
    full_feature_set = c(accuracy = 83.87, f_measure = 0.839, precision = 0.840, recall = 0.839),
    correlation_feature_selection = c(accuracy = 89.51, f_measure = 0.904, precision = 0.919, recall = 0.895)
  )
},

MODEL_RATE_EMOTION_VALUES_RANDOMFOREST = {
  rbind(
    full_feature_set = c(accuracy = 87.09, f_measure = 0.843, precision = 0.844, recall = 0.871),
    correlation_feature_selection = c(accuracy = 91.93, f_measure = 0.918, precision = 0.917, recall = 0.919)
  )
},

MODEL_RATE_EMOTION_VALUES_SVM = {
  rbind(
    full_feature_set = c(accuracy = 87.09, f_measure = 0.866, precision = 0.862, recall = 0.871),
    correlation_feature_selection = c(accuracy = 90.32, f_measure = 0.905, precision = 0.907, recall = 0.903)
  )
},

MODEL_RATE_EMOTION_VALUES_LSTM-DNN = {
  rbind(
    full_feature_set = c(accuracy = 87.09, f_measure = 0.864, precision = 0.861, recall = 0.871),
    correlation_feature_selection = c(accuracy = 92.74, f_measure = 0.926, precision = 0.925, recall = 0.927)
  )
},
  MODEL_VALIDATION = {10-fold cross validation},
 FINAL_NOTES = {discuss. unsure how to report features for different models, subsets...} 
}


@Article{nag2022on,
  AUTHOR = {Nag, Sayan and Basu, Medha and Sanyal, Shankha and Banerjee, Archi and Ghosh, Dipak},
  JOURNAL = {Physica A: Statistical Mechanics and its Applications},
  NOTE = {Cited by: 19},
  TITLE = {On the application of deep learning and multifractal techniques to classify emotions and instruments using Indian Classical Music},
  TYPE = {Article},
  VOLUME = {597},
  YEAR = {2022},
  DOI = {10.1016/j.physa.2022.127261},
  ABSTRACT = {Music is often considered as the language of emotions. The way it stimulates the emotional appraisal across people from different communities, culture and demographics has long been known and hence categorizing on the basis of emotions is indeed an intriguing basic research area. Indian Classical Music (ICM) is famous for its ambiguous nature, i.e. its ability to evoke a number of mixed emotions through only a single musical narration, and hence classifying evoked emotions from ICM becomes a more challenging task. With the rapid advancements in the field of Deep Learning, this Music Emotion Recognition (MER) task is becoming more and more relevant and robust, hence can be applied to one of the most challenging test case i.e. classifying emotions elicited from ICM. In this paper we present a new dataset called JUMusEmoDB which presently has 1600 audio clips (approximately 30 s each) where 400 clips each correspond to happy, sad, calm and anxiety emotional scales. The initial annotations and emotional classification of the database was done based on an emotional rating test (5-point Likert scale) performed by 100 participants. The clips have been taken from different conventional ‘raga’ renditions played in two Indian stringed instruments – sitar and sarod by eminent maestros of ICM and digitized in 44.1 kHz frequency. The ragas, which are unique to ICM, are described as musical structures capable of inducing different moods or emotions. For supervised classification purposes, we have used Convolutional Neural Network (CNN) based architectures (resnet50, mobilenet v2.0, squeezenet v1.0 and a proposed ODE-Net) on corresponding music spectrograms of the 6400 sub-clips (where every clip was segmented into 4 sub-clips) which contain both time as well as frequency domain information. Along with emotion classification, instrument classification based response was also attempted on the same dataset using the CNN based architectures. In this context, a nonlinear technique, Multifractal Detrended Fluctuation Analysis (MFDFA) was also applied on the musical clips to classify them on the basis of complexity values extracted from the method. The initial classification accuracy obtained from the applied methods are quite inspiring and have been corroborated with ANOVA results to determine the statistical significance. This type of CNN based classification algorithm using a rich corpus of Indian Classical Music is unique even in the global perspective and can be replicated in other modalities of music also. The link to this newly developed dataset has been provided in the dataset description section of the paper. This dataset is still under development and we plan to include more data containing other emotional as well as instrumental entities into consideration. © 2022 Elsevier B.V.},
  KEYWORDS = {Classification (of information); Convolutional neural networks; Deep learning; Frequency domain analysis; Music; Network architecture; Convolutional neural network; Emotion; Emotion recognition; Indian classical music; Learning techniques; Multifractal detrended fluctuation analysis; Multifractal technique; Music emotions; Network-based architectures; Research areas; Fractals},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85127732290&doi = 10.1016},
  AUTHOR_KEYWORDS = {Classification; CNN; Emotions; Indian Classical Music; Instruments; MFDFA},
  PARADIGM = {classification},
  NOTES_CA = {include, classification},
  NOTES_TE = {include, basic emotion classification},
  EMOTIONS = {happy, sad, calm, angry},
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = {30},
  STIMULUS_DURATION_UNIT = {seconds},
  STIMULUS_N = {1600}, %% each clip segmented into 4 subclips for total of 6400...
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = {convolutional neural network, Multifractal Detrended Fluctuation Analysis, ResNet50, MobileNet v.20, SqueezeNet v1.0, ODE-Net}, %% ODE-Net proposed 
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_NOTES = {discuss; image-processing approach applied to spectrograms for emotional classification !EXCL!}
}


@Article{nguyen2017an,
  AUTHOR = {Nguyen, Van Loi and Kim, Donglim and Ho, Van Phi and Lim, Younghwan},
  JOURNAL = {International Journal of Electrical and Computer Engineering},
  NOTE = {Cited by: 14},
  NUMBER = {3},
  PAGES = {1246 – 1254},
  TITLE = {A new recognition method for visualizing music emotion},
  TYPE = {Article},
  VOLUME = {7},
  YEAR = {2017},
  DOI = {10.11591/ijece.v7i3.pp1246-1254},
  ABSTRACT = {This paper proposes an emotion detection method using a combination of dimensional approach and categorical approach. Thayer's model is divided into discrete emotion sections based on the level of arousal and valence. The main objective of the method is to increase the number of detected emotions which is used for emotion visualization. To evaluate the suggested method, we conducted various experiments with supervised learning and feature selection strategies. We collected 300 music clips with emotions annotated by music experts. Two feature sets are employed to create two training models for arousal and valence dimensions of Thayer's model. Finally, 36 music emotions are detected by proposed method. The results showed that the suggested algorithm achieved the highest accuracy when using RandomForest classifier with 70% and 57.3% for arousal and valence, respectively. These rates are better than previous studies. © 2017 Institute of Advanced Engineering and Science.},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85021156960&doi = 10.11591},
  AUTHOR_KEYWORDS = {Feature extraction; Music emotion recognition algorithm; Music information retrieval; Music mood detection},
  PARADIGM = {classification},
  NOTES_CA = {include, classification},
  NOTES_TE = {include},
  EMOTIONS = {288 emotion labels}, %% used for classification of VA. Presumably quadrant detection
  EMOTION_LOCUS = {perceived},
  STIMULUS_TYPE = {AMG},
  STIMULUS_DURATION = {30},
  STIMULUS_DURATION_UNIT = {s},
  STIMULUS_N = {300},
  FEATURE_N = {397},
  PARTICIPANT_N = {not specified}, %% AMG
  PARTICIPANT_EXPERTISE = {experts}, %% AMG
  PARTICIPANT_ORIGIN = {not specified},
  PARTICIPANT_SAMPLING = {not specified},
  PARTICIPANT_TASK = {annotate},
  FEATURE_CATEGORIES = {Harmony, Rhythm, Dynamic, Timbre, Melody},
  FEATURE_SOURCE = {Sound Description Toolbox, MIR Toolbox, Marsyas, PsySound},
  FEATURE_REDUCTION_METHOD = {WEKA},
  MODEL_CATEGORY = {classification},
  MODEL_DETAIL = {RandomForest},
  MODEL_MEASURE = {Classification accuracy},
  MODEL_COMPLEXITY_PARAMETERS = {Feature reduction: Attribute Evaluator = CfsSubsetEval,  Search Method= BestFirst},
  MODEL_RATE_EMOTION_NAMES = {valence, arousal},
  MODEL_RATE_EMOTION_VALUES_AROUSAL = {rbind(
  level_1 = c('level_1' = 44,'level_2' = 0,'level_3' = 4,'level_4' = 1,'level_5' = 0,'level_6' = 1, 'recall' = 88),
  level_2 = c(0,47,1,0,2,0,94),
  level_3 = c(15,0,24,8,0,3,48),
  level_4 = c(6,0,16,15,0,13,30),
  level_5 = c(0,3,0,0,47,0,94),
  level_6 = c(0,0,4,13,0,33,66)
) },
MODEL_RATE_EMOTION_VALUES_VALENCE = {rbind(
  level_1 = c('level_1' = 20,'level_2' = 0,'level_3' = 11,'level_4' = 6,'level_5' = 9,'level_6' = 4, 'recall' = 40),
  level_2 = c(1,36,0,2,7,4,72),
  level_3 = c(12,0,23,11,3,1,46),
  level_4 = c(4,0,11,31,4,0,62),
  level_5 = c(9,0,4,5,29,3,58),
  level_6 = c(8,3,2,1,3,33,66)
)
},
  MODEL_VALIDATION = {10-fold cross-validation},
  FINAL_NOTES = {}
}


@Article{panda2020no,
  AUTHOR = {Panda, Renato and Malheiro, Ricardo and Paiva, Rui Pedro},
  JOURNAL = {IEEE Transactions on Affective Computing},
  NOTE = {Cited by: 83; All Open Access, Green Open Access},
  NUMBER = {4},
  PAGES = {614 – 626},
  TITLE = {Novel Audio Features for Music Emotion Recognition},
  TYPE = {Article},
  VOLUME = {11},
  YEAR = {2020},
  DOI = {10.1109/TAFFC.2018.2820691},
  ABSTRACT = {This work advances the music emotion recognition state-of-the-art by proposing novel emotionally-relevant audio features. We reviewed the existing audio features implemented in well-known frameworks and their relationships with the eight commonly defined musical concepts. This knowledge helped uncover musical concepts lacking computational extractors, to which we propose algorithms - namely related with musical texture and expressive techniques. To evaluate our work, we created a public dataset of 900 audio clips, with subjective annotations following Russell's emotion quadrants. The existent audio features (baseline) and the proposed features (novel) were tested using 20 repetitions of 10-fold cross-validation. Adding the proposed features improved the F1-score to 76.4 percent (by 9 percent), when compared to a similar number of baseline-only features. Moreover, analysing the features relevance and results uncovered interesting relations, namely the weight of specific features and musical concepts to each emotion quadrant, and warrant promising new directions for future research in the field of music emotion recognition, interactive media, and novel music interfaces. © 2010-2012 IEEE.},
  KEYWORDS = {Feature extraction; Speech recognition; Textures; 10-fold cross-validation; Affective Computing; Audio database; Emotion recognition; Interactive media; Music information retrieval; Music interfaces; Musical concepts; Audio acoustics},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85044777756&doi = 10.1109},
  AUTHOR_KEYWORDS = {Affective computing; audio databases; emotion recognition; feature extraction; music information retrieval},
  PARADIGM = {classification},
  NOTES_CA = {include, quadrant classification},
  NOTES_TE = {include, classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = {AllMusic},
  STIMULUS_DURATION = {30},
  STIMULUS_DURATION_UNIT = {s},
  STIMULUS_N = {900},
  FEATURE_N = { },
  PARTICIPANT_N = {not specified},
  PARTICIPANT_EXPERTISE = {experts},
  PARTICIPANT_ORIGIN = {not specified},
  PARTICIPANT_SAMPLING = {not specified},
  PARTICIPANT_TASK = {c(experts = 'annotate', subjects = 'rate')}, %% task given to subjects, for which no information seems to be provided. subjects rate VA and the experts previously categorized
  FEATURE_CATEGORIES = {},
  FEATURE_SOURCE = { }, %% Requires discussion
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = {20 x 10-fold cross validation},
  FINAL_NOTES = {DISCUSS}
}


@Article{sorussa2020em,
  AUTHOR = {Sorussa, Kanawat and Choksuriwong, Anant and Karnjanadecha, Montri},
  JOURNAL = {ECTI Transactions on Computer and Information Technology},
  NOTE = {Cited by: 4; All Open Access, Gold Open Access},
  NUMBER = {1},
  PAGES = {53 – 66},
  TITLE = {Emotion classi cation system for digital music with a cascaded technique},
  TYPE = {Article},
  VOLUME = {14},
  YEAR = {2020},
  DOI = {10.37936/ecti-cit.2020141.205317},
  ABSTRACT = {Music selection is diffcult without effcient organization based on metadata or tags, and one effective tag scheme is based on the emotion expressed by the music. However, manual annotation is labor intensive and unstable because the perception of music emotion varies from person to person. This paper presents an emotion classi cation system for digital music with a resolution of eight emotional classes. Russell’s emotion model was adopted as common ground for emotional annotation. The music information retrieval (MIR) toolbox was employed to extract acoustic features from audio les. The classi cation system utilized a supervised machine learning technique to recognize acoustic features and create predictive models. Four predictive models were proposed and compared. The models were composed by crossmatching two types of neural networks, the Levenberg-Marquardt (LM) and resilient backpropagation (Rprop), with two types of structures: a traditional multiclass model and the cascaded structure of a binary-class model. The performance of each model was evaluated via the MediaEval Database for Emotional Analysis (DEAM) benchmark. The best result was achieved by the model trained with the cascaded Rprop neural network (accuracy of 89.5%). In addition, correlation coeffcient analysis showed that timbre features were the most impactful for prediction. Our work offers an opportunity for a competitive advantage in music classi cation because only a few music providers currently tag music with emotional terms. © 2020, ECTI Association Sirindhon International Institute of Technology. All rights reserved.},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85084006024&doi = 10.37936},
  AUTHOR_KEYWORDS = {Articial neural networks; Classi cation algorithms; Emotion recognition; Music information retrieval},
  PARADIGM = {classification},
  NOTES_CA = {include, classification},
  NOTES_TE = {include, classification},
  EMOTIONS = {valence, arousal},
  EMOTION_LOCUS = {perceived},
  STIMULUS_TYPE = {DEAM},
  STIMULUS_DURATION = {not specified},
  STIMULUS_DURATION_UNIT = {not specified},
  STIMULUS_N = {1802}, %% report train or test?
  FEATURE_N = {122},
  PARTICIPANT_N = {not specified},
  PARTICIPANT_EXPERTISE = {not specified},
  PARTICIPANT_ORIGIN = {MTurk},
  PARTICIPANT_SAMPLING = {crowdsourced},
  PARTICIPANT_TASK = {rate},
  FEATURE_CATEGORIES = {dynamics, rhythm, timbre, pitch, tonality}, %% check against Panda
  FEATURE_SOURCE = {MIR Toolbox},
  FEATURE_REDUCTION_METHOD = {none},
  MODEL_CATEGORY = {classification},
  MODEL_DETAIL = {Levenberg-Marquardt neural network, Rprop neural network}, %% resilient back-propogation
  MODEL_MEASURE = {classification accuracy},
  MODEL_COMPLEXITY_PARAMETERS = {multiclass lm: architecture = 122-10-2, gradient min = 1.0e-07, failing max = 10\n; lvl 1 cascaded lm: architecture = 122-10-2, gradient min = 1.0e-07, failing max = 10;\n lvl 2 cascaded lm: architecture = 122-10-2, gradient min = 1.0e-07, failing max = 10;\n lvl 3 cascaded lm: architecture = 122-10-2, gradient min = 1.0e-21, failing max = 10;\n multiclassr prop: architecture = 122-10-8, gradient min = 1.0e-50, failing max = 300;\n lvl 1 cascaded rprop: architecture = 122-10-2, gradient min = 1.0e-50, failing max = 300;\n lvl 2 cascaded rprop: architecture = 122-10-2, gradient min = 1.0e-100, failing max = 300;\n lvl 3 cascaded rprop: architecture = 122-10-2, gradient min = 1.0e-100, failing max = 300},
  MODEL_RATE_EMOTION_NAMES = {valence, arousal},
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = {validation set}, % train, test and validation sets. They report 7/3 ratio (training set/test set)
  FINAL_NOTES = {discuss. Unsure how to report results}
}


@Article{yang2021an,
  AUTHOR = {Yang, Jing},
  JOURNAL = {Frontiers in Psychology},
  NOTE = {Cited by: 14; All Open Access, Gold Open Access, Green Open Access},
  TITLE = {A Novel Music Emotion Recognition Model Using Neural Network Technology},
  TYPE = {Article},
  VOLUME = {12},
  YEAR = {2021},
  DOI = {10.3389/fpsyg.2021.760060},
  ABSTRACT = {Music plays an extremely important role in people’s production and life. The amount of music is growing rapidly. At the same time, the demand for music organization, classification, and retrieval is also increasing. Paying more attention to the emotional expression of creators and the psychological characteristics of music are also indispensable personalized needs of users. The existing music emotion recognition (MER) methods have the following two challenges. First, the emotional color conveyed by the first music is constantly changing with the playback of the music, and it is difficult to accurately express the ups and downs of music emotion based on the analysis of the entire music. Second, it is difficult to analyze music emotions based on the pitch, length, and intensity of the notes, which can hardly reflect the soul and connotation of music. In this paper, an improved back propagation (BP) algorithm neural network is used to analyze music data. Because the traditional BP network tends to fall into local solutions, the selection of initial weights and thresholds directly affects the training effect. This paper introduces artificial bee colony (ABC) algorithm to improve the structure of BP neural network. The output value of the ABC algorithm is used as the weight and threshold of the BP neural network. The ABC algorithm is responsible for adjusting the weights and thresholds, and feeds back the optimal weights and thresholds to the BP neural network system. BP neural network with ABC algorithm can improve the global search ability of the BP network, while reducing the probability of the BP network falling into the local optimal solution, and the convergence speed is faster. Through experiments on public music data sets, the experimental results show that compared with other comparative models, the MER method used in this paper has better recognition effect and faster recognition speed. © Copyright © 2021 Yang.},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85116855247&doi = 10.3389},
  AUTHOR_KEYWORDS = {ABC algorithm; BP neural network; emotion recognition; MediaEval Emotion in Music data set; music},
  PARADIGM = {classification},
  NOTES_CA = {include, classification},
  NOTES_TE = {include, classification},
  EMOTIONS = {happy, sad, nervous, calm},
  EMOTION_LOCUS = {perceived},
  STIMULUS_TYPE = {MediaEval},
  STIMULUS_DURATION = {45},
  STIMULUS_DURATION_UNIT =  {s},
  STIMULUS_N = {744},
  FEATURE_N = {}, %% 7 in best model
  PARTICIPANT_N = {specified elsewhere},
  PARTICIPANT_EXPERTISE = {specified elsewhere},
  PARTICIPANT_ORIGIN = {specified elsewhere},
  PARTICIPANT_SAMPLING = {specified elsewhere},
  PARTICIPANT_TASK = {specified elsewhere},
  FEATURE_CATEGORIES = {timbre, dynamic, },
  FEATURE_SOURCE =  {not specified},
  FEATURE_REDUCTION_METHOD =  {back-propogation},
  MODEL_CATEGORY = {regression},
  MODEL_DETAIL = {back-propogation neural network, artificial bee colony algorithm, SVM, KNN, GMM, BP},
  MODEL_MEASURE = {MAE, RMSE, R2},
  MODEL_COMPLEXITY_PARAMETERS = {not specified},
  MODEL_RATE_EMOTION_NAMES = {happy, sad, nervous, calm}, 
  MODEL_RATE_EMOTION_VALUES = {
rbind(
    SVM = c(MAE_valence = 1.1253, RMSE_valence = 0.1392, R2_valence = 0.4374,
            MAE_arousal = 1.3091, RMSE_arousal = 0.1405, R2_arousal = 0.5663),
    KNN = c(1.0974,0.2988,0.4403,
            1.1686,0.3265,0.5881),
    GMM = c(1.2342,0.3072,0.4236,
            1.4105,0.3136,0.5712),
    BP = c(1.1066,0.1190,0.4576,
           1.1987,0.1284,0.6284),
    proposed = c(0.8872,0.1066,0.4606,
                 0.9156,0.1322,0.6687)
)},
  MODEL_VALIDATION = {80/20 train-test split},
  FINAL_NOTES = {discuss}
}


@Article{yeh2014po,
  AUTHOR = {Yeh, Chia-Hung and Tseng, Wen-Yu and Chen, Chia-Yen and Lin, Yu-Dun and Tsai, Yi-Ren and Bi, Hsuan-I and Lin, Yu-Ching and Lin, Ho-Yi},
  JOURNAL = {Multimedia Tools and Applications},
  NOTE = {Cited by: 11},
  NUMBER = {3},
  PAGES = {2103 – 2128},
  TITLE = {Popular music representation: chorus detection & emotion recognition},
  TYPE = {Article},
  VOLUME = {73},
  YEAR = {2014},
  DOI = {10.1007/s11042-013-1687-2},
  ABSTRACT = {This paper proposes a popular music representation strategy based on the song’s emotion. First, a piece of popular music is decomposed into chorus and verse segments through the proposed chorus detection algorithm. Three descriptive features: intensity, frequency band and rhythm regularity are extracted from the structured segments for emotion detection. A hierarchical Adaboost classifier is employed to recognize the emotion of a piece of popular music. The general emotion of the music is classified according to Thayer’s model into four emotions: happy, angry, depressed and relaxed. Experiments conducted on a 350-popular-music database show the average recall and precision of our proposed chorus detection are approximately 95 % and 84 %, respectively; and the average precision rate of emotion detection is 92 %. Additional tests are performed on songs with cover versions in different lyrics and languages, and the resultant precision rate is 90 %. The proposes approaches have been tested and proven by the professional online music company, KKBOX Inc. and show promising performance for effectively and efficiently identifying the emotions of a variety of popular music. © 2013, Springer Science+Business Media New York.},
  KEYWORDS = {Multimedia systems; Chorus; Emotion; MFCCs; Popular music; Rhythm; Verse; Adaptive boosting},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-84912027522&doi = 10.1007},
  AUTHOR_KEYWORDS = {Adaboost; Chorus; Emotion; MFCCs; Popular music; Rhythm; Verse},
  PARADIGM = {classification},
  NOTES_CA = {include, classification},
  NOTES_TE = {include, or unclear, report classification broadly/vaguely},
  EMOTIONS = {happy, angry, depressed, relaxed},
  EMOTION_LOCUS = {perceived},
  STIMULUS_TYPE = {chorus of popular music, chorus of MIREX 2009 collection},
  STIMULUS_DURATION = {},
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = {350},
  FEATURE_N = {3}, %% intensity, rhythm regularity, frequency subband
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = {classification},
  MODEL_DETAIL = {Adaboost},
  MODEL_MEASURE = {precision, recall},
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_NOTES = {}
}


@Article{zhang2017fe,
  AUTHOR = {Zhang, Jiang Long and Huang, Xiang Lin and Yang, Li Fang and Xu, Ye and Sun, Shu Tao},
  JOURNAL = {Multimedia Systems},
  NOTE = {Cited by: 10},
  NUMBER = {2},
  PAGES = {251 – 264},
  TITLE = {Feature selection and feature learning in arousal dimension of music emotion by using shrinkage methods},
  TYPE = {Article},
  VOLUME = {23},
  YEAR = {2017},
  DOI = {10.1007/s00530-015-0489-y},
  ABSTRACT = {Music emotion recognition is an important topic in music information retrieval area. A lot of acoustic features are used to train a music classification or regression emotion model. However, these existing features may not be efficient for classification or regression task. Furthermore, most works do not explain why these features do work for classification. In our work, eight features are extracted to represent the arousal dimension of music emotion, and various commonly used statistical learning methods such as Logistic Regression, and tree-based methods are applied to interpret important features. Then the shrinkage methods are applied to feature selection and classification in music emotion recognition for the first time. Our tests show that the proposed approaches are efficient for feature selection just as entropy-based filter methods, and better than wrapper methods. The shrinkage methods can produce more continuous and low variance model than wrapper methods. Then, we discover that the most useful features are low specific loudness sensation coefficients (low-SONE), root mean square and loudness-flux. Moreover, the shrinkage methods apply in logistic regression perform better for classification than most of other methods. We get an average accuracy rate of 83.8 .  2015, Springer-Verlag Berlin Heidelberg.},
  KEYWORDS = {Feature extraction; Regression analysis; Shrinkage; Speech recognition; Feature selection and classification; Features learning; Features selection; Music classification; Music information retrieval; Shrinkage methods; Statistical learning; Statistical learning methods; Classification (of information)},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-84944937222&doi = 10.1007},
  AUTHOR_KEYWORDS = {Features learning; Features selection; Music arousal dimension classification; Shrinkage method; Statistical learning},
  PARADIGM = {classification},
  NOTES_CA = {include, classification},
  NOTES_TE = {include},
  EMOTIONS = {arousal},
  EMOTION_LOCUS = {perceived},
  STIMULUS_TYPE = {Chinese popular songs},
  STIMULUS_DURATION = {30},
  STIMULUS_DURATION_UNIT = {s},
  STIMULUS_N = {171},
  FEATURE_N = {8},
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = {dynamic},
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = {Relief-F, CFS, Pearson correlation filter, Spearman correlation filter, information gain, gain-ratio },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_NOTES = {discuss, insufficient details on classification} 
}


@Article{bai2016di,
  AUTHOR = {Bai, Junjie and Feng, Lixiao and Peng, Jun and Shi, Jinliang and Luo, Kan and Li, Zuojin and Liao, Lu and Wang, Yingxu},
  JOURNAL = {INTERNATIONAL JOURNAL OF COGNITIVE INFORMATICS AND NATURAL INTELLIGENCE},
  MONTH = {OCT-DEC},
  NUMBER = {4},
  PAGES = {74-89},
  TITLE = {Dimensional Music Emotion Recognition by Machine Learning},
  VOLUME = {10},
  YEAR = {2016},
  DOI = {10.4018/IJCINI.2016100104},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.4018/IJCINI.2016100104},
  ISSN = {1557-3958},
  EISSN = {1557-3966},
  ORCID = {Wang, Yingxu/0000-0003-0445-3632 Peng, Jun/0000-0001-6800-0064 luo, kan/0000-0003-2317-6714},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include},
  EMOTIONS = {valence, arousal},
  EMOTION_LOCUS = {not specified},
  STIMULUS_TYPE = {MediaEval},
  STIMULUS_DURATION = {45},
  STIMULUS_DURATION_UNIT = {s},
  STIMULUS_N = {744}, %% break into train and test sets?
  FEATURE_N = {548}, %% pre-reduction. after reduction, 139 for PCA and 276 for ReliefF
  PARTICIPANT_N = {not specified},
  PARTICIPANT_EXPERTISE = {not specified}, %% could try to pull this from original study
  PARTICIPANT_ORIGIN = {not specified},
  PARTICIPANT_SAMPLING = {not specified},
  PARTICIPANT_TASK = {rate},
  FEATURE_CATEGORIES = {rhythm, dynamic, timbre, harmony, pitch},
  FEATURE_SOURCE = {Daubechies, MIRToolbox, Sound Description toolbox},
  FEATURE_REDUCTION_METHOD = {pca, reliefF},
  MODEL_CATEGORY = {regression},
  MODEL_DETAIL = {SVR, MARS, RFR, RNN},
  MODEL_MEASURE = {R2},
  MODEL_COMPLEXITY_PARAMETERS = {549},
  MODEL_RATE_EMOTION_NAMES = {valence, arousal},
  MODEL_RATE_EMOTION_VALUES = {},
  MODEL_VALIDATION = {none},
  FINAL_NOTES = {}
}


@Article{battcock2021in,
  AUTHOR = {Battcock, Aimee and Schutz, Michael},
  JOURNAL = {JOURNAL OF NEW MUSIC RESEARCH},
  MONTH = {OCT 20},
  NUMBER = {5},
  PAGES = {447-468},
  TITLE = {Individualized interpretation: Exploring structural and interpretive effects on evaluations of emotional content in Bach's Well Tempered Clavier},
  VOLUME = {50},
  YEAR = {2021},
  DOI = {10.1080/09298215.2021.1979050},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1080/09298215.2021.1979050},
  ISSN = {0929-8215},
  EISSN = {1744-5027},
  EARLYACCESSDATE = {JAN 2022},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include, commonality analysis},
  EMOTIONS = {valence, arousal},
  EMOTION_LOCUS = {perceived},
  STIMULUS_TYPE = {prelude},
  STIMULUS_DURATION = {8},
  STIMULUS_DURATION_UNIT = {measure},
  STIMULUS_N = {336}, %% 48 excerpts times 9 performers
  FEATURE_N = {3},
  PARTICIPANT_N = {180},
  PARTICIPANT_EXPERTISE = {nonmusicians},
  PARTICIPANT_ORIGIN = {canada},
  PARTICIPANT_SAMPLING = {convenience},
  PARTICIPANT_TASK = {rate},
  FEATURE_CATEGORIES = {melody, harmony, rhythm},
  FEATURE_SOURCE = {manual},
  FEATURE_REDUCTION_METHOD = {none},
  MODEL_CATEGORY = {regression},
  MODEL_DETAIL = {commonality analysis, multiple regression},
  MODEL_MEASURE = {R2},
  MODEL_COMPLEXITY_PARAMETERS = {4},
  MODEL_RATE_EMOTION_NAMES = {valence, arousal},
  MODEL_RATE_EMOTION_VALUES = {c('valence'  = 0.808, 'arousal' = 0.788)},
  MODEL_VALIDATION = {none},
  FINAL_NOTES = {}
}


@Article{beveridge2018po,
  AUTHOR = {Beveridge, Scott and Knox, Don},
  JOURNAL = {PSYCHOLOGY OF MUSIC},
  MONTH = {MAY},
  NUMBER = {3},
  PAGES = {411-423},
  TITLE = {Popular music and the role of vocal melody in perceived emotion},
  VOLUME = {46},
  YEAR = {2018},
  DOI = {10.1177/0305735617713834},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1177/0305735617713834},
  ISSN = {0305-7356},
  EISSN = {1741-3087},
  ORCID = {Knox, Don/0000-0003-1303-1183},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include, R},
  EMOTIONS = {valence, arousal},
EMOTION_LOCUS = {perceived},
STIMULUS_TYPE = {stimuli composed in style of popular music; vocal line substituted for utterances},
STIMULUS_DURATION = {~30},
STIMULUS_DURATION_UNIT = {s},
STIMULUS_N = {study 1: 100; study 2: 20},
FEATURE_N = {16},
PARTICIPANT_N = {study 1: 196; study 2: 30}, %% each excerpt assessed by 49
PARTICIPANT_EXPERTISE = {not specified},
PARTICIPANT_ORIGIN = {audio or psychology programs},
PARTICIPANT_SAMPLING = {convenience},
PARTICIPANT_TASK = {rate},
FEATURE_CATEGORIES = {dynamics, timbre, harmony, rhythm}, %% their breakdown of features likely differs from panda's, but use similar category names p. 414
FEATURE_SOURCE = {MIDI Toolbox, jSymbloic}, %% report on midi features but not audio features
FEATURE_REDUCTION_METHOD = {recursive feature elimination},
MODEL_CATEGORY = {regression},
MODEL_DETAIL = {random forest},
MODEL_MEASURE = {R2, AIC},
MODEL_COMPLEXITY_PARAMETERS = { },
MODEL_RATE_EMOTION_NAMES = {valence, arousal},
MODEL_RATE_EMOTION_VALUES = {mrbind(internal_validation_model = c('valence' = 0.34, 'arousal' = 0.79) ,
external_validation_model = c('valence' = 0.61, 'arousal' = 0.69))},
MODEL_VALIDATION = {cross validation, external validation (study 2)},
FINAL_NOTES = {models differ in number of relevant features.}
}



@Article{cao2023th,
  AUTHOR = {Cao, Yujing and Park, Jinwan},
  JOURNAL = {IEEE ACCESS},
  PAGES = {141192-141204},
  TITLE = {The Analysis of Music Emotion and Visualization Fusing Long Short-Term Memory Networks Under the Internet of Things},
  VOLUME = {11},
  YEAR = {2023},
  DOI = {10.1109/ACCESS.2023.3341926},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1109/ACCESS.2023.3341926},
  ISSN = {2169-3536},
  PARADIGM = {regression},
  NOTES_CA = {include, single-modality reported},
  NOTES_TE = {include, some quality issues},
  EMOTIONS = {valence, arousal},
  EMOTION_LOCUS = {not specified},
  STIMULUS_TYPE = {million songs dataset}, %% blues, classical, country, disco, hiphop, jazz, metal, pop, reggae, rock
  STIMULUS_DURATION = {not specified},
  STIMULUS_DURATION_UNIT = {not specified},
  STIMULUS_N = {1000}, %% it's called the million songs dataset, but authors say it contains 10 genres x 100 songs? webpage also provides access to random 10,000 subset. unclear what they used
  FEATURE_N = {not specified}, %% "rich musical features covering rhythm, melody, harmony, and music type"
  PARTICIPANT_N = {not specified}, %% use million songs database. I think it auto-fetches tags from last.fm
  PARTICIPANT_EXPERTISE = {not specified}, %% use million songs database
  PARTICIPANT_ORIGIN = {online},
  PARTICIPANT_SAMPLING = {convenience},
  PARTICIPANT_TASK = {not specified},
  FEATURE_CATEGORIES = {not specified}, %% complete list here: http://millionsongdataset.com/pages/example-track-description/
  FEATURE_SOURCE = {million song dataset},
  FEATURE_REDUCTION_METHOD = {}, %% unclear whether the STS is the feature reduction method. Mention inspiration from Scaled Additional Interaction Regression and Gapprox, and dyanmic-voltage and Frequency scaling 

  MODEL_CATEGORY = {regression},
  MODEL_DETAIL = {RNN, LTSM network, STS}, %% long-short-term memory network; sequence-to-sequence
  MODEL_MEASURE = {R2}, 
  MODEL_COMPLEXITY_PARAMETERS = {1 pre-training layer; 256 hidden units; 1*128 fusion features; 0.001 learning rate; Batch size: 128;  Regularization intensity and discard rate: 0.2}, %% as reported in manuscript, excluding a few tuning parameters
  MODEL_RATE_EMOTION_NAMES = {valence, arousal},
  MODEL_RATE_EMOTION_VALUES = {c('valence' = 0.456, 'arousal' = 0.439)}, %% results of model without IoT features. Unclear what model they're reporting on but likely the same RNN using LTSM and STS
  MODEL_VALIDATION = {cross-validation},
  FINAL_NOTES = {}
}


@Article{chen2017co,
  AUTHOR = {Chen, Yu-An and Wang, Ju-Chiang and Yang, Yi-Hsuan and Chen, Homer H.},
  JOURNAL = {IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING},
  MONTH = {JUL},
  NUMBER = {7},
  PAGES = {1409-1420},
  TITLE = {Component Tying for Mixture Model Adaptation in Personalization of Music Emotion Recognition},
  VOLUME = {25},
  YEAR = {2017},
  DOI = {10.1109/TASLP.2017.2693565},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1109/TASLP.2017.2693565},
  ISSN = {2329-9290},
  EISSN = {2329-9304},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include},
  EMOTIONS = {valence, arousal, 38 mood tags}, %% propose AMG1608 dataset
  EMOTION_LOCUS = {not specified},
  STIMULUS_TYPE = {AMG1608}, %% use AMG1608 data set
  STIMULUS_DURATION = {30},
  STIMULUS_DURATION_UNIT = {seconds},
  STIMULUS_N = {1838},
  FEATURE_N = {72},
  PARTICIPANT_N = {665},
  PARTICIPANT_EXPERTISE = {not specified},
  PARTICIPANT_ORIGIN = {22 from campus, 643 from mechanical turk},
  PARTICIPANT_SAMPLING = {convenience, online},
  PARTICIPANT_TASK = {annotation},
  FEATURE_CATEGORIES = {Timbre = 67, Temporal? = 4}, %% temporal: shape statistics of audio signal? not sure if fits entirely
  FEATURE_SOURCE = {Yaafe, MIRToolbox},
  FEATURE_REDUCTION_METHOD = {not specified},
  MODEL_CATEGORY = {regression},
  MODEL_DETAIL = {acoustic emotion gaussians, GMM, emotion GMM, MAP linear regression },
  MODEL_MEASURE = {R2, E},
  MODEL_COMPLEXITY_PARAMETERS = {frame size = 50ms, frame overlap = 25ms, segment = 16 frames, segment overlap = 12 frames; component tying methods = c(single group tying,  quadrant-wise tying, hierarchical-tying, random tying)},
  MODEL_RATE_EMOTION_NAMES = {valence, arousal},
  MODEL_RATE_EMOTION_VALUES = {
rbind(
  baseline = c('euclid' = 0.45, 'r2_valence' = -0.84, 'r2_arousal' =  -0.06,
  'euclid.sd' = 0.09, 'r2_valence.sd' = 0.09, 'r2_arousal.sd' = 0.09),
  mfcc = c(0.3,0.14, 0.61, 0.15,0.01,0.01),
  tonal = c(0.29,0.13, 0.65, 0.15,0.03,0.02),
  spectral = c(0.32,0.06, 0.56, 0.16,0.02,0.02),
  temporal = c(0.36,0.08, 0.30, 0.17,0.01,0.02),
  all = c(0.29,0.15, 0.67, 0.15,0.01,0.02)
)},
#MODEL_RATE_PERFORMANCE_SD = {
#rbind(
#  baseline = c('euclid' = 0.09, 'valence' = 0.09, 'arousal' =  0.08),
#  mfcc = c(0.15,0.01,0.01),
#  tonal = c(0.15,0.03,0.02),
#  spectral = c(0.16,0.02,0.02),
#  temporal = c(0.17,0.01,0.02),
#  all = c(0.15,0.01,0.02)
#)
#},
#MODEL_RATE_PERFORMANCE_PERSONALIZED_COMPONENT_TYING = {
#rbind(
#  no_personal_annotations = c('valence' = -0.28, 'arousal' =  0.36),
# 20_personal_annotations_component_tying_method = c(0.01, 0.54)
#)
# },
  MODEL_VALIDATION = {three-fold cross-validation},
  FINAL_NOTES = {}
 
}


@Article{chin2018pr,
  AUTHOR = {Chin, Yu-Hao and Wang, Jia-Ching and Wang, Ju-Chiang and Yang, Yi-Hsuan},
  JOURNAL = {IEEE TRANSACTIONS ON AFFECTIVE COMPUTING},
  MONTH = {OCT-DEC},
  NUMBER = {4},
  PAGES = {541-549},
  TITLE = {Predicting the Probability Density Function of Music Emotion Using Emotion Space Mapping},
  VOLUME = {9},
  YEAR = {2018},
  DOI = {10.1109/TAFFC.2016.2628794},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1109/TAFFC.2016.2628794},
  ISSN = {1949-3045},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include, R2},
  EMOTIONS = {valence, arousal},
  EMOTION_LOCUS = {perceived},
  STIMULUS_TYPE = {NTUMIR, MediaEval2013},
  STIMULUS_DURATION = {NTUMIR: 30, MediaEval2013: specified elsewhere},
  STIMULUS_DURATION_UNIT = {seconds},
  STIMULUS_N = {NTUMIR: 60, MediaEval2013: 744},
  FEATURE_N = {NTUMIR: ?; MediaEval: 6669},
  PARTICIPANT_N = {c(NTUMIR = 99, MediaEval2013 = ?)},
  PARTICIPANT_EXPERTISE = {no limitation},
  PARTICIPANT_ORIGIN = {NTUMIR: Taiwan; MediaEval2013: United States},
  PARTICIPANT_SAMPLING = {NTUMIR: specified on web; MediaEval2013: MTurk},
  PARTICIPANT_TASK = {rate},
  FEATURE_CATEGORIES = {NTUMIR: Harmony = 10, Timbre = 90, Temporal? = 6, rhythm = 61, lyrics = ?, MediaEval2013: Harmony, pitch, timbre, dynamic, other)
)
},
  FEATURE_SOURCE = {NTUMIR: MIRToolbox; MediaEval2013: openSMILE},
  FEATURE_REDUCTION_METHOD = {kernel principal component analysis},
  MODEL_CATEGORY = {regression},
  MODEL_DETAIL = {support vector regression, kNN, kernal principle component analysis},
  MODEL_MEASURE = {R2, JS},
  MODEL_COMPLEXITY_PARAMETERS_NTUMIR = {NTUMIR Emotion space mapping: kpca dimensionality = 15, k=8; Simple svr: 15 principal components; Ensemble svr: 5 models per grid cell, 5*8*8 models; MediaEval: KPCA dimensionality = 450, z = 8}, %% NTUMIR: only reporting on table2, MediaEval only reporting on table3
MODEL_RATE_EMOTION_NAMES = {valence, arousal},
MODEL_RATE_EMOTION_VALUES_NTUMIR = {
rbind(
proposed_knn_kpca_features = c('R2' = 0.622, 'JS' = 0.069, 'R2.sd' = 0.288, 'JS.sd' = 0.066),
simple_svr_kpca_features = c(0.584, 0.085, 0.265, 0.059),
ensemble_svr = c(0.531, 0.102, 0.244, 0.073) 
) # reported in other study
}, 
MODEL_RATE_EMOTION_VALUES_MEDIAEVAL2013 = {
  rbind(
    simple_svr_kpca_features = c('R2' = 0.334, 'JS' = 0.130, 'R2.sd'= 0.034, 'JS.sd' = 0.009),
    proposed_knn_kpca_features = c(0.554, 0.108, 0.044, 0.013)
  )
},
%%MODEL_RATE_EMOTION_SD_NTUMIR = {
%%  rbind(
%%    proposed_knn_kpca_features = c('R2' = 0.288, 'JS' = 0.066),
%%    simple_svr_kpca_features = c(0.265, 0.059),
%%    ensemble_svr = c(0.244, 0.073) %% reported in other study
%%  )
%%},
%%MODEL_RATE_EMOTION_SD_MEDIAEVAL2013 = {
%%  rbind(
%%    simple_svr_kpca_features = c('R2' = 0.034, 'JS' = 0.009),
%%    proposed_knn_kpca_features = c(0.044, 0.013)
%%  )
%%},
 MODEL_VALIDATION = {NTUMIR: LOO, MediaEval2013: five-fold cross-validation},
 FINAL_NOTES = {discuss}
}


@Article{coutinho2017sh,
  AUTHOR = {Coutinho, Eduardo and Schuller, Bjorn},
  JOURNAL = {PLOS ONE},
  MONTH = {JUN 28},
  NUMBER = {6},
  TITLE = {Shared acoustic codes underlie emotional communication in music and speech-Evidence from deep transfer learning},
  VOLUME = {12},
  YEAR = {2017},
  DOI = {10.1371/journal.pone.0179289},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1371/journal.pone.0179289},
  ISSN = {1932-6203},
  ARTICLE = {e0179289},
  ORCID = {Coutinho, Eduardo/0000-0001-5234-1497 Schuller, Bj{\"o}rn Wolfgang/0000-0002-6478-8699},
  RESEARCHERID = {Coutinho, Eduardo/K-1391-2019 Schuller, Bj{\"o}rn Wolfgang/D-3241-2011},
  PARADIGM = {regression},
  NOTES_CA = {include, can report intradomain accuracies},
  NOTES_TE = {include},
  EMOTIONS = {valence, arousal},
  EMOTION_LOCUS = {perceived},
  STIMULUS_TYPE = {MediaEval2014: specified elsewhere; MP_db: classical, contemporary western art, baroque, bossa nova, rock, pop, heavy metal, film music},
  STIMULUS_DURATION = {variable},
  STIMULUS_DURATION_UNIT = {seconds},
  STIMULUS_N = {MediaEval2014: 1000: music perception database 1: 6, music perception database 2: 9, music perception database 3: 8, music perception database 4: 7
},
  FEATURE_N = {130},
  PARTICIPANT_N = {c(MediaEval2014 = 'specified elsewhere',
  MP_db1 = 35,
  MP_db2 = 39,
  MP_db3 = 52,
  MP_db4 = 38)
},
  PARTICIPANT_EXPERTISE = {not specified},
  PARTICIPANT_ORIGIN = {?},
  PARTICIPANT_SAMPLING = {crowd source, convenience},
  PARTICIPANT_TASK = {rate},
  FEATURE_CATEGORIES = { pitch, timbre, dynamic}, %% i simplify their table. they call some cues vocal, but they seem to fit better with panda's timbre features. includes jitter and shimmer which aren't in panda tables,
  FEATURE_SOURCE = {ComParE},
  FEATURE_REDUCTION_METHOD =  {denoising autoencoder},
  MODEL_CATEGORY = {pitch, timbre, dynamic}, %% i simplify their table. they call some cues vocal, but they seem to fit better with panda's timbre features. includes jitter and shimmer which aren't in panda tables
  MODEL_DETAIL = {denoising autoencoders, 260 inputs, 2 hidden layers, 2 output units with linear activation functions, 10 repetitions, learning rate = 5x10e^-6, first layer size = 150, second layer size = 10, gaussian noise = 0.5, performance averaged across best five trials, trained on music and speech
},
  MODEL_MEASURE = {rmse, r, ccc},
  MODEL_COMPLEXITY_PARAMETERS = {65 low level descriptors plus first order derivatives},
  MODEL_RATE_EMOTION_NAMES = {valence, arousal},
  MODEL_RATE_EMOTION_VALUES = {rbind(
  valence = c('rmse' = 0.265,
              'r' = 0.175,
              'ccc' = 0.090),
  arousal = c('rmse' = 0.194,
              'r' = 0.517,
              'ccc' = 0.317)
) },
  MODEL_VALIDATION = {nested LOO cross validation},
  FINAL_NOTES = {}
}


@Article{deng2015em,
  AUTHOR = {Deng, James J. and Leung, Clement H. C. and Milani, Alfredo and Chen, Li},
  JOURNAL = {ACM TRANSACTIONS ON INTERACTIVE INTELLIGENT SYSTEMS},
  MONTH = {MAR},
  NUMBER = {1},
  TITLE = {Emotional States Associated with Music: Classification, Prediction of Changes, and Consideration in Recommendation},
  VOLUME = {5},
  YEAR = {2015},
  DOI = {10.1145/2723575},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1145/2723575},
  ISSN = {2160-6455},
  EISSN = {2160-6463},
  ORCID = {Chen, Li/0000-0002-5842-838X MILANI, Alfredo/0000-0003-4534-1805},
  COMMENT = {discussed},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include},
  EMOTIONS = {valence, arousal, resonance},
  EMOTION_LOCUS = {perceived},
  STIMULUS_TYPE = {Western classical},
  STIMULUS_DURATION = {30},
  STIMULUS_DURATION_UNIT = {seconds},
  STIMULUS_N = {275},
  FEATURE_N = {201},
  PARTICIPANT_N = {65},
  PARTICIPANT_EXPERTISE = {musicians, nonmusicians},
  PARTICIPANT_ORIGIN = {Hong Kong},
  PARTICIPANT_SAMPLING = {convenience},
  PARTICIPANT_TASK = {rate},
  FEATURE_CATEGORIES = {dynamic, timbre, tonal, rhythm},
  FEATURE_SOURCE = {MIRToolbox, marsyas},
  FEATURE_REDUCTION_METHOD = {graph embedding, sequential floating forward search (feature selection)},
  MODEL_CATEGORY = {regression},
  MODEL_DETAIL = {support vector regression, sparse bayesian regression, variational bayesian regression},
  MODEL_MEASURE = {R2},
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = {arousal, valence, resonance},
  MODEL_RATE_EMOTION_VALUES = {rbind(
  svr = c(r2_arousal = 0.7249, explained_variance_arousal = 0.7556,
          r2_valence = 0.6119, explained_variance_valence = 0.6142,
          r2_resonance = 0.5374, explained_variance_resonance = 0.5496),
  sbr = c(r2_arousal = 0.7381, explained_variance_arousal = 0.7395,
          r2_valence = 0.6296, explained_variance_valence = 0.6376,
          r2_resonance = 0.5456, explained_variance_resonance = 0.5558),
  vbr = c(r2_arousal = 0.7108, explained_variance_arousal = 0.7415),
          r2_valence = 0.6328, explained_variance_valence = 0.6340,
          r2_resonance = 0.5554, explained_variance_resonance = 0.563)
 },
  MODEL_VALIDATION = {cross-validation},
  FINAL_NOTES = {}
}


@Article{gingras2014be,
  AUTHOR = {Gingras, Bruno and Marin, Manuela M. and Fitch, W. Tecumseh},
  JOURNAL = {Quarterly Journal of Experimental Psychology},
  NOTE = {Cited by: 32},
  NUMBER = {7},
  PAGES = {1428 – 1446},
  TITLE = {Beyond intensity: Spectral features effectively predict music-induced subjective arousal},
  TYPE = {Article},
  VOLUME = {67},
  YEAR = {2014},
  DOI = {10.1080/17470218.2013.863954},
  ABSTRACT = {Emotions in music are conveyed by a variety of acoustic cues. Notably, the positive association between sound intensity and arousal has particular biological relevance. However, although amplitude normalization is a common procedure used to control for intensity in music psychology research, direct comparisons between emotional ratings of original and amplitude-normalized musical excerpts are lacking.In this study, 30 nonmusicians retrospectively rated the subjective arousal and pleasantness induced by 84 six-second classical music excerpts, and an additional 30 nonmusicians rated the same excerpts normalized for amplitude. Following the cue-redundancy and Brunswik lens models of acoustic communication, we hypothesized that arousal and pleasantness ratings would be similar for both versions of the excerpts, and that arousal could be predicted effectively by other acoustic cues besides intensity.Although the difference in mean arousal and pleasantness ratings between original and amplitude-normalized excerpts correlated significantly with the amplitude adjustment, ratings for both sets of excerpts were highly correlated and shared a similar range of values, thus validating the use of amplitude normalization in music emotion research. Two acoustic parameters, spectral flux and spectral entropy, accounted for 65% of the variance in arousal ratings for both sets, indicating that spectral features can effectively predict arousal. Additionally, we confirmed that amplitude-normalized excerpts were adequately matched for loudness. Overall, the results corroborate our hypotheses and support the cue-redundancy and Brunswik lens models. © 2013 The Experimental Psychology Society.},
  KEYWORDS = {Acoustic Stimulation; Acoustics; Adult; Arousal; Emotions; Female; Humans; Linear Models; Loudness Perception; Male; Music; Predictive Value of Tests; Questionnaires; Recognition (Psychology); Young Adult; acoustics; adult; arousal; auditory stimulation; emotion; female; hearing; human; male; music; physiology; predictive value; psychology; questionnaire; recognition; statistical model; young adult},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-84902376903&doi = 10.1080},
  AUTHOR_KEYWORDS = {Arousal; Brunswik lens model; Emotion; Intensity; Music},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include, has R2},
  EMOTIONS = {pleasantness, arousal},
  EMOTION_LOCUS = {perceived},
  STIMULUS_TYPE = {piano trio},
  STIMULUS_DURATION = {6},
  STIMULUS_DURATION_UNIT = {seconds},
  STIMULUS_N = {84},
  FEATURE_N = {
  pre_fitting = 21,
  post_fitting_arousal_original_mlr = 5,  
  post_fitting_arousal_normalized_mlr = 3,
  post_fitting_valence_original_mlr = 3,  
  post_fitting_valence_normalized_mlr = 3,  
  post_fitting_arousal_original_mlr_common = 3,
  post_fitting_arousal_normalized_mlr_common = 3,
  post_fitting_valence_original_mlr_common = 3,  
  post_fitting_valence_normalized_mlr_common = 3,  
  post_fitting_arousal_original_pls = 6,
  post_fitting_arousal_normalized_pls = 6,
  post_fitting_valence_original_pls = 6,
  post_fitting_valence_normalized_pls = 6,
  normalized_mlr_common: common predictors bw original and rms-normalized sets}, 
  PARTICIPANT_N = {60},
  PARTICIPANT_EXPERTISE = {nonmusicians},
  PARTICIPANT_ORIGIN = {not specified}, %% presumably austria
  PARTICIPANT_SAMPLING = {convenience},
  PARTICIPANT_TASK = {rate},
  FEATURE_CATEGORIES = {dynamics, timbre, rhythm, tonality},
  FEATURE_SOURCE = {mirtoolbox, praat},
  FEATURE_REDUCTION_METHOD = {stepwise-forward procedure based on BIC},
  MODEL_CATEGORY = {regression},
  MODEL_DETAIL = {multiple linear regression, partial least squares regression},
  MODEL_MEASURE = {R2},
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = {pleasantness, arousal},
  MODEL_RATE_EMOTION_VALUES = { rbind(
  arousal_original_mlr = .848,
  arousal_normalized_mlr =  .674,
  pleasantness_original_mlr =  .386,
  pleasantness_normalized_mlr = .289, 
  arousal_original_mlr = .667, 
  arousal_normalized_mlr =  .674, 
  pleasantness_original_mlr =  .286, 
  pleasantness_normalized_mlr = .283, 
  arousal_original_pls = .779,
  arousal_normalized_pls = .652,
  pleasantness_original_pls = .533,
  pleasantness_normalized_pls = .367
) # normalized_mlr_common: common predictors bw original and rms-normalized sets.
},
  MODEL_VALIDATION = {pls: 10-fold cross validation},
  FINAL_NOTES = {}
}


@Article{grekow2018au,
  AUTHOR = {Grekow, Jacek},
  JOURNAL = {JOURNAL OF INFORMATION AND TELECOMMUNICATION},
  MONTH = {JUL 3},
  NUMBER = {3},
  PAGES = {322-333},
  TITLE = {Audio features dedicated to the detection and tracking of arousal and valence in musical compositions},
  VOLUME = {2},
  YEAR = {2018},
  DOI = {10.1080/24751839.2018.1463749},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1080/24751839.2018.1463749},
  ISSN = {2475-1839},
  EISSN = {2475-1847},
  RESEARCHERID = {Grekow, Jacek/M-9500-2015},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include},
  EMOTIONS = {arousal, valence},
  EMOTION_LOCUS = {perceived},
  STIMULUS_TYPE = {classical, jazz, blues, country, disco, hip-hop, metal, pop, reggae, rock},
  STIMULUS_DURATION = {6},
  STIMULUS_DURATION_UNIT = {seconds},
  STIMULUS_N = {324},
  FEATURE_N = {654},
  PARTICIPANT_N = {5},
  PARTICIPANT_EXPERTISE = {music experts},
  PARTICIPANT_ORIGIN = {not specified},
  PARTICIPANT_SAMPLING = {convenience},
  PARTICIPANT_TASK = {rate},
  FEATURE_CATEGORIES = {rhythm, timbre, harmony, dynamics, melody, high-level},
  FEATURE_SOURCE = {essentia, marsyas},
  FEATURE_REDUCTION_METHOD = {'WrapperSubsetEval'},
  MODEL_CATEGORY = {regression},
  MODEL_DETAIL = {SMOreg},
  MODEL_MEASURE = {R2, MAE},
  MODEL_COMPLEXITY_PARAMETERS = {attribute_selection: WrapperSubsetEval, search_method: BestFirst},
  MODEL_RATE_EMOTION_NAMES = {arousal, valence},
  MODEL_RATE_EMOTION_VALUES_TOOLBOXES = { rbind(
   essentia_before_attribute_selection = c(R2_arousal = .48, MAE_arousal = .18,  
                                           R2_valence = .27, MAE_valence = 0.17),
   essentia_after_attribute_selection = c(R2_arousal = .79, MAE_arousal = .09,  
                                         R2_valence = .58, MAE_valence = 0.10),
   marsyas_before_attribute_selection = c(R2_arousal = .63, MAE_arousal = .13,  
                                          R2_valence = .15, MAE_valence = 0.16),
   marsyas_after_attribute_selection = c(R2_arousal = .73, MAE_arousal = .11,  
                                          R2_valence = .25, MAE_valence = 0.14)
)},
MODEL_RATE_EMOTION_VALUES_FEATURE_SUBSETS = {
rbind(
   lowlevel = c(R2_arousal = .74, MAE_arousal = .10,  R2_valence = .49, MAE_valence = 0.12),
   rhythm = c(R2_arousal = .68, MAE_arousal = .11,  R2_valence = .15, MAE_valence = 0.15),
   tonal = c(R2_arousal = .53, MAE_arousal = .14,  R2_valence = .48, MAE_valence = 0.12),  
   lowlevel_rhythm = c(R2_arousal = 0.79, MAE_arousal = 0.09,  R2_valence = 0.40, MAE_valence = 0.12),
   lowlevel_tonal = c(R2_arousal = 0.74, MAE_arousal = 0.10,  R2_valence = 0.56, MAE_valence = 0.10),
   rhythm_tonal = c(R2_arousal = 0.74, MAE_arousal = 0.11,  R2_valence = 0.52, MAE_valence = 0.11),
   lowlevel_rhythm_tonal = c(R2_arousal = 0.79, MAE_arousal = 0.09,  R2_valence = 0.58, MAE_valence = 0.10)
)
},
  MODEL_VALIDATION = {10-fold cross validation},
  FINAL_NOTES = {}
}


@Article{grekow2021mu,
  AUTHOR = {Grekow, Jacek},
  JOURNAL = {JOURNAL OF INTELLIGENT INFORMATION SYSTEMS},
  MONTH = {DEC},
  NUMBER = {3},
  PAGES = {531-546},
  TITLE = {Music emotion recognition using recurrent neural networks and pretrained models},
  VOLUME = {57},
  YEAR = {2021},
  DOI = {10.1007/s10844-021-00658-5},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1007/s10844-021-00658-5},
  ISSN = {0925-9902},
  EISSN = {1573-7675},
  ORCID = {Grekow, Jacek/0000-0003-2094-0107},
  RESEARCHERID = {Grekow, Jacek/M-9500-2015},
  EARLYACCESSDATE = {AUG 2021},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include},
  EMOTIONS = {valence, arousal},
  EMOTION_LOCUS = {perceived},
  STIMULUS_TYPE = {classical, jazz, blues, country, disco, hip-hop, metal, pop, reggae, rock}, %% training data: GTZAN
  STIMULUS_DURATION = {6},
  STIMULUS_DURATION_UNIT = {seconds},
  STIMULUS_N = {324},
  FEATURE_N = {653},
  PARTICIPANT_N = {5},
  PARTICIPANT_EXPERTISE = {music experts},
  PARTICIPANT_ORIGIN = {not specified},
  PARTICIPANT_SAMPLING = {convenience},
  PARTICIPANT_TASK = {rate},
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = {essentia, marsyas},
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = {classification, neural network},
  MODEL_DETAIL = {recurrent neural networks, long short-term memory units},
  MODEL_MEASURE = {R2, MAE},
  MODEL_COMPLEXITY_PARAMETERS = {training_epochs = 100, n layers = 1-2, lstm_units = 124 or 248},
  MODEL_RATE_EMOTION_NAMES = {valence, arousal},
MODEL_RATE_EMOTION_VALUES_CHROMA.MARSYAS.54= {
rbind(
linear.regression = c(R2_arousal = 0.4, MAE_arousal = 0.17, R2_valence = 0.02, MAE_valence = 0.18),
SMOreg = c(0.43,0.16,0.04,0.17),
RNN1.124LSTM = c(0.43,0.17,0.02,0.18),
RNN2.124.124LSTM = c(0.48,0.15,0.03,0.17),
RNN3.248LSTM = c(0.41,0.17,0.02,0.18),
RNN4.248.248LSTM = c(0.46,0.16,0.02,0.18)
)
},
MODEL_RATE_EMOTION_VALUES_MFCC = {
  rbind(
    linear.regression = c(R2_arousal = 0.61, MAE_arousal = 0.13, R2_valence = 0.07, MAE_valence = 0.17),
    SMOreg = c(0.6,0.13,0.10,0.17),
    RNN1.124LSTM = c(0.58,0.14,0.14,0.17),
    RNN2.124.124LSTM = c(0.66,0.12,0.11,0.16),
    RNN3.248LSTM = c(0.61,0.13,0.13,0.16),
    RNN4.248.248LSTM = c(0.64,0.12,0.14,0.15)
  )
},
MODEL_RATE_EMOTION_VALUES_ESSENTIA = {
  rbind(
    linear.regression = c(R2_arousal = 0.07, MAE_arousal = 0.25, R2_valence = 0.06, MAE_valence = 0.19),
    SMOreg = c(0.48,0.18,0.27,0.17),
    RNN1.124LSTM = c(0.54,0.14,0.21,0.16),
    RNN2.124.124LSTM = c(0.67,0.12,0.32,0.13),
    RNN3.248LSTM = c(0.58,0.14,0.32,0.14),
    RNN4.248.248LSTM = c(0.69,0.11,0.40,0.13),
    RNN5.529LSTM = c(0.61,0.13,0.29,0.15),
    RNN6.529.529LSTM = c(0.68,0.12,0.36,0.14)
  )
},

MODEL_RATE_EMOTION_VALUES_PRETRAINED = {
  rbind(
    RNN1.124LSTM = c(R2_arousal = 0.63, MAE_arousal = 0.13, R2_valence = 0.35, MAE_valence = 0.14),
    RNN2.124.124LSTM = c(0.73,0.11,0.42,0.13),
    RNN3.248LSTM = c(0.68,0.12,0.36,0.14),
    RNN4.248.248LSTM = c(0.73,0.11,0.46,0.12),
    RNN5.529LSTM = c(0.69,0.12,0.38,0.13),
    RNN6.529.529LSTM = c(0.71,0.12,0.46,0.12)
  )
},
  MODEL_VALIDATION = {10-fold cross validation},
  FINAL_NOTES = {}
}


@Article{griffiths2021am,
  AUTHOR = {Griffiths, Darryl and Cunningham, Stuart and Weinel, Jonathan and Picking, Richard},
  JOURNAL = {JOURNAL OF NEW MUSIC RESEARCH},
  MONTH = {AUG 8},
  NUMBER = {4},
  PAGES = {355-372},
  TITLE = {A multi-genre model for music emotion recognition using linear regressors},
  VOLUME = {50},
  YEAR = {2021},
  DOI = {10.1080/09298215.2021.1977336},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1080/09298215.2021.1977336},
  ISSN = {0929-8215},
  EISSN = {1744-5027},
  ORCID = {Cunningham, Stuart/0000-0002-5348-7700 Weinel, Jonathan/0000-0001-5347-3897},
  RESEARCHERID = {Cunningham, Stuart/HSH-5303-2023},
  EARLYACCESSDATE = {SEP 2021},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include, linear regression},
  EMOTIONS = {valence, arousal},
  EMOTION_LOCUS = {perceived},
  STIMULUS_TYPE = {},
  STIMULUS_DURATION = {variable},
  STIMULUS_DURATION_UNIT = {s},
  STIMULUS_N = {study 1: 20; study 2: 40)}, % three outliers
  FEATURE_N = {before_selection = 45, after_selection = 6},
  PARTICIPANT_N = {study1=44, study2 = 158},
  PARTICIPANT_EXPERTISE = {not specified},
  PARTICIPANT_ORIGIN = {Auditory Mailing List, Social Media},
  PARTICIPANT_SAMPLING = {convenience},
  PARTICIPANT_TASK = {self-report}, %% categorical emotions with intensity prompt; reverse-engineered for circumplex representation
  FEATURE_CATEGORIES = {before selection: dynamic, timbre, tonality, rhythm; after_selection: dynamic, timbre},
  FEATURE_SOURCE = {MIR Toolbox, Matlab Audio Analysis Library},
  FEATURE_REDUCTION_METHOD = {Spearman correlation ranking},
  MODEL_CATEGORY = {perceived},
  MODEL_DETAIL = {multiple linear regression},
  MODEL_MEASURE = {R2, MSE, SSE},
  MODEL_COMPLEXITY_PARAMETERS = {4},
  MODEL_RATE_EMOTION_NAMES = {valence, arousal},
  MODEL_RATE_EMOTION_VALUES = {list(
  study2 = c(r2_arousal = 0.708, sse_arousal = 29.700, rmse_arousal = 0.084, r2.95l_arousal = 0.691, r2.95u_arousal = 1.059,
                  r2_valence = 0.674, sse_valence = 30.581, rmse_valence = 0.897, r2.95l_valence = 0.562, r2.95u_valence = 0.894),
  study2.no.outliers = c(0.85,12.341,0.585,0.841,1.118,
                  0.776,19.7,0.74,0.63,0.91)
)}, %% only reported results of study 2, as study 1 regressed only individual features
  MODEL_VALIDATION = {study2}, %% study 2 is the validation of model developed in study 1
  FINAL_NOTES = {}
}


@Article{hu2017cr,
  AUTHOR = {Hu, Xiao and Yang, Yi-Hsuan},
  JOURNAL = {IEEE TRANSACTIONS ON AFFECTIVE COMPUTING},
  MONTH = {APR-JUN},
  NUMBER = {2},
  PAGES = {228-240},
  TITLE = {Cross-Dataset and Cross-Cultural Music Mood Prediction: A Case on Western and Chinese Pop Songs},
  VOLUME = {8},
  YEAR = {2017},
  DOI = {10.1109/TAFFC.2016.2523503},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1109/TAFFC.2016.2523503},
  ISSN = {1949-3045},
  ORCID = {Hu, Xiao/0000-0003-3994-0385},
  RESEARCHERID = {Hu, Xiao/A-7645-2013 Hu, Xiao/AAD-8405-2020},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include},
  EMOTIONS = {valence, arousal},
  EMOTION_LOCUS = {perceived},
  STIMULUS_TYPE = {MER60, CH818, AMG1608},
  STIMULUS_DURATION = {MER60 = 30, CH818 = 30, AMG1608 = 60},
  STIMULUS_DURATION_UNIT = {s},
  STIMULUS_N = {MER60: 60, CH818: 818, AMG1608: 1608},
  FEATURE_N = {15}, %% 23 in table, but 15 reported
  PARTICIPANT_N = {}, %% 40 per clip, 3 per clip, 15-32 per clip
  PARTICIPANT_EXPERTISE =  {MER60 = nonexperts, CH818 = experts, AMG1608 =nonexperts },
  PARTICIPANT_ORIGIN = {MER60 = Taiwan, CH818 = China, AMG1608 = MTurk},
  PARTICIPANT_SAMPLING = {MER60 = convenience, CH818 = experts, AMG1608 = crowdsource},
  PARTICIPANT_TASK = {rate},
  FEATURE_CATEGORIES = {Loudness, Pitch, Rhythm, Tonality, Timbre},
  FEATURE_SOURCE = {MIR Toolbox, PsySound, Chroma Toolbox,, Tempogram Toolbox},
  FEATURE_REDUCTION_METHOD = {step-wise forward feature selection algorithm},
  MODEL_CATEGORY = {regression},
  MODEL_DETAIL = {support vector regression with radial basis function,
exp1: train = complete set, test = complete set, factor examined = combined features;\n
exp2: train = selected set (random), test = complete set, factor examined = size of training data;\n
exp3: train = selected set (controlled), test = complete set, factor examined = reliability level (close to that of mer 60) of training dataset;\n
exp4: train = selected set (controlled), test = selected set (controlled), factor examined = reliability level (close to that of mer 60) of training and test datasets;\n
exp5: train = selected sets (varied), test = selected sets (varied), factor examined = reliability level (varied) of training and test datasets},
  MODEL_MEASURE = {R2},
  MODEL_COMPLEXITY_PARAMETERS = {parameters optimized through grid searches},
  MODEL_RATE_EMOTION_NAMES = {valence, arousal},
  MODEL_RATE_EMOTION_VALUES = {rbind(mer_mer = c(exp1_valence = 0.18, exp2_valence = 0.18, 
exp3_valence = 0.18, exp4_valence = 0.18,
exp1_arousal = 0.68, exp2_arousal = 0.68, 
exp3_arousal = 0.68, exp4_arousal = 0.68),
        ch_mer = c(0.27,0.19,0.25,0.25,0.8,0.74,0.75,0.75),
        amg_mer = c(0.40,0.22,0.2,0.2,0.75,0.67,0.65,0.65),
        mer_ch = c(0.13,0.13,0.13,0.53,0.7,0.7,0.7,0.59),
        ch_ch = c(0.25,0.15,0.10,0.28,0.77,0.72,0.74,0.81),
        amg_ch = c(0.21,0.06,0.04,0.2,0.68,0.67,0.63,0.66),
        mer_amg = c(0.08,0.08,0.08,0.33,0.61,0.61,0.61,0.73),
        ch_amg = c(0.07,0.02,0.02,0.13,0.66,0.57,0.59,0.71),
        amg_amg = c(0.14,0.03,0.06,0.19,0.73,0.63,0.63,0.72),
        within = c(0.19,0.12,0.11,0.22,0.73,0.68,0.68,0.74),
        cross = c(0.19,0.12,0.12,0.28,0.7,0.66,0.66,0.68),
        all = c(0.19,0.12,0.12,0.26,0.71,0.67,0.67,0.7))
 }, 
  MODEL_VALIDATION = {10-fold cross validation},
  FINAL_NOTES = {unsure how to split up individual experiments and include details about them}
}


@Article{hu2022de,
  AUTHOR = {Hu, Xiao and Li, Fanjie and Liu, Ruilun},
  JOURNAL = {APPLIED SCIENCES-BASEL},
  MONTH = {SEP},
  NUMBER = {18},
  TITLE = {Detecting Music-Induced Emotion Based on Acoustic Analysis and Physiological Sensing: A Multimodal Approach},
  VOLUME = {12},
  YEAR = {2022},
  DOI = {10.3390/app12189354},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.3390/app12189354},
  ARTICLE = {9354},
  EISSN = {2076-3417},
  ORCID = {Hu, Xiao/0000-0003-3994-0385 Li, Fanjie/0000-0001-7016-6354},
  RESEARCHERID = {Hu, Xiao/AAD-8405-2020},
  COMMENT = {includes audio-only model},
  PARADIGM = {regression},
  NOTES_CA = {include , single-modality reported},
  NOTES_TE = {include, includes audio only classification},
  EMOTIONS = {valence, arousal},
  EMOTION_LOCUS = {perceived},
  STIMULUS_TYPE = {MIREX Grand Challenge on User Experience (Jamendo)}, 
  STIMULUS_DURATION = {not specified}, %% full tracks possibly, average playtime 50s
  STIMULUS_DURATION_UNIT = {s},
  STIMULUS_N = {956},
  FEATURE_N = {231}, %% just reporting audio features
  PARTICIPANT_N = {30},
  PARTICIPANT_EXPERTISE = {diverse},
  PARTICIPANT_ORIGIN = {Hong Kong},
  PARTICIPANT_SAMPLING = {convenience}, %% assumed
  PARTICIPANT_TASK = {survey, rate},
  FEATURE_CATEGORIES = {dynamic, rhythm, timbre, tonal, pitch},
  FEATURE_SOURCE = {Librosa},
  FEATURE_REDUCTION_METHOD = {none},
  MODEL_CATEGORY = {classification},
  MODEL_DETAIL = {SVM with RBF},
  MODEL_MEASURE = {F1, AUC},
  MODEL_COMPLEXITY_PARAMETERS = {undersample majority class, 
  oversample minority class using SVM borderline synthetic minority oversampling},
  MODEL_RATE_EMOTION_NAMES = {valence, arousal},
  MODEL_RATE_EMOTION_VALUES = {
  rbind(
  imbalanced.data = c(F1_arousal = 71.38, AUC_arousal = 81.31, F1_valence = 54.74, AUC_valence = 63.86),
  resampled.data = c(72.93,81.41,58.09,53.72)
  )
  },
  MODEL_VALIDATION = {10-fold cross validation},
  FINAL_NOTES = {}
}


@Article{koh2023me,
  AUTHOR = {Koh, En Yan and Cheuk, Kin Wai and Heung, Kwan Yee and Agres, Kat R. and Herremans, Dorien},
  JOURNAL = {SENSORS},
  MONTH = {JAN},
  NUMBER = {1},
  TITLE = {MERP: A Music Dataset with Emotion Ratings and Raters' Profile Information},
  VOLUME = {23},
  YEAR = {2023},
  DOI = {10.3390/s23010382},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.3390/s23010382},
  ARTICLE = {382},
  EISSN = {1424-8220},
  ORCID = {Herremans, Dorien/0000-0001-8607-1640 Cheuk, Kin Wai/0000-0003-3213-8242 Agres, Kathleen/0000-0001-7260-2447},
  RESEARCHERID = {Herremans, Dorien/G-9599-2018 Cheuk, Kin Wai/HZJ-8015-2023},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include},
  EMOTIONS = {valence, arousal},
  EMOTION_LOCUS = {perceived},
  STIMULUS_TYPE = {Free Music Archive},
  STIMULUS_DURATION = {full},
  STIMULUS_DURATION_UNIT = {s},
  STIMULUS_N = {54},
  FEATURE_N = {260},
  PARTICIPANT_N = {277},
  PARTICIPANT_EXPERTISE = {nonexperts},
  PARTICIPANT_ORIGIN = {MTurk},
  PARTICIPANT_SAMPLING = {crowdsourcing},
  PARTICIPANT_TASK = {rate}, %% continuous
  FEATURE_CATEGORIES = {timbre, dynamic, pitch},
  FEATURE_SOURCE = {openSMILE},
  FEATURE_REDUCTION_METHOD = {},
  MODEL_CATEGORY = {regression},
  MODEL_DETAIL = {}, %% move to complexity parameters? 
  MODEL_MEASURE = {R},
  MODEL_COMPLEXITY_PARAMETERS = {fully connected model: nodes = 512-256-1, dropout = 50%, leakyReLu = 0.1, smoothing = Gaussian kernel: sigma = 1.5, size = 7; LTSM model: model type = RNN, hidden dimension = 512; hyper parameter tuning = trial and error, batch size = 8, learning rate = 0.0001},
  MODEL_RATE_EMOTION_NAMES = {valence, arousal},
  MODEL_RATE_EMOTION_VALUES = {
  rbind(fully.connected = c(MSE_valence = 0.0315, R_valence = 0.1314, 
                          MSE_arousal = 0.0333, R_arousal = 0.3507),
        LSTM = c(0.0532,0.0599,0.0441,0.2992)
        )
},
  MODEL_VALIDATION = {five-fold cross validation with 100 epochs},
  FINAL_NOTES = {}
}


@Article{malheiro2018em, 
  AUTHOR = {Malheiro, Ricardo and Panda, Renato and Gomes, Paulo and Paiva, Rui Pedro},
  JOURNAL = {IEEE TRANSACTIONS ON AFFECTIVE COMPUTING},
  MONTH = {APR-JUN},
  NUMBER = {2},
  PAGES = {240-254},
  TITLE = {Emotionally-Relevant Features for Classification and Regression of Music Lyrics},
  VOLUME = {9},
  YEAR = {2018},
  DOI = {10.1109/TAFFC.2016.2598569},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1109/TAFFC.2016.2598569},
  ISSN = {1949-3045},
  ORCID = {Malheiro, Ricardo/0000-0002-3010-2732 Panda, Renato/0000-0003-2539-5590 Paiva, Rui Pedro/0000-0003-3215-3960},
  RESEARCHERID = {Malheiro, Ricardo/L-9369-2017 Panda, Renato/AAK-7581-2020 Paiva, Rui Pedro/D-9602-2018},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_NOTES = {discuss, could not find any audio features. only lyric features. !EXCL!}
}


@Article{markov2014mu,
  AUTHOR = {Markov, Konstantin and Matsui, Tomoko},
  JOURNAL = {IEEE Access},
  NOTE = {Cited by: 73; All Open Access, Gold Open Access},
  PAGES = {688 – 697},
  TITLE = {Music genre and emotion recognition using Gaussian processes},
  TYPE = {Article},
  VOLUME = {2},
  YEAR = {2014},
  DOI = {10.1109/ACCESS.2014.2333095},
  ABSTRACT = {Gaussian Processes (GPs) are Bayesian nonparametric models that are becoming more and more popular for their superior capabilities to capture highly nonlinear data relationships in various tasks, such as dimensionality reduction, time series analysis, novelty detection, as well as classical regression and classification tasks. In this paper, we investigate the feasibility and applicability of GP models for music genre classification and music emotion estimation. These are two of the main tasks in the music information retrieval (MIR) field. So far, the support vector machine (SVM) has been the dominant model used in MIR systems. Like SVM, GP models are based on kernel functions and Gram matrices; but, in contrast, they produce truly probabilistic outputs with an explicit degree of prediction uncertainty. In addition, there exist algorithms for GP hyperparameter learning - something the SVM framework lacks. In this paper, we built two systems, one for music genre classification and another for music emotion estimation using both SVM and GP models, and compared their performances on two databases of similar size. In all cases, the music audio signal was processed in the same way, and the effects of different feature extraction methods and their various combinations were also investigated. The evaluation experiments clearly showed that in both music genre classification and music emotion estimation tasks the GP performed consistently better than the SVM. The GP achieved a 13.6% relative genre classification error reduction and up to an 11% absolute increase of the coefficient of determination in the emotion estimation task. © 2014 IEEE.},
  KEYWORDS = {Classification (of information); Feature extraction; Gaussian distribution; Gaussian noise (electronic); Image retrieval; Support vector machines; Time series analysis; Coefficient of determination; Dimensionality reduction; Feature extraction methods; Gaussian Processes; Music emotions; Music genre classification; Music information retrieval; Prediction uncertainty; Audio acoustics},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-84923317754&doi = 10.1109},
  AUTHOR_KEYWORDS = {Gaussian processes; Music emotion estimation; Music genre classification},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include, mediaeval, R2 metrics},
  EMOTIONS = {valence, arousal},
  EMOTION_LOCUS = {perceived},
  STIMULUS_TYPE = {MediaEval},
  STIMULUS_DURATION = {45},
  STIMULUS_DURATION_UNIT = {s},
  STIMULUS_N = {1000},
  FEATURE_N = {model 1: 52, model 2 = 68, model 3 = 260, model 4 = 388}, %% between 52 and 388 depending on model
  PARTICIPANT_N = {100},
  PARTICIPANT_EXPERTISE = {specified elsewhere},
  PARTICIPANT_ORIGIN = {specified elsewhere},
  PARTICIPANT_SAMPLING = {specified elsewhere},
  PARTICIPANT_TASK = {specified elsewhere},
  FEATURE_CATEGORIES = {timbre, speech, pitch},
  FEATURE_SOURCE = {Marsyas},
  FEATURE_REDUCTION_METHOD = {none},
  MODEL_CATEGORY = {regression},
  MODEL_DETAIL = { exp 1: SVR + Radial basis function; exp 2 = Gaussian Process Regression},
  MODEL_MEASURE = {R2},
  MODEL_COMPLEXITY_PARAMETERS = {SVR kernel: radial basis function; GPR kernel: squared exponential kernel},
  MODEL_RATE_EMOTION_NAMES = {valence, arousal},
  MODEL_RATE_EMOTION_VALUES = {
  rbind(
  MFCC = c('R2_arousal' = 0.63, 'R2_arousal.SD' = 0.064, 'R2_valence' = 0.356, 'R2_valence.SD' = 0.085),
  MFCC.TMBR = c(0.63,0.065,0.354,0.084),
  MFCC.TMBR.SCF.SFM = c(0.686,0.045,0.398,0.075),
  MFCC.TMBR.SCF.SFM.CHR.LSP = c(0.587,0.066,0.341,0.097)
)
  },
  MODEL_VALIDATION = {10-fold cross validation},
  FINAL_NOTES = {}
}


@Article{medina2020em,
  AUTHOR = {Medina, Yesid Ospitia and Beltran, Jose Ramon and Baldassarri, Sandra},
  JOURNAL = {PERSONAL AND UBIQUITOUS COMPUTING},
  MONTH = {2020 APR 15},
  TITLE = {Emotional classification of music using neural networks with the MediaEval dataset},
  YEAR = {2020},
  DOI = {10.1007/s00779-020-01393-4},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1007/s00779-020-01393-4},
  ISSN = {1617-4909},
  EISSN = {1617-4917},
  ORCID = {Ospitia, Yesid/0000-0002-5494-2787 Beltran, Jose Ramon/0000-0002-7500-4650 Baldassarri, Sandra/0000-0002-9315-6391},
  RESEARCHERID = {Ospitia, Yesid/AAD-6729-2021 Beltran, Jose Ramon/K-7693-2015 Baldassarri, Sandra/L-6033-2014},
  EARLYACCESSDATE = {APR 2020},
  PARADIGM = {classification},
  NOTES_CA = {include},
  NOTES_TE = {include, classification},
  EMOTIONS = {valence, arousal},
  EMOTION_LOCUS = {perceived},
  STIMULUS_TYPE = {MediaEval},
  STIMULUS_DURATION = {45},
  STIMULUS_DURATION_UNIT = {s},
  STIMULUS_N = {1802},
  FEATURE_N = {260},
  PARTICIPANT_N = {specified elsewhere},
  PARTICIPANT_EXPERTISE = {specified elsewhere},
  PARTICIPANT_ORIGIN = {specified elsewhere},
  PARTICIPANT_SAMPLING = {specified elsewhere},
  PARTICIPANT_TASK = {specified elsewhere},
  FEATURE_CATEGORIES = {specified elsewhere},
  FEATURE_SOURCE = {openSMILE},
  FEATURE_REDUCTION_METHOD = {principal component analysis},
  MODEL_CATEGORY = {classification},
  MODEL_DETAIL = {multilayer perceptron},
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = {Valence Multilayer Perceptron: learning rate = 0.070, hidden layers = 1, neurons = 64, RMSE = 0.23; Arousal Multilayer Perceptron: learning rate = 0.060, hidden layers = 1, neurons = 64,RMSE = 0.24},
  MODEL_RATE_EMOTION_NAMES = {valence, arousal},
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_NOTES = {discuss, unsure how to report results. Quality issues? OvR detection !EXCL!}
}


@Article{orjesek2022en,
  AUTHOR = {Orjesek, Richard and Jarina, Roman and Chmulik, Michal},
  JOURNAL = {MULTIMEDIA TOOLS AND APPLICATIONS},
  MONTH = {FEB},
  NUMBER = {4},
  PAGES = {5017-5031},
  TITLE = {End-to-end music emotion variation detection using iteratively reconstructed deep features},
  VOLUME = {81},
  YEAR = {2022},
  DOI = {10.1007/s11042-021-11584-7},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1007/s11042-021-11584-7},
  ISSN = {1380-7501},
  EISSN = {1573-7721},
  ORCID = {Chmulik, Michal/0000-0002-0513-5129 Jarina, Roman/0000-0002-0478-5808},
  RESEARCHERID = {Chmulik, Michal/IQW-1183-2023 Jarina, Roman/E-2541-2018},
  EARLYACCESSDATE = {JAN 2022},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include, both R2 and classification included},
  EMOTIONS = {valence, arousal},
  EMOTION_LOCUS = {perceived},
  STIMULUS_TYPE = {DEAM}, %% use DEAM dataset
  STIMULUS_DURATION = {45},
  STIMULUS_DURATION_UNIT = {s},
  STIMULUS_N = {431}, %% subset of DEAM
  FEATURE_N = {specified elsewhere},
  PARTICIPANT_N = {specified elsewhere},
  PARTICIPANT_EXPERTISE = {specified elsewhere},
  PARTICIPANT_ORIGIN = {specified elsewhere},
  PARTICIPANT_SAMPLING = {specified elsewhere},
  PARTICIPANT_TASK = {rate},
  FEATURE_CATEGORIES = {specified elsewhere},
  FEATURE_SOURCE = {OpenSMILE}, %% specified elsewhere. Take notes on features...
  FEATURE_REDUCTION_METHOD = {not specified},
  MODEL_CATEGORY = {regression},
  MODEL_DETAIL = {rbind(deep-neural-network = c('1D convolution layer', 'autoencoder-based layer with iterative reconstruction', 'bidirectional gated recurrent unit'),
'time-distributed-layer' = c('CNN', 'iterative reconstruction', 'bidirectional gated recurrent unit'))},
  MODEL_MEASURE = {RMSE, PCC},
  MODEL_COMPLEXITY_PARAMETERS = {Optimization: back-propogation through time, adam optimizer (default parameters)},
  MODEL_RATE_EMOTION_NAMES = {arousal, valence},
  MODEL_RATE_EMOTION_VALUES = {OpenSMILE = rbind('CNN-BiGRU.audio.deam.1' = c(arousal_rmse.mean = 0.116, arousal_rmse.sd = 0.004, arousal_pcc.mean = 0.63, arousal_pcc.sd = 0.01, 
												   valence_rmse.mean = 0.123, valence_rmse.sd = 0.003, valence_pcc.mean = 0.618, valence_pcc.sd = 0.005),
'CNN-IR-BiGRU.audio.deam.1' = c(arousal_rmse.mean = 0.105, arousal_rmse.sd = 0.001, arousal_pcc.mean = 0.66, arousal_pcc.sd = 0.004, 
			 valence_rmse.mean = 0.114, valence_rmse.sd = 0.0005, valence_pcc.mean = 0.637, valence_pcc.sd = 0.007)
)
},
  MODEL_VALIDATION = {not specified},
  FINAL_NOTES = {}
}


@Article{panwar2019ar,
  AUTHOR = {Panwar, Sharaj and Rad, Paul and Choo, Kim-Kwang Raymond and Roopaei, Mehdi},
  JOURNAL = {JOURNAL OF SUPERCOMPUTING},
  MONTH = {JUN},
  NUMBER = {6, SI},
  PAGES = {2986-3009},
  TITLE = {Are you emotional or depressed? Learning about your emotional state from your music using machine learning},
  VOLUME = {75},
  YEAR = {2019},
  DOI = {10.1007/s11227-018-2499-y},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1007/s11227-018-2499-y},
  ISSN = {0920-8542},
  EISSN = {1573-0484},
  ORCID = {Choo, Kim-Kwang Raymond/0000-0001-9208-5336},
  RESEARCHERID = {Choo, Kim-Kwang Raymond/A-3634-2009 najafirad, peyman/ACB-9554-2022},
  PARADIGM = {regression},
  NOTES_CA = {include, authors add detail about VA accuracy},
  NOTES_TE = {include, DEAM},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_NOTES = {discuss: beyond scope? !EXCL!}
}


@Article{saizclar2022pr,
  AUTHOR = {Saiz-Clar, Elena and Angel Serrano, Miguel and Manuel Reales, Jose},
  JOURNAL = {PSYCHOLOGY OF MUSIC},
  MONTH = {JUL},
  NUMBER = {4},
  PAGES = {1107-1120},
  TITLE = {Predicting emotions in music using the onset curve},
  VOLUME = {50},
  YEAR = {2022},
  DOI = {10.1177/03057356211031658},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1177/03057356211031658},
  ISSN = {0305-7356},
  EISSN = {1741-3087},
  ORCID = {Serrano, Miguel-Angel/0000-0002-6574-4532 Serrano, Miguel-Angel/0000-0002-6574-4532 Saiz-Clar, Elena/0000-0002-8714-2305},
  RESEARCHERID = {Serrano, Miguel-Angel/ABE-7279-2021 Serrano, Miguel-Angel/AAZ-4393-2020},
  EARLYACCESSDATE = {AUG 2021},
  PARADIGM = {regression},
  NOTES_CA = {exclude, no modeling task},
  NOTES_TE = {include, R2},
  EMOTIONS = {activation, valence},
  EMOTION_LOCUS = {not specified},
  STIMULUS_TYPE = {piano pieces, classical music}, %% piano works from vieillard et al. (2008)
  STIMULUS_DURATION = {8-12},
  STIMULUS_DURATION_UNIT = {s},
  STIMULUS_N = {40},
  FEATURE_N = {22}, %% five in final model
  PARTICIPANT_N = {16},
  PARTICIPANT_EXPERTISE = {no formal training},
  PARTICIPANT_ORIGIN = {not specified}, %% presumably Spain
  PARTICIPANT_SAMPLING = {not specified}, %% presumably convenience
  PARTICIPANT_TASK = {rate, categorize},
  FEATURE_CATEGORIES = {rhythm},
  FEATURE_SOURCE = {MIRToolbox},
  FEATURE_REDUCTION_METHOD = {correlation significance, PCA},
  MODEL_CATEGORY = {regression},
  MODEL_DETAIL = {MLR, five predictors following PCA},
  MODEL_MEASURE = {R2, eta2},
  MODEL_COMPLEXITY_PARAMETERS = {23},
  MODEL_RATE_EMOTION_NAMES = {valence, activation},
  MODEL_RATE_EMOTION_VALUES = {MIRToolbox = mrbind('multiple linear regression.onset curves.1.1' = c(activation_r2.r2 = 0.537, activation_eta2.eta2 = 0.537,
  valence_r2.r2 = 0.218, valence_eta2.eta2 = 0.218))},
  MODEL_VALIDATION = {cross-validation},
  FINAL_NOTES = {}
}


@Article{vempala2024pr,
  AUTHOR = {Vempala, Naresh and Russo, Frank and Lab, Smart},
  MONTH = {},
  JOURNAL = {CMMR 2012},
  TITLE = {Predicting Emotion from Music Audio Features Using Neural Networks},
  YEAR = {2024},
  DOI = {10.32920/25413184},
  ABSTRACT = {{$<$}p{$ > $}We describe our implementation of two neural networks: a static feedforward network, and an Elman network, for predicting mean valence/arousal ratings of participants for musical excerpts based on audio features. Thirteen audio features were extracted from 12 classical music excerpts (3 from each emotion quadrant). Valence/arousal ratings were collected from 45 participants for the static network, and 9 participants for the Elman network. For the Elman network, each excerpt was temporally segmented into four, sequential chunks of equal duration. Networks were trained on eight of the 12 excerpts and tested on the remaining four. The static network predicted values that closely matched mean participant ratings of valence and arousal. The Elman network did a good job of predicting the arousal trend but not the valence trend. Our study indicates that neural networks can be trained to identify statistical consistencies across audio features to predict valence/arousal values.{$<$}/p{$ > $}},
  KEYWORDS = {Audio Event Detection; Melody Extraction; Emotion Recognition; Affective Computing; Speech Emotion},
  SOURCE = {open_alex},
  URL = {https://doi.org/10.32920/25413184},
  BDSK = {https://doi.org/10.32920/25413184},
  DATE = {2024-05-13 15:05:26 +0100},
  DATE.1 = {2024-05-13 15:05:26 +0100},
  LA = {en},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include, classification of quadrant},
  EMOTIONS = {valence, arousal},
  EMOTION_LOCUS = {perceived},
  STIMULUS_TYPE = {classical},
  STIMULUS_DURATION = {120},
  STIMULUS_DURATION_UNIT = {s},
  STIMULUS_N = {12},
  FEATURE_N = {13},
  PARTICIPANT_N = {54}, %% 45 for static network, 9 for Elman network
  PARTICIPANT_EXPERTISE = {c('standard network' = 'limited musical training',
  'elman network' = 'musical training')},
  PARTICIPANT_ORIGIN = {not specified}, %% presumably canada
  PARTICIPANT_SAMPLING = {not specified}, %% presumably convenience
  PARTICIPANT_TASK = {rate},
  FEATURE_CATEGORIES = {dynamic, timbre, rhythm, harmony},
  FEATURE_SOURCE = {MIRToolbox},
  FEATURE_REDUCTION_METHOD = {not specified},
  MODEL_CATEGORY = {classification, neural nets},
  MODEL_DETAIL = {static neural network, elman neural network},
  MODEL_MEASURE = {classification accuracy},
  MODEL_COMPLEXITY_PARAMETERS = {Static network: input units = 13, hidden units = 13, output units = 2, training epochs = 10000, learning rate = 0.3, context units = NA; Elman network: input units = 13, hidden units = 13, output units = 2, training epochs = 10000, learning rate = 0.3, context units = 13},
  MODEL_RATE_EMOTION_NAMES = {valence, arousal},
  MODEL_RATE_EMOTION_VALUES = {MIRToolbox = mrbind('static neural network.1.1.1' = c(classification_accuracy.mean = 85.7),
  'elman neural network.1.1.1' = c(classification_accuracy.mean = 54.3))}, %% final epoch accuracy: 60%. Can retrieve values from graph potentially
  MODEL_VALIDATION = {train-test split},
  FINAL_NOTES = {proceedings article !EXCL!} 
}


@Article{wang2021ac,
  AUTHOR = {Wang, Xin and Wei, Yujia and Heng, Lena and McAdams, Stephen},
  JOURNAL = {FRONTIERS IN PSYCHOLOGY},
  MONTH = {SEP 29},
  TITLE = {A Cross-Cultural Analysis of the Influence of Timbre on Affect Perception in Western Classical Music and Chinese Music Traditions},
  VOLUME = {12},
  YEAR = {2021},
  DOI = {10.3389/fpsyg.2021.732865},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.3389/fpsyg.2021.732865},
  ISSN = {1664-1078},
  ARTICLE = {732865},
  ORCID = {McAdams, Stephen/0000-0002-6744-9035 Heng, Lena/0000-0002-4395-2576},
  RESEARCHERID = {McAdams, Stephen/GQB-0225-2022 Wei, Yujia/IAQ-8917-2023},
  PARADIGM = {regression},
  NOTES_CA = {include, acoustic model present in paper},
  NOTES_TE = {include},
  EMOTIONS = {valence, tension arousal, energy arousal},
  EMOTION_LOCUS = {perceived},
  STIMULUS_TYPE = {chinese, western classical music},
  STIMULUS_DURATION = {not specified},
  STIMULUS_DURATION_UNIT = {s},
  STIMULUS_N = {48}, %% 2 melodies * 4 quadrants * 6 timbres
  FEATURE_N = {18}, %% three from Timbre Toolbox
  PARTICIPANT_N = {160},
  PARTICIPANT_EXPERTISE = {musicians, nonmusicians},
  PARTICIPANT_ORIGIN = {Canada, China},
  PARTICIPANT_SAMPLING = {not specified}, %% presumably convenience
  PARTICIPANT_TASK = {rate}, %%
  FEATURE_CATEGORIES = {timbre},
  FEATURE_SOURCE = {MIRToolbox, Timbre Toolbox},
  FEATURE_REDUCTION_METHOD = {PLSR},
  MODEL_CATEGORY = {regression},
  MODEL_DETAIL = {PLSR},
  MODEL_MEASURE = {R2, Q2},
  MODEL_COMPLEXITY_PARAMETERS = {two pcs},
  MODEL_RATE_EMOTION_NAMES = {valence, tension arousal, energy arousal},
  MODEL_RATE_EMOTION_VALUES = {
  MIRToolbox = mrbind(
  PLSR.timbre.western.1 = c(
  valence_r2.r2 = 0.48, valence_q2.q2 = 0.32, valence_pc1.pc1 = 0.22, valence_pc2.pc2 = 0.38, 
  'tension arousal_r2.r2' = 0.70, 'tension arousal_q2.q2' = 0.65, 'tension arousal_pc1.pc1' = 0.44, 'tension arousal_pc2.pc2' = 0.18, 
  'energy arousal_r2.r2'= 0.70, 'energy arousal_q2.q2' = 0.62, 'energy arousal_pc1.pc1' = 0.37, 'energy arousal_pc2.pc2' = 0.25),
  PLSR.timbre.chinese.1 = c(
  valence_r2.r2 = 0.55, valence_q2.q2 = 0.44, valence_pc1.pc1 = 0.32, valence_pc2.pc2 = 0.31, 
  'tension arousal_r2.r2' = 0.36, 'tension arousal_q2.q2' = 0.22, 'tension arousal_pc1.pc1' = 0.44, 'tension arousal_pc2.pc2' = 0.13, 
  'energy arousal_r2.r2' = 0.67, 'energy arousal_q2.q2' = 0.60, 'energy arousal_pc1.pc1' = 0.38, 'energy arousal_pc2.pc2' = 0.25)
  )
  },
  MODEL_VALIDATION = {6-fold cross validation},
  FINAL_NOTES = {}
}


@Article{wang2022co,
  AUTHOR = {Wang, Xin and Wang, Li and Xie, Lingyun},
  JOURNAL = {Applied Sciences (Switzerland)},
  NOTE = {Cited by: 7; All Open Access, Gold Open Access},
  NUMBER = {12},
  TITLE = {Comparison and Analysis of Acoustic Features of Western and Chinese Classical Music Emotion Recognition Based on V‐A Model},
  TYPE = {Article},
  VOLUME = {12},
  YEAR = {2022},
  DOI = {10.3390/app12125787},
  ABSTRACT = {Music emotion recognition is increasingly becoming important in scientific research and practical applications. Due to the differences in musical characteristics between Western and Chinese classical music, it is necessary to investigate the distinctions in music emotional feature sets to improve the accuracy of cross‐cultural emotion recognition models. Therefore, a comparative study on emotion recognition in Chinese and Western classical music was conducted. Using the V‐A model as an emotional perception model, approximately 1000 pieces of Western and Chinese classical excerpts in total were selected, and approximately 20‐dimension feature sets for different emotional dimensions of different datasets were finally extracted. We considered different kinds of al-gorithms at each step of the training process, from pre‐processing to feature selection and regression model selection. The results reveal that the combination of MaxAbsScaler pre‐processing and the wrapper method using the recursive feature elimination algorithm based on extremely randomized trees is the optimal algorithm. The harmonic change detection function is a culturally universal fea-ture, whereas spectral flux is a culturally specific feature for Chinese classical music. It is also found that pitch features are more significant for Western classical music, whereas loudness and rhythm features are more significant for Chinese classical music. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85132193259&doi = 10.3390},
  AUTHOR_KEYWORDS = {classical music; extreme random tree; feature selection; music emotion recognition; V‐A model},
  PARADIGM = {regression},
  NOTES_CA = {include, though reports negative r^2 values in model comparison in table 2},
  NOTES_TE = {include},
  EMOTIONS = {valence, arousal},
  EMOTION_LOCUS = {perceived},
  STIMULUS_TYPE = {emoMusic, Soundtracks, Chinese Classical Music Dataset},
  STIMULUS_DURATION = {emoMusic: 45; soundtracks: 10-30; chinese: 30}, %% ~30 seconds for Chinese Classical dataset, 10-30 for Soundtracks, ~45 for emoMusic
  STIMULUS_DURATION_UNIT = {s},
  STIMULUS_N = {emoMusic: 1000, soundtracks: 360, chinese: 500}, %% emoMusic contains full 1000? 
  FEATURE_N = {variable, 557 before feature selection}, %% dimensionality BEFORE reduction
  PARTICIPANT_N = {c(emoMusic = '10 min per excerpt', soundtracks = 12, chinese = 20)},
  PARTICIPANT_EXPERTISE = {c(emoMusic = 'nonexperts', soundtracks = '10 years of musical training',
  chinese = 'audio technology')},
  PARTICIPANT_ORIGIN = {variable},
  PARTICIPANT_SAMPLING = {variable},
  PARTICIPANT_TASK = {rate},
  FEATURE_CATEGORIES = {Timbre, Pitch, Dynamic, Rhythm},
  FEATURE_SOURCE = {MIRToolbox, Essentia},
  FEATURE_REDUCTION_METHOD = {StandardScaler, Min-MaxScaler, MaxAbsScaler, Quantile Transformer Uniform distribution (QTU), Quantile Transformer with Gaussian distribution (QTG)},
  MODEL_CATEGORY = {regression},
  MODEL_DETAIL = {extremely randomized tree regression},
  MODEL_MEASURE = {R2},
  MODEL_COMPLEXITY_PARAMETERS = {MaxAbsScaler method, 20 combinations of pre-processing and feature selection were evaluated to obtain the optimal method},
  MODEL_RATE_EMOTION_NAMES = {valence, arousal},
  MODEL_RATE_EMOTION_VALUES = {
  library = mrbind(ERT.1.western.1 =  c(valence_r2.globalOptimal = 0.695, 
  valence_nFeatures.globalOptimal = 42, valence_r2.localOptimal = 0.674, valence_nFeatures.localOptimal = 21,
  arousal_r2.globalOptimal = 0.572, 
  arousal_nFeatures.globalOptimal = 38, arousal_r2.localOptimal = 0.560, arousal_nFeatures.localOptimal = 17),
  ERT.1.chinese.1 =  c(valence_r2.globalOptimal = 0.621, 
  valence_nFeatures.globalOptimal = 121, valence_r2.localOptimal = 0.607, valence_nFeatures.localOptimal = 21,
  arousal_r2.globalOptimal = 0.673, 
  arousal_nFeatures.globalOptimal = 76, arousal_r2.localOptimal = 0.652, arousal_nFeatures.localOptimal = 22)
  )
  },
  MODEL_VALIDATION = {five-fold cross validation},
  FINAL_NOTES = {}
}


@Article{wang2022cr,
  AUTHOR = {Wang, Xin and Wei, Yujia and Yang, Dasheng},
  JOURNAL = {COGNITIVE COMPUTATION AND SYSTEMS},
  MONTH = {JUN},
  NUMBER = {2, SI},
  PAGES = {116-129},
  TITLE = {Cross-cultural analysis of the correlation between musical elements and emotion},
  VOLUME = {4},
  YEAR = {2022},
  DOI = {10.1049/ccs2.12032},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1049/ccs2.12032},
  EISSN = {2517-7567},
  RESEARCHERID = {Wei, Yujia/IAQ-8917-2023},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include, regression},
  EMOTIONS = {valence, tension arousal, energy arousal},
  EMOTION_LOCUS = {perceived},
  STIMULUS_TYPE = {chinese solo, chinese ensemble, western solo, western ensemble,
  Chinese classical dataset, MediaEval, Emotify, AMG1608}, %% stimuli sampled from other data sets
  STIMULUS_DURATION = {10-15},
  STIMULUS_DURATION_UNIT = {s},
  STIMULUS_N = {146}, %% 515 before reliability test 
  FEATURE_N = {15},
  PARTICIPANT_N = {30},
  PARTICIPANT_EXPERTISE = {no restrictions}, 
  PARTICIPANT_ORIGIN = {China},
  PARTICIPANT_SAMPLING = {online, convenience}, %% presumably convenience
  PARTICIPANT_TASK = {rate},
  FEATURE_CATEGORIES = {Timbre, Rhythm, Dynamics, Pitch},
  FEATURE_SOURCE = {ppmBatch, MIRToolbox, expert annotation},
  FEATURE_REDUCTION_METHOD = {PCA},%% part of PLSR
  MODEL_CATEGORY = {regression},
  MODEL_DETAIL = {PLSR},
  MODEL_MEASURE = {R2},
  MODEL_COMPLEXITY_PARAMETERS = {unsure},
  MODEL_RATE_EMOTION_NAMES = {valence, tension arousal, energy arousal},
  MODEL_RATE_EMOTION_VALUES = {bind_field(
  various.pls.mixed.chineseClassicalEnsembles.1 = c(valence_r2.r2 = 0.85, valence_q2.q2 = 0.73, valence_pc1.pc1 = 0.36, valence_pc2.pc2 = 0.14,
                                            'tension arousal_r2.r2' = 0.77, 'tension arousal_q2.q2' = 0.62, 'tension arousal_pc1.pc1' = 0.35, 'tension arousal_pc2.pc2' = 0.21,
                                            'energy arousal_r2.r2' = 0.91, 'energy arousal_q2.q2' = 0.87, 'energy arousal_pc1.pc1' = 0.37, 'energy arousal_pc2.pc2' = 0.14),
  various.pls.mixed.chineseClassicalSolo.1 = c(0.64,0.3,0.22,0.16,0.54,0.17,0.21,0.17,0.76,0.5,0.21,0.12),
  various.pls.mixed.westernClassicalEnsembles.1 = c(.62,.4,.23,.17,.6,.44,.28,.14,.81,.7,.29,.14),
  various.pls.mixed.westernClassicalSolo.1 = c(.8,.64,.25,.14,.77,.64,.27,.14,.9,.87,.27,.16)
)
},
  MODEL_VALIDATION = {six-fold cross validation},
  FINAL_NOTES = {}
}


@Article{xia2022st,
  AUTHOR = {Xia, Yu and Xu, Fumei},
  JOURNAL = {Mathematical Problems in Engineering},
  NOTE = {Cited by: 6; All Open Access, Gold Open Access},
  TITLE = {Study on Music Emotion Recognition Based on the Machine Learning Model Clustering Algorithm},
  TYPE = {Article},
  VOLUME = {2022},
  YEAR = {2022},
  DOI = {10.1155/2022/9256586},
  ABSTRACT = {In recent years, the explosive growth of online music resources makes it difficult to retrieve and manage music information. To efficiently retrieve and classify music information has become a hot research topic. Thayer's two-dimensional emotion plane is selected as the basis for establishing the music emotion database. Music is divided into five categories, the concept of continuous emotion perception is introduced, and music emotion is regarded as a point on a two-dimensional emotional plane, together with the two sentiment variables to determine its location. The artificial labeling method is used to determine the position range of the five types of emotions on the emotional plane, and the regression method is used to obtain the relationship between the VA value and the music features so that the music emotion classification problem is transformed into a regression problem. A regression-based music emotion classification system is designed and implemented, which mainly includes a training part and a testing part. In the training part, three algorithms, namely, polynomial regression, support vector regression, and k-plane piecewise regression, are used to obtain the regression model. In the test part, the input music data is regressed and predicted to obtain its VA value and then classified, and the system performance is considered by classification accuracy. Results show that the combined method of support vector regression and k-plane piecewise regression improves the accuracy by 3 to 4 percentage points compared to using one algorithm alone; compared with the traditional classification method based on a support vector machine, the accuracy improves by 6 percentage points. Music emotion is classified by algorithms such as support vector machine classification, K-neighborhood classification, fuzzy neural network classification, fuzzy K-neighborhood classification, Bayesian classification, and Fisher linear discrimination, among which the support vector machine, fuzzy K-neighborhood, and the accuracy rate of music emotion classification realized by Fisher linear discriminant algorithm are more than 80%; a new algorithm "mixed classifier"is proposed, and the music emotion recognition rate based on this algorithm reaches 84.9%. © 2022 Yu Xia and Fumei Xu.},
  KEYWORDS = {Behavioral research; Classification (of information); Clustering algorithms; Fuzzy neural networks; Information management; Regression analysis; Speech recognition; Support vector machines; Vectors; Classifieds; Emotion recognition; K neighborhoods; Music emotion classifications; Music emotions; Music information; Percentage points; Piecewise regression; Support vector regressions; Two-dimensional; Emotion Recognition},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85140847927&doi = 10.1155},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include, although some quality issues},
  EMOTIONS = {},
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = {},
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = {},
  MODEL_CATEGORY = { },
  MODEL_DETAIL = {},
  MODEL_MEASURE = {},
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_NOTES = {quality issues !EXCL!}
}


@Article{xie2020mu,
  AUTHOR = {Xie, Baijun and Kim, Jonathan C. and Park, Chung Hyuk},
  JOURNAL = {APPLIED SCIENCES-BASEL},
  MONTH = {FEB},
  NUMBER = {3},
  TITLE = {Musical Emotion Recognition with Spectral Feature Extraction Based on a Sinusoidal Model with Model-Based and Deep-Learning Approaches},
  VOLUME = {10},
  YEAR = {2020},
  DOI = {10.3390/app10030902},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.3390/app10030902},
  ARTICLE = {902},
  EISSN = {2076-3417},
  ORCID = {Park, Chung Hyuk/0000-0003-0742-6541 Xie, Baijun/0000-0001-5080-198X},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include},
  EMOTIONS = {arousal, valence},
  EMOTION_LOCUS = {perceived},
  STIMULUS_TYPE = {1000 songs dataset},
  STIMULUS_DURATION = {45},
  STIMULUS_DURATION_UNIT = {s},
  STIMULUS_N = {744}, 
  FEATURE_N = {not specified},
  PARTICIPANT_N = {minimum of 10 human annotators},
  PARTICIPANT_EXPERTISE = {specified elsewhere},
  PARTICIPANT_ORIGIN = {specified elsewhere},
  PARTICIPANT_SAMPLING = {specified elsewhere},
  PARTICIPANT_TASK = {rate},
  FEATURE_CATEGORIES = {timbre},
  FEATURE_SOURCE = {openSMILE},
  FEATURE_REDUCTION_METHOD = {Sinusoidal Transform Coding},
  MODEL_CATEGORY = {regression},
  MODEL_DETAIL = { principal component regression (PCR), partial least squares (PLS) regression, and a feedforward networ.  peak-picking routine operation in a spectral envelope estimation vocoder (SEEVOC) framework},
  MODEL_MEASURE = {RMSE},
  MODEL_COMPLEXITY_PARAMETERS = {n principal components pca: 1 to 150, n predictor components pls 1 - 20, n hidden layers cnn: 1 to 30},
  MODEL_RATE_EMOTION_NAMES = {valence, arousal},
  MODEL_RATE_EMOTION_VALUES = {bind_field('openSMILE.pls.baseline features.100songs.1' = c(arousal_rmse.rmse = 0.146, valence_rmse.rmse = 0.156, arousal_r.r = 0.785, valence_r.r = 0.601), 
                   'openSMILE.pls.stc.100songs.1' = c(0.149, 0.156, 0.775, 0.600), 
                   'openSMILE.pls.baseline features and stc.100songs.1' = c(0.144, 0.151, 0.793,0.630),
                   'openSMILE.pcr.baseline features.100songs.1' = c(0.147, 0.156,0.781,0.612),
                   'openSMILE.pcr.stc.100songs.1' = c(0.150,0.157,0.770,0.585),
                   'openSMILE.pcr.baseline features and stc.100songs.1' = c(0.144,0.150,0.793,0.634),
                   'openSMILE.ff.baseline features.100songs.1' = c(0.172,0.169,0.732,0.570),
                   'openSMILE.ff.stc.100songs.1' = c(0.179,0.172,0.730,.551),
                   'openSMILE.ff.baseline features and stc.100songs.1' = c(0.165,0.158,.754,0.609)
)
 }, %% not reporting on classification as those involve spectrogram images instead.
  MODEL_VALIDATION = {10-fold cross-validation},
  FINAL_NOTES = {}
}


@Article{xu2021us,
  AUTHOR = {Xu, Liang and Sun, Zaoyi and Wen, Xin and Huang, Zhengxi and Chao, Chi-ju and Xu, Liuchang},
  JOURNAL = {PEERJ COMPUTER SCIENCE},
  MONTH = {NOV 15},
  TITLE = {Using machine learning analysis to interpret the relationship between music emotion and lyric features},
  VOLUME = {7},
  YEAR = {2021},
  DOI = {10.7717/peerj-cs.785},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.7717/peerj-cs.785},
  ARTICLE = {e785},
  EISSN = {2376-5992},
  ORCID = {Xu, Liang/0000-0003-3889-927X Sun, Zaoyi/0000-0002-4551-3606},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include, audio prediction and R2 included},
  EMOTIONS = {valence, arousal},
  EMOTION_LOCUS =  {perceived},
  STIMULUS_TYPE = {Chinese pop songs from PSIC3839},
  STIMULUS_DURATION = {specified elsewhere},
  STIMULUS_DURATION_UNIT = {s},
  STIMULUS_N = {2372 (subset of PSIC3839, total n: 3839)},  
  FEATURE_N = {50 PCA features}, %%
  PARTICIPANT_N = {specified elsewhere},
  PARTICIPANT_EXPERTISE = {specified elsewhere},
  PARTICIPANT_ORIGIN = {China},
  PARTICIPANT_SAMPLING = {specified elsewhere},
  PARTICIPANT_TASK = {rate},
  FEATURE_CATEGORIES = {rhythm, timbre, pitch, harmony},
  FEATURE_SOURCE = {librosa},
  FEATURE_REDUCTION_METHOD = {PCA, later grid parameter search},
  MODEL_CATEGORY = {regression},
  MODEL_DETAIL = {mlr, random forest regression},
  MODEL_MEASURE = {r2, rmse},
  MODEL_COMPLEXITY_PARAMETERS = {Arousal RFR: n_estimators = 156, max_depth = 10,  min_samples_leaf = 8 min_samples_split = 18 max_features = 0.5; Valence RFR: n_estimators = 179, max_depth = 15,  min_samples_leaf = 13 min_samples_split = 22 max_features = 0.5},
  MODEL_RATE_EMOTION_NAMES = {valence, arousal},
  MODEL_RATE_EMOTION_VALUES = {bind_field(
  'librosa.random forest regression.audio.psic3839.1' = c(arousal_r2.r2 = 0.629, arousal_rmse.rmse = 0.147,valence_r2.r2 = 0.371, valence_rmse.rmse = 0.214)
) },
  MODEL_VALIDATION = {10-fold cross validation},
  FINAL_NOTES = {discuss}
}


@Article{zhang2016br,
  AUTHOR = {Zhang, JiangLong and Huang, XiangLin and Yang, Lifang and Nie, Liqiang},
  JOURNAL = {NEUROCOMPUTING},
  MONTH = {OCT 5},
  NUMBER = {SI},
  PAGES = {333-341},
  TITLE = {Bridge the semantic gap between pop music acoustic feature and emotion: Build an interpretable model},
  VOLUME = {208},
  YEAR = {2016},
  DOI = {10.1016/j.neucom.2016.01.099},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1016/j.neucom.2016.01.099},
  ISSN = {0925-2312},
  EISSN = {1872-8286},
  ORCID = {Zhang, Jianglong/0000-0002-9079-2499},
  PARADIGM = {classification},
  NOTES_CA = {include},
  NOTES_TE = {include, classification},
  EMOTIONS = {valence, arousal},
  EMOTION_LOCUS = {perceived},
  STIMULUS_TYPE = {Chinese popular music from Zhang et al. (2015) },
  STIMULUS_DURATION = {Specified elsewhere},
  STIMULUS_DURATION_UNIT = {s},
  STIMULUS_N = {Specified elsewhere},
  FEATURE_N = {119},
  PARTICIPANT_N = {Specified elsewhere},
  PARTICIPANT_EXPERTISE = {Specified elsewhere},
  PARTICIPANT_ORIGIN = {Specified elsewhere}, %% presumably china
  PARTICIPANT_SAMPLING = {Specified elsewhere},
  PARTICIPANT_TASK = {Rate},
  FEATURE_CATEGORIES = {dynamic, timbre, rhythm, tonality }, %% "we make an assumption that the arousal is only dependent on the intensity features, and valence is dependent on the whole feature sets" 
  FEATURE_SOURCE = {MA Toolbox, MIR Toolbox, coversongs},
  FEATURE_REDUCTION_METHOD = {filter methods, wrapper methods, shrinkage methods. Details: Relief-R-2, Relief-F-3, Correlation-Based (Pearson, Spearman, information-gain, gain-ratio) Forward Selection-AIC, L1-regularized logistic regression},
  MODEL_CATEGORY = {classification},
  MODEL_DETAIL = {penalized logistic regression, regularized SVM, tested models: LR,
LDA, Quadratic Discriminant Analysis (QDA), KNN, Tree, Random-
Forest (RF), Bagging (Bag), Boosting tree (Boost), Support Vector
Machine with Radial kernel (SVM-R), Stagewise Penalty Logistic
Regression (PLR), Support Vector Machine with Polynomial kernel
(SVM-P), Support Vector Machine with Linear kernel (SVM-L), L1-
LR, L2-LR, L1-SVM, L2-SVM and Shrunken Centroids Regularized
Discriminant Analysis (RDA).},
  MODEL_MEASURE = {Accuracy},
  MODEL_COMPLEXITY_PARAMETERS = {Tuning parameters defined through 10-fold cross validation},
  MODEL_RATE_EMOTION_NAMES = {valence, arousal},
  MODEL_RATE_EMOTION_VALUES = {  match_named_values('l1 logistic regression.intensity.zhang2015so.1' = c(arousal_accuracy.mean = .84, valence_accuracy.mean = 0.64), 
                   'l2 logistic regression.intensity.zhang2015so.1' = c(arousal_accuracy.mean = 0.83, valence_accuracy.mean = 0.68), 
                   'l1 support vector regression.intensity.zhang2015so.1' = c(arousal_accuracy.mean = 0.83, valence_accuracy.median = 0.72), 
                   'reguliarized discriminant analysis.intensity.zhang2015so.1' = c(arousal_accuracy.mean = 0.83), 
                   'logistic regression.intensity.zhang2015so.1' = c(arousal_accuracy.mean = 0.8),
                   'polynomial svm.intensity.zhang2015so.1'  = c(arousal_accuracy.mean = 0.774),
                    'linear svm.intensity.zhang2015so.1'  = c(arousal_accuracy.mean = 81.4)
)
},
  MODEL_VALIDATION = {10-fold cross validation (for tuning parameters)},
  FINAL_NOTES = {}
}


@Article{zhang2019us,
  AUTHOR = {Zhang, Le-kai and Sun, Shou-qian and Xing, Bai-xi and Luo, Rui-ming and Zhang, Ke-jun},
  JOURNAL = {FRONTIERS OF INFORMATION TECHNOLOGY \& ELECTRONIC ENGINEERING},
  MONTH = {JUL},
  NUMBER = {7},
  PAGES = {964-974},
  TITLE = {Using psychophysiological measures to recognize personal music emotional experience},
  VOLUME = {20},
  YEAR = {2019},
  DOI = {10.1631/FITEE.1800101},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1631/FITEE.1800101},
  ISSN = {2095-9184},
  EISSN = {2095-9230},
  RESEARCHERID = {zhang, ke/AAH-8217-2019},
  PARADIGM = {regression},
  NOTES_CA = {include, single-modality reported},
  NOTES_TE = {include, use a subset of the results},
  EMOTIONS = {valence, arousal},
  EMOTION_LOCUS = {perceived},
  STIMULUS_TYPE = {Chinese folk music. Adopted from Xing et al. (2015)},
  STIMULUS_DURATION = {10},
  STIMULUS_DURATION_UNIT = {s},
  STIMULUS_N = {420}, %% cite Xing et al. (2015). Stimuli divided into 21 groups each with 20 stimuli (five per quadrant). Each participant listened to 3 groups
  FEATURE_N = {64}, %% 63 found relevant for valence, 64 for arousal (from feature selection method including other sources)
  PARTICIPANT_N = {21},
  PARTICIPANT_EXPERTISE = {no specific music training},
  PARTICIPANT_ORIGIN = {not specified}, %% presumably china
  PARTICIPANT_SAMPLING = {volunteer},
  PARTICIPANT_TASK = {rate},
  FEATURE_CATEGORIES = {timbre},
  FEATURE_SOURCE = {marsyas},
  FEATURE_REDUCTION_METHOD = {ANOVA, PCA. PCA may not be relevant to music-only model},
  MODEL_CATEGORY = {regression},
  MODEL_DETAIL =  {linear regression, ridge regression, SVM (three kernels), decision trees, kNN, multilayer perceptron, Nu support vector regression},
  MODEL_MEASURE = {MSE, CC},
  MODEL_COMPLEXITY_PARAMETERS = {Scikit learn defaults, RBF kernel},
  MODEL_RATE_EMOTION_NAMES = {valence, arousal},
  MODEL_RATE_EMOTION_VALUES = {bind_field(
  marsyas.NuSVR.music.xing2014.1 = c(arousal_mse.mse = 0.0437, arousal_cc.cc = 0.425, valence_mse.mse = 0.0373, valence_cc.cc = 0.420)
)},
  MODEL_VALIDATION = {10 x 10 cross validation},
  FINAL_NOTES = {}
}


@Article{zhang2023mo,
  AUTHOR = {Zhang, Meixian and Zhu, Yonghua and Zhang, Wenjun and Zhu, Yunwen and Feng, Tianyu},
  JOURNAL = {MULTIMEDIA TOOLS AND APPLICATIONS},
  MONTH = {FEB},
  NUMBER = {5},
  PAGES = {7319-7341},
  TITLE = {Modularized composite attention network for continuous music emotion recognition},
  VOLUME = {82},
  YEAR = {2023},
  DOI = {10.1007/s11042-022-13577-6},
  SOURCE = {web_of_science},
  BDSK = {https://doi.org/10.1007/s11042-022-13577-6},
  ISSN = {1380-7501},
  EISSN = {1573-7721},
  ORCID = {Zhang, Meixian/0000-0002-6696-2814},
  RESEARCHERID = {Zhu, Yonghua/HSI-1360-2023 Feng, Tian-Yu/AAX-3892-2020},
  EARLYACCESSDATE = {AUG 2022},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include, R2},
  EMOTIONS = {valence, arousal},
  EMOTION_LOCUS = {perceived},
  STIMULUS_TYPE =  {PMEmo, DEAM}, %% PMEmo: new, DEAM: benchmark
  STIMULUS_DURATION = {DEAM: 45, PMEmo: specified elsewhere}, %% PMEmo has 794. Rest removed due to insufficieent annotation #
  STIMULUS_DURATION_UNIT = {s},
  STIMULUS_N = {DEAM: 744, PMEmo: 206},
  FEATURE_N = { }, %% how to report
  PARTICIPANT_N = {specified elsewhere},
  PARTICIPANT_EXPERTISE = {specified elsewhere},
  PARTICIPANT_ORIGIN = {specified elsewhere},
  PARTICIPANT_SAMPLING = {specified elsewhere},
  PARTICIPANT_TASK = {rate},
  FEATURE_CATEGORIES = {filter banks, handcrafted},
  FEATURE_SOURCE = {filter bank output (size = 120*120), 60 handcrafted features (size = 60*60)},
  FEATURE_REDUCTION_METHOD = {weighted attention module, feature augmentation},
  MODEL_CATEGORY =  {regression},
  MODEL_DETAIL = {Modularized Composite Attention Network. Case 1: no sample reconstruction. Case 2: no handcrafted features (or feature augmentation). Case 3: No self-attention mechanism or BLSTM. Case 4: no style embedding module}, %% new method
  MODEL_MEASURE = {RMSE, MAE, CCC},
  MODEL_COMPLEXITY_PARAMETERS = {Batch size: 15, mixup technique applied to spectrograms, first convolutional layer alpha: 0.2, learning rate: 0.001, decreasing by 1/10 every 10 epochs. Adam optimizer. Dropout ratio 0.3 and 0.2,
alpha = 0.18, beta = 0.49},
  MODEL_RATE_EMOTION_NAMES = {valence, arousal},
  MODEL_RATE_EMOTION_VALUES = {bind_field(
   'ns.MCAN.handcrafted and filter bank.deam.1' = c(valence_ccc.mean = 0.309, valence_rmse.mean = 0.112, valence_mae.mean = 0.811,
                              valence_ccc.sd = 0.358, valence_rmse.sd = 0.010, valence_mae.sd = 0.009,
                       arousal_ccc.mean = 0.502, arousal_rmse.mean = 0.109, arousal_mae.mean = 0.764,
                       arousal_ccc.sd = 0.287, arousal_rmse.sd = 0.011, arousal_mae.sd = 0.023),
   'ns.MCAN. handcrafted and filter bank.PMEmo.1' = c(.215,.144,.892,0.265,.102,.030,.401,.135,.901,.315,0.057,0.103)
 )}, %% only reporting final results (not five cases. Can add if important)
  MODEL_VALIDATION = {ablation, train/test},
  FINAL_NOTES = {}
}


@Article{zhang2024ap,
  AUTHOR = {Zhang, Yao and Cai, Delin and Zhang, Dongmei},
  JOURNAL = {Environment and Social Psychology}, %% dubious journal??
  NOTE = {Cited by: 0; All Open Access, Gold Open Access},
  NUMBER = {4},
  TITLE = {Application and algorithm optimization of music emotion recognition in piano performance evaluation},
  TYPE = {Article},
  VOLUME = {9},
  YEAR = {2024},
  DOI = {10.54517/esp.v9i4.2344},
  ABSTRACT = {In the current research, we integrate distinct learning modalities—Curriculum Learning (CL) and Reinforcement Learning (RL)—in an attempt to develop and optimize Music Emotion Recognition (MER) in piano performance. Classical approaches have never been successful when applied in the field of determining the degree of emotion in the music of the piano, owing to the substantial complexity required. Addressing this particular issue is the primary motivation for the present endeavour. In an approach that’s comparable to how human beings acquire information, it trains the RL agent CL in phases; such an approach improves the student’s learning model in understanding the diverse emotions expressed by musical compositions. A higher rating of performance can be achieved after learning the model to recognize emotions more effectively and precisely. A set of piano melodies with emotional content notes has been included in the EMOPIA repository for use when conducting the process of evaluation. In order to benchmark the proposed approach with different models, parameters including R2, Mean Absolute Error (MAE), and Root Mean Square Error (RMSE) were deployed. Studies indicate that the recommended approach accurately recognizes the emotions expressed by piano-playing music. In challenging tasks like MER, the significance of implementing the CL paradigm with the RL has been emphasized using the outcomes mentioned earlier. © 2024 by author(s).},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85184876855&doi = 10.54517},
  AUTHOR_KEYWORDS = {Curriculum Learning; Machine Learning; MBE; Music Emotion Recognition; piano music; Reinforcement Learning; RMSE},
  PARADIGM = {regression},
  NOTES_CA = {include},
  NOTES_TE = {include, classification of quadrants},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_NOTES = {dubious !EXCL!}  %% dubious journal??
}
