%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Tuomas Eerola at 2024-05-13 16:08:33 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@article{abadisubramanian_2015,
	abstract = {In this work, we present DECAF-a multimodal data set for decoding user physiological responses to affective multimedia content. Different from data sets such as DEAP {$[$}15{$]$} and MAHNOB-HCI {$[$}31{$]$}, DECAF contains (1) brain signals acquired using the Magnetoencephalogram (MEG) sensor, which requires little physical contact with the user's scalp and consequently facilitates naturalistic affective response, and (2) explicit and implicit emotional responses of 30 participants to 40 one-minute music video segments used in {$[$}15{$]$} and 36 movie clips, thereby enabling comparisons between the EEG versus MEG modalities as well as movie versus music stimuli for affect recognition. In addition to MEG data, DECAF comprises synchronously recorded near-infra-red (NIR) facial videos, horizontal Electrooculogram (hEOG), Electrocardiogram (ECG), and trapezius-Electromyogram (tEMG) peripheral physiological responses. To demonstrate DECAF's utility, we present (i) a detailed analysis of the correlations between participants' self-assessments and their physiological responses and (ii) single-trial classification results for valence, arousal and dominance, with performance evaluation against existing data sets. DECAF also contains time-continuous emotion annotations for movie clips from seven users, which we use to demonstrate dynamic emotion prediction.},
	author = {Abadi, Mojtaba and Subramanian, Ramanathan and Kia, Seyed and Avesani, Paolo and Patras, Ioannis and Sebe, Nicu},
	c1 = {Department of Information Engineering and Computer Science, University of Trento, Italy; Semantic Knowledge an Innovation Lab (SKIL), Telecom Italia; Advanced Digital Sciences Center, University of Illinois at Urbana-Champaign, Singapore; NeuroInformatics Lab, Fondazione Bruno Kessler, Trento, Italy; NeuroInformatics Lab, Fondazione Bruno Kessler, Trento, Italy; School of Computer Science and Electronic Engineering, Queen Mary, University of London; Department of Information Engineering and Computer Science, University of Trento, Italy},
	date = {2015-07-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:28 +0100},
	doi = {10.1109/taffc.2015.2392932},
	isbn = {1949-3045},
	journal = {IEEE transactions on affective computing},
	keywords = {Affective Computing; Emotion Recognition; Deep Learning for EEG; Multimodal Data; EEG Analysis},
	la = {en},
	number = {3},
	pages = {209--222},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {DECAF: MEG-Based Multimodal Database for Decoding Affective Physiological Responses},
	url = {https://doi.org/10.1109/taffc.2015.2392932},
	volume = {6},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/taffc.2015.2392932}}

@article{goshvarpourabbasi_2017,
	abstract = {The purpose of the current study was to examine the effectiveness of Matching Pursuit (MP) algorithm in emotion recognition.Electrocardiogram (ECG) and galvanic skin responses (GSR) of 11 healthy students were collected while subjects were listening to emotional music clips. Applying three dictionaries, including two wavelet packet dictionaries (Coiflet, and Daubechies) and discrete cosine transform, MP coefficients were extracted from ECG and GSR signals. Next, some statistical indices were calculated from the MP coefficients. Then, three dimensionality reduction methods, including Principal Component Analysis (PCA), Linear Discriminant Analysis, and Kernel PCA were applied. The dimensionality reduced features were fed into the Probabilistic Neural Network in subject-dependent and subject-independent modes. Emotion classes were described by a two-dimensional emotion space, including four quadrants of valence and arousal plane, valence based, and arousal based emotional states.Using PCA, the highest recognition rate of 100{\%} was achieved for sigma = 0.01 in all classification schemes. In addition, the classification performance of ECG features was evidently better than that of GSR features. Similar results were obtained for subject-dependent emotion classification mode.An accurate emotion recognition system was proposed using MP algorithm and wavelet dictionaries.},
	author = {Goshvarpour, Atefeh and Abbasi, Ataollah},
	c1 = {{$[$}Computational Neuroscience Laboratory, Department of Biomedical Engineering, Faculty of Electrical Engineering, Sahand University of Technology, Tabriz, Iran{$]$}; {$[$}Computational Neuroscience Laboratory, Department of Biomedical Engineering, Faculty of Electrical Engineering, Sahand University of Technology, Tabriz, Iran{$]$}},
	date = {2017-12-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1016/j.bj.2017.11.001},
	isbn = {2319-4170},
	journal = {Biomedical journal},
	keywords = {Emotion Recognition; Affective Computing; Speech Emotion; Deep Learning for EEG; Epilepsy Detection},
	la = {en},
	number = {6},
	pages = {355--368},
	publisher = {Elsevier BV},
	title = {An accurate emotion recognition system using ECG and GSR signals and matching pursuit method},
	url = {https://doi.org/10.1016/j.bj.2017.11.001},
	volume = {40},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1016/j.bj.2017.11.001}}

@article{aljanakiyang_2017,
	abstract = {Music emotion recognition (MER) field rapidly expanded in the last decade. Many new methods and new audio features are developed to improve the performance of MER algorithms. However, it is very difficult to compare the performance of the new methods because of the data representation diversity and scarcity of publicly available data. In this paper, we address these problems by creating a data set and a benchmark for MER. The data set that we release, a MediaEval Database for Emotional Analysis in Music (DEAM), is the largest available data set of dynamic annotations (valence and arousal annotations for 1,802 songs and song excerpts licensed under Creative Commons with 2Hz time resolution). Using DEAM, we organized the 'Emotion in Music' task at MediaEval Multimedia Evaluation Campaign from 2013 to 2015. The benchmark attracted, in total, 21 active teams to participate in the challenge. We analyze the results of the benchmark: the winning algorithms and feature-sets. We also describe the design of the benchmark, the evaluation procedures and the data cleaning and transformations that we suggest. The results from the benchmark suggest that the recurrent neural network based approaches combined with large feature-sets work best for dynamic MER.},
	author = {Aljanaki, Anna and Yang, Yi and Soleymani, Mohammad},
	c1 = {Utrecht University, Utrecht, the Netherlands; Academia Sinica, Taipei, Taiwan; Swiss Center for Affective Sciences, University of Geneva, Geneva, Switzerland},
	date = {2017-03-10},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1371/journal.pone.0173392},
	isbn = {1932-6203},
	journal = {PloS one},
	keywords = {Emotion Recognition; Music Information Retrieval; Audio Event Detection; Affective Computing; Melody Extraction},
	la = {en},
	number = {3},
	pages = {e0173392--e0173392},
	publisher = {Public Library of Science},
	title = {Developing a benchmark for emotional analysis of music},
	url = {https://doi.org/10.1371/journal.pone.0173392},
	volume = {12},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1371/journal.pone.0173392}}

@article{keelawatthammasan_2019,
	abstract = {Emotion recognition during music listening using electroencephalogram (EEG) has gained more attention from researchers, recently.Many studies focused on accuracy on one subject while subject-independent performance evaluation was still unclear.In this paper, the objective is to create an emotion recognition model that can be applied to multiple subjects.By adopting convolutional neural networks (CNNs), advantage could be gained from utilizing information from electrodes and time steps.Using CNNs also does not need feature extraction which might leave out other related but unobserved features.CNNs with three to seven convolutional layers were deployed in this research.We measured their performance with a binary classification task for compositions of emotions including arousal and valence.The results showed that our method captured EEG signal patterns from numerous subjects by 10-fold cross validation with 81.54{\%} and 86.87{\%} accuracy from arousal and valence respectively.The method also showed a higher capability of generalization to unseen subjects than the previous method as can be observed from the results of leave-one-subject-out validation.},
	author = {Keelawat, Panayu and Thammasan, Nattapong and Kijsirikul, Boonserm and Numao, Masayuki},
	c1 = {Department of Computer Engineering Chulalongkorn University Bangkok, Thailand; Nattapong Thammasan Human Media Interaction University of Twente Enschede, Netherlands; The Institute of Scientific and Industrial Research Osaka University Osaka, Japan; Human Media Interaction, University of Twente, Enschede, Netherlands.; Department of Computer Engineering Chulalongkorn University Bangkok, Thailand; Nattapong Thammasan Human Media Interaction University of Twente Enschede, Netherlands; The Institute of Scientific and Industrial Research Osaka University Osaka, Japan; Department of Computer Engineering Chulalongkorn University Bangkok, Thailand; Nattapong Thammasan Human Media Interaction University of Twente Enschede, Netherlands; The Institute of Scientific and Industrial Research Osaka University Osaka, Japan},
	date = {2019-03-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/cspa.2019.8696054},
	keywords = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Epilepsy Detection},
	la = {en},
	title = {Subject-Independent Emotion Recognition During Music Listening Based on EEG Using Deep Convolutional Neural Networks},
	url = {https://doi.org/10.1109/cspa.2019.8696054},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/cspa.2019.8696054}}

@article{chenli_2021,
	abstract = {Recently, emotion classification from electroencephalogram (EEG) data has attracted much attention. As EEG is an unsteady and rapidly changing voltage signal, the features extracted from EEG usually change dramatically, whereas emotion states change gradually. Most existing feature extraction approaches do not consider these differences between EEG and emotion. Microstate analysis could capture important spatio-temporal properties of EEG signals. At the same time, it could reduce the fast-changing EEG signals to a sequence of prototypical topographical maps. While microstate analysis has been widely used to study brain function, few studies have used this method to analyze how brain responds to emotional auditory stimuli. In this study, the authors proposed a novel feature extraction method based on EEG microstates for emotion recognition. Determining the optimal number of microstates automatically is a challenge for applying microstate analysis to emotion. This research proposed dual-threshold-based atomize and agglomerate hierarchical clustering (DTAAHC) to determine the optimal number of microstate classes automatically. By using the proposed method to model the temporal dynamics of auditory emotion process, we extracted microstate characteristics as novel temporospatial features to improve the performance of emotion recognition from EEG signals. We evaluated the proposed method on two datasets. For public music-evoked EEG Dataset for Emotion Analysis using Physiological signals, the microstate analysis identified 10 microstates which together explained around 86{\%} of the data in global field power peaks. The accuracy of emotion recognition achieved 75.8{\%} in valence and 77.1{\%} in arousal using microstate sequence characteristics as features. Compared to previous studies, the proposed method outperformed the current feature sets. For the speech-evoked EEG dataset, the microstate analysis identified nine microstates which together explained around 85{\%} of the data. The accuracy of emotion recognition achieved 74.2{\%} in valence and 72.3{\%} in arousal using microstate sequence characteristics as features. The experimental results indicated that microstate characteristics can effectively improve the performance of emotion recognition from EEG signals.},
	author = {Chen, Jing and Li, Haifeng and Ma, Lin and Bo, Hongjian and Soong, Frank and Shi, Yaohui},
	c1 = {School of Computer Science and Technology, Faculty of Computing, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Faculty of Computing, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Faculty of Computing, Harbin Institute of Technology, Harbin, China; Shenzhen Academy of Aerospace Technology, Shenzhen, China; Speech Group, Microsoft Research Asia, Beijing, China; Heilongjiang Provincial Hospital, Harbin, China},
	date = {2021-07-14},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3389/fnins.2021.689791},
	isbn = {1662-453X},
	journal = {Frontiers in neuroscience},
	keywords = {EEG Analysis; Emotion Recognition; Deep Learning for EEG; Speech Emotion; Affective Computing},
	la = {en},
	publisher = {Frontiers Media},
	title = {Dual-Threshold-Based Microstate Analysis on Characterizing Temporal Dynamics of Affective Process and Emotion Recognition From EEG Signals},
	url = {https://doi.org/10.3389/fnins.2021.689791},
	volume = {15},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.3389/fnins.2021.689791}}

@article{mollahosseinihasani_2019,
	abstract = {Automated affective computing in the wild setting is a challenging problem in computer vision. Existing annotated databases of facial expressions in the wild are small and mostly cover discrete emotions (aka the categorical model). There are very limited annotated facial databases for affective computing in the continuous dimensional model (e.g., valence and arousal). To meet this need, we collected, annotated, and prepared for public distribution a new database of facial emotions in the wild (called AffectNet). AffectNet contains more than 1,000,000 facial images from the Internet by querying three major search engines using 1250 emotion related keywords in six different languages. About half of the retrieved images were manually annotated for the presence of seven discrete facial expressions and the intensity of valence and arousal. AffectNet is by far the largest database of facial expression, valence, and arousal in the wild enabling research in automated facial expression recognition in two different emotion models. Two baseline deep neural networks are used to classify images in the categorical model and predict the intensity of valence and arousal. Various evaluation metrics show that our deep neural network baselines can perform better than conventional machine learning methods and off-the-shelf facial expression recognition systems.},
	author = {Mollahosseini, Ali and Hasani, Behzad and Mahoor, Mohammad},
	c1 = {Department of Electrical and Computer Engineering, University of Denver, Denver, CO; Department of Electrical and Computer Engineering, University of Denver, Denver, CO; Department of Electrical and Computer Engineering, University of Denver, Denver, CO},
	date = {2019-01-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1109/taffc.2017.2740923},
	isbn = {1949-3045},
	journal = {IEEE transactions on affective computing},
	keywords = {Affective Computing; Emotion Recognition; Emotional Expressions; Facial Expression; Face Perception},
	la = {en},
	number = {1},
	pages = {18--31},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild},
	url = {https://doi.org/10.1109/taffc.2017.2740923},
	volume = {10},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/taffc.2017.2740923}}

@article{livingstonerusso_2018,
	abstract = {The RAVDESS is a validated multimodal database of emotional speech and song. The database is gender balanced consisting of 24 professional actors, vocalizing lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity, with an additional neutral expression. All conditions are available in face-and-voice, face-only, and voice-only formats. The set of 7356 recordings were each rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity and test-retest intrarater reliability were reported. Corrected accuracy and composite "goodness" measures are presented to assist researchers in the selection of stimuli. All recordings are made freely available under a Creative Commons license and can be downloaded at https://doi.org/10.5281/zenodo.1188976.},
	author = {Livingstone, Steven and Russo, Frank},
	c1 = {Department of Computer Science and Information Systems, University of Wisconsin-River Falls, Wisconsin, WI, United States of America; Department of Psychology, Ryerson University, Toronto, Canada; Department of Psychology, Ryerson University, Toronto, Canada},
	date = {2018-05-16},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1371/journal.pone.0196391},
	isbn = {1932-6203},
	journal = {PloS one},
	keywords = {Speech Emotion; Emotion Recognition; Facial Expression; Affective Computing; Speech Perception},
	la = {en},
	number = {5},
	pages = {e0196391--e0196391},
	publisher = {Public Library of Science},
	title = {The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English},
	url = {https://doi.org/10.1371/journal.pone.0196391},
	volume = {13},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1371/journal.pone.0196391}}

@article{alakusturkoglu_2020,
	abstract = {Electronics LettersVolume 56, Issue 25 p. 1364-1367 Special Issue: Current Trends in Cognitive Science and Brain Computing Research and ApplicationsFree Access Emotion recognition with deep learning using GAMEEMO data set T. B. Alakus, Corresponding Author T. B. Alakus talhaburakalakus{\char64}klu.edu.tr orcid.org/0000-0003-3136-3341 Kirklareli University, Faculty of Engineering, Department of Software Engineering, Kirklareli, TurkeySearch for more papers by this authorI. Turkoglu, I. Turkoglu Firat University, Faculty of Technology, Department of Software Engineering, Elazig, TurkeySearch for more papers by this author T. B. Alakus, Corresponding Author T. B. Alakus talhaburakalakus{\char64}klu.edu.tr orcid.org/0000-0003-3136-3341 Kirklareli University, Faculty of Engineering, Department of Software Engineering, Kirklareli, TurkeySearch for more papers by this authorI. Turkoglu, I. Turkoglu Firat University, Faculty of Technology, Department of Software Engineering, Elazig, TurkeySearch for more papers by this author First published: 22 October 2020 https://doi.org/10.1049/el.2020.2460Citations: 10AboutSectionsPDF ToolsRequest permissionExport citationAdd to favoritesTrack citation ShareShare Give accessShare full text accessShare full-text accessPlease review our Terms and Conditions of Use and check box below to share full-text version of article.I have read and accept the Wiley Online Library Terms and Conditions of UseShareable LinkUse the link below to share a full-text version of this article with your friends and colleagues. Learn more.Copy URL Share a linkShare onFacebookTwitterLinkedInRedditWechat Abstract Emotion recognition is actively used in brain--computer interface, health care, security, e-commerce, education and entertainment applications to increase and control human--machine interaction. Therefore, emotions affect people's lives and decision-making mechanisms throughout their lives. However, the fact that emotions vary from person to person, being an abstract concept and being dependent on internal and external factors makes the studies in this field difficult. In recent years, studies based on electroencephalography (EEG) signals, which perform emotion analysis in a more robust and reliable way, have gained momentum. In this article, emotion analysis based on EEG signals was performed to predict positive and negative emotions. The study consists of four parts. In the first part, EEG signals were obtained from the GAMEEMO data set. In the second stage, the spectral entropy values of the EEG signals of all channels were calculated and these values were classified by the bidirectional long-short term memory architecture in the third stage. In the last stage, the performance of the deep-learning architecture was evaluated with accuracy, sensitivity, specificity and receiver operating characteristic (ROC) curve. With the proposed method, an accuracy of 76.91{\%} and a ROC value of 90{\%} were obtained. Introduction Emotion can be defined as the voluntary or involuntary reaction of people against an external stimulus while performing actions such as talking, thinking, communicating, learning, making decisions etc. Since all these and similar actions are carried out through emotions, emotions have a great impact on daily life. While negative emotions affect people both physically and psychologically, positive emotions make people more successful in society and bring better living conditions {$[$}1, 2 {$]$}. There are many different emotion analysis studies in order to comprehend the nature and behaviour of emotions. However, the fact that the concept of emotion is abstract and does not have an objective result makes it difficult to analyse emotions {$[$}3, 4 {$]$}. In addition, a large number of methods to collect and process emotion data makes the analysis process more difficult and time-consuming. For these reasons, a computer-based system is needed {$[$}5 {$]$}. Emotions can be obtained through physical and non-physical methods. Examples of these include voice signals, body language, facial expressions and physical activities. Since these methods are easy to apply, data can be obtained quickly and easily. However, during the data collection phase, emotions can be manipulated intentionally or unintentionally by the subjects. While the voice signals are collected, subjects can imitate their voices and similarly hide their facial expressions {$[$}6 {$]$}. Therefore, the fact that the data obtained by these methods are both incomplete and untrustworthy caused the need for a more reliable system and increased the importance of physiological signals such as electroencephalography (EEG) {$[$}7 {$]$}. EEG signals are the most widely used method in this area because of their ease of use, low cost and portable models {$[$}8 {$]$}. There are two types of emotional patterns in the literature, discrete and dimensional. There are eight basic emotions (anger, joy, trust, fear, surprise, sadness, disgust and anticipation) in the discrete emotion model {$[$}9 {$]$}. In the dimensional model, emotions are expressed not by their names but according to their positions in the arousal--valence plane {$[$}10 {$]$}. In this plane, emotions are divided into four main areas. Valence axis refers to the x -axis, and this axis indicates whether the emotion is negative or positive. Y -axis expresses arousal and emotions are ordered from low to high according to the degree of activity. The arousal--valence plane is given in Fig. 1. The plane is divided into four different zones, as can be seen in Fig. 1. While there are high arousal positive valence emotions in the first zone, there are high valence negative emotions in the second zone. In the third and fourth zones, there are emotions of negative valence-low arousal and positive valence-low arousal, respectively. In this model, emotions are named according to their location in the coordinate plane rather than their names. For example, the emotion of happiness is expressed as high arousal positive valence. Similar inferences can be made for other types of emotions. In this study, dimensional emotion model was used and emotions were evaluated as positive-valence and negative-valence. Fig. 1Open in figure viewerPowerPoint Example of arousal--valence dimensional emotion model The study consists of four stages. In the first step, EEG signals were collected from the GAMEEMO data set. In the second step, the spectral entropy values of each EEG signal were calculated. Then these values were classified with the bidirectional long-short term memory (BiLSTM) deep-learning model and the prediction process was carried out. In the last stage, the performance of the BiLSTM model was measured with different evaluation metrics. The main contributions of the study can be summarised as follows: To the best of our knowledge, the GAMEEMO data set was analysed for the first time in this study with the BiLSTM deep-learning model. With this study, it was observed that EEG signals obtained from a portable device can also be used for emotion analysis. The rest of the work is organized as follows: studies conducted with EEG signals are mentioned in the related works section. In the data and methods section, general information about the data set, the spectral entropy and BiLSTM model used in this study are given. In the application results section, the performance of the BiLSTM model was examined and the results were discussed. In the conclusion section, the study was examined and explanations were made based on possible future applications. Related works In this section, emotion analysis studies performed with EEG signals are examined. The authors in {$[$}11 {$]$} used the LSTM model for emotion prediction and classified the EEG signals for it. The DEAP data set was used in the study and a classification process was performed for valence, arousal and liking classes. The signals have not been preprocessed and classified directly in the LSTM architecture. The study was validated with four-fold cross-validation and the performance of the LSTM model was evaluated with the accuracy metric only. At the end of the study, the accuracy of 0.8565 for arousal, 0.8545 for valence and 0.8799 for liking was obtained. Authors in {$[$}12 {$]$} carried out emotion analysis using deep learning. The DEAP data set was used in the study and the feature extraction was carried out before classification. In the feature extraction phase, the signals were transformed with empirical model decomposition and variational model decomposition and the power spectral density, and the first difference values of intrinsic mode functions were collected from these converted signals. Then the signals were classified with both support vector machines (SVMs) and deep neural network. After the classification process, accuracies of 0.6125 for arousal and 0.6250 for valence were reached. The authors in {$[$}13 {$]$} performed emotion analysis with principal component analysis and deep learning. As in other studies, the DEAP data set was used in this study. The signals were first transformed into five different bands with fast Fourier transform and power spectral values were obtained from each band. Then the values were normalised and classified with deep-learning network. Valence emotions were predicted with 0.5342 and arousal emotions with 0.5205 accuracies with the proposed method. Data and methods In this study, EEG signals belonging to the GAMEEMO {$[$}14 {$]$} data set are used. The data set contains EEG signals of 28 people. Unlike conventional EEG collecting devices, the data were obtained with a portable EEG device (Emotiv EPOC + 14-Channel Wireless EEG Headset). The EEG device used has 14 channels in total as AF3, AF4, F3, F4, F7, F8, FC5, FC6, O1, O2, P7, P8, T7 and T8. The sampling rate of the obtained signals is 128 Hz. The data set contains raw and preprocessed signals. Since noise-free data were used in this study, pre-processed data were considered. In order to obtain emotions, the subjects played four computer games and each subject played games for 5 min. There are a total of 1568 (4 ×14 ×28) EEG data in the data set. The number 4 refers to the stimuli used. This value is 4 because 4 games were played in the data set. The number 14 indicates the number of EEG channels, while number 28 refers to the subjects. Sample length of each EEG data is 38,252. More technical and detailed information about the data set can be obtained from {$[$}14 {$]$}. Researchers who want to use GAMEEMO data can access the data from the link provided (https://data.mendeley.com/datasets/b3pn4kwpmn/3 ). In the study, feature extraction was carried out and spectral entropy values of EEG signals were calculated. Spectral entropy measures how sharp the spectrum of a signal is {$[$}15 {$]$}. A signal with a sharp spectrum, such as the sum of sinusoids, has low spectral entropy. In contrast, a flat spectrum signal such as white noise has high spectral entropy. Spectral entropy treats the normalised power distribution in the frequency domain of the signal as a probability distribution and calculates the Shannon entropy. Shannon entropy in this context is the spectral entropy of the signal. Spectral entropy is effectively used in fault detection and diagnosis {$[$}16, 17 {$]$}, speech recognition {$[$}18 {$]$} and biomedical signal processing {$[$}19 {$]$}. In this study, the spectral entropy value of each EEG signal was calculated and these values were used to classify with BiLSTM. In this study, recurrent neural network was used instead of traditional CNN architectures because of their success in time series applications {$[$}20-22 {$]$}. For this reason, a recurrent neural network model --bidirectional LSTM, was used in the proposed study. Bidirectional LSTMs are an extension of the LSTM model and have been proposed to improve model performance in classification problems. In the BiLSTM architecture, input values train two LSTMs instead of one. Therefore, information flows both from the past to the future and from the future to the past. In traditional LSTM architectures, information from the future is evaluated and preserved, while in BLSTM architecture, information from both the past and the future is preserved and valued. Owing to this advantage, BiLSTM is more successful than LSTM {$[$}23 {$]$}. Thus, BiLSTM was considered in the study. The graphical abstract of the study is given in Fig. 2. Fig. 2Open in figure viewerPowerPoint Flow chart of the study Application results In this study, the EEG signals of the GAMEEMO data set were classified and positive--negative emotions were predicted. BiLSTM was used for the classification process and the performance of the deep-learning model was measured with accuracy, sensitivity, specificity and receiver operating characteristic (ROC) values. The parameters of the developed BiLSTM model can be summarised as follows: EEG data, whose spectral entropy values were calculated, were used in the input layer. Then the 128-unit BiLSTM layer was designed. ReLU function was used as an activation function. Then, the data were transformed into a one-dimensional vector by the flattening process. Later, the batch normalisation was performed and the data were normalised. Dropout was used to prevent overfitting problem and its degree was set to 0.25. Finally, a fully connected layer was designed and the number of neurons was determined as 512. In the classification layer, the sigmoid function has been used and the binary classification process has been made. Stochastic gradient descent was applied as an optimiser with default values. The loss of the model was calculated by binary cross-entropy. The epoch value was chosen to be 250. To validate the model, the train-test split approach was used and 80{\%} of the data was used for training and 20{\%} for testing. All of these parameters were determined by trial and error approach and the parameters giving the best result were used in the study. Table 1 shows the classification results of the BiLSTM model. Table 1. Classification results of positive and negative emotions Accuracy, {\%} Sensitivity, {\%} Specificity, {\%} ROC 76.91 76.93 76.89 0.90 As seen in Table 1, positive and negative emotions were classified with an accuracy rate of 0.7691 with the proposed BiLSTM model. In addition, the sensitivity value was 0.7693 and the specificity value was 0.7689. ROC value was measured as 0.90 for both classes. The graph of the ROC is given in Fig. 3. The area under the curve (AUC) score is used effectively in biomedical studies and is expressed as a better analysis {$[$}24 {$]$}. In order for the classification process to be considered good, the AUC score must be >0.8 {$[$}24 {$]$}. Furthermore, the AUC score between 0.9 and 1.0 indicates that the classification is excellent {$[$}24 {$]$}. In this study, the AUC score was calculated as 0.9, indicating that the proposed method is effective and successful. Fig. 3Open in figure viewerPowerPoint ROC curve of positive and negative emotions (class 0 refers to negative emotions, class 1 refers to positive emotions) These results were also compared with the machine-learning algorithm results used in the original article. The comparison results are given in Table 2. Since there is only one study in the literature with this data set, only the results in the original article could be examined. According to the results given in Table 2, it is seen that the proposed deep-learning method was better than the machine-learning algorithms used in the original study. While 73 and 66{\%} accuracy values were achieved for SVM and KNN, respectively, this rate increased to ∼77{\%} with the BiLSTM model. According to these results, it can be inferred that the deep-learning method is at least as successful and even better as existing machine-learning methods. Table 2. Comparison of classification results Reference SVM (accuracy) KNN (accuracy) BiLSTM (accuracy) {$[$}14 {$]$} 73{\%} 66{\%} ---the proposed method ------76.93{\%} Conclusion In this study, positive and negative emotions were analysed using the EEG data of the GAMEEMO data set. In the first part of the study, pre-processed data were obtained from the data set. Then, spectral entropy values were collected from the data of each EEG channel and these values were used in the BiLSTM model. In the final phase, the classification process was made with BiLSTM and the performance of the deep-learning model was measured with accuracy, sensitivity, specificity and ROC values. With the proposed method, 76.91{\%} accuracy, 76.93{\%} sensitivity, 76.89{\%} specificity and 90{\%} ROC values were achieved. In addition, the proposed method was compared with the machine-learning algorithms used in the original article and it was observed that the proposed method was at least as successful as them. In the future, this data set will be examined in more detail and comparisons will be made using different deep-learning algorithms and signal processing methods. Emotions will be examined with both binary-class classification and multi-class classification. Emotions are of great importance in human life. In daily life, we use our emotions intentionally or unintentionally. Therefore, emotion analysis studies are important for understanding emotions and determining their behaviour. References 1Naji M. Firoozabadi M. Azadfallah P.: 'Emotion classification during music listening from forehead biosignals ', Signal. Image. Video. Process., 2015, 9, pp. 1365--1375, doi: 10.1007/s11760-013-0591-6 2Alakus T.B. Turkoglu I.: 'EEG based emotion analysis systems ', TBV J. Comput. Sci. Eng., 2018, 11, (1 ), pp. 26--39 3Michalopoulos K. Bourbakis N.: 'Application of multiscale entropy on EEG signals for emotion detection '. IEEE EMBS Int. Conf. Information Technology Applications in Biomedicine, Orlando, FL, USA, February 2017, pp. 341--344, doi: 10.1109/BHI.2017.7897275 4Alakus T.B. Turkoglu I.: 'Feature selection with sequential forward selection algorithm from emotion estimation based on EEG signals ', Sakarya Univ. J. Sci., 2019, 23, (6 ), pp. 1096--1105, doi: 10.16984/saufenbilder.501799 5Turnip A. Simbolon A.I. Amri M.F. et al.: 'Backpropagation neural networks training for EEG-SSVEP classification of emotion recognition ', Internetwork. Indonesia J., 2017, 9, (1 ), pp. 53--57 6Alakus T.B. Turkoglu I.: 'Determination of effective EEG channels for discrimination of positive and negative emotions with wavelet decomposition and support vector machines ', Int. J. Inform. Technol., 2019, 12, (3 ), pp. 229--237 7Yan J. Chen S. Deng S.: 'A EEG-based emotion recognition model with rhythm and time characteristics ', Brain. Inform., 2019, 6, (1 ), pp. 1--8, doi: 10.1186/s40708-019-0100-y 8Pan J. Li Y. Wang J.: 'An EEG-based brain-computer interface for emotion recognition '. Int. Joint Conf. Neural Networks, Vancouver, BC, Canada, July 2016, pp. 2063--2067, doi: 10.1109/IJCNN.2016.7727453 9Mason W.A. Capitanio J.P.: 'Basic emotions: a reconstruction ', Emot. Rev., 2012, 4, (3 ), pp. 238--244, doi: 10.1177/1754073912439763 10Russel A.: 'Core affect and psychological construction of emotion ', Psychol. Rev., 2003, 110, (1 ), pp. 145--150 11Alhagry S. Fahmy A.A. El-Khoribi R.A.: 'Emotion recognition based on EEG using LSTM recurrent neural network ', Int. J. Adv. Comput. Sci. Appl., 2017, 8, (10 ), pp. 355--358, doi: 10.14569/IJACSA.2017.081046 12Pandey P. Seeja K.R.: 'Subject independent emotion recognition from EEG using VMD and deep learning ', J. King Saud Univ. --Comput. Inform. Sci., 2019, 2019, pp. 53--58, doi: 10.1016/j.jksuci.2019.11.003 13Jirayucharoensak S. Pan-Ngum S. Israsena P.: 'EEG-based emotion recognition using deep learning network with principal component based covariate shift adaptation ', Sci. World J., 2014, 2014, pp. 1--10, 627892, doi: 10.1155/2014/627892 14Alakus T.B. Gonen M. Turkoglu I.: 'Database for an emotion recognition system based on EEG signals and various computer games --GAMEEMO ', Biomed. Signal Proc. Control, 2020, 60, pp. 1--12, doi: 10.1016/j.bspc.2020.101951 15Vanluchene A.L.G. Vereecke H. Thas O. et al.: 'Spectral entropy as an electroencephalographic measure of anesthetic drug effect: a comparison with bispectral index and processed midlatency auditory evoked response ', Anesthesiology, 2004, 101, (1 ), pp. 34--42 16Pan Y.N. Chen J. Li X.L.: 'Spectral entropy: a complementary index for rolling element bearing performance degradation assessment ', Proc. Inst. Mech. Eng. C, J. Mech. Eng. Sci., 2009, 223, (5 ), pp. 1223--1231, doi: 10.1243/09544062JMES1224 17Sharma V. Parey A.: 'A review of gear fault diagnosis using various condition indicators ', Procedia Eng., 2016, 144, pp. 256--263, doi: 10.1016/j.proeng.2016.05.131 18Majstorovic N. Andric M. Mikluc D.: 'Entropy-based algorithm for speech recognition in noisy environment '. Telecommunications Forum, Belgrade, Serbia, November 2011, pp. 667--670, doi: 10.1109/TELFOR.2011.6143635 19Vakkuri A. Yli-Hankala A. Talja P. et al.: 'Time-frequency balanced spectral entropy as a measure of anesthetic drug effect in central nervous system during sevoflurane, propofol, and thiopental anesthesia ', Acta Anaesthesiol. Scand., 2004, 48, (2 ), pp. 145--153, doi: 10.1111/j.0001-5172.2004.00323.x 20Hewamalage H. Bergmeir C. Bandara K.: 'Recurrent neural networks for time series forecasting: current status and future directions ', Int. J. Forecast., 2020, 2020, pp. 1--40, doi: 10.1016/j.ijforecast.2020.06.008 21Guo T. Xu Z. Yao X. et al.: 'Robust online time series prediction with recurrent neural networks '. Int. Conf. Data Science and Advanced Analytics, Montreal, QC, Canada, October 2016, pp. 816--825, doi: 10.1109/DSAA.2016.92 22Connor J.T. Martin R.D. Atlas L.E.: 'Recurrent neural networks and robust time series prediction ', IEEE Trans. Neural Netw., 1994, 5, (2 ), pp. 240--254, doi: 10.1109/72.279188 23Graves A. Fernandez S. Schmidhuber J.: 'Bidirectional LSTM networks for improved phoneme classification and recognition '. Int. Conf. Artificial Neural Networks: Formal Models and Their Applications, Warsaw, Poland, September 2005, doi: 10.1007/11550907{\_}126 24Mandrekar J.N.: 'Receiver operating characteristic curve in diagnostic test assessment ', J. Thorac. Oncol., 2010, 5, (9 ), pp. 1315--1316, doi: 10.1097/JTO.0b013e3181ec173d Citing Literature Volume56, Issue25December 2020Pages 1364-1367 FiguresReferencesRelatedInformation},
	author = {Alaku{\c s}, Talha and T{\"u}rko{\u g}lu, İ̇brahim},
	c1 = {Kirklareli UniversityFaculty of EngineeringDepartment of Software EngineeringKirklareliTurkey; Firat UniversityFaculty of TechnologyDepartment of Software EngineeringElazigTurkey},
	date = {2020-10-22},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1049/el.2020.2460},
	isbn = {0013-5194},
	journal = {Electronics letters},
	keywords = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Speech Emotion},
	la = {en},
	number = {25},
	pages = {1364--1367},
	publisher = {Institution of Engineering and Technology},
	title = {Emotion recognition with deep learning using GAMEEMO data set},
	url = {https://doi.org/10.1049/el.2020.2460},
	volume = {56},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1049/el.2020.2460}}

@article{katsigiannisramzan_2018,
	abstract = {In this paper, we present DREAMER, a multimodal database consisting of electroencephalogram (EEG) and electrocardiogram (ECG) signals recorded during affect elicitation by means of audio-visual stimuli. Signals from 23 participants were recorded along with the participants self-assessment of their affective state after each stimuli, in terms of valence, arousal, and dominance. All the signals were captured using portable, wearable, wireless, low-cost, and off-the-shelf equipment that has the potential to allow the use of affective computing methods in everyday applications. A baseline for participant-wise affect recognition using EEG and ECG-based features, as well as their fusion, was established through supervised classification experiments using support vector machines (SVMs). The self-assessment of the participants was evaluated through comparison with the self-assessments from another study using the same audio-visual stimuli. Classification results for valence, arousal, and dominance of the proposed database are comparable to the ones achieved for other databases that use nonportable, expensive, medical grade devices. These results indicate the prospects of using low-cost devices for affect recognition applications. The proposed database will be made publicly available in order to allow researchers to achieve a more thorough evaluation of the suitability of these capturing devices for affect recognition applications.},
	author = {Katsigiannis, Stamos and Ramzan, Naeem},
	c1 = {School of Engineering and Computing, University of the West of Scotland, Paisley, U.K.; School of Engineering and Computing, University of the West of Scotland, Paisley, U.K.},
	date = {2018-01-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/jbhi.2017.2688239},
	isbn = {2168-2194},
	journal = {IEEE journal of biomedical and health informatics},
	keywords = {Emotion Recognition; Affective Computing; Deep Learning for EEG; EEG Analysis; Emotion Regulation},
	la = {en},
	number = {1},
	pages = {98--107},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {DREAMER: A Database for Emotion Recognition Through EEG and ECG Signals From Wireless Low-cost Off-the-Shelf Devices},
	url = {https://doi.org/10.1109/jbhi.2017.2688239},
	volume = {22},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/jbhi.2017.2688239}}

@article{trigeorgisringeval_2016,
	abstract = {The automatic recognition of spontaneous emotions from speech is a challenging task. On the one hand, acoustic features need to be robust enough to capture the emotional content for various styles of speaking, and while on the other, machine learning algorithms need to be insensitive to outliers while being able to model the context. Whereas the latter has been tackled by the use of Long Short-Term Memory (LSTM) networks, the former is still under very active investigations, even though more than a decade of research has provided a large set of acoustic descriptors. In this paper, we propose a solution to the problem of `context-aware'emotional relevant feature extraction, by combining Convolutional Neural Networks (CNNs) with LSTM networks, in order to automatically learn the best representation of the speech signal directly from the raw time representation. In this novel work on the so-called end-to-end speech emotion recognition, we show that the use of the proposed topology significantly outperforms the traditional approaches based on signal processing techniques for the prediction of spontaneous and natural emotions on the RECOLA database.},
	author = {Trigeorgis, George and Ringeval, Fabien and Brueckner, Raymond and Marchi, Erik and Nicolaou, Mihalis and Schuller, Bj{\"o}rn and Zafeiriou, Stefanos},
	c1 = {Department of Computing, Imperial College London, London, UK; Universitat Passau, Passau, Bayern, DE; MMK, Technische Universitat Munchen, Munich, Germany; Nuance Communications Deutschland GmbH, Germany; MMK, Technische Universitat Munchen, Munich, Germany; Department of Computing, Goldsmiths, University of London, UK; AudEERING UG, Germany; Chair of Complex \& Intelligent Systems, University of Passau, Passau, Germany; Department of Computing, Imperial College London, London, UK; Department of Computing, Imperial College London, London, UK},
	date = {2016-03-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1109/icassp.2016.7472669},
	keywords = {Emotion Recognition; Affective Computing; Feature Extraction; Audio-Visual Speech Recognition; Speech Emotion},
	la = {en},
	title = {Adieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network},
	url = {https://doi.org/10.1109/icassp.2016.7472669},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/icassp.2016.7472669}}

@article{lisun_2022,
	abstract = {Recognizing the emotional states of humans through EEG signals are of great significance to the progress of human-computer interaction. The present study aimed to perform automatic recognition of music-evoked emotions through region-specific information and dynamic functional connectivity of EEG signals and a deep learning neural network. EEG signals of 15 healthy volunteers were collected when different emotions (high-valence-arousal vs. low-valence-arousal) were induced by a musical experimental paradigm. Then a sequential backward selection algorithm combining with deep neural network called Xception was proposed to evaluate the effect of different channel combinations on emotion recognition. In addition, we also assessed whether dynamic functional network of frontal cortex, constructed through different trial number, may affect the performance of emotion cognition. Results showed that the binary classification accuracy based on all 30 channels was 70.19{\%}, the accuracy based on all channels located in the frontal region was 71.05{\%}, and the accuracy based on the best channel combination in the frontal region was 76.84{\%}. In addition, we found that the classification performance increased as longer temporal functional network of frontal cortex was constructed as input features. In sum, emotions induced by different musical stimuli can be recognized by our proposed approach though region-specific EEG signals and time-varying functional network of frontal cortex. Our findings could provide a new perspective for the development of EEG-based emotional recognition systems and advance our understanding of the neural mechanism underlying emotion processing.},
	author = {Li, Jun and Sun, Lechan and Huang, Min and Xu, Yichen and Li, Rihui},
	c1 = {College of Information Engineering, Nanchang Hangkong University, China; College of Information Engineering, Nanchang Hangkong University, China; College of Aviation Service and Music, Nanchang Hangkong University, China; College of Aviation Service and Music, Nanchang Hangkong University, China; College of Aviation Service and Music, Nanchang Hangkong University, China; Department of Psychiatry and Behavioral Sciences, Center for Interdisciplinary Brain Sciences Research, Stanford University, United States},
	date = {2022-05-02},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3389/fnins.2022.884475},
	isbn = {1662-453X},
	journal = {Frontiers in neuroscience},
	keywords = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Speech Emotion},
	la = {en},
	publisher = {Frontiers Media},
	title = {Enhancing Emotion Recognition Using Region-Specific Electroencephalogram Data and Dynamic Functional Connectivity},
	url = {https://doi.org/10.3389/fnins.2022.884475},
	volume = {16},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.3389/fnins.2022.884475}}

@article{zhengzhu_2019,
	abstract = {In this paper, we investigate stable patterns of electroencephalogram (EEG) over time for emotion recognition using a machine learning approach. Up to now, various findings of activated patterns associated with different emotions have been reported. However, their stability over time has not been fully investigated yet. In this paper, we focus on identifying EEG stability in emotion recognition. We systematically evaluate the performance of various popular feature extraction, feature selection, feature smoothing and pattern classification methods with the DEAP dataset and a newly developed dataset called SEED for this study. Discriminative Graph regularized Extreme Learning Machine with differential entropy features achieves the best average accuracies of 69.67 and 91.07 percent on the DEAP and SEED datasets, respectively. The experimental results indicate that stable patterns exhibit consistency across sessions; the lateral temporal areas activate more for positive emotions than negative emotions in beta and gamma bands; the neural patterns of neutral emotions have higher alpha responses at parietal and occipital sites; and for negative emotions, the neural patterns have significant higher delta responses at parietal and occipital sites and higher gamma responses at prefrontal sites. The performance of our emotion recognition models shows that the neural patterns are relatively stable within and between sessions.},
	author = {Zheng, Wei‐Long and Zhu, Jie and Lu, Bao‐Liang},
	c1 = {Department of Computer Science and Engineering, Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China},
	date = {2019-07-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1109/taffc.2017.2712143},
	isbn = {1949-3045},
	journal = {IEEE transactions on affective computing},
	keywords = {Deep Learning for EEG; Emotion Recognition; EEG Analysis; Epilepsy Detection; Affective Computing},
	la = {en},
	number = {3},
	pages = {417--429},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {Identifying Stable Patterns over Time for Emotion Recognition from EEG},
	url = {https://doi.org/10.1109/taffc.2017.2712143},
	volume = {10},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/taffc.2017.2712143}}

@article{jirayucharoensakpan-ngum_2014,
	abstract = {Automatic emotion recognition is one of the most challenging tasks. To detect emotion from nonstationary EEG signals, a sophisticated learning algorithm that can represent high-level abstraction is required. This study proposes the utilization of a deep learning network (DLN) to discover unknown feature correlation between input signals that is crucial for the learning task. The DLN is implemented with a stacked autoencoder (SAE) using hierarchical feature learning approach. Input features of the network are power spectral densities of 32-channel EEG signals from 32 subjects. To alleviate overfitting problem, principal component analysis (PCA) is applied to extract the most important components of initial input features. Furthermore, covariate shift adaptation of the principal components is implemented to minimize the nonstationary effect of EEG signals. Experimental results show that the DLN is capable of classifying three different levels of valence and arousal with accuracy of 49.52{\%} and 46.03{\%}, respectively. Principal component based covariate shift adaptation enhances the respective classification accuracy by 5.55{\%} and 6.53{\%}. Moreover, DLN provides better performance compared to SVM and naive Bayes classifiers.},
	author = {Jirayucharoensak, Suwicha and Pan-ngum, Setha and Israsena, Pasin},
	c1 = {Department of Computer Engineering, Faculty of Engineering, Chulalongkorn University, Bangkok 10330, Thailand; National Electronics and Computer Technology Center, Thailand Science Park, Khlong Luang, Pathum Thani 12120, Thailand; Department of Computer Engineering, Faculty of Engineering, Chulalongkorn University, Bangkok 10330, Thailand; National Electronics and Computer Technology Center, Thailand Science Park, Khlong Luang, Pathum Thani 12120, Thailand},
	date = {2014-01-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1155/2014/627892},
	isbn = {1537-744X},
	journal = {The scientific world journal/TheScientificWorldjournal},
	keywords = {Deep Learning for EEG; Emotion Recognition; EEG Analysis; Deep Learning; Affective Computing},
	la = {en},
	pages = {1--10},
	publisher = {Hindawi Publishing Corporation},
	title = {EEG-Based Emotion Recognition Using Deep Learning Network with Principal Component Based Covariate Shift Adaptation},
	url = {https://doi.org/10.1155/2014/627892},
	volume = {2014},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1155/2014/627892}}

@article{zhaoadib_2016,
	abstract = {This paper demonstrates a new technology that can infer a person's emotions from RF signals reflected off his body. EQ-Radio transmits an RF signal and analyzes its reflections off a person's body to recognize his emotional state (happy, sad, etc.). The key enabler underlying EQ-Radio is a new algorithm for extracting the individual heartbeats from the wireless signal at an accuracy comparable to on-body ECG monitors. The resulting beats are then used to compute emotion-dependent features which feed a machine-learning emotion classifier. We describe the design and implementation of EQ-Radio, and demonstrate through a user study that its emotion recognition accuracy is on par with state-of-the-art emotion recognition systems that require a person to be hooked to an ECG monitor.},
	author = {Zhao, M. and Adib, Fadel and Katabi, Dina},
	c1 = {Massachusetts Institute of Technology; Massachusetts Institute of Technology; Massachusetts Institute of Technology},
	date = {2016-10-03},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1145/2973750.2973762},
	keywords = {Emotion Recognition; Deep Learning for EEG; Affective Computing; EEG Analysis; ECG Signal},
	la = {en},
	title = {Emotion recognition using wireless signals},
	url = {https://doi.org/10.1145/2973750.2973762},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1145/2973750.2973762}}

@article{baveyedellandrea_2015,
	abstract = {Research in affective computing requires ground truth data for training and benchmarking computational models for machine-based emotion understanding.In this paper, we propose a large video database, namely LIRIS-ACCEDE, for affective content analysis and related applications, including video indexing, summarization or browsing.In contrast to existing datasets with very few video resources and limited accessibility due to copyright constraints, LIRIS-ACCEDE consists of 9,800 good quality video excerpts with a large content diversity.All excerpts are shared under Creative Commons licenses and can thus be freely distributed without copyright issues.Affective annotations were achieved using crowdsourcing through a pair-wise video comparison protocol, thereby ensuring that annotations are fully consistent, as testified by a high inter-annotator agreement, despite the large diversity of raters' cultural backgrounds.In addition, to enable fair comparison and landmark progresses of future affective computational models, we further provide four experimental protocols and a baseline for prediction of emotions using a large set of both visual and audio features.The dataset (the video clips, annotations, features and protocols) is publicly},
	author = {Baveye, Yoann and Dellandr{\'e}a, Emmanuel and Chamaret, Christel and Chen, Liming},
	c1 = {Extraction de Caract{\'e}ristiques et Identification; Technicolor R \& I {$[$}Cesson S{\'e}vign{\'e}{$]$}; Extraction de Caract{\'e}ristiques et Identification; Technicolor R \& I {$[$}Cesson S{\'e}vign{\'e}{$]$}; Extraction de Caract{\'e}ristiques et Identification},
	date = {2015-01-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/taffc.2015.2396531},
	isbn = {1949-3045},
	journal = {IEEE transactions on affective computing},
	keywords = {Affective Computing; Emotion Recognition; Databases; Video Summarization; Music Information Retrieval},
	la = {en},
	number = {1},
	pages = {43--55},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {LIRIS-ACCEDE: A Video Database for Affective Content Analysis},
	url = {https://doi.org/10.1109/taffc.2015.2396531},
	volume = {6},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/taffc.2015.2396531}}

@article{zhuangzeng_2017,
	abstract = {This paper introduces a method for feature extraction and emotion recognition based on empirical mode decomposition (EMD). By using EMD, EEG signals are decomposed into Intrinsic Mode Functions (IMFs) automatically. Multidimensional information of IMF is utilized as features, the first difference of time series, the first difference of phase, and the normalized energy. The performance of the proposed method is verified on a publicly available emotional database. The results show that the three features are effective for emotion recognition. The role of each IMF is inquired and we find that high frequency component IMF1 has significant effect on different emotional states detection. The informative electrodes based on EMD strategy are analyzed. In addition, the classification accuracy of the proposed method is compared with several classical techniques, including fractal dimension (FD), sample entropy, differential entropy, and discrete wavelet transform (DWT). Experiment results on DEAP datasets demonstrate that our method can improve emotion recognition performance.},
	author = {Zhuang, Ning and Zeng, Ying and Li, Tong and Zhang, Chi and Zhang, Hanming and Yan, Bin},
	c1 = {China National Digital Switching System Engineering and Technological Research Center, Zhengzhou 450002, China; China National Digital Switching System Engineering and Technological Research Center, Zhengzhou 450002, China; Key Laboratory for NeuroInformation of Ministry of Education, School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, China; China National Digital Switching System Engineering and Technological Research Center, Zhengzhou 450002, China; China National Digital Switching System Engineering and Technological Research Center, Zhengzhou 450002, China; China National Digital Switching System Engineering and Technological Research Center, Zhengzhou 450002, China; China National Digital Switching System Engineering and Technological Research Center, Zhengzhou 450002, China},
	date = {2017-01-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1155/2017/8317357},
	isbn = {2314-6133},
	journal = {BioMed research international},
	keywords = {Emotion Recognition; Feature Extraction; Deep Learning for EEG; Affective Computing; Signal Decomposition},
	la = {en},
	pages = {1--9},
	publisher = {Hindawi Publishing Corporation},
	title = {Emotion Recognition from EEG Signals Using Multidimensional Information in EMD Domain},
	url = {https://doi.org/10.1155/2017/8317357},
	volume = {2017},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1155/2017/8317357}}

@article{dzedzickiskaklauskas_2020,
	abstract = {Automated emotion recognition (AEE) is an important issue in various fields of activities which use human emotional reactions as a signal for marketing, technical equipment, or human--robot interaction. This paper analyzes scientific research and technical papers for sensor use analysis, among various methods implemented or researched. This paper covers a few classes of sensors, using contactless methods as well as contact and skin-penetrating electrodes for human emotion detection and the measurement of their intensity. The results of the analysis performed in this paper present applicable methods for each type of emotion and their intensity and propose their classification. The classification of emotion sensors is presented to reveal area of application and expected outcomes from each method, as well as their limitations. This paper should be relevant for researchers using human emotion evaluation and analysis, when there is a need to choose a proper method for their purposes or to find alternative decisions. Based on the analyzed human emotion recognition sensors and methods, we developed some practical applications for humanizing the Internet of Things (IoT) and affective computing systems.},
	author = {Dzedzickis, Andrius and Kaklauskas, Art{\=u}ras and Bu{\v c}inskas, Vytautas},
	c1 = {Faculty of Mechanics, Vilnius Gediminas Technical University, J. Basanaviciaus g. 28, LT-03224 Vilnius, Lithuania; Faculty of Civil engineering, Vilnius Gediminas Technical University, Sauletekio ave. 11, LT-10223 Vilnius, Lithuania; Faculty of Mechanics, Vilnius Gediminas Technical University, J. Basanaviciaus g. 28, LT-03224 Vilnius, Lithuania},
	date = {2020-01-21},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/s20030592},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Emotion Recognition; Affective Computing; Head Gesture Recognition; Speech Emotion; Eye Movement Analysis},
	la = {en},
	number = {3},
	pages = {592--592},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Human Emotion Recognition: Review of Sensors and Methods},
	url = {https://doi.org/10.3390/s20030592},
	volume = {20},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.3390/s20030592}}

@article{schuller_2018,
	abstract = {Tracing 20 years of progress in making machines hear our emotions based on speech signal properties.},
	author = {Schuller, Bj{\"o}rn},
	c1 = {University of Augsburg, Germany},
	date = {2018-04-24},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1145/3129340},
	isbn = {0001-0782},
	journal = {Communications of the ACM},
	keywords = {Emotion Recognition; Speech Emotion; Audio-Visual Speech Recognition; Affective Computing; Facial Expression Analysis},
	la = {en},
	number = {5},
	pages = {90--99},
	publisher = {Association for Computing Machinery},
	title = {Speech emotion recognition},
	url = {https://doi.org/10.1145/3129340},
	volume = {61},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1145/3129340}}

@article{muhlallison_2014,
	abstract = {Affective states, moods and emotions, are an integral part of human nature: they shape our thoughts, govern the behavior of the individual, and influence our interpersonal relationships. The last decades have seen a growing interest in the automatic detection of such states from voice, facial expression, and physiological signals, primarily with the goal of enhancing human-computer interaction with an affective component. With the advent of brain-computer interface research, the idea of affective brain-computer interfaces (aBCI), enabling affect detection from brain signals, arose. In this article, we set out to survey the field of neurophysiology-based affect detection. We outline possible applications of aBCI in a general taxonomy of brain-computer interface approaches and introduce the core concepts of affect and their neurophysiological fundamentals. We show that there is a growing body of literature that evidences the capabilities, but also the limitations and challenges of affect detection from neurophysiological activity.},
	author = {M{\"u}hl, Christian and Allison, Brendan and Nijholt, Anton and Chanel, Guillaume},
	c1 = {Inria Bordeaux - Sud-Ouest, Talence, France; ASPEN Lab, Electrical and Computer Engineering Department, Old Dominion University, Norfolk, VA, USA; Department of Cognitive Science, University of California at San Diego, La Jolla, CA, USA; Faculty EEMCS, Human Media Interaction, University of Twente, Enschede, The Netherlands; Swiss Center for Affective Sciences --University of Geneva, Campus Biotech, Gen{\`e}ve, Switzerland},
	date = {2014-04-03},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1080/2326263x.2014.912881},
	isbn = {2326-2621},
	journal = {Brain computer interfaces},
	keywords = {Affective Computing; Brain-Computer Interfaces; Emotion Recognition; Human-Computer Interaction},
	la = {en},
	number = {2},
	pages = {66--84},
	publisher = {Taylor \& Francis},
	title = {A survey of affective brain computer interfaces: principles, state-of-the-art, and challenges},
	url = {https://doi.org/10.1080/2326263x.2014.912881},
	volume = {1},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1080/2326263x.2014.912881}}

@article{miranda-correaabadi_2021,
	abstract = {We present AMIGOS- A dataset for Multimodal research of affect, personality traits and mood on Individuals and GrOupS. Different to other databases, we elicited affect using both short and long videos in two social contexts, one with individual viewers and one with groups of viewers. The database allows the multimodal study of the affective responses, by means of neuro-physiological signals of individuals in relation to their personality and mood, and with respect to the social context and videos' duration. The data is collected in two experimental settings. In the first one, 40 participants watched 16 short emotional videos. In the second one, the participants watched 4 long videos, some of them alone and the rest in groups. The participants' signals, namely, Electroencephalogram (EEG), Electrocardiogram (ECG) and Galvanic Skin Response (GSR), were recorded using wearable sensors. Participants' frontal HD video and both RGB and depth full body videos were also recorded. Participants emotions have been annotated with both self-assessment of affective levels (valence, arousal, control, familiarity, liking and basic emotions) felt during the videos as well as external-assessment of levels of valence and arousal. We present a detailed correlation analysis of the different dimensions as well as baseline methods and results for single-trial classification of valence and arousal, personality traits, mood and social context. The database is made publicly available.},
	author = {Miranda-Correa, Juan and Abadi, Mojtaba and Sebe, Nicu and Patras, Ioannis},
	c1 = {School of Computer Science and Electronic Engineering, Queen Mary University of London, London, United Kingdom; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; School of Computer Science and Electronic Engineering, Queen Mary University of London, London, United Kingdom},
	date = {2021-04-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1109/taffc.2018.2884461},
	isbn = {1949-3045},
	journal = {IEEE transactions on affective computing},
	keywords = {Affective Computing; Emotion Recognition; Personality Data; Emotion Dynamics; Multimodal Data},
	la = {en},
	number = {2},
	pages = {479--493},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {AMIGOS: A Dataset for Affect, Personality and Mood Research on Individuals and Groups},
	url = {https://doi.org/10.1109/taffc.2018.2884461},
	volume = {12},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/taffc.2018.2884461}}

@article{marinmoraleshiguera-trujillo_2018,
	abstract = {Abstract Affective Computing has emerged as an important field of study that aims to develop systems that can automatically recognize emotions. Up to the present, elicitation has been carried out with non-immersive stimuli. This study, on the other hand, aims to develop an emotion recognition system for affective states evoked through Immersive Virtual Environments. Four alternative virtual rooms were designed to elicit four possible arousal-valence combinations, as described in each quadrant of the Circumplex Model of Affects. An experiment involving the recording of the electroencephalography (EEG) and electrocardiography (ECG) of sixty participants was carried out. A set of features was extracted from these signals using various state-of-the-art metrics that quantify brain and cardiovascular linear and nonlinear dynamics, which were input into a Support Vector Machine classifier to predict the subject's arousal and valence perception. The model's accuracy was 75.00{\%} along the arousal dimension and 71.21{\%} along the valence dimension. Our findings validate the use of Immersive Virtual Environments to elicit and automatically recognize different emotional states from neural and cardiac dynamics; this development could have novel applications in fields as diverse as Architecture, Health, Education and Videogames.},
	author = {Mar{\'\i}n‐Morales, Javier and Higuera-Trujillo, Juan and Greco, Alberto and Guixeres, Jaime and Llinares, Carmen and Scilingo, Enzo and Alca{\~n}{\'\i}z, Mariano and Valenza, Gaetano},
	c1 = {Instituto de Investigaci{\'o}n e Innovaci{\'o}n en Bioingenier{\'\i}a, Universitat Polit{\`e}cnica de Val{\`e}ncia, Val{\`e}ncia, Spain; Instituto de Investigaci{\'o}n e Innovaci{\'o}n en Bioingenier{\'\i}a, Universitat Polit{\`e}cnica de Val{\`e}ncia, Val{\`e}ncia, Spain; Bioengineering and Robotics Research Centre E Piaggio \& Department of Information Engineering, University of Pisa, Pisa, Italy; Instituto de Investigaci{\'o}n e Innovaci{\'o}n en Bioingenier{\'\i}a, Universitat Polit{\`e}cnica de Val{\`e}ncia, Val{\`e}ncia, Spain; Instituto de Investigaci{\'o}n e Innovaci{\'o}n en Bioingenier{\'\i}a, Universitat Polit{\`e}cnica de Val{\`e}ncia, Val{\`e}ncia, Spain; Bioengineering and Robotics Research Centre E Piaggio \& Department of Information Engineering, University of Pisa, Pisa, Italy; Instituto de Investigaci{\'o}n e Innovaci{\'o}n en Bioingenier{\'\i}a, Universitat Polit{\`e}cnica de Val{\`e}ncia, Val{\`e}ncia, Spain; Bioengineering and Robotics Research Centre E Piaggio \& Department of Information Engineering, University of Pisa, Pisa, Italy},
	date = {2018-09-12},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1038/s41598-018-32063-4},
	isbn = {2045-2322},
	journal = {Scientific reports},
	keywords = {Affective Computing; Emotion Recognition; Emotion Regulation; Human-Computer Interaction; Speech Emotion},
	la = {en},
	number = {1},
	publisher = {Nature Portfolio},
	title = {Affective computing in virtual reality: emotion recognition from brain and heartbeat dynamics using wearable sensors},
	url = {https://doi.org/10.1038/s41598-018-32063-4},
	volume = {8},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1038/s41598-018-32063-4}}

@article{santamaria-granadosmunoz-organero_2019,
	abstract = {Recommender systems have been based on context and content, and now the technological challenge of making personalized recommendations based on the user emotional state arises through physiological signals that are obtained from devices or sensors. This paper applies the deep learning approach using a deep convolutional neural network on a dataset of physiological signals (electrocardiogram and galvanic skin response), in this case, the AMIGOS dataset. The detection of emotions is done by correlating these physiological signals with the data of arousal and valence of this dataset, to classify the affective state of a person. In addition, an application for emotion recognition based on classic machine learning algorithms is proposed to extract the features of physiological signals in the domain of time, frequency, and non-linear. This application uses a convolutional neural network for the automatic feature extraction of the physiological signals, and through fully connected network layers, the emotion prediction is made. The experimental results on the AMIGOS dataset show that the method proposed in this paper achieves a better precision of the classification of the emotional states, in comparison with the originally obtained by the authors of this dataset.},
	author = {Santamar{\'\i}a-Granados, Luz and Mu{\~n}oz-Organero, Mario and Ram{\'\i}rez-Gonz{\'a}lez, Gustavo and Abdulhay, Enas and Arunkumar, N.},
	c1 = {Faculty of Systems Engineering, Universidad Santo Tom{\'a}s, Tunja, Colombia; Telematics Engineering Department, UC3M-BS Institute of Financial Big Data, Universidad Carlos III de Madrid, Leganes, Spain; Telematics Department, University of Cauca, Popay{\'a}n, Colombia; Department of Biomedical Engineering, Jordan University of Science and Technology, Irbid, Jordan; Department of Electronics and Instrumentation, SASTRA University, Thanjavur, India},
	date = {2019-01-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1109/access.2018.2883213},
	isbn = {2169-3536},
	journal = {IEEE access},
	keywords = {Emotion Recognition; Affective Computing; Deep Learning for EEG; Deep Learning; Affective Design},
	la = {en},
	pages = {57--67},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {Using Deep Convolutional Neural Network for Emotion Detection on a Physiological Signals Dataset (AMIGOS)},
	url = {https://doi.org/10.1109/access.2018.2883213},
	volume = {7},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/access.2018.2883213}}

@article{nardellivalenza_2015,
	abstract = {This paper reports on how emotional states elicited by affective sounds can be effectively recognized by means of estimates of Autonomic Nervous System (ANS) dynamics. Specifically, emotional states are modeled as a combination of arousal and valence dimensions according to the well-known circumplex model of affect, whereas the ANS dynamics is estimated through standard and nonlinear analysis of Heart rate variability (HRV) exclusively, which is derived from the electrocardiogram (ECG). In addition, Lagged Poincar{\'e}Plots of the HRV series were also taken into account. The affective sounds were gathered from the International Affective Digitized Sound System and grouped into four different levels of arousal (intensity) and two levels of valence (unpleasant and pleasant). A group of 27 healthy volunteers were administered with these standardized stimuli while ECG signals were continuously recorded. Then, those HRV features showing significant changes (p {\$}<;{\$} 0.05 from statistical tests) between the arousal and valence dimensions were used as input of an automatic classification system for the recognition of the four classes of arousal and two classes of valence. Experimental results demonstrated that a quadratic discriminant classifier, tested through Leave-One-Subject-Out procedure, was able to achieve a recognition accuracy of 84.72 percent on the valence dimension, and 84.26 percent on the arousal dimension.},
	author = {Nardelli, M. and Valenza, Gaetano and Greco, Alberto and Lanat{\`a}, Antonio and Scilingo, Enzo},
	c1 = {Department Department of Information Engineering \& Research Centre E. Piaggio, University of Pisa, Pisa, Italy; Department Department of Information Engineering \& Research Centre E. Piaggio, University of Pisa, Pisa, Italy; Department Department of Information Engineering \& Research Centre E. Piaggio, University of Pisa, Pisa, Italy; Department Department of Information Engineering \& Research Centre E. Piaggio, University of Pisa, Pisa, Italy; Department Department of Information Engineering \& Research Centre E. Piaggio, University of Pisa, Pisa, Italy},
	date = {2015-10-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1109/taffc.2015.2432810},
	isbn = {1949-3045},
	journal = {IEEE transactions on affective computing},
	keywords = {Affective Computing; Emotion Recognition; Heart Rate Variability; Emotion Regulation; Speech Emotion},
	la = {en},
	number = {4},
	pages = {385--394},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {Recognizing Emotions Induced by Affective Sounds through Heart Rate Variability},
	url = {https://doi.org/10.1109/taffc.2015.2432810},
	volume = {6},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/taffc.2015.2432810}}

@article{haodong_2019,
	abstract = {Emotion recognition based on multi-channel electroencephalograph (EEG) signals is becoming increasingly attractive. However, the conventional methods ignore the spatial characteristics of EEG signals, which also contain salient information related to emotion states. In this paper, a deep learning framework based on a multiband feature matrix (MFM) and a capsule network (CapsNet) is proposed. In the framework, the frequency domain, spatial characteristics, and frequency band characteristics of the multi-channel EEG signals are combined to construct the MFM. Then, the CapsNet model is introduced to recognize emotion states according to the input MFM. Experiments conducted on the dataset for emotion analysis using EEG, physiological, and video signals (DEAP) indicate that the proposed method outperforms most of the common models. The experimental results demonstrate that the three characteristics contained in the MFM were complementary and the capsule network was more suitable for mining and utilizing the three correlation characteristics.},
	author = {Hao, Chao and Dong, Liang and Liu, Yongli and Lu, Bao-Yun},
	c1 = {School of Computer Science and Technology, Henan Polytechnic University, Jiaozuo 454000, China; School of Computer Science and Technology, Henan Polytechnic University, Jiaozuo 454000, China; School of Computer Science and Technology, Henan Polytechnic University, Jiaozuo 454000, China; School of Computer Science and Technology, Henan Polytechnic University, Jiaozuo 454000, China},
	date = {2019-05-13},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/s19092212},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Speech Emotion},
	la = {en},
	number = {9},
	pages = {2212--2212},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Emotion Recognition from Multiband EEG Signals Using CapsNet},
	url = {https://doi.org/10.3390/s19092212},
	volume = {19},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.3390/s19092212}}

@article{nakisarastgoo_2018,
	abstract = {There is currently no standard or widely accepted subset of features to effectively classify different emotions based on electroencephalogram (EEG) signals. While combining all possible EEG features may improve the classification performance, it can lead to high dimensionality and worse performance due to redundancy and inefficiency. To solve the high-dimensionality problem, this paper proposes a new framework to automatically search for the optimal subset of EEG features using evolutionary computation (EC) algorithms. The proposed framework has been extensively evaluated using two public datasets (MAHNOB, DEAP) and a new dataset acquired with a mobile EEG sensor. The results confirm that EC algorithms can effectively support feature selection to identify the best EEG features and the best channels to maximize performance over a four-quadrant emotion classification problem. These findings are significant for informing future development of EEG-based emotion classification because low-cost mobile EEG sensors with fewer electrodes are becoming popular for many new applications.},
	author = {Nakisa, Bahareh and Rastgoo, Mohammad and Tjondronegoro, Dian and Chandran, Vinod},
	c1 = {Science and Engineering Faculty, Queensland University of Technology, Brisbane, Qld, Australia; Science and Engineering Faculty, Queensland University of Technology, Brisbane, Qld, Australia; School of Business and Tourism, Southern Cross University, Gold Coast, Qld, Australia; Science and Engineering Faculty, Queensland University of Technology, Brisbane, Qld, Australia},
	date = {2018-03-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1016/j.eswa.2017.09.062},
	isbn = {0957-4174},
	journal = {Expert systems with applications},
	keywords = {Deep Learning for EEG; Emotion Recognition; EEG Analysis; Affective Computing; Feature Extraction},
	la = {en},
	pages = {143--155},
	publisher = {Elsevier BV},
	title = {Evolutionary computation algorithms for feature selection of EEG-based emotion recognition using mobile sensors},
	url = {https://doi.org/10.1016/j.eswa.2017.09.062},
	volume = {93},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1016/j.eswa.2017.09.062}}

@article{lisong_2018,
	abstract = {Recognizing cross-subject emotions based on brain imaging data, e.g., EEG, has always been difficult due to the poor generalizability of features across subjects. Thus, systematically exploring the ability of different EEG features to identify emotional information across subjects is crucial. Prior related work has explored this question based only on one or two kinds of features, and different findings and conclusions have been presented. In this work, we aim at a more comprehensive investigation on this question with a wider range of feature types, including 18 kinds of linear and nonlinear EEG features. The effectiveness of these features was examined on two publicly accessible datasets, namely, the dataset for emotion analysis using physiological signals (DEAP) and the SJTU emotion EEG dataset (SEED). We adopted the support vector machine (SVM) approach and the `leave-one-subject-out' verification strategy to evaluate recognition performance. Using automatic feature selection methods, the highest mean recognition accuracy of 59.06{$\backslash$}{\%} (AUC{\$}={\$}0.605) on the DEAP dataset and of 83.33{$\backslash$}{\%} (AUC{\$}={\$}0.904) on the SEED dataset were reached. Furthermore, using manually operated feature selection on the SEED dataset, we explored the importance of different EEG features in cross-subject emotion recognition from multiple perspectives, including different channels, brain regions, rhythms and feature types. For example, we found that the Hjorth parameter of mobility in the beta rhythm achieved the best mean recognition accuracy compared to the other features. Through a pilot correlation analysis, we further examined the highly correlated features, for a better understanding of the implications hidden in those features that allow for differentiating cross-subject emotions. Various remarkable observations have been made. The results of this paper validate the possibility of exploring robust EEG features in cross-subject emotion recognition.},
	author = {Li, Xiang and Song, Dawei and Zhang, Peng and Zhang, Yazhou and Hou, Yuexian and Hu, Bin},
	c1 = {Tianjin Key Laboratory of Cognitive Computing and Application, Tianjin University, Tianjin, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Computing and Communications, The Open University, Milton Keynes, United Kingdom; Tianjin Key Laboratory of Cognitive Computing and Application, Tianjin University, Tianjin, China; Tianjin Key Laboratory of Cognitive Computing and Application, Tianjin University, Tianjin, China; Tianjin Key Laboratory of Cognitive Computing and Application, Tianjin University, Tianjin, China; School of Information Science and Engineering, Lanzhou University, Lanzhou, China},
	date = {2018-03-19},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3389/fnins.2018.00162},
	isbn = {1662-453X},
	journal = {Frontiers in neuroscience},
	keywords = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Affective Computing; Epilepsy Detection},
	la = {en},
	publisher = {Frontiers Media},
	title = {Exploring EEG Features in Cross-Subject Emotion Recognition},
	url = {https://doi.org/10.3389/fnins.2018.00162},
	volume = {12},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.3389/fnins.2018.00162}}

@article{xingli_2019,
	abstract = {EEG-based automatic emotion recognition can help brain-inspired robots in improving their interactions with humans. This paper presents a novel framework for emotion recognition using multi-channel electroencephalogram (EEG). The framework consists of a linear EEG mixing model and an emotion timing model. Our proposed framework considerably decomposes the EEG source signals from the collected EEG signals and improves classification accuracy by using the context correlations of the EEG feature sequences. Specially, Stack AutoEncoder (SAE) is used to build and solve the linear EEG mixing model and the emotion timing model is based on the Long Short-Term Memory Recurrent Neural Network (LSTM-RNN). The framework was implemented on the DEAP dataset for an emotion recognition experiment, where the mean accuracy of emotion recognition achieved 81.10{\%} in valence and 74.38{\%} in arousal, and the effectiveness of our framework was verified. Our framework exhibited a better performance in emotion recognition using multi-channel EEG than the compared conventional approaches in the experiments.},
	author = {Xing, Xiaofen and Li, Zhenqi and Xu, Tianyuan and Shu, Lin and Hu, Bin and Xu, Xiangmin},
	c1 = {School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; School of Information Science and Engineering, Lanzhou University, Lanzhou, China; School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China},
	date = {2019-06-12},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3389/fnbot.2019.00037},
	isbn = {1662-5218},
	journal = {Frontiers in neurorobotics},
	keywords = {Deep Learning for EEG; Emotion Recognition; EEG Analysis; Affective Computing; Speech Emotion},
	la = {en},
	publisher = {Frontiers Media},
	title = {SAE+LSTM: A New Framework for Emotion Recognition From Multi-Channel EEG},
	url = {https://doi.org/10.3389/fnbot.2019.00037},
	volume = {13},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.3389/fnbot.2019.00037}}

@article{lansourina_2019,
	abstract = {Affective brain-computer interface (aBCI) introduces personal affective factors to human-computer interaction. The state-of-the-art aBCI tailors its classifier to each individual user to achieve accurate emotion classification. A subject-independent classifier that is trained on pooled data from multiple subjects generally leads to inferior accuracy, due to the fact that electroencephalography patterns vary from subject to subject. Transfer learning or domain adaptation techniques have been leveraged to tackle this problem. Existing studies have reported successful applications of domain adaptation techniques on SEED dataset. However, little is known about the effectiveness of the domain adaptation techniques on other affective datasets or in a cross-dataset application. In this paper, we focus on a comparative study on several state-of-the-art domain adaptation techniques on two datasets: 1) DEAP and 2) SEED. We demonstrate that domain adaptation techniques can improve the classification accuracy on both datasets, but not so effective on DEAP as on SEED. Then, we explore the efficacy of domain adaptation in a cross-dataset setting when the data are collected under different environments using different devices and experimental protocols. Here, we propose to apply domain adaptation to reduce the intersubject variance as well as technical discrepancies between datasets, and then train a subject-independent classifier on one dataset and test on the other. Experiment results show that using domain adaptation technique in a transductive adaptation setting can improve the accuracy significantly by 7.25{\%}-13.40{\%} compared to the baseline accuracy where no domain adaptation technique is used.},
	author = {Lan, Zirui and Sourina, Olga and Wang, Lipo and Scherer, Reinhold and M{\"u}ller-Putz, Gernot},
	c1 = {Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Institute of Neural Engineering, Graz University of Technology, Graz, Austria; Institute of Neural Engineering, Graz University of Technology, Graz, Austria},
	date = {2019-03-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/tcds.2018.2826840},
	isbn = {2379-8920},
	journal = {IEEE transactions on cognitive and developmental systems},
	keywords = {Affective Computing; Emotion Recognition; Brain-Computer Interfaces; Speech Emotion; Human-Computer Interaction},
	la = {en},
	number = {1},
	pages = {85--94},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {Domain Adaptation Techniques for EEG-Based Emotion Recognition: A Comparative Study on Two Public Datasets},
	url = {https://doi.org/10.1109/tcds.2018.2826840},
	volume = {11},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/tcds.2018.2826840}}

@article{dominguez-jimenezcampo-landines_2020,
	abstract = {Emotions are affective states related to physiological responses. This study proposes a model for recognition of three emotions: amusement, sadness, and neutral from physiological signals with the purpose of developing a reliable methodology for emotion recognition using wearable devices. Target emotions were elicited in 37 volunteers using video clips while two biosignals were recorded: photoplethysmography, which provides information about heart rate, and galvanic skin response. These signals were analyzed in frequency and time domains to obtain a set of features. Several feature selection techniques and classifiers were evaluated. The best model was obtained with random forest recursive feature elimination, for feature selection, and a support vector machine for classification. The results show that it is possible to detect amusement, sadness, and neutral emotions using only galvanic skin response features. The system was able to recognize the three target emotions with accuracy up to 100{\%} when evaluated on the test data set.},
	author = {Dom{\'\i}nguez-Jim{\'e}nez, J. A. and Campo-Landines, Kiara and Mart{\'\i}nez-Santos, Juan and Delahoz, E. and Contreras-Ortiz, Sonia},
	c1 = {Universidad Tecnol{\'o}gica de Bol{\'\i}var, Km 1 V{\'\i}a Turbaco, Cartagena de Indias, Colombia; Universidad Tecnol{\'o}gica de Bol{\'\i}var, Km 1 V{\'\i}a Turbaco, Cartagena de Indias, Colombia; Universidad Tecnol{\'o}gica de Bol{\'\i}var, Km 1 V{\'\i}a Turbaco, Cartagena de Indias, Colombia; Universidad Tecnol{\'o}gica de Bol{\'\i}var, Km 1 V{\'\i}a Turbaco, Cartagena de Indias, Colombia; Universidad Tecnol{\'o}gica de Bol{\'\i}var, Km 1 V{\'\i}a Turbaco, Cartagena de Indias, Colombia},
	date = {2020-01-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1016/j.bspc.2019.101646},
	isbn = {1746-8094},
	journal = {Biomedical signal processing and control},
	keywords = {Emotion Recognition; Affective Computing; Emotion Regulation; Physiological Signals; Speech Emotion},
	la = {en},
	pages = {101646--101646},
	publisher = {Elsevier BV},
	title = {A machine learning model for emotion recognition from physiological signals},
	url = {https://doi.org/10.1016/j.bspc.2019.101646},
	volume = {55},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1016/j.bspc.2019.101646}}

@article{cimtayekmekcioglu_2020,
	abstract = {The electroencephalogram (EEG) has great attraction in emotion recognition studies due to its resistance to deceptive actions of humans. This is one of the most significant advantages of brain signals in comparison to visual or speech signals in the emotion recognition context. A major challenge in EEG-based emotion recognition is that EEG recordings exhibit varying distributions for different people as well as for the same person at different time instances. This nonstationary nature of EEG limits the accuracy of it when subject independency is the priority. The aim of this study is to increase the subject-independent recognition accuracy by exploiting pretrained state-of-the-art Convolutional Neural Network (CNN) architectures. Unlike similar studies that extract spectral band power features from the EEG readings, raw EEG data is used in our study after applying windowing, pre-adjustments and normalization. Removing manual feature extraction from the training system overcomes the risk of eliminating hidden features in the raw data and helps leverage the deep neural network's power in uncovering unknown features. To improve the classification accuracy further, a median filter is used to eliminate the false detections along a prediction interval of emotions. This method yields a mean cross-subject accuracy of 86.56{\%} and 78.34{\%} on the Shanghai Jiao Tong University Emotion EEG Dataset (SEED) for two and three emotion classes, respectively. It also yields a mean cross-subject accuracy of 72.81{\%} on the Database for Emotion Analysis using Physiological Signals (DEAP) and 81.8{\%} on the Loughborough University Multimodal Emotion Dataset (LUMED) for two emotion classes. Furthermore, the recognition model that has been trained using the SEED dataset was tested with the DEAP dataset, which yields a mean prediction accuracy of 58.1{\%} across all subjects and emotion classes. Results show that in terms of classification accuracy, the proposed approach is superior to, or on par with, the reference subject-independent EEG emotion recognition studies identified in literature and has limited complexity due to the elimination of the need for feature extraction.},
	author = {{\c C}imtay, Y{\"u}cel and Ekmekcio{\v g}lu, Erhan},
	c1 = {Institute for Digital Technologies, Loughborough University London, London E20 3BS, UK;; Institute for Digital Technologies, Loughborough University London, London E20 3BS, UK;},
	date = {2020-04-04},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/s20072034},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; ECG Signal},
	la = {en},
	number = {7},
	pages = {2034--2034},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Investigating the Use of Pretrained Convolutional Neural Network on Cross-Subject and Cross-Dataset EEG Emotion Recognition},
	url = {https://doi.org/10.3390/s20072034},
	volume = {20},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.3390/s20072034}}

@article{kwonshin_2018,
	abstract = {The purpose of this study is to improve human emotional classification accuracy using a convolution neural networks (CNN) model and to suggest an overall method to classify emotion based on multimodal data. We improved classification performance by combining electroencephalogram (EEG) and galvanic skin response (GSR) signals. GSR signals are preprocessed using by the zero-crossing rate. Sufficient EEG feature extraction can be obtained through CNN. Therefore, we propose a suitable CNN model for feature extraction by tuning hyper parameters in convolution filters. The EEG signal is preprocessed prior to convolution by a wavelet transform while considering time and frequency simultaneously. We use a database for emotion analysis using the physiological signals open dataset to verify the proposed process, achieving 73.4{\%} accuracy, showing significant performance improvement over the current best practice models.},
	author = {Kwon, Yea and Shin, Sae and Kim, Shin},
	c1 = {Department of Computer Science, Yonsei University, 50 Yonsei-ro, Seodaemun-gu, Seoul 03722, Korea;; Department of Computer Science, Yonsei University, 50 Yonsei-ro, Seodaemun-gu, Seoul 03722, Korea;; Department of Computer Science, Yonsei University, 50 Yonsei-ro, Seodaemun-gu, Seoul 03722, Korea;},
	date = {2018-04-30},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/s18051383},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Emotion Recognition; Deep Learning for EEG; Affective Computing; EEG Analysis; Head Gesture Recognition},
	la = {en},
	number = {5},
	pages = {1383--1383},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Electroencephalography Based Fusion Two-Dimensional (2D)-Convolution Neural Networks (CNN) Model for Emotion Recognition System},
	url = {https://doi.org/10.3390/s18051383},
	volume = {18},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.3390/s18051383}}

@article{mixu_2018,
	abstract = {Many studies have been done on the emotion recognition based on multi-channel electroencephalogram (EEG) signals.This paper explores the influence of the emotion recognition accuracy of EEG signals in different frequency bands and different number of channels.We classified the emotional states in the valence and arousal dimensions using different combinations of EEG channels. Firstly, DEAP default preprocessed data were normalized. Next, EEG signals were divided into four frequency bands using discrete wavelet transform, and entropy and energy were calculated as features of K-nearest neighbor Classifier.The classification accuracies of the 10, 14, 18 and 32 EEG channels based on the Gamma frequency band were 89.54{\%}, 92.28{\%}, 93.72{\%} and 95.70{\%} in the valence dimension and 89.81{\%}, 92.24{\%}, 93.69{\%} and 95.69{\%} in the arousal dimension. As the number of channels increases, the classification accuracy of emotional states also increases, the classification accuracy of the gamma frequency band is greater than that of the beta frequency band followed by the alpha and theta frequency bands.This paper provided better frequency bands and channels reference for emotion recognition based on EEG.},
	author = {Mi, Li and Xu, Hongpei and Liu, Xingwang and Lu, Shengfu},
	c1 = {Department of Automation, Faculty of Information Technology, Beijing University of Technology, Beijing 100124, China; The Beijing International Collaboration Base on Brain Informatics and Wisdom Services, Beijing 100024, China; Department of Automation, Faculty of Information Technology, Beijing University of Technology, Beijing 100124, China; The Beijing International Collaboration Base on Brain Informatics and Wisdom Services, Beijing 100024, China; Department of Automation, Faculty of Information Technology, Beijing University of Technology, Beijing 100124, China; The Beijing International Collaboration Base on Brain Informatics and Wisdom Services, Beijing 100024, China; Department of Automation, Faculty of Information Technology, Beijing University of Technology, Beijing 100124, China; The Beijing International Collaboration Base on Brain Informatics and Wisdom Services, Beijing 100024, China},
	date = {2018-07-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3233/thc-174836},
	isbn = {0928-7329},
	journal = {Technology and health care},
	keywords = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Epilepsy Detection; Affective Computing},
	la = {en},
	pages = {509--519},
	publisher = {IOS Press},
	title = {Emotion recognition from multichannel EEG signals using K-nearest neighbor classification},
	url = {https://doi.org/10.3233/thc-174836},
	volume = {26},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.3233/thc-174836}}

@article{liuwu_2020,
	abstract = {Emotion classification based on brain-computer interface (BCI) systems is an appealing research topic. Recently, deep learning has been employed for the emotion classifications of BCI systems and compared to traditional classification methods improved results have been obtained. In this paper, a novel deep neural network is proposed for emotion classification using EEG systems, which combines the Convolutional Neural Network (CNN), Sparse Autoencoder (SAE), and Deep Neural Network (DNN) together. In the proposed network, the features extracted by the CNN are first sent to SAE for encoding and decoding. Then the data with reduced redundancy are used as the input features of a DNN for classification task. The public datasets of DEAP and SEED are used for testing. Experimental results show that the proposed network is more effective than conventional CNN methods on the emotion recognitions. For the DEAP dataset, the highest recognition accuracies of 89.49{\%} and 92.86{\%} are achieved for valence and arousal, respectively. For the SEED dataset, however, the best recognition accuracy reaches 96.77{\%}. By combining the CNN, SAE, and DNN and training them separately, the proposed network is shown as an efficient method with a faster convergence than the conventional CNN.},
	author = {Liu, Junxiu and Wu, Guopei and Luo, Yuling and Qiu, Senhui and Yang, Su and Li, Wei and Bi, Yifei},
	c1 = {School of Electronic Engineering, Guangxi Normal University, Guilin, China; School of Electronic Engineering, Guangxi Normal University, Guilin, China; School of Electronic Engineering, Guangxi Normal University, Guilin, China; Guangxi Key Laboratory of Wireless Wideband Communication and Signal Processing, Guilin, China; School of Electronic Engineering, Guangxi Normal University, Guilin, China; Department of Computer Science and Software Engineering, Xi'an Jiaotong-Liverpool University, Suzhou, China; Academy for Engineering \& Technology, Fudan University, Shanghai, China; Department of Electronic Engineering, The University of York, York, United Kingdom; College of Foreign Languages, University of Shanghai for Science and Technology, Shanghai, China; Department of Psychology, The University of York, York, United Kingdom},
	date = {2020-09-02},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3389/fnsys.2020.00043},
	isbn = {1662-5137},
	journal = {Frontiers in systems neuroscience},
	keywords = {Emotion Recognition; Deep Learning for EEG; Affective Computing; Deep Learning; Speech Emotion},
	la = {en},
	publisher = {Frontiers Media},
	title = {EEG-Based Emotion Classification Using a Deep Neural Network and Sparse Autoencoder},
	url = {https://doi.org/10.3389/fnsys.2020.00043},
	volume = {14},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.3389/fnsys.2020.00043}}

@article{ozerdempolat_2017,
	abstract = {Emotion plays an important role in human interaction. People can explain their emotions in terms of word, voice intonation, facial expression, and body language. However, brain-computer interface (BCI) systems have not reached the desired level to interpret emotions. Automatic emotion recognition based on BCI systems has been a topic of great research in the last few decades. Electroencephalogram (EEG) signals are one of the most crucial resources for these systems. The main advantage of using EEG signals is that it reflects real emotion and can easily be processed by computer systems. In this study, EEG signals related to positive and negative emotions have been classified with preprocessing of channel selection. Self-Assessment Manikins was used to determine emotional states. We have employed discrete wavelet transform and machine learning techniques such as multilayer perceptron neural network (MLPNN) and k-nearest neighborhood (kNN) algorithm to classify EEG signals. The classifier algorithms were initially used for channel selection. EEG channels for each participant were evaluated separately, and five EEG channels that offered the best classification performance were determined. Thus, final feature vectors were obtained by combining the features of EEG segments belonging to these channels. The final feature vectors with related positive and negative emotions were classified separately using MLPNN and kNN algorithms. The classification performance obtained with both the algorithms are computed and compared. The average overall accuracies were obtained as 77.14 and 72.92{\%} by using MLPNN and kNN, respectively.},
	author = {{\"O}zerdem, Mehmet and Polat, Hasan},
	c1 = {Electrical and Electronics Engineering, Dicle University, 21000, Diyarbakır, Turkey; Electrical and Electronics Engineering, Mus Alparslan University, 49000, Mu{\c s}, Turkey},
	date = {2017-07-15},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1007/s40708-017-0069-3},
	isbn = {2198-4026},
	journal = {Brain informatics},
	keywords = {Emotion Recognition; EEG Analysis; Affective Computing; Deep Learning for EEG; Speech Emotion},
	la = {en},
	number = {4},
	pages = {241--252},
	publisher = {Springer Science+Business Media},
	title = {Emotion recognition based on EEG features in movie clips with channel selection},
	url = {https://doi.org/10.1007/s40708-017-0069-3},
	volume = {4},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1007/s40708-017-0069-3}}

@article{zhangchen_2016,
	abstract = {Electroencephalogram (EEG) signals recorded from sensor electrodes on the scalp can directly detect the brain dynamics in response to different emotional states. Emotion recognition from EEG signals has attracted broad attention, partly due to the rapid development of wearable computing and the needs of a more immersive human-computer interface (HCI) environment. To improve the recognition performance, multi-channel EEG signals are usually used. A large set of EEG sensor channels will add to the computational complexity and cause users inconvenience. ReliefF-based channel selection methods were systematically investigated for EEG-based emotion recognition on a database for emotion analysis using physiological signals (DEAP). Three strategies were employed to select the best channels in classifying four emotional states (joy, fear, sadness and relaxation). Furthermore, support vector machine (SVM) was used as a classifier to validate the performance of the channel selection results. The experimental results showed the effectiveness of our methods and the comparison with the similar strategies, based on the F-score, was given. Strategies to evaluate a channel as a unity gave better performance in channel reduction with an acceptable loss of accuracy. In the third strategy, after adjusting channels' weights according to their contribution to the classification accuracy, the number of channels was reduced to eight with a slight loss of accuracy (58.51{\%} $\pm$10.05{\%} versus the best classification accuracy 59.13{\%} $\pm$11.00{\%} using 19 channels). In addition, the study of selecting subject-independent channels, related to emotion processing, was also implemented. The sensors, selected subject-independently from frontal, parietal lobes, have been identified to provide more discriminative information associated with emotion processing, and are distributed symmetrically over the scalp, which is consistent with the existing literature. The results will make a contribution to the realization of a practical EEG-based emotion recognition system.},
	author = {Zhang, Jianhai and Chen, Ming and Zhao, Shaokai and Hu, Sanqing and Shi, Zhiguo and Cao, Yu},
	c1 = {College of Computer Science, Hangzhou Dianzi University, Hangzhou 310018, China; College of Computer Science, Hangzhou Dianzi University, Hangzhou 310018, China; College of Computer Science, Hangzhou Dianzi University, Hangzhou 310018, China; College of Computer Science, Hangzhou Dianzi University, Hangzhou 310018, China; Department of Information Science and Electronic Engineering, Zhejiang University, Hangzhou 310012, China; Department of Computer Science, The University of Massachusetts Lowell, Lowell, MA 01854, USA},
	date = {2016-09-22},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3390/s16101558},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Epilepsy Detection; Affective Computing},
	la = {en},
	number = {10},
	pages = {1558--1558},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {ReliefF-Based EEG Sensor Selection Methods for Emotion Recognition},
	url = {https://doi.org/10.3390/s16101558},
	volume = {16},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.3390/s16101558}}

@article{yanghan_2019,
	abstract = {We present a multi-column CNN-based model for emotion recognition from EEG signals. Recently, a deep neural network is widely employed for extracting features and recognizing emotions from various biosignals including EEG signals. A decision from a single CNN-based emotion recognizing module shows improved accuracy than the conventional handcrafted feature-based modules. To further improve the accuracy of the CNN-based modules, we devise a multi-column structured model, whose decision is produced by a weighted sum of the decisions from individual recognizing modules. We apply the model to EEG signals from DEAP dataset for comparison and demonstrate the improved accuracy of our model.},
	author = {Yang, Heekyung and Han, Jongdae and Min, Kyungha},
	c1 = {Industry-Academy Cooperation Foundation, Sangmyung University, Seoul 03016, Korea; Department of Computer Science, Sangmyung University, Seoul 03016, Korea; Department of Computer Science, Sangmyung University, Seoul 03016, Korea},
	date = {2019-10-31},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3390/s19214736},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Emotion Recognition; Deep Learning for EEG; Affective Computing; EEG Analysis},
	la = {en},
	number = {21},
	pages = {4736--4736},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {A Multi-Column CNN Model for Emotion Recognition from EEG Signals},
	url = {https://doi.org/10.3390/s19214736},
	volume = {19},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.3390/s19214736}}

@article{lihuang_2017,
	abstract = {The method presented in this study can be applied in many fields, such as mental health care, entertainment consumption behavior, society safety, and so on.For example, in the mental health care field, an automatic emotion analysis system can be constructed with our method to monitor the emotional variation of the subjects.With accurate and objective emotion analysis results from EEG signals, our method can provide useful treatment effect information to the medical staff.},
	author = {Li, Youjun and Huang, Jiajin and Zhou, Hu and Zhong, Ning},
	c1 = {Institute of International WIC, Beijing University of Technology, Beijing 100124, China;; Institute of International WIC, Beijing University of Technology, Beijing 100124, China;; Institute of International WIC, Beijing University of Technology, Beijing 100124, China;; Institute of International WIC, Beijing University of Technology, Beijing 100124, China;; Knowledge Information Systems Lab, Department of Life Science and Informatics, Maebashi Institute of Technology, Maebashi 371-0816, Japan},
	date = {2017-10-13},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/app7101060},
	isbn = {2076-3417},
	journal = {Applied sciences},
	keywords = {Emotion Recognition; Affective Computing; Emotions; Color Psychology; Speech Emotion},
	la = {en},
	number = {10},
	pages = {1060--1060},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Human Emotion Recognition with Electroencephalographic Multidimensional Features by Hybrid Deep Neural Networks},
	url = {https://doi.org/10.3390/app7101060},
	volume = {7},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.3390/app7101060}}

@article{thammasanmoriyama_2016,
	abstract = {Although emotion detection using electroencephalogram (EEG) data has become a highly active area of research over the last decades, little attention has been paid to stimulus familiarity, a crucial subjectivity issue. Using both our experimental data and a sophisticated database (DEAP dataset), we investigated the effects of familiarity on brain activity based on EEG signals. Focusing on familiarity studies, we allowed subjects to select the same number of familiar and unfamiliar songs; both resulting datasets demonstrated the importance of reporting self-emotion based on the assumption that the emotional state when experiencing music is subjective. We found evidence that music familiarity influences both the power spectra of brainwaves and the brain functional connectivity to a certain level. We conducted an additional experiment using music familiarity in an attempt to recognize emotional states; our empirical results suggested that the use of only songs with low familiarity levels can enhance the performance of EEG-based emotion classification systems that adopt fractal dimension or power spectral density features and support vector machine, multilayer perceptron or C4.5 classifier. This suggests that unfamiliar songs are most appropriate for the construction of an emotion recognition system.},
	author = {Thammasan, Nattapong and Moriyama, Koichi and Fukui, Kenichi and Numao, Masayuki},
	c1 = {Institute of Scientific and Industrial Research (ISIR), Osaka University, Ibaraki-shi, Osaka, 567-0047, Japan; Department of Computer Science and Engineering, Nagoya Institute of Technology, Showa-ku, Nagoya, 466-8555, Japan; Institute of Scientific and Industrial Research (ISIR), Osaka University, Ibaraki-shi, Osaka, 567-0047, Japan; Institute of Scientific and Industrial Research (ISIR), Osaka University, Ibaraki-shi, Osaka, 567-0047, Japan},
	date = {2016-04-29},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1007/s40708-016-0051-5},
	isbn = {2198-4026},
	journal = {Brain informatics},
	keywords = {EEG Analysis; Emotion Recognition; Deep Learning for EEG; Affective Computing; Speech Emotion},
	la = {en},
	number = {1},
	pages = {39--50},
	publisher = {Springer Science+Business Media},
	title = {Familiarity effects in EEG-based emotion recognition},
	url = {https://doi.org/10.1007/s40708-016-0051-5},
	volume = {4},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1007/s40708-016-0051-5}}

@article{marinmoralesllinares_2020,
	abstract = {Emotions play a critical role in our daily lives, so the understanding and recognition of emotional responses is crucial for human research. Affective computing research has mostly used non-immersive two-dimensional (2D) images or videos to elicit emotional states. However, immersive virtual reality, which allows researchers to simulate environments in controlled laboratory conditions with high levels of sense of presence and interactivity, is becoming more popular in emotion research. Moreover, its synergy with implicit measurements and machine-learning techniques has the potential to impact transversely in many research areas, opening new opportunities for the scientific community. This paper presents a systematic review of the emotion recognition research undertaken with physiological and behavioural measures using head-mounted displays as elicitation devices. The results highlight the evolution of the field, give a clear perspective using aggregated analysis, reveal the current open issues and provide guidelines for future research.},
	author = {Mar{\'\i}n‐Morales, Javier and Llinares, Carmen and Guixeres, Jaime and Alca{\~n}{\'\i}z, Mariano},
	c1 = {Instituto de Investigaci{\'o}n e Innovaci{\'o}n en Bioingenier{\'\i}a, Universitat Polit{\`e}cnica de Val{\`e}ncia, 46022 Val{\`e}ncia, Spain;; Instituto de Investigaci{\'o}n e Innovaci{\'o}n en Bioingenier{\'\i}a, Universitat Polit{\`e}cnica de Val{\`e}ncia, 46022 Val{\`e}ncia, Spain;; Instituto de Investigaci{\'o}n e Innovaci{\'o}n en Bioingenier{\'\i}a, Universitat Polit{\`e}cnica de Val{\`e}ncia, 46022 Val{\`e}ncia, Spain;; Instituto de Investigaci{\'o}n e Innovaci{\'o}n en Bioingenier{\'\i}a, Universitat Polit{\`e}cnica de Val{\`e}ncia, 46022 Val{\`e}ncia, Spain;},
	date = {2020-09-10},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/s20185163},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Affective Computing; Emotion Recognition; Human-Computer Interaction},
	la = {en},
	number = {18},
	pages = {5163--5163},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Emotion Recognition in Immersive Virtual Reality: From Statistics to Affective Computing},
	url = {https://doi.org/10.3390/s20185163},
	volume = {20},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.3390/s20185163}}

@article{candrayuwono_2015,
	abstract = {When dealing with patients with psychological or emotional symptoms, medical practitioners are often faced with the problem of objectively recognizing their patients' emotional state. In this paper, we approach this problem using a computer program that automatically extracts emotions from EEG signals. We extend the finding of Koelstra et. al {$[$}IEEE trans. affective comput., vol. 3, no. 1, pp. 18-31, 2012{$]$} using the same dataset (i.e. the DEAP: dataset for emotion analysis using electroencephalogram, physiological and video signals), where we observed that the accuracy can be further improved using wavelet features extracted from shorter time segments. More precisely, we achieved accuracy of 65{\%} for both valence and arousal using the wavelet entropy of 3 to 12 seconds signal segments. This improvement in accuracy entails an important discovery that information on emotions contained in the EEG signal may be better described in term of wavelets and in shorter time segments.},
	author = {Candra, Henry and Yuwono, Mitchell and Chai, Rifai and Handojoseno, A. and Elamvazuthi, Irraivan and Nguyen, Hung and Su, Steven},
	c1 = {Faculty of Engineering and Information Technolnov., Universirv of Technologies, Sydney, New South Wales, Australia; Faculty of Engineering and Information Technolnov., Universirv of Technologies, Sydney, New South Wales, Australia; Faculty of Engineering and Information Technolnov., Universirv of Technologies, Sydney, New South Wales, Australia; Faculty of Engineering and Information Technolnov., Universirv of Technologies, Sydney, New South Wales, Australia; Faculty of Engineering and Information Technolnov., Universirv of Technologies, Sydney, New South Wales, Australia; Faculty of Engineering and Information Technolnov., Universirv of Technologies, Sydney, New South Wales, Australia; Dept. of Electrical and Electronic Engineering Universiti Teknologi PETRONAS, Tronoh, Malaysia},
	date = {2015-08-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/embc.2015.7320065},
	keywords = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Affective Computing; Epilepsy Detection},
	la = {en},
	title = {Investigation of window size in classification of EEG-emotion signal with wavelet entropy and support vector machine},
	url = {https://doi.org/10.1109/embc.2015.7320065},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/embc.2015.7320065}}

@article{kossaifiwalecki_2021,
	abstract = {Natural human-computer interaction and audio-visual human behaviour sensing systems, which would achieve robust performance in-the-wild are more needed than ever as digital devices are increasingly becoming an indispensable part of our life. Accurately annotated real-world data are the crux in devising such systems. However, existing databases usually consider controlled settings, low demographic variability, and a single task. In this paper, we introduce the SEWA database of more than 2,000 minutes of audio-visual data of 398 people coming from six cultures, 50 percent female, and uniformly spanning the age range of 18 to 65 years old. Subjects were recorded in two different contexts: while watching adverts and while discussing adverts in a video chat. The database includes rich annotations of the recordings in terms of facial landmarks, facial action units (FAU), various vocalisations, mirroring, and continuously valued valence, arousal, liking, agreement, and prototypic examples of (dis)liking. This database aims to be an extremely valuable resource for researchers in affective computing and automatic human sensing and is expected to push forward the research in human behaviour analysis, including cultural studies. Along with the database, we provide extensive baseline experiments for automatic FAU detection and automatic valence, arousal, and (dis)liking intensity estimation.},
	author = {Kossaifi, Jean and Walecki, Robert and Panagakis, Yannis and Shen, Jie and Schmitt, Maximilian and Ringeval, Fabien and Han, Jing and Pandit, Vedhas and Toisoul, Antoine and Schuller, Bj{\"o}rn and Star, Kam and Hajiyev, Elnar and Panti{\'c}, Maja},
	c1 = {Department of Computing, Imperial College London, London, United Kingdom; Department of Computing, Imperial College London, London, United Kingdom; Department of Computing, Imperial College London, London, United Kingdom; Department of Computing, Imperial College London, London, United Kingdom; Chair Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Augsburg, Bavaria, Germany; Universit{\'e}Grenoble Alpes, France; Chair Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Augsburg, Bavaria, Germany; Chair Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Augsburg, Bavaria, Germany; Department of Computing, Imperial College London, London, United Kingdom; Department of Computing, Imperial College London, London, United Kingdom; Playgen, London, United Kingdom; Real eyes, London, United Kingdom; Department of Computing, Imperial College London, London, United Kingdom},
	date = {2021-03-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/tpami.2019.2944808},
	isbn = {0162-8828},
	journal = {IEEE transactions on pattern analysis and machine intelligence},
	keywords = {Affective Computing; Databases; Audiovisual Interaction; Emotion Recognition; Aesthetic Intelligence},
	la = {en},
	number = {3},
	pages = {1022--1040},
	publisher = {IEEE Computer Society},
	title = {SEWA DB: A Rich Database for Audio-Visual Emotion and Sentiment Research in the Wild},
	url = {https://doi.org/10.1109/tpami.2019.2944808},
	volume = {43},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/tpami.2019.2944808}}

@article{cowensauter_2019,
	abstract = {What would a comprehensive atlas of human emotions include? For 50 years, scientists have sought to map emotion-related experience, expression, physiology, and recognition in terms of the "basic six"-anger, disgust, fear, happiness, sadness, and surprise. Claims about the relationships between these six emotions and prototypical facial configurations have provided the basis for a long-standing debate over the diagnostic value of expression (for review and latest installment in this debate, see Barrett et al., p. 1). Building on recent empirical findings and methodologies, we offer an alternative conceptual and methodological approach that reveals a richer taxonomy of emotion. Dozens of distinct varieties of emotion are reliably distinguished by language, evoked in distinct circumstances, and perceived in distinct expressions of the face, body, and voice. Traditional models-both the basic six and affective-circumplex model (valence and arousal)-capture a fraction of the systematic variability in emotional response. In contrast, emotion-related responses (e.g., the smile of embarrassment, triumphant postures, sympathetic vocalizations, blends of distinct expressions) can be explained by richer models of emotion. Given these developments, we discuss why tests of a basic-six model of emotion are not tests of the diagnostic value of facial expression more generally. Determining the full extent of what facial expressions can tell us, marginally and in conjunction with other behavioral and contextual cues, will require mapping the high-dimensional, continuous space of facial, bodily, and vocal signals onto richly multifaceted experiences using large-scale statistical modeling and machine-learning methods.},
	author = {Cowen, Alan and Sauter, Disa and Tracy, Jessica and Keltner, Dacher},
	c1 = {Department of Psychology, University of California, Berkeley; Faculty of Social and Behavioural Sciences, University of Amsterdam; Department of Psychology, University of British Columbia; Department of Psychology, University of California, Berkeley},
	date = {2019-07-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1177/1529100619850176},
	isbn = {1529-1006},
	journal = {Psychological science in the public interest},
	keywords = {Emotional Expressions; Emotion Recognition; Affective Computing; Emotion Regulation; Facial Expression},
	la = {en},
	number = {1},
	pages = {69--90},
	publisher = {SAGE Publishing},
	title = {Mapping the Passions: Toward a High-Dimensional Taxonomy of Emotional Experience and Expression},
	url = {https://doi.org/10.1177/1529100619850176},
	volume = {20},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1177/1529100619850176}}

@article{grecovalenza_2017,
	abstract = {Physiological sensors and interfaces for mental healthcare are becoming of great interest in research and commercial fields. Specifically, biomedical sensors and related ad hoc signal processing methods can be profitably used for supporting objective, psychological assessments. However, a simple system able to automatically classify the emotional state of a healthy subject is still missing. To overcome this important limitation, we here propose the use of convex optimization-based electrodermal activity (EDA) framework and clustering algorithms to automatically discern arousal and valence levels induced by affective sound stimuli. EDA recordings were gathered from 25 healthy volunteers, using only one EDA sensor to be placed on fingers. Standardized stimuli were chosen from the International Affective Digitized Sound System database, and grouped into four different levels of arousal (i.e., the levels of emotional intensity) and two levels of valence (i.e., how unpleasant/pleasant a sound can be perceived). Experimental results demonstrated that our system is able to achieve a recognition accuracy of 77.33{\%} on the arousal dimension, and 84{\%} on the valence dimension.},
	author = {Greco, Alberto and Valenza, Gaetano and Citi, Luca and Scilingo, Enzo},
	c1 = {Research Center E.Piaggio, Department of Information Engineering, School of Engineering, University of Pisa, Pisa, Italy; Research Center E.Piaggio, Department of Information Engineering, School of Engineering, University of Pisa, Pisa, Italy; {$[$}School of Computer Science and Electronic Engineering, University of Essex, Colchester, U.K.{$]$}; Research Center E.Piaggio, Department of Information Engineering, School of Engineering, University of Pisa, Pisa, Italy},
	date = {2017-02-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/jsen.2016.2623677},
	isbn = {1530-437X},
	journal = {IEEE sensors journal},
	keywords = {Emotion Recognition; Affective Computing; Audio Event Detection; EEG Analysis; Environmental Sound Recognition},
	la = {en},
	number = {3},
	pages = {716--725},
	publisher = {IEEE Sensors Council},
	title = {Arousal and Valence Recognition of Affective Sounds Based on Electrodermal Activity},
	url = {https://doi.org/10.1109/jsen.2016.2623677},
	volume = {17},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/jsen.2016.2623677}}

@article{abdelwahabbusso_2018,
	abstract = {The performance of speech emotion recognition is affected by the differences in data distributions between train (source domain) and test (target domain) sets used to build and evaluate the models. This is a common problem, as multiple studies have shown that the performance of emotional classifiers drop when they are exposed to data that does not match the distribution used to build the emotion classifiers. The difference in data distributions becomes very clear when the training and testing data come from different domains, causing a large performance gap between validation and testing performance. Due to the high cost of annotating new data and the abundance of unlabeled data, it is crucial to extract as much useful information as possible from the available unlabeled data. This study looks into the use of adversarial multitask training to extract a common representation between train and test domains. The primary task is to predict emotional attribute-based descriptors for arousal, valence, or dominance. The secondary task is to learn a common representation where the train and test domains cannot be distinguished. By using a gradient reversal layer, the gradients coming from the domain classifier are used to bring the source and target domain representations closer. We show that exploiting unlabeled data consistently leads to better emotion recognition performance across all emotional dimensions. We visualize the effect of adversarial training on the feature representation across the proposed deep learning architecture. The analysis shows that the data representations for the train and test domains converge as the data is passed to deeper layers of the network. We also evaluate the difference in performance when we use a shallow neural network versus a {$\backslash$}emph{\{}deep neural network{\}} (DNN) and the effect of the number of shared layers used by the task and domain classifiers.},
	author = {Abdelwahab, Mohammed and Busso, Carlos},
	c1 = {Department of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, TX, USA; Department of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, TX, USA},
	date = {2018-12-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:46 +0100},
	doi = {10.1109/taslp.2018.2867099},
	isbn = {2329-9290},
	journal = {IEEE/ACM transactions on audio, speech, and language processing},
	keywords = {Emotion Recognition; Affective Computing; Speech Emotion; Audio Event Detection; Audio-Visual Speech Recognition},
	la = {en},
	number = {12},
	pages = {2423--2435},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {Domain Adversarial for Acoustic Emotion Recognition},
	url = {https://doi.org/10.1109/taslp.2018.2867099},
	volume = {26},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/taslp.2018.2867099}}

@article{qingqiao_2019,
	abstract = {Electroencephalogram (EEG) signal-based emotion recognition has attracted wide interests in recent years and has been broadly adopted in medical, affective computing, and other relevant fields. However, the majority of the research reported in this field tends to focus on the accuracy of classification whilst neglecting the interpretability of emotion progression. In this paper, we propose a new interpretable emotion recognition approach with the activation mechanism by using machine learning and EEG signals. This paper innovatively proposes the emotional activation curve to demonstrate the activation process of emotions. The algorithm first extracts features from EEG signals and classifies emotions using machine learning techniques, in which different parts of a trial are used to train the proposed model and assess its impact on emotion recognition results. Second, novel activation curves of emotions are constructed based on the classification results, and two emotion coefficients, i.e., the correlation coefficients and entropy coefficients. The activation curve can not only classify emotions but also reveals to a certain extent the emotional activation mechanism. Finally, a weight coefficient is obtained from the two coefficients to improve the accuracy of emotion recognition. To validate the proposed method, experiments have been carried out on the DEAP and SEED dataset. The results support the point that emotions are progressively activated throughout the experiment, and the weighting coefficients based on the correlation coefficient and the entropy coefficient can effectively improve the EEG-based emotion recognition accuracy.},
	author = {Qing, Chunmei and Qiao, Rui and Xu, Xiangmin and Cheng, Yongqiang},
	c1 = {School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; Department of Computer Science and Technology, University of Hull, Hull, U.K.},
	date = {2019-01-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1109/access.2019.2928691},
	isbn = {2169-3536},
	journal = {IEEE access},
	keywords = {Emotion Recognition; Affective Computing; Emotion Regulation; Deep Learning for EEG; EEG Analysis},
	la = {en},
	pages = {94160--94170},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {Interpretable Emotion Recognition Using EEG Signals},
	url = {https://doi.org/10.1109/access.2019.2928691},
	volume = {7},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/access.2019.2928691}}

@article{moonjang_2018,
	abstract = {Emotion recognition based on electroencephalography (EEG) has received attention as a way to implement human-centric services. However, there is still much room for improvement, particularly in terms of the recognition accuracy. In this paper, we propose a novel deep learning approach using convolutional neural networks (CNNs) for EEG-based emotion recognition. In particular, we employ brain connectivity features that have not been used with deep learning models in previous studies, which can account for synchronous activations of different brain regions. In addition, we develop a method to effectively capture asymmetric brain activity patterns that are important for emotion recognition. Experimental results confirm the effectiveness of our approach.},
	author = {Moon, Seong-Eun and Jang, Sungil and Lee, Jongseok},
	c1 = {School of Integrated Technology, Yonsei University, Republic of Korea{\#}TAB{\#}; School of Integrated Technology, Yonsei University, Republic of Korea{\#}TAB{\#}; School of Integrated Technology, Yonsei University, Republic of Korea{\#}TAB{\#}},
	date = {2018-04-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1109/icassp.2018.8461315},
	keywords = {Emotion Recognition; Deep Learning for EEG; Affective Computing; EEG Analysis; Sensory Processing},
	la = {en},
	title = {Convolutional Neural Network Approach for Eeg-Based Emotion Recognition Using Brain Connectivity and its Spatial Information},
	url = {https://doi.org/10.1109/icassp.2018.8461315},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/icassp.2018.8461315}}

@article{islamislam_2021,
	abstract = {Emotion recognition using Artificial Intelligence (AI) is a fundamental prerequisite to improve Human-Computer Interaction (HCI). Recognizing emotion from Electroencephalogram (EEG) has been globally accepted in many applications such as intelligent thinking, decision-making, social communication, feeling detection, affective computing, etc. Nevertheless, due to having too low amplitude variation related to time on EEG signal, the proper recognition of emotion from this signal has become too challenging. Usually, considerable effort is required to identify the proper feature or feature set for an effective feature-based emotion recognition system. To extenuate the manual human effort of feature extraction, we proposed a deep machine-learning-based model with Convolutional Neural Network (CNN). At first, the one-dimensional EEG data were converted to Pearson's Correlation Coefficient (PCC) featured images of channel correlation of EEG sub-bands. Then the images were fed into the CNN model to recognize emotion. Two protocols were conducted, namely, protocol-1 to identify two levels and protocol-2 to recognize three levels of valence and arousal that demonstrate emotion. We investigated that only the upper triangular portion of the PCC featured images reduced the computational complexity and size of memory without hampering the model accuracy. The maximum accuracy of 78.22{\%} on valence and 74.92{\%} on arousal were obtained using the internationally authorized DEAP dataset.},
	author = {Islam, Rabiul and Islam, Milon and Rahman, Mustafizur and Mondal, Chayan and Singha, Suvojit and Ahmad, Mohiuddin and Awal, Abdul and Islam, Saiful and Moni, Mohammad},
	c1 = {Electrical and Electronic Engineering, Bangladesh Army University of Engineering \& Technology, Natore, 6431, Bangladesh; Electrical and Electronic Engineering, Khulna University of Engineering \& Technology, Khulna, 9203, Bangladesh; Computer Science and Engineering, Khulna University of Engineering \& Technology, Khulna, 9203, Bangladesh; Electrical and Electronic Engineering, Jashore University of Science and Technology, Jashore, 7408, Bangladesh; Electrical and Electronic Engineering, Khulna University of Engineering \& Technology, Khulna, 9203, Bangladesh; Electrical and Electronic Engineering, Khulna University of Engineering \& Technology, Khulna, 9203, Bangladesh; Electrical and Electronic Engineering, Khulna University of Engineering \& Technology, Khulna, 9203, Bangladesh; Electronics and Communication Engineering, Khulna University, Khulna, 9208, Bangladesh; School of Information and Communication Technology, Griffith University, Gold Coast, Australia; School of Health and Rehabilitation Sciences, The University of Queensland, St Lucia, QLD, 4072, Australia},
	date = {2021-09-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1016/j.compbiomed.2021.104757},
	isbn = {0010-4825},
	journal = {Computers in biology and medicine},
	keywords = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Feature Extraction},
	la = {en},
	pages = {104757--104757},
	publisher = {Elsevier BV},
	title = {EEG Channel Correlation Based Model for Emotion Recognition},
	url = {https://doi.org/10.1016/j.compbiomed.2021.104757},
	volume = {136},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1016/j.compbiomed.2021.104757}}

@article{linyang_2014,
	abstract = {Electroencephalography (EEG)-based emotion classification during music listening has gained increasing attention nowadays due to its promise of potential applications such as musical affective brain-computer interface (ABCI), neuromarketing, music therapy, and implicit multimedia tagging and triggering. However, music is an ecologically valid and complex stimulus that conveys certain emotions to listeners through compositions of musical elements. Using solely EEG signals to distinguish emotions remained challenging. This study aimed to assess the applicability of a multimodal approach by leveraging the EEG dynamics and acoustic characteristics of musical contents for the classification of emotional valence and arousal. To this end, this study adopted machine-learning methods to systematically elucidate the roles of the EEG and music modalities in the emotion modeling. The empirical results suggested that when whole-head EEG signals were available, the inclusion of musical contents did not improve the classification performance. The obtained performance of 74\~{}76{\%} using solely EEG modality was statistically comparable to that using the multimodality approach. However, if EEG dynamics were only available from a small set of electrodes (likely the case in real-life applications), the music modality would play a complementary role and augment the EEG results from around 61-67{\%} in valence classification and from around 58-67{\%} in arousal classification. The musical timber appeared to replace less-discriminative EEG features and led to improvements in both valence and arousal classification, whereas musical loudness was contributed specifically to the arousal classification. The present study not only provided principles for constructing an EEG-based multimodal approach, but also revealed the fundamental insights into the interplay of the brain activity and musical contents in emotion modeling.},
	author = {Lin, Yuan-Pin and Yang, Yi‐Hsuan and Jung, Tzyy-Ping},
	c1 = {Swartz Center for Computational Neuroscience, Institute for Neural Computation, University of California, San Diego, La Jolla, CA, USA; Music and Audio Computing Lab, Research Center for IT Innovation, Academia Sinica, Taipei, Taiwan; Swartz Center for Computational Neuroscience, Institute for Neural Computation, University of California, San Diego, La Jolla, CA, USA},
	date = {2014-05-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3389/fnins.2014.00094},
	isbn = {1662-453X},
	journal = {Frontiers in neuroscience},
	keywords = {EEG Analysis; Emotion Recognition; Deep Learning for EEG; Music Perception; Affective Computing},
	la = {en},
	publisher = {Frontiers Media},
	title = {Fusion of electroencephalographic dynamics and musical contents for estimating emotional responses in music listening},
	url = {https://doi.org/10.3389/fnins.2014.00094},
	volume = {8},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.3389/fnins.2014.00094}}

@article{pihotjahjadi_2020,
	abstract = {Emotion recognition using brain wave signals involves using high dimensional electroencephalogram (EEG) data. In this paper, a window selection method based on mutual information is introduced to select an appropriate signal window to reduce the length of the signals. The motivation of the windowing method comes from EEG emotion recognition being computationally costly and the data having low signal-to-noise ratio. The aim of the windowing method is to find a reduced signal where the emotions are strongest. In this paper, it is suggested, that using only the signal section which best describes emotions improves the classification of emotions. This is achieved by iteratively comparing different-length EEG signals at different time locations using the mutual information between the reduced signal and emotion labels as criterion. The reduced signal with the highest mutual information is used for extracting the features for emotion classification. In addition, a viable framework for emotion recognition is introduced. Experimental results on publicly available datasets, DEAP and MAHNOB-HCI, show significant improvement in emotion recognition accuracy.},
	author = {Piho, Laura and Tjahjadi, Tardi},
	c1 = {School of Engineering, University of Warwick, Coventry, United Kingdom; School of Engineering, University of Warwick, Coventry, United Kingdom},
	date = {2020-10-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1109/taffc.2018.2840973},
	isbn = {1949-3045},
	journal = {IEEE transactions on affective computing},
	keywords = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Speech Emotion},
	la = {en},
	number = {4},
	pages = {722--735},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {A Mutual Information Based Adaptive Windowing of Informative EEG for Emotion Recognition},
	url = {https://doi.org/10.1109/taffc.2018.2840973},
	volume = {11},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/taffc.2018.2840973}}

@article{baveyechamaret_2018,
	abstract = {In our present society, the cinema has become one of the major forms of entertainment providing unlimited contexts of emotion elicitation for the emotional needs of human beings. Since emotions are universal and shape all aspects of our interpersonal and intellectual experience, they have proved to be a highly multidisciplinary research field, ranging from psychology, sociology, neuroscience, etc., to computer science. However, affective multimedia content analysis work from the computer science community benefits but little from the progress achieved in other research fields. In this paper, a multidisciplinary state-of-the-art for affective movie content analysis is given, in order to promote and encourage exchanges between researchers from a very wide range of fields. In contrast to other state-of-the-art papers on affective video content analysis, this work confronts the ideas and models of psychology, sociology, neuroscience, and computer science. The concepts of aesthetic emotions and emotion induction, as well as the different representations of emotions are introduced, based on psychological and sociological theories. Previous global and continuous affective video content analysis work, including video emotion recognition and violence detection, are also presented in order to point out the limitations of affective video content analysis work.},
	author = {Baveye, Yoann and Chamaret, Christel and Dellandr{\'e}a, Emmanuel and Chen, Liming},
	c1 = {Institut de Recherche en Communications et en Cybern{\'e}tique de Nantes; Technicolor R \& I {$[$}Cesson S{\'e}vign{\'e}{$]$}; Extraction de Caract{\'e}ristiques et Identification; Extraction de Caract{\'e}ristiques et Identification},
	date = {2018-10-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/taffc.2017.2661284},
	isbn = {1949-3045},
	journal = {IEEE transactions on affective computing},
	keywords = {Affective Computing; Emotion Recognition},
	la = {en},
	number = {4},
	pages = {396--409},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {Affective Video Content Analysis: A Multidisciplinary Insight},
	url = {https://doi.org/10.1109/taffc.2017.2661284},
	volume = {9},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/taffc.2017.2661284}}

@article{lakhanbanluesombatkul_2019,
	abstract = {For several decades, electroencephalography (EEG) has featured as one of the most commonly used tools in emotional state recognition via monitoring of distinctive brain activities. An array of datasets has been generated with the use of diverse emotion-eliciting stimuli and the resulting brainwave responses conventionally captured with high-end EEG devices. However, the applicability of these devices is to some extent limited by practical constraints and may prove difficult to be deployed in highly mobile context omnipresent in everyday happenings. In this study, we evaluate the potential of OpenBCI to bridge this gap by first comparing its performance to research grade EEG system, employing the same algorithms that were applied on benchmark datasets. Moreover, for the purpose of emotion classification, we propose a novel method to facilitate the selection of audio-visual stimuli of high/low valence and arousal. Our setup entailed recruiting 200 healthy volunteers of varying years of age to identify the top 60 affective video clips from a total of 120 candidates through standardized self assessment, genre tags, and unsupervised machine learning. In addition, 43 participants were enrolled to watch the pre-selected clips during which emotional EEG brainwaves and peripheral physiological signals were collected. These recordings were analyzed and extracted features fed into a classification model to predict whether the elicited signals were associated with a high or low level of valence and arousal. As it turned out, our prediction accuracies were decidedly comparable to those of previous studies that utilized more costly EEG amplifiers for data acquisition.},
	author = {Lakhan, Payongkit and Banluesombatkul, Nannapas and Changniam, Vongsagon and Dhithijaiyratn, Ratwade and Leelaarporn, Pitshaporn and Boonchieng, Ekkarat and Hompoonsup, Supanida and Wilaiprasitporn, Theerawit},
	c1 = {Bio-Inspired Robotics and Neural Engineering Lab, School of Information Science and Technology, Vidyasirimedhi Institute of Science and Technology, Rayong, Thailand; Bio-Inspired Robotics and Neural Engineering Lab, School of Information Science and Technology, Vidyasirimedhi Institute of Science and Technology, Rayong, Thailand; Department of Tool and Materials Engineering, King Mongkut's University of Technology Thonburi, Bangkok, Thailand; Department of Electrical Engineering, Chulalongkorn University, Bangkok, Thailand; Bio-Inspired Robotics and Neural Engineering Lab, School of Information Science and Technology, Vidyasirimedhi Institute of Science and Technology, Rayong, Thailand; Center of Excellence in Community Health Informatics, Chiang Mai University, Chiang Mai, Thailand; Learning Institute, King Mongkut's University of Technology Thonburi, Bangkok, Thailand; Bio-Inspired Robotics and Neural Engineering Lab, School of Information Science and Technology, Vidyasirimedhi Institute of Science and Technology, Rayong, Thailand},
	date = {2019-11-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/jsen.2019.2928781},
	isbn = {1530-437X},
	journal = {IEEE sensors journal},
	keywords = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Affective Computing; Epilepsy Detection},
	la = {en},
	number = {21},
	pages = {9896--9907},
	publisher = {IEEE Sensors Council},
	title = {Consumer Grade Brain Sensing for Emotion Recognition},
	url = {https://doi.org/10.1109/jsen.2019.2928781},
	volume = {19},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/jsen.2019.2928781}}

@article{wangwu_2020,
	abstract = {Electroencephalogram (EEG), as a direct response to brain activity, can be used to detect mental states and physical conditions. Among various EEG-based emotion recognition studies, due to the non-linear, non-stationary and the individual difference of EEG signals, traditional recognition methods still have the disadvantages of complicated feature extraction and low recognition rates. Thus, this paper first proposes a novel concept of electrode-frequency distribution maps (EFDMs) with short-time Fourier transform (STFT). Residual block based deep convolutional neural network (CNN) is proposed for automatic feature extraction and emotion classification with EFDMs. Aim at the shortcomings of the small amount of EEG samples and the challenge of differences in individual emotions, which makes it difficult to construct a universal model, this paper proposes a cross-datasets emotion recognition method of deep model transfer learning. Experiments carried out on two publicly available datasets. The proposed method achieved an average classification score of 90.59{\%} based on a short length of EEG data on SEED, which is 4.51{\%} higher than the baseline method. Then, the pre-trained model was applied to DEAP through deep model transfer learning with a few samples, resulted an average accuracy of 82.84{\%}. Finally, this paper adopts the gradient weighted class activation mapping (Grad-CAM) to get a glimpse of what features the CNN has learned during training from EFDMs and concludes that the high frequency bands are more favorable for emotion recognition.},
	author = {Wang, Fei and Wu, Shichao and Zhang, Weiwei and Xu, Zongfeng and Zhang, Yahui and Wu, Chengdong and Coleman, Sonya},
	c1 = {Faculty of Robot Science and Engineering, Northeastern University, Shenyang, 110169, China; Faculty of Robot Science and Engineering, Northeastern University, Shenyang, 110169, China; Faculty of Robot Science and Engineering, Northeastern University, Shenyang, 110169, China; College of Information Science and Engineering, Northeastern University, Shenyang, 110819, China; College of Information Science and Engineering, Northeastern University, Shenyang, 110819, China; Faculty of Robot Science and Engineering, Northeastern University, Shenyang, 110169, China; Intelligent Systems Research Centre, Ulster University, Londonderry, United Kingdom},
	date = {2020-09-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1016/j.neuropsychologia.2020.107506},
	isbn = {0028-3932},
	journal = {Neuropsychologia},
	keywords = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Epilepsy Detection},
	la = {en},
	pages = {107506--107506},
	publisher = {Elsevier BV},
	title = {Emotion recognition with convolutional neural network and EEG-based EFDMs},
	url = {https://doi.org/10.1016/j.neuropsychologia.2020.107506},
	volume = {146},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1016/j.neuropsychologia.2020.107506}}

@article{rouastadam_2021,
	abstract = {Automatic human affect recognition is a key step towards more natural human-computer interaction. Recent trends include recognition in the wild using a fusion of audiovisual and physiological sensors, a challenging setting for conventional machine learning algorithms. Since 2010, novel deep learning algorithms have been applied increasingly in this field. In this paper, we review the literature on human affect recognition between 2010 and 2017, with a special focus on approaches using deep neural networks. By classifying a total of 950 studies according to their usage of shallow or deep architectures, we are able to show a trend towards deep learning. Reviewing a subset of 233 studies that employ deep neural networks, we comprehensively quantify their applications in this field. We find that deep learning is used for learning of (i) spatial feature representations, (ii) temporal feature representations, and (iii) joint feature representations for multimodal sensor data. Exemplary state-of-the-art architectures illustrate the progress. Our findings show the role deep architectures will play in human affect recognition, and can serve as a reference point for researchers working on related applications.},
	author = {Rouast, Philipp and Adam, Marc and Chiong, Raymond},
	c1 = {School of Electrical Engineering and Computing, University of Newcastle, Callaghan, NSW, Australia; School of Electrical Engineering and Computing, University of Newcastle, Callaghan, NSW, Australia; School of Electrical Engineering and Computing, University of Newcastle, Callaghan, NSW, Australia},
	date = {2021-04-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1109/taffc.2018.2890471},
	isbn = {1949-3045},
	journal = {IEEE transactions on affective computing},
	keywords = {Affective Computing; Emotion Recognition; Deep Learning; Human-Computer Interaction},
	la = {en},
	number = {2},
	pages = {524--543},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {Deep Learning for Human Affect Recognition: Insights and New Developments},
	url = {https://doi.org/10.1109/taffc.2018.2890471},
	volume = {12},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/taffc.2018.2890471}}

@article{al-machotelmachot_2019a,
	abstract = {One of the main objectives of Active and Assisted Living (AAL) environments is to ensure that elderly and/or disabled people perform/live well in their immediate environments; this can be monitored by among others the recognition of emotions based on non-highly intrusive sensors such as Electrodermal Activity (EDA) sensors. However, designing a learning system or building a machine-learning model to recognize human emotions while training the system on a specific group of persons and testing the system on a totally a new group of persons is still a serious challenge in the field, as it is possible that the second testing group of persons may have different emotion patterns. Accordingly, the purpose of this paper is to contribute to the field of human emotion recognition by proposing a Convolutional Neural Network (CNN) architecture which ensures promising robustness-related results for both subject-dependent and subject-independent human emotion recognition. The CNN model has been trained using a grid search technique which is a model hyperparameter optimization technique to fine-tune the parameters of the proposed CNN architecture. The overall concept's performance is validated and stress-tested by using MAHNOB and DEAP datasets. The results demonstrate a promising robustness improvement regarding various evaluation metrics. We could increase the accuracy for subject-independent classification to 78{\%} and 82{\%} for MAHNOB and DEAP respectively and to 81{\%} and 85{\%} subject-dependent classification for MAHNOB and DEAP respectively (4 classes/labels). The work shows clearly that while using solely the non-intrusive EDA sensors a robust classification of human emotion is possible even without involving additional/other physiological signals.},
	author = {Al Machot, Fadi and Elmachot, Ali and Ali, Mouhannad and Al Machot, Elyan and Kyamakya, Kyandoghere},
	c1 = {Research Center Borstel-Leibniz Lung Center, 23845 Borstel, Germany; Faculty of Mechanical and Electrical Engineering, University of Damascus, Damascus, Syria;; Institute for Smart Systems Technologies, Alpen-Adira University, 9020 Klagenfurt, Austria;; Faculty of Medicine, Dresden University of Technology, 01069 Dresden, Germany;; Institute for Smart Systems Technologies, Alpen-Adira University, 9020 Klagenfurt, Austria;},
	date = {2019-04-07},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:06:07 +0100},
	doi = {10.3390/s19071659},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Emotion Recognition; Deep Learning for EEG; Affective Computing; EEG Analysis; Epilepsy Detection},
	la = {en},
	number = {7},
	pages = {1659--1659},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors},
	url = {https://doi.org/10.3390/s19071659},
	volume = {19},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.3390/s19071659}}

@article{cimtayekmekcioglu_2020,
	abstract = {Multimodal emotion recognition has gained traction in affective computing research community to overcome the limitations posed by the processing a single form of data and to increase recognition robustness.In this study, a novel emotion recognition system is introduced, which is based on multiple modalities including facial expressions, galvanic skin response (GSR) and electroencephalogram (EEG).This method follows a hybrid fusion strategy and yields a maximum one-subject-out accuracy of 81.2{\%} and a mean accuracy of 74.2{\%} on our bespoke multimodal emotion dataset (LUMED-2) for 3 emotion classes: sad, neutral and happy.Similarly, our approach yields a maximum one-subject-out accuracy of 91.5{\%} and a mean accuracy of 53.8{\%} on the Database for Emotion Analysis using Physiological Signals (DEAP) for varying numbers of emotion classes, 4 in average, including angry, disgust, afraid, happy, neutral, sad and surprised.The presented model is particularly useful in determining the correct emotional state in the case of natural deceptive facial expressions.In terms of emotion recognition accuracy, this study is superior to, or on par with, the reference subject-independent multimodal emotion recognition studies introduced in the literature.},
	author = {{\c C}imtay, Y{\"u}cel and Ekmekcio{\v g}lu, Erhan and Caglar‐Ozhan, Seyma},
	c1 = {Institute for Digital Technologies, Loughborough University London, London E20 3BS, U.K.; Institute for Digital Technologies, Loughborough University London, London E20 3BS, U.K.; Department of Computer Education and Instructional Technology, Hacettepe University, 06800 Ankara, Turkey},
	date = {2020-01-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/access.2020.3023871},
	isbn = {2169-3536},
	journal = {IEEE access},
	keywords = {Emotion Recognition; Affective Computing; Emotions; Multimodal Data; Speech Emotion},
	la = {en},
	pages = {168865--168878},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {Cross-Subject Multimodal Emotion Recognition Based on Hybrid Fusion},
	url = {https://doi.org/10.1109/access.2020.3023871},
	volume = {8},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/access.2020.3023871}}

@article{liuzhang_2021,
	abstract = {Emotions are closely related to human behavior, family, and society. Changes in emotions can cause differences in electroencephalography (EEG) signals, which show different emotional states and are not easy to disguise. EEG-based emotion recognition has been widely used in human-computer interaction, medical diagnosis, military, and other fields. In this paper, we describe the common steps of an emotion recognition algorithm based on EEG from data acquisition, preprocessing, feature extraction, feature selection to classifier. Then, we review the existing EEG-based emotional recognition methods, as well as assess their classification effect. This paper will help researchers quickly understand the basic theory of emotion recognition and provide references for the future development of EEG. Moreover, emotion is an important representation of safety psychology.},
	author = {Liu, Haoran and Zhang, Ying and Li, Yujun and Kong, Xianwen},
	c1 = {The Boiler and Pressure Vessel Safety Inspection Institute of Henan Province, China; Patent Examination Cooperation (Henan) Center of the Patent Office, CNIPA, China; The Boiler and Pressure Vessel Safety Inspection Institute of Henan Province, China; The Boiler and Pressure Vessel Safety Inspection Institute of Henan Province, China},
	date = {2021-10-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3389/fncom.2021.758212},
	isbn = {1662-5188},
	journal = {Frontiers in computational neuroscience},
	keywords = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Affective Computing; Head Gesture Recognition},
	la = {en},
	publisher = {Frontiers Media},
	title = {Review on Emotion Recognition Based on Electroencephalography},
	url = {https://doi.org/10.3389/fncom.2021.758212},
	volume = {15},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.3389/fncom.2021.758212}}

@article{zhaoadib_2018,
	abstract = {This paper demonstrates a new technology that can infer a person's emotions from RF signals reflected off his body. EQ-Radio transmits an RF signal and analyzes its reflections off a person's body to recognize his emotional state (happy, sad, etc.). The key enabler underlying EQ-Radio is a new algorithm for extracting the individual heartbeats from the wireless signal at an accuracy comparable to on-body ECG monitors. The resulting beats are then used to compute emotion-dependent features which feed a machine-learning emotion classifier. We describe the design and implementation of EQ-Radio, and demonstrate through a user study that its emotion recognition accuracy is on par with state-of-the-art emotion recognition systems that require a person to be hooked to an ECG monitor.},
	author = {Zhao, M. and Adib, Fadel and Katabi, Dina},
	c1 = {Massachusetts Institute of Technology, Cambridge, MA; Massachusetts Institute of Technology, Cambridge, MA; Massachusetts Institute of Technology, Cambridge, MA},
	date = {2018-08-22},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1145/3236621},
	isbn = {0001-0782},
	journal = {Communications of the ACM},
	keywords = {Emotion Recognition; Deep Learning for EEG; Affective Computing; EEG Analysis; ECG Signal},
	la = {en},
	number = {9},
	pages = {91--100},
	publisher = {Association for Computing Machinery},
	title = {Emotion recognition using wireless signals},
	url = {https://doi.org/10.1145/3236621},
	volume = {61},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1145/3236621}}

@article{girardilanubile_2017,
	abstract = {Emotion recognition from biometrics is relevant to a wide range of application domains, including healthcare. Existing approaches usually adopt multi-electrodes sensors that could be expensive or uncomfortable to be used in real-life situations. In this study, we investigate whether we can reliably recognize high vs. low emotional valence and arousal by relying on noninvasive low cost EEG, EMG, and GSR sensors. We report the results of an empirical study involving 19 subjects. We achieve state-of-the-art classification performance for both valence and arousal even in a cross-subject classification setting, which eliminates the need for individual training and tuning of classification models.},
	author = {Girardi, Daniela and Lanubile, Filippo and Novielli, Nicole},
	c1 = {University of Bari `Aldo Moro'; University of Bari `Aldo Moro'; University of Bari `Aldo Moro'},
	date = {2017-10-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/acii.2017.8273589},
	keywords = {Emotion Recognition; Epilepsy Detection; EEG Analysis; Affective Computing; Deep Learning for EEG},
	la = {en},
	title = {Emotion detection using noninvasive low cost sensors},
	url = {https://doi.org/10.1109/acii.2017.8273589},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/acii.2017.8273589}}

@article{menezessamara_2017,
	abstract = {One of the challenges in virtual environments is the difficulty users have in interacting with these increasingly complex systems. Ultimately, endowing machines with the ability to perceive users emotions will enable a more intuitive and reliable interaction. Consequently, using the electroencephalogram as a bio-signal sensor, the affective state of a user can be modelled and subsequently utilised in order to achieve a system that can recognise and react to the user's emotions. This paper investigates features extracted from electroencephalogram signals for the purpose of affective state modelling based on Russell's Circumplex Model. Investigations are presented that aim to provide the foundation for future work in modelling user affect to enhance interaction experience in virtual environments. The DEAP dataset was used within this work, along with a Support Vector Machine and Random Forest, which yielded reasonable classification accuracies for Valence and Arousal using feature vectors based on statistical measurements and band power from the {$\backslash$}'z, {$\backslash$}b{\{}eta{\}}, {$\backslash$}'z, and {$\backslash$}'z{$\backslash$}'z waves and High Order Crossing of the EEG signal.},
	author = {Menezes, Maria and Samara, Anas and Galway, Leo and Sant'Anna, Anita and Verikas, Antanas and Alonso‐Fernandez, Fernando and Wang, H. and Bond, Raymond},
	c1 = {Center for Applied Intelligent Systems Research, Halmstad University, Halmstad, Sweden; School of Computing and Mathematics, Ulster University Belfast, Belfast, UK; School of Computing and Mathematics, Ulster University Belfast, Belfast, UK; Center for Applied Intelligent Systems Research, Halmstad University, Halmstad, Sweden; Center for Applied Intelligent Systems Research, Halmstad University, Halmstad, Sweden; Center for Applied Intelligent Systems Research, Halmstad University, Halmstad, Sweden; School of Computing and Mathematics, Ulster University Belfast, Belfast, UK; School of Computing and Mathematics, Ulster University Belfast, Belfast, UK},
	date = {2017-08-22},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1007/s00779-017-1072-7},
	isbn = {1617-4909},
	journal = {Personal and ubiquitous computing},
	keywords = {Emotion Recognition; Affective Computing; Deep Learning for EEG; EEG Analysis; Sensory Processing},
	la = {en},
	number = {6},
	pages = {1003--1013},
	publisher = {Springer Science+Business Media},
	title = {Towards emotion recognition for virtual environments: an evaluation of eeg features on benchmark dataset},
	url = {https://doi.org/10.1007/s00779-017-1072-7},
	volume = {21},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1007/s00779-017-1072-7}}

@article{moerlandbroekens_2017,
	abstract = {This article provides the first survey of computational models of emotion in reinforcement learning (RL) agents. The survey focuses on agent/robot emotions, and mostly ignores human user emotions. Emotions are recognized as functional in decision-making by influencing motivation and action selection. Therefore, computational emotion models are usually grounded in the agent's decision making architecture, of which RL is an important subclass. Studying emotions in RL-based agents is useful for three research fields. For machine learning (ML) researchers, emotion models may improve learning efficiency. For the interactive ML and human--robot interaction community, emotions can communicate state and enhance user investment. Lastly, it allows affective modelling researchers to investigate their emotion theories in a successful AI agent class. This survey provides background on emotion theory and RL. It systematically addresses (1) from what underlying dimensions (e.g. homeostasis, appraisal) emotions can be derived and how these can be modelled in RL-agents, (2) what types of emotions have been derived from these dimensions, and (3) how these emotions may either influence the learning efficiency of the agent or be useful as social signals. We also systematically compare evaluation criteria, and draw connections to important RL sub-domains like (intrinsic) motivation and model-based RL. In short, this survey provides both a practical overview for engineers wanting to implement emotions in their RL agents, and identifies challenges and directions for future emotion-RL research.},
	author = {Moerland, Thomas and Broekens, Joost and Jonker, Catholijn},
	c1 = {Delft University of Technology, Mekelweg 4, 2628 CD, Delft, The Netherlands; Delft University of Technology, Mekelweg 4, 2628 CD, Delft, The Netherlands; Delft University of Technology, Mekelweg 4, 2628 CD, Delft, The Netherlands},
	date = {2017-08-25},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1007/s10994-017-5666-0},
	isbn = {0885-6125},
	journal = {Machine learning},
	keywords = {Emotion Recognition; Affective Computing; Emotion Perception; Reinforcement Learning; Human Perception of Robots},
	la = {en},
	number = {2},
	pages = {443--480},
	publisher = {Springer Science+Business Media},
	title = {Emotion in reinforcement learning agents and robots: a survey},
	url = {https://doi.org/10.1007/s10994-017-5666-0},
	volume = {107},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1007/s10994-017-5666-0}}

@article{zhangchen_2020,
	abstract = {Emotion is the human brain reacting to objective things. In real life, human emotions are complex and changeable, so research into emotion recognition is of great significance in real life applications. Recently, many deep learning and machine learning methods have been widely applied in emotion recognition based on EEG signals. However, the traditional machine learning method has a major disadvantage in that the feature extraction process is usually cumbersome, which relies heavily on human experts. Then, end-to-end deep learning methods emerged as an effective method to address this disadvantage with the help of raw signal features and time-frequency spectrums. Here, we investigated the application of several deep learning models to the research field of EEG-based emotion recognition, including deep neural networks (DNN), convolutional neural networks (CNN), long short-term memory (LSTM), and a hybrid model of CNN and LSTM (CNN-LSTM). The experiments were carried on the well-known DEAP dataset. Experimental results show that the CNN and CNN-LSTM models had high classification performance in EEG-based emotion recognition, and their accurate extraction rate of RAW data reached 90.12 and 94.17{\%}, respectively. The performance of the DNN model was not as accurate as other models, but the training speed was fast. The LSTM model was not as stable as the CNN and CNN-LSTM models. Moreover, with the same number of parameters, the training speed of the LSTM was much slower and it was difficult to achieve convergence. Additional parameter comparison experiments with other models, including epoch, learning rate, and dropout probability, were also conducted in the paper. Comparison results prove that the DNN model converged to optimal with fewer epochs and a higher learning rate. In contrast, the CNN model needed more epochs to learn. As for dropout probability, reducing the parameters by \~{}50{\%} each time was appropriate.},
	author = {Zhang, Yaqing and Chen, Jinling and Tan, Jen and Chen, Yuxuan and Chen, Yunyi and Li, Dihan and Lei, Yang and Su, Jian and Huang, Xin and Che, Wenliang},
	c1 = {Department of Cardiology, Shanghai Tenth People's Hospital, Tongji University School of Medicine, Shanghai, China; Department of Software Engineering, School of Informatics Xiamen University (National Demonstative Software School), Xiamen, China; Department of Software Engineering, School of Informatics Xiamen University (National Demonstative Software School), Xiamen, China; Department of Computer and Software, Institute of System Science, National University of Singapore, Singapore, Singapore; Department of Software Engineering, School of Informatics Xiamen University (National Demonstative Software School), Xiamen, China; Department of Software Engineering, School of Informatics Xiamen University (National Demonstative Software School), Xiamen, China; Department of Software Engineering, School of Informatics Xiamen University (National Demonstative Software School), Xiamen, China; Department of Software Engineering, School of Informatics Xiamen University (National Demonstative Software School), Xiamen, China; Nanjing University of Information Science and Technology, Nanjing, China; School of Software, Jiangxi Normal University, Nanchang, China; Department of Cardiology, Shanghai Tenth People's Hospital, Tongji University School of Medicine, Shanghai, China},
	date = {2020-12-23},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3389/fnins.2020.622759},
	isbn = {1662-453X},
	journal = {Frontiers in neuroscience},
	keywords = {Emotion Recognition; Deep Learning for EEG; Affective Computing; EEG Analysis; Deep Learning},
	la = {en},
	publisher = {Frontiers Media},
	title = {An Investigation of Deep Learning Models for EEG-Based Emotion Recognition},
	url = {https://doi.org/10.3389/fnins.2020.622759},
	volume = {14},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.3389/fnins.2020.622759}}

@article{muszynskitian_2021,
	abstract = {Recognizing emotional reactions of movie audiences to affective movie content is a challenging task in affective computing. Previous research on induced emotion recognition has mainly focused on using audio-visual movie content. Nevertheless, the relationship between the perceptions of the affective movie content (perceived emotions) and the emotions evoked in the audiences (induced emotions) is unexplored. In this work, we studied the relationship between perceived and induced emotions of movie audiences. Moreover, we investigated multimodal modelling approaches to predict movie induced emotions from movie content based features, as well as physiological and behavioral reactions of movie audiences. To carry out analysis of induced and perceived emotions, we first extended an existing database for movie affect analysis by annotating perceived emotions in a crowd-sourced manner. We find that perceived and induced emotions are not always consistent with each other. In addition, we show that perceived emotions, movie dialogues, and aesthetic highlights are discriminative for movie induced emotion recognition besides spectators' physiological and behavioral reactions. Also, our experiments revealed that induced emotion recognition could benefit from including temporal information and performing multimodal fusion. Moreover, our work deeply investigated the gap between affective content analysis and induced emotion recognition by gaining insight into the relationships between aesthetic highlights, induced emotions, and perceived emotions.},
	author = {Muszy{\'n}ski, Micha{\l} and Tian, Leimin and Lai, Catherine and Moore, Johanna and Κωστούλας, Θεόδωρος and Lombardo, Patrizia and Pun, Thierry and Chanel, Guillaume},
	c1 = {University of Geneva, Geneva, Switzerland; University of Edinburgh and Monash University, Clayton, VIC, Australia; University of Edinburgh, Edinburgh, United Kingdom; University of Edinburgh, Edinburgh, United Kingdom; Bournemouth University, Poole, United Kingdom; University of Geneva, Geneva, Switzerland; University of Geneva, Geneva, Switzerland; University of Geneva, Geneva, Switzerland; University of Geneva, Geneva, Switzerland},
	date = {2021-01-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1109/taffc.2019.2902091},
	isbn = {1949-3045},
	journal = {IEEE transactions on affective computing},
	keywords = {Emotion Recognition; Affective Computing; Emotions; Speech Emotion; Affective Design},
	la = {en},
	number = {1},
	pages = {36--52},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {Recognizing Induced Emotions of Movie Audiences from Multimodal Information},
	url = {https://doi.org/10.1109/taffc.2019.2902091},
	volume = {12},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/taffc.2019.2902091}}

@article{athavipachpan-ngum_2019,
	abstract = {For future healthcare applications, which are increasingly moving towards out-of-hospital or home-based caring models, the ability to remotely and continuously monitor patients'conditions effectively are imperative. Among others, emotional state is one of the conditions that could be of interest to doctors or caregivers. This paper discusses a preliminary study to develop a wearable device that is a low cost, single channel, dry contact, in-ear EEG suitable for non-intrusive monitoring. All aspects of the designs, engineering, and experimenting by applying machine learning for emotion classification, are covered. Based on the valence and arousal emotion model, the device is able to classify basic emotion with 71.07{\%} accuracy (valence), 72.89{\%} accuracy (arousal), and 53.72{\%} (all four emotions). The results are comparable to those measured from the more conventional EEG headsets at T7 and T8 scalp positions. These results, together with its earphone-like wearability, suggest its potential usage especially for future healthcare applications, such as home-based or tele-monitoring systems as intended.},
	author = {Athavipach, Chanavit and Pan-ngum, Setha and Israsena, Pasin},
	c1 = {Department of Computer Engineering, Faculty of Engineering, Chulalongkorn University, Phayathai Road, Wang Mai, Pathumwan, Bangkok 10330, Thailand; Department of Computer Engineering, Faculty of Engineering, Chulalongkorn University, Phayathai Road, Wang Mai, Pathumwan, Bangkok 10330, Thailand; National Electronics and Computer Technology Center, 112 Thailand Science Park, Phahonyothin Road, Khlong Nueng, Khlong Luang, Pathumthani 12120, Thailand},
	date = {2019-09-17},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/s19184014},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {EEG Analysis; Deep Learning for EEG; Emotion Recognition; Speech Emotion; Affective Computing},
	la = {en},
	number = {18},
	pages = {4014--4014},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {A Wearable In-Ear EEG Device for Emotion Monitoring},
	url = {https://doi.org/10.3390/s19184014},
	volume = {19},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.3390/s19184014}}

@article{asgharkhan_2019,
	abstract = {Much attention has been paid to the recognition of human emotions with the help of electroencephalogram (EEG) signals based on machine learning technology. Recognizing emotions is a challenging task due to the non-linear property of the EEG signal. This paper presents an advanced signal processing method using the deep neural network (DNN) for emotion recognition based on EEG signals. The spectral and temporal components of the raw EEG signal are first retained in the 2D Spectrogram before the extraction of features. The pre-trained AlexNet model is used to extract the raw features from the 2D Spectrogram for each channel. To reduce the feature dimensionality, spatial, and temporal based, bag of deep features (BoDF) model is proposed. A series of vocabularies consisting of 10 cluster centers of each class is calculated using the k-means cluster algorithm. Lastly, the emotion of each subject is represented using the histogram of the vocabulary set collected from the raw-feature of a single channel. Features extracted from the proposed BoDF model have considerably smaller dimensions. The proposed model achieves better classification accuracy compared to the recently reported work when validated on SJTU SEED and DEAP data sets. For optimal classification performance, we use a support vector machine (SVM) and k-nearest neighbor (k-NN) to classify the extracted features for the different emotional states of the two data sets. The BoDF model achieves 93.8{\%} accuracy in the SEED data set and 77.4{\%} accuracy in the DEAP data set, which is more accurate compared to other state-of-the-art methods of human emotion recognition.},
	author = {Asghar, Muhammad and Khan, Muhammad and Fawad, Fawad and Amin, Yasar and Rizwan, Muhammad and Rahman, MuhibUr and Badnava, Salman and Mirjavadi, Seyed},
	c1 = {Department of Telecommunication Engineering, University of Engineering and Technology, Taxila 47050, Pakistan; Department of Telecommunication Engineering, University of Engineering and Technology, Taxila 47050, Pakistan; Department of Telecommunication Engineering, University of Engineering and Technology, Taxila 47050, Pakistan; Department of Telecommunication Engineering, University of Engineering and Technology, Taxila 47050, Pakistan; Department of Computer Engineering, University of Engineering and Technology, Taxila 47050, Pakistan; Department of Electrical Engineering, Polytechnique Montreal, Montreal, QC H3T 1J4, Canada; Department of Computer Science and Engineering, College of Engineering, Qatar University, P.O. Box 2713 Doha, Qatar; Department of Mechanical and Industrial Engineering, College of Engineering, Qatar University, P.O. Box 2713 Doha, Qatar},
	date = {2019-11-28},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/s19235218},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; ECG Signal},
	la = {en},
	number = {23},
	pages = {5218--5218},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {EEG-Based Multi-Modal Emotion Recognition using Bag of Deep Features: An Optimal Feature Selection Approach},
	url = {https://doi.org/10.3390/s19235218},
	volume = {19},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.3390/s19235218}}

@article{hanzhang_2017,
	abstract = {Over the last decade, automatic emotion recognition has become well established. The gold standard target is thereby usually calculated based on multiple annotations from different raters. All related efforts assume that the emotional state of a human subject can be identified by a 'hard' category or a unique value. This assumption tries to ease the human observer's subjectivity when observing patterns such as the emotional state of others. However, as the number of annotators cannot be infinite, uncertainty remains in the emotion target even if calculated from several, yet few human annotators. The common procedure to use this same emotion target in the learning process thus inevitably introduces noise in terms of an uncertain learning target. In this light, we propose a 'soft' prediction framework to provide a more human-like and comprehensive prediction of emotion. In our novel framework, we provide an additional target to indicate the uncertainty of human perception based on the inter-rater disagreement level, in contrast to the traditional framework which is merely producing one single prediction (category or value). To exploit the dependency between the emotional state and the newly introduced perception uncertainty, we implement a multi-task learning strategy. To evaluate the feasibility and effectiveness of the proposed soft prediction framework, we perform extensive experiments on a time- and value-continuous spontaneous audiovisual emotion database including late fusion results. We show that the soft prediction framework with multi-task learning of the emotional state and its perception uncertainty significantly outperforms the individual tasks in both the arousal and valence dimensions.},
	author = {Han, Jing and Zhang, Zixing and Schmitt, Maximilian and Panti{\'c}, Maja and Schuller, Bj{\"o}rn},
	c1 = {University of Augsburg \& University of Passau, Augsburg, Germany; University of Passau, Passau, Germany; University of Augsburg \& University of Passau, Augsburg, Germany; Imperial College London, London, United Kingdom; University of Augsburg \& University of Passau, Augsburg, Germany},
	date = {2017-10-19},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1145/3123266.3123383},
	keywords = {Affective Computing; Emotion Recognition; Speech Emotion},
	la = {en},
	title = {From Hard to Soft},
	url = {https://doi.org/10.1145/3123266.3123383},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3123266.3123383}}

@article{dingrobinson_2023,
	abstract = {The high temporal resolution and the asymmetric spatial activations are essential attributes of electroencephalogram (EEG) underlying emotional processes in the brain. To learn the temporal dynamics and spatial asymmetry of EEG towards accurate and generalized emotion recognition, we propose TSception, a multi-scale convolutional neural network that can classify emotions from EEG. TSception consists of dynamic temporal, asymmetric spatial, and high-level fusion layers, which learn discriminative representations in the time and channel dimensions simultaneously. The dynamic temporal layer consists of multi-scale 1D convolutional kernels whose lengths are related to the sampling rate of EEG, which learns the dynamic temporal and frequency representations of EEG. The asymmetric spatial layer takes advantage of the asymmetric EEG patterns for emotion, learning the discriminative global and hemisphere representations. The learned spatial representations will be fused by a high-level fusion layer. Using more generalized cross-validation settings, the proposed method is evaluated on two publicly available datasets DEAP and MAHNOB-HCI. The performance of the proposed network is compared with prior reported methods such as SVM, KNN, FBFgMDM, FBTSC, Unsupervised learning, DeepConvNet, ShallowConvNet, and EEGNet. TSception achieves higher classification accuracies and F1 scores than other methods in most of the experiments. The codes are available at https://github.com/yi-ding-cs/TSception},
	author = {Ding, Yi and Robinson, Neethu and Zhang, Su and Zeng, Qiuhao and Guan, Cuntai},
	c1 = {School of Computer Science and Engineering, Nanyang Technological University, 54761 Singapore, Singapore, Singapore, 639798; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computing Science and Engineering, Nanyang Technological University, 54761 Singapore, Singapore, Singapore, 639798; School of Computer Science and Engineering, Nanyang Technological University, 54761 Singapore, Singapore, Singapore; School of Computer Science and Engineering, Nanyang Technological University, 54761 Singapore, North West, Singapore, 639798},
	date = {2023-07-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/taffc.2022.3169001},
	isbn = {1949-3045},
	journal = {IEEE transactions on affective computing},
	keywords = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Epilepsy Detection},
	la = {en},
	number = {3},
	pages = {2238--2250},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {TSception: Capturing Temporal Dynamics and Spatial Asymmetry From EEG for Emotion Recognition},
	url = {https://doi.org/10.1109/taffc.2022.3169001},
	volume = {14},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/taffc.2022.3169001}}

@article{heli_2020,
	abstract = {With the continuous development of portable noninvasive human sensor technologies such as brain--computer interfaces (BCI), multimodal emotion recognition has attracted increasing attention in the area of affective computing. This paper primarily discusses the progress of research into multimodal emotion recognition based on BCI and reviews three types of multimodal affective BCI (aBCI): aBCI based on a combination of behavior and brain signals, aBCI based on various hybrid neurophysiology modalities and aBCI based on heterogeneous sensory stimuli. For each type of aBCI, we further review several representative multimodal aBCI systems, including their design principles, paradigms, algorithms, experimental results and corresponding advantages. Finally, we identify several important issues and research directions for multimodal emotion recognition based on BCI.},
	author = {He, Zhipeng and Li, Zina and Yang, Fuzhou and Wang, Lei and Li, Jingcong and Zhou, Chengju and Pan, Jiahui},
	c1 = {School of Software, South China Normal University, Foshan 528225, China; School of Computer, South China Normal University, Guangzhou 510641, China; School of Software, South China Normal University, Foshan 528225, China; School of Software, South China Normal University, Foshan 528225, China; School of Software, South China Normal University, Foshan 528225, China; School of Software, South China Normal University, Foshan 528225, China; School of Software, South China Normal University, Foshan 528225, China},
	date = {2020-09-29},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/brainsci10100687},
	isbn = {2076-3425},
	journal = {Brain sciences},
	keywords = {Emotion Recognition; Affective Computing; Brain-Computer Interfaces; BCI Technology; BCI Communication},
	la = {en},
	number = {10},
	pages = {687--687},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Advances in Multimodal Emotion Recognition Based on Brain--Computer Interfaces},
	url = {https://doi.org/10.3390/brainsci10100687},
	volume = {10},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.3390/brainsci10100687}}

@article{vinolavimaladevi_2015,
	abstract = {This paper presents the various emotion classification and recognition systems which implement methods aiming at improving Human Machine Interaction. The modalities and approaches used for affect detection vary and contribute to accuracy and efficacy in detecting emotions of human beings. This paper discovers them in a comparison and descriptive manner. Various applications that use the methodologies in different contexts to address the challenges in real time are discussed. This survey also describes the databases that can be used as standard data sets in the process of emotion identification. Thus an integrated discussion of methods, databases used and applications pertaining to the emerging field of Affective Computing (AC) is done and surveyed.This paper presents the various emotion classification and recognition systems which implement methods aiming at improving Human Machine Interaction. The modalities and approaches used for affect detection vary and contribute to accuracy and efficacy in detecting emotions of human beings. This paper discovers them in a comparison and descriptive manner. Various applications that use the methodologies in different contexts to address the challenges in real time are discussed. This survey also describes the databases that can be used as standard data sets in the process of emotion identification. Thus an integrated discussion of methods, databases used and applications pertaining to the emerging field of Affective Computing (AC) is done and surveyed.},
	author = {Vinola, C and Vimaladevi, K.},
	c1 = {Francis Xavier Engineering College; P.S.R.Engineering College},
	date = {2015-12-16},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.5565/rev/elcvia.795},
	isbn = {1577-5097},
	journal = {ELCVIA. Electronic letters on computer vision and image analysis},
	keywords = {Emotion Recognition; Affective Computing; Human-Computer Interaction; Affective Design; Emotions},
	la = {en},
	number = {2},
	pages = {24--24},
	publisher = {Computer Vision Center Press},
	title = {A Survey on Human Emotion Recognition Approaches, Databases and Applications},
	url = {https://doi.org/10.5565/rev/elcvia.795},
	volume = {14},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.5565/rev/elcvia.795}}

@article{lotfianbusso_2019,
	abstract = {This study introduces a method to design a curriculum for machine-learning to maximize the efficiency during the training process of deep neural networks (DNNs) for speech emotion recognition. Previous studies in other machine-learning problems have shown the benefits of training a classifier following a curriculum where samples are gradually presented in increasing level of difficulty. For speech emotion recognition, the challenge is to establish a natural order of difficulty in the training set to create the curriculum. We address this problem by assuming that, ambiguous samples for humans are also ambiguous for computers. Speech samples are often annotated by multiple evaluators to account for differences in emotion perception across individuals. While some sentences with clear emotional content are consistently annotated, sentences with more ambiguous emotional content present important disagreement between individual evaluations. We propose to use the disagreement between evaluators as a measure of difficulty for the classification task. We propose metrics that quantify the inter-evaluation agreement to define the curriculum for regression problems and binary and multi-class classification problems. The experimental results consistently show that relying on a curriculum based on agreement between human judgments leads to statistically significant improvements over baselines trained without a curriculum.},
	author = {Lotfian, Reza and Busso, Carlos},
	c1 = {Erik Jonsson School of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, TX, USA; Erik Jonsson School of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, TX, USA},
	date = {2019-04-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/taslp.2019.2898816},
	isbn = {2329-9290},
	journal = {IEEE/ACM transactions on audio, speech, and language processing},
	keywords = {Emotion Recognition; Affective Computing; Speech Emotion; Environmental Sound Recognition; Deep Learning},
	la = {en},
	number = {4},
	pages = {815--826},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {Curriculum Learning for Speech Emotion Recognition From Crowdsourced Labels},
	url = {https://doi.org/10.1109/taslp.2019.2898816},
	volume = {27},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/taslp.2019.2898816}}

@article{gannounialedaily_2021,
	abstract = {Recognizing emotions using biological brain signals requires accurate and efficient signal processing and feature extraction methods. Existing methods use several techniques to extract useful features from a fixed number of electroencephalography (EEG) channels. The primary objective of this study was to improve the performance of emotion recognition using brain signals by applying a novel and adaptive channel selection method that acknowledges that brain activity has a unique behavior that differs from one person to another and one emotional state to another. Moreover, we propose identifying epochs, which are the instants at which excitation is maximum, during the emotion to improve the system's accuracy. We used the zero-time windowing method to extract instantaneous spectral information using the numerator group-delay function to accurately detect the epochs in each emotional state. Different classification scheme were defined using QDC and RNN and evaluated using the DEAP database. The experimental results showed that the proposed method is highly competitive compared with existing studies of multi-class emotion recognition. The average accuracy rate exceeded 89{\%}. Compared with existing algorithms dealing with 9 emotions, the proposed method enhanced the accuracy rate by 8{\%}. Moreover, experiment shows that the proposed system outperforms similar approaches discriminating between 3 and 4 emotions only. We also found that the proposed method works well, even when applying conventional classification algorithms.},
	author = {Gannouni, Sofien and Aledaily, Arwa and Belwafi, Kais and Aboalsamh, Hatim},
	c1 = {Computer Science Department, College of Computer and Information Sciences, King Saud University, Riyadh, 11543, Saudi Arabia; Computer Science Department, College of Computer and Information Sciences, King Saud University, Riyadh, 11543, Saudi Arabia; Computer Science Department, College of Computer and Information Sciences, King Saud University, Riyadh, 11543, Saudi Arabia; Computer Science Department, College of Computer and Information Sciences, King Saud University, Riyadh, 11543, Saudi Arabia},
	date = {2021-03-29},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1038/s41598-021-86345-5},
	isbn = {2045-2322},
	journal = {Scientific reports},
	keywords = {Emotion Recognition; Affective Computing; Epilepsy Detection; EEG Analysis; Speech Emotion},
	la = {en},
	number = {1},
	publisher = {Nature Portfolio},
	title = {Emotion detection using electroencephalography signals and a zero-time windowing-based epoch estimation and relevant electrode identification},
	url = {https://doi.org/10.1038/s41598-021-86345-5},
	volume = {11},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1038/s41598-021-86345-5}}

@article{gatticalzolari_2018,
	abstract = {Abstract The human emotional reactions to stimuli delivered by different sensory modalities is a topic of interest for many disciplines, from Human-Computer-Interaction to cognitive sciences. Different databases of stimuli eliciting emotional reaction are available, tested on a high number of participants. Interestingly, stimuli within one database are always of the same type. In other words, to date, no data was obtained and compared from distinct types of emotion-eliciting stimuli from the same participant. This makes it difficult to use different databases within the same experiment, limiting the complexity of experiments investigating emotional reactions. Moreover, whereas the stimuli and the participants'rating to the stimuli are available, physiological reactions of participants to the emotional stimuli are often recorded but not shared. Here, we test stimuli delivered either through a visual, auditory, or haptic modality in a within participant experimental design. We provide the results of our study in the form of a MATLAB structure including basic demographics on the participants, the participant's self-assessment of his/her emotional state, and his/her physiological reactions (i.e., skin conductance).},
	author = {Gatti, Elia and Calzolari, Elena and Maggioni, Emanuela and Obrist, Marianna},
	c1 = {Sussex Computer Human Interaction (SCHI) Lab, School of Engineering and Informatics, University of Sussex, Brighton, UK; Imperial College, London, UK; Sussex Computer Human Interaction (SCHI) Lab, School of Engineering and Informatics, University of Sussex, Brighton, UK; Sussex Computer Human Interaction (SCHI) Lab, School of Engineering and Informatics, University of Sussex, Brighton, UK},
	date = {2018-06-26},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1038/sdata.2018.120},
	isbn = {2052-4463},
	journal = {Scientific data},
	keywords = {Sensory Analysis; Emotional Responses; Affective Computing; Emotion Recognition; Emotions},
	la = {en},
	number = {1},
	publisher = {Nature Portfolio},
	title = {Emotional ratings and skin conductance response to visual, auditory and haptic stimuli},
	url = {https://doi.org/10.1038/sdata.2018.120},
	volume = {5},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1038/sdata.2018.120}}

@article{larradetniewiadomski_2020,
	abstract = {Emotion, mood, and stress recognition (EMSR) has been studied in laboratory settings for decades. In particular, physiological signals are widely used to detect and classify affective states in lab conditions. However, physiological reactions to emotional stimuli have been found to differ in laboratory and natural settings. Thanks to recent technological progress (e.g., in wearables) the creation of EMSR systems for a large number of consumers during their everyday activities is increasingly possible. Therefore, datasets created in the wild are needed to insure the validity and the exploitability of EMSR models for real-life applications. In this paper, we initially present common techniques used in laboratory settings to induce emotions for the purpose of physiological dataset creation. Next, advantages and challenges of data collection in the wild are discussed. To assess the applicability of existing datasets to real-life applications, we propose a set of categories to guide and compare at a glance different methodologies used by researchers to collect such data. For this purpose, we also introduce a visual tool called Graphical Assessment of Real-life Application-Focused Emotional Dataset (GARAFED). In the last part of the paper, we apply the proposed tool to compare existing physiological datasets for EMSR in the wild and to show possible improvements and future directions of research. We wish for this paper and GARAFED to be used as guidelines for researchers and developers who aim at collecting affect-related data for real-life EMSR-based applications.},
	author = {Larradet, Fanny and Niewiadomski, Rados{\l}aw and Barresi, Giacinto and Caldwell, Darwin and Mattos, Leonardo},
	c1 = {Advanced Robotics, Istituto Italiano di Tecnologia, Genoa, Italy; Contact Unit, Istituto Italiano di Tecnologia, Genoa, Italy; Department of Psychology and Cognitive Science, University of Trento, Rovereto, Italy; Rehab Technologies, Istituto Italiano di Tecnologia, Genoa, Italy; Advanced Robotics, Istituto Italiano di Tecnologia, Genoa, Italy; Advanced Robotics, Istituto Italiano di Tecnologia, Genoa, Italy},
	date = {2020-07-15},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3389/fpsyg.2020.01111},
	isbn = {1664-1078},
	journal = {Frontiers in psychology},
	keywords = {Emotion Recognition; Emotion Regulation; Affective Computing; Speech Emotion; Physiological Signals},
	la = {en},
	publisher = {Frontiers Media},
	title = {Toward Emotion Recognition From Physiological Signals in the Wild: Approaching the Methodological Issues in Real-Life Data Collection},
	url = {https://doi.org/10.3389/fpsyg.2020.01111},
	volume = {11},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.3389/fpsyg.2020.01111}}

@article{chengwang_2019,
	abstract = {Abstract This article studies whether heart sound signals can be used for emotion recognition. First, we built a small emotion heart sound database, and simultaneously recorded the participants'ECG for comparative analysis. Second, according to the characteristics of the heart sound signals, two emotion evaluation indicators were proposed: HRV of heart sounds (difference between successive heartbeats) and DSV of heart sounds (the ratio of diastolic to systolic duration variability). Then, we extracted linear and nonlinear features from two emotion evaluation indicators to recognize four kinds of emotions. Moreover, we used valence dimension, arousal dimension and valence-arousal synthesis as evaluation standards. The experimental results demonstrated that heart sound signals can be used for emotion recognition. It was more effective to achieve recognition results by combining the features of HRV and DSV of heart sounds. Finally, the average accuracy of four emotion recognitions on valence dimension, arousal dimension and valence-arousal synthesis was up to 96.875{\%}, 88.5417{\%} and 81.25{\%}, respectively.},
	author = {Cheng, Xiefeng and Wang, Yue and Dai, Shicheng and Zhao, Pei and Liu, Qifa},
	c1 = {College of Electronic and Optical Engineering, Nanjing University of Posts and Telecommunications, Nanjing, 210003, China; College of Electronic and Optical Engineering, Nanjing University of Posts and Telecommunications, Nanjing, 210003, China; College of Electronic and Optical Engineering, Nanjing University of Posts and Telecommunications, Nanjing, 210003, China; Pediatric Cardiology, Xin Hua Hospital Affiliated to Shanghai Jiao Tong University School of Medicine, Shanghai, 200092, China; College of Telecommunication and Information Engineering, Nanjing University of Posts and Telecommunications, Nanjing, 210003, China},
	date = {2019-04-24},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1038/s41598-019-42826-2},
	isbn = {2045-2322},
	journal = {Scientific reports},
	keywords = {Emotion Recognition; Affective Computing; Heart Sound; Speech Emotion},
	la = {en},
	number = {1},
	publisher = {Nature Portfolio},
	title = {Heart sound signals can be used for emotion recognition},
	url = {https://doi.org/10.1038/s41598-019-42826-2},
	volume = {9},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1038/s41598-019-42826-2}}

@article{zhaojia_2021,
	abstract = {Humans are emotional creatures. Multiple modalities are often involved when we express emotions, whether we do so explicitly (such as through facial expression and speech) or implicitly (e.g., via text or images). Enabling machines to have emotional intelligence, i.e., recognizing, interpreting, processing, and simulating emotions, is becoming increasingly important. In this tutorial, we discuss several key aspects of multimodal emotion recognition (MER).},
	author = {Zhao, Sicheng and Jia, Guoli and Yang, Jufeng and Ding, Guiguang and Keutzer, Kurt},
	c1 = {Research Scientist, Columbia University, New York, New York, USA; College of Computer Science, Nankai University, Tianjin, China; College of Computer Science, Nankai University, Tianjin, China; School of Software, Tsinghua University, Beijing, China; Electrical Engineering and Computer Science, University of California, Berkeley, California, USA},
	date = {2021-11-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1109/msp.2021.3106895},
	isbn = {1053-5888},
	journal = {IEEE signal processing magazine},
	keywords = {Emotion Recognition; Affective Computing; Speech Emotion; Multimodal Data; Facial Expression},
	la = {en},
	number = {6},
	pages = {59--73},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {Emotion Recognition From Multiple Modalities: Fundamentals and methodologies},
	url = {https://doi.org/10.1109/msp.2021.3106895},
	volume = {38},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/msp.2021.3106895}}

@article{braunschubert_2019,
	abstract = {Drivers in negative emotional states, such as anger or sadness, are prone to perform bad at driving, decreasing overall road safety for all road users. Recent advances in affective computing, however, allow for the detection of such states and give us tools to tackle the connected problems within automotive user interfaces. We see potential in building a system which reacts upon possibly dangerous driver states and influences the driver in order to drive more safely. We compare different interaction approaches for an affective automotive interface, namely Ambient Light, Visual Notification, a Voice Assistant, and an Empathic Assistant. Results of a simulator study with 60 participants (30 each with induced sadness/anger) indicate that an emotional voice assistant with the ability to empathize with the user is the most promising approach as it improves negative states best and is rated most positively. Qualitative data also shows that users prefer an empathic assistant but also resent potential paternalism. This leads us to suggest that digital assistants are a valuable platform to improve driver emotions in automotive environments and thereby enable safer driving.},
	author = {Braun, Michael and Schubert, Jonas and Pfleging, Bastian and Alt, Florian},
	c1 = {BMW Group Research, New Technologies, Innovations, 85748 Garching, Germany; LMU Munich, 80337 Munich, Germany; LMU Munich, 80337 Munich, Germany; Eindhoven University of Technology, 5612 Eindhoven, The Netherlands; LMU Munich, 80337 Munich, Germany; CODE Research Institute, Bundeswehr University, 81739 Munich, Germany; LMU Munich, 80337 Munich, Germany},
	date = {2019-03-25},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/mti3010021},
	isbn = {2414-4088},
	journal = {Multimodal technologies and interaction},
	keywords = {Affective Computing; Driver Fatigue; Emotion Recognition; Affective Design; Emotions},
	la = {en},
	number = {1},
	pages = {21--21},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Improving Driver Emotions with Affective Strategies},
	url = {https://doi.org/10.3390/mti3010021},
	volume = {3},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.3390/mti3010021}}

@article{balanmoise_2019,
	abstract = {There has been steady progress in the field of affective computing over the last two decades that has integrated artificial intelligence techniques in the construction of computational models of emotion. Having, as a purpose, the development of a system for treating phobias that would automatically determine fear levels and adapt exposure intensity based on the user's current affective state, we propose a comparative study between various machine and deep learning techniques (four deep neural network models, a stochastic configuration network, Support Vector Machine, Linear Discriminant Analysis, Random Forest and k-Nearest Neighbors), with and without feature selection, for recognizing and classifying fear levels based on the electroencephalogram (EEG) and peripheral data from the DEAP (Database for Emotion Analysis using Physiological signals) database. Fear was considered an emotion eliciting low valence, high arousal and low dominance. By dividing the ratings of valence/arousal/dominance emotion dimensions, we propose two paradigms for fear level estimation---the two-level (0---no fear and 1---fear) and the four-level (0---no fear, 1---low fear, 2---medium fear, 3---high fear) paradigms. Although all the methods provide good classification accuracies, the highest F scores have been obtained using the Random Forest Classifier---89.96{\%} and 85.33{\%} for the two-level and four-level fear evaluation modality.},
	author = {B{\u a}lan, Oana and Moise, Gabriela and Moldoveanu, Alin and Leordeanu, Marius and Moldoveanu, Florica},
	c1 = {Department of Computer Science and Engineering, Faculty of Automatic Control and Computers, University POLITEHNICA of Bucharest, 060042 Bucharest, Romania; Department of Computer Science, Information Technology, Mathematics and Physics (ITIMF), Petroleum-Gas University of Ploiesti, 100680 Ploiesti, Romania; Department of Computer Science and Engineering, Faculty of Automatic Control and Computers, University POLITEHNICA of Bucharest, 060042 Bucharest, Romania; Department of Computer Science and Engineering, Faculty of Automatic Control and Computers, University POLITEHNICA of Bucharest, 060042 Bucharest, Romania; Department of Computer Science and Engineering, Faculty of Automatic Control and Computers, University POLITEHNICA of Bucharest, 060042 Bucharest, Romania},
	date = {2019-04-11},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/s19071738},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Affective Computing; Emotion Recognition; Deep Learning for EEG; Deep Learning; EEG Analysis},
	la = {en},
	number = {7},
	pages = {1738--1738},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Fear Level Classification Based on Emotional Dimensions and Machine Learning Techniques},
	url = {https://doi.org/10.3390/s19071738},
	volume = {19},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.3390/s19071738}}

@article{zhaogholaminejad_2019,
	abstract = {Due to the subjective responses of different subjects to physical stimuli, emotion recognition methodologies from physiological signals are increasingly becoming personalized. Existing works mainly focused on modeling the involved physiological corpus of each subject, without considering the psychological factors, such as interest and personality. The latent correlation among different subjects has also been rarely examined. In this article, we propose to investigate the influence of personality on emotional behavior in a hypergraph learning framework. Assuming that each vertex is a compound tuple (subject, stimuli), multi-modal hypergraphs can be constructed based on the personality correlation among different subjects and on the physiological correlation among corresponding stimuli. To reveal the different importance of vertices, hyperedges, and modalities, we learn the weights for each of them. As the hypergraphs connect different subjects on the compound vertices, the emotions of multiple subjects can be simultaneously recognized. In this way, the constructed hypergraphs are vertex-weighted multi-modal multi-task ones. The estimated factors, referred to as emotion relevance, are employed for emotion recognition. We carry out extensive experiments on the ASCERTAIN dataset and the results demonstrate the superiority of the proposed method, as compared to the state-of-the-art emotion recognition approaches.},
	author = {Zhao, Sicheng and Gholaminejad, Amir and Ding, Guiguang and Gao, Yue and Han, Jungong and Keutzer, Kurt},
	c1 = {Tsinghua University, China and University of California Berkeley, Berkeley; University of California Berkeley, Berkeley; Tsinghua University, Beijing, China; Tsinghua University, Beijing, China; Lancaster University, Lancaster, UK; University of California Berkeley, Berkeley},
	date = {2019-01-24},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1145/3233184},
	isbn = {1551-6857},
	journal = {ACM transactions on multimedia computing, communications and applications/ACM transactions on multimedia computing communications and applications},
	keywords = {Emotion Recognition; Affective Computing; Physiological Signals; Speech Emotion},
	la = {en},
	number = {1s},
	pages = {1--18},
	publisher = {Association for Computing Machinery},
	title = {Personalized Emotion Recognition by Personality-Aware High-Order Learning of Physiological Signals},
	url = {https://doi.org/10.1145/3233184},
	volume = {15},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1145/3233184}}

@article{liumeng_2016,
	abstract = {Human brain behavior is very complex and it is difficult to interpret. Human emotion might come from brain activities. However, the relationship between human emotion and brain activities is far from clear. In recent years, more and more researchers are trying to discover this relationship by recording brain signals such as electroencephalogram (EEG) signals with the associated emotion information extracted from other modalities such as facial expression. In this paper, machine learning based methods are used to model this relationship in the publicly available dataset DEAP (Database for Emotional Analysis using Physiological Signals). Different features are extracted from raw EEG recordings. Then Maximum Relevance Minimum Redundancy (mRMR) was used for feature selection. These features are fed into machine learning methods to build the prediction models to extract the emotion information from EEG signals. The models are evaluated on this dataset and satisfactory results are achieved.},
	author = {Liu, Jingxin and Meng, Hongying and Nandi, Asoke and Li, Maozhen},
	c1 = {Department of Electronic and Computer Engineering Brunel University London London United Kingdom; Department of Electronic and Computer Engineering Brunel University London London United Kingdom; Dept of Electronic and Computer Engineering, Brunel University London, London, United Kingdom; The Key Laboratory of Embedded Systems and Service, Tongji University, Shanghai, China; Dept of Electronic and Computer Engineering, Brunel University London, London, United Kingdom; The Key Laboratory of Embedded Systems and Service, Tongji University, Shanghai, China},
	date = {2016-08-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/fskd.2016.7603437},
	keywords = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Speech Emotion},
	la = {en},
	title = {Emotion detection from EEG recordings},
	url = {https://doi.org/10.1109/fskd.2016.7603437},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/fskd.2016.7603437}}

@article{yuvarajmurugappan_2014,
	abstract = {In addition to classic motor signs and symptoms, individuals with Parkinson's disease (PD) are characterized by emotional deficits. Ongoing brain activity can be recorded by electroencephalograph (EEG) to discover the links between emotional states and brain activity. This study utilized machine-learning algorithms to categorize emotional states in PD patients compared with healthy controls (HC) using EEG. Twenty non-demented PD patients and 20 healthy age-, gender-, and education level-matched controls viewed happiness, sadness, fear, anger, surprise, and disgust emotional stimuli while fourteen-channel EEG was being recorded. Multimodal stimulus (combination of audio and visual) was used to evoke the emotions. To classify the EEG-based emotional states and visualize the changes of emotional states over time, this paper compares four kinds of EEG features for emotional state classification and proposes an approach to track the trajectory of emotion changes with manifold learning. From the experimental results using our EEG data set, we found that (a) bispectrum feature is superior to other three kinds of features, namely power spectrum, wavelet packet and nonlinear dynamical analysis; (b) higher frequency bands (alpha, beta and gamma) play a more important role in emotion activities than lower frequency bands (delta and theta) in both groups and; (c) the trajectory of emotion changes can be visualized by reducing subject-independent features with manifold learning. This provides a promising way of implementing visualization of patient's emotional state in real time and leads to a practical system for noninvasive assessment of the emotional impairments associated with neurological disorders.},
	author = {Yuvaraj, Rajamanickam and Murugappan, M. and Ibrahim, Norlinah and Sundaraj, Kenneth and Omar, Mohammad and Mohamad, Khairiyah and Palaniappan, Ramaswamy},
	c1 = {School of Mechatronic Engineering, University Malaysia Perlis (UniMAP), Campus Ulu Pauh, Arau, 02600, Perlis, Malaysia; School of Mechatronic Engineering, University Malaysia Perlis (UniMAP), Campus Ulu Pauh, Arau, 02600, Perlis, Malaysia; Neurology Unit, Department of Medicine, UKM Medical Center, Jalan Yaacob Latiff, 56000, Bandar Tun Razak, Kuala Lumpur, Malaysia; School of Mechatronic Engineering, University Malaysia Perlis (UniMAP), Campus Ulu Pauh, Arau, 02600, Perlis, Malaysia; School of Mechatronic Engineering, University Malaysia Perlis (UniMAP), Campus Ulu Pauh, Arau, 02600, Perlis, Malaysia; Neurology Unit, Department of Medicine, UKM Medical Center, Jalan Yaacob Latiff, 56000, Bandar Tun Razak, Kuala Lumpur, Malaysia; School of Computing, University of Kent, Medway, UK},
	date = {2014-12-01},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1016/j.ijpsycho.2014.07.014},
	isbn = {0167-8760},
	journal = {International journal of psychophysiology},
	keywords = {EEG Analysis; Deep Learning for EEG; Emotion Recognition; Epilepsy Detection; Affective Computing},
	la = {en},
	number = {3},
	pages = {482--495},
	publisher = {Elsevier BV},
	title = {Optimal set of EEG features for emotional state classification and trajectory visualization in Parkinson's disease},
	url = {https://doi.org/10.1016/j.ijpsycho.2014.07.014},
	volume = {94},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1016/j.ijpsycho.2014.07.014}}

@article{castillocastrogonzalez_2016,
	abstract = {This paper introduces the architecture of an emotion-aware ambient intelligent and gerontechnological project named "Improvement of the Elderly Quality of Life and Care through Smart Emotion Regulation". The objective of the proposal is to find solutions for improving the quality of life and care of the elderly who can or want to continue living at home by using emotion regulation techniques. A series of sensors is used for monitoring the elderlies' facial and gestural expression, activity and behaviour, as well as relevant physiological data. This way the older people's emotions are inferred and recognized. Music, colour and light are the stimulating means to regulate their emotions towards a positive and pleasant mood. Then, the paper proposes a gerontechnological software architecture that enables real-time, continuous monitoring of the elderly and provides the best-tailored reactions of the ambience in order to regulate the older person's emotions towards a positive mood. After describing the benefits of the approach for emotion recognition and regulation in the elderly, the eight levels that compose the architecture are described.},
	author = {Castillo, Jos{\'e} and Castro‐Gonz{\'a}lez, {\'A}lvaro and Fern{\'a}ndez-Caballero, Antonio and Latorre, Jos{\'e} and Pastor, Jos{\'e} and Fern{\'a}ndez-Sotos, Alicia and Salichs, Miguel},
	c1 = {Instituto de Investigaci{\'o}n en Inform{\'a}tica de Albacete, Universidad de Castilla-La Mancha, 02071, Albacete, Spain; Robotics Lab, Universidad Carlos III de Madrid, 28911, Madrid, Spain; Instituto de Investigaci{\'o}n en Inform{\'a}tica de Albacete, Universidad de Castilla-La Mancha, 02071, Albacete, Spain; Instituto de Investigaci{\'o}n en Discapacidades Neurol{\'o}gicas, Universidad de Castilla-La Mancha, 02071, Albacete, Spain; Instituto de Tecnolog{\'\i}as Audiovisuales, Universidad de Castilla-La Mancha, 16071, Cuenca, Spain; Facultad de Educaci{\'o}n de Albacete, Universidad de Castilla-La Mancha, 02071, Albacete, Spain; Robotics Lab, Universidad Carlos III de Madrid, 28911, Madrid, Spain},
	date = {2016-02-12},
	date-added = {2024-05-13 15:05:25 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1007/s12559-016-9383-y},
	isbn = {1866-9956},
	journal = {Cognitive computation},
	keywords = {Emotion Recognition; Emotion Perception; Affective Computing; Emotions},
	la = {en},
	number = {2},
	pages = {357--367},
	publisher = {Springer Science+Business Media},
	title = {Software Architecture for Smart Emotion Recognition and Regulation of the Ageing Adult},
	url = {https://doi.org/10.1007/s12559-016-9383-y},
	volume = {8},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1007/s12559-016-9383-y}}

@article{serenoscott_2015,
	abstract = {Visual emotion word processing has been in the focus of recent psycholinguistic research. In general, emotion words provoke differential responses in comparison to neutral words. However, words are typically processed within a context rather than in isolation. For instance, how does one's inner emotional state influence the comprehension of emotion words? To address this question, the current study examined lexical decision responses to emotionally positive, negative, and neutral words as a function of induced mood as well as their word frequency. Mood was manipulated by exposing participants to different types of music. Participants were randomly assigned to one of three conditions-no music, positive music, and negative music. Participants' moods were assessed during the experiment to confirm the mood induction manipulation. Reaction time results confirmed prior demonstrations of an interaction between a word's emotionality and its frequency. Results also showed a significant interaction between participant mood and word emotionality. However, the pattern of results was not consistent with mood-congruency effects. Although positive and negative mood facilitated responses overall in comparison to the control group, neither positive nor negative mood appeared to additionally facilitate responses to mood-congruent words. Instead, the pattern of findings seemed to be the consequence of attentional effects arising from induced mood. Positive mood broadens attention to a global level, eliminating the category distinction of positive-negative valence but leaving the high-low arousal dimension intact. In contrast, negative mood narrows attention to a local level, enhancing within-category distinctions, in particular, for negative words, resulting in less effective facilitation.},
	author = {Sereno, Sara and Scott, Graham and Yao, Bo and Thaden, Elske and O'Donnell, Patrick},
	c1 = {Institute of Neuroscience and Psychology, University of Glasgow, Glasgow, UK; Applied Psychology Research Group, School of Media, Culture and Society, University of the West of Scotland, Paisley, UK; School of Psychological Sciences, University of Manchester, Manchester, UK; School of Psychology, University of Glasgow, Glasgow, UK; School of Psychology, University of Glasgow, Glasgow, UK},
	date = {2015-08-24},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3389/fpsyg.2015.01191},
	isbn = {1664-1078},
	journal = {Frontiers in psychology},
	keywords = {Speech Emotion; Emotion Recognition; Emotion Regulation; Affective Computing; Decision-making},
	la = {en},
	publisher = {Frontiers Media},
	title = {Emotion word processing: does mood make a difference?},
	url = {https://doi.org/10.3389/fpsyg.2015.01191},
	volume = {6},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.3389/fpsyg.2015.01191}}

@article{tianmuszynski_2017,
	abstract = {Predicting the emotional response of movie audiences to affective movie content is a challenging task in affective computing. Previous work has focused on using audiovisual movie content to predict movie induced emotions. However, the relationship between the audience's perceptions of the affective movie content (perceived emotions) and the emotions evoked in the audience (induced emotions) remains unexplored. In this work, we address the relationship between perceived and induced emotions in movies, and identify features and modelling approaches effective for predicting movie induced emotions. First, we extend the LIRIS-ACCEDE database by annotating perceived emotions in a crowd-sourced manner, and find that perceived and induced emotions are not always consistent. Second, we show that dialogue events and aesthetic highlights are effective predictors of movie induced emotions. In addition to movie based features, we also study physiological and behavioural measurements of audiences. Our experiments show that induced emotion recognition can benefit from including temporal context and from including multimodal information. Our study bridges the gap between affective content analysis and induced emotion prediction.},
	author = {Tian, Leimin and Muszy{\'n}ski, Micha{\l} and Lai, Catherine and Moore, Johanna and Κωστούλας, Θεόδωρος and Lombardo, Patrizia and Pun, Thierry and Chanel, Guillaume},
	c1 = {School of Informatics, University of Edinburgh; Computer Vision and Multimedia Laboratory, University of Geneva; School of Informatics, University of Edinburgh; School of Informatics, University of Edinburgh; Computer Vision and Multimedia Laboratory \& Swiss Center for Affective Sciences, University of Geneva; Faculty of Science and Technology, Bournemouth University, UK; Department of Modern French, University of Geneva; Computer Vision and Multimedia Laboratory \& Swiss Center for Affective Sciences, University of Geneva; Computer Vision and Multimedia Laboratory \& Swiss Center for Affective Sciences, University of Geneva},
	date = {2017-10-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1109/acii.2017.8273575},
	keywords = {Affective Computing; Emotion Recognition; Emotions; Aspect-based Sentiment Analysis; Narrative Persuasion},
	la = {en},
	title = {Recognizing induced emotions of movie audiences: Are induced and perceived emotions the same?},
	url = {https://doi.org/10.1109/acii.2017.8273575},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/acii.2017.8273575}}

@article{kumarchong_2018,
	abstract = {Correlation analysis is an extensively used technique that identifies interesting relationships in data. These relationships help us realize the relevance of attributes with respect to the target class to be predicted. This study has exploited correlation analysis and machine learning-based approaches to identify relevant attributes in the dataset which have a significant impact on classifying a patient's mental health status. For mental health situations, correlation analysis has been performed in Weka, which involves a dataset of depressive disorder symptoms and situations based on weather conditions, as well as emotion classification based on physiological sensor readings. Pearson's product moment correlation and other different classification algorithms have been utilized for this analysis. The results show interesting correlations in weather attributes for bipolar patients, as well as in features extracted from physiological data for emotional states.},
	author = {Kumar, Sunil and Chong, Ilyoung},
	c1 = {Department of Information and Communications Engineering, Hankuk University of Foreign Studies, Seoul 02450, Korea; Department of Information and Communications Engineering, Hankuk University of Foreign Studies, Seoul 02450, Korea},
	date = {2018-12-19},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/ijerph15122907},
	isbn = {1660-4601},
	journal = {International journal of environmental research and public health/International journal of environmental research and public health},
	keywords = {Affective Computing; Emotion Recognition; Neuroimaging Data Analysis; Personality Data},
	la = {en},
	number = {12},
	pages = {2907--2907},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Correlation Analysis to Identify the Effective Data in Machine Learning: Prediction of Depressive Disorder and Emotion States},
	url = {https://doi.org/10.3390/ijerph15122907},
	volume = {15},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.3390/ijerph15122907}}

@article{patelraghunandan_2021,
	abstract = {Abstract Many studies on brain--computer interface (BCI) have sought to understand the emotional state of the user to provide a reliable link between humans and machines. Advanced neuroimaging methods like electroencephalography (EEG) have enabled us to replicate and understand a wide range of human emotions more precisely. This physiological signal, i.e., EEG-based method is in stark comparison to traditional non-physiological signal-based methods and has been shown to perform better. EEG closely measures the electrical activities of the brain (a nonlinear system) and hence entropy proves to be an efficient feature in extracting meaningful information from raw brain waves. This review aims to give a brief summary of various entropy-based methods used for emotion classification hence providing insights into EEG-based emotion recognition. This study also reviews the current and future trends and discusses how emotion identification using entropy as a measure to extract features, can accomplish enhanced identification when using EEG signal.},
	author = {Patel, Pragati and Raghunandan, R and Annavarapu, Ramesh},
	c1 = {Department of Physics, School of Physical, Chemical, and Applied Sciences, Pondicherry University, Puducherry, 605014, India; Department of Physics, School of Physical, Chemical, and Applied Sciences, Pondicherry University, Puducherry, 605014, India; Department of Physics, School of Physical, Chemical, and Applied Sciences, Pondicherry University, Puducherry, 605014, India},
	date = {2021-10-05},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1186/s40708-021-00141-5},
	isbn = {2198-4026},
	journal = {Brain informatics},
	keywords = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Affective Computing; Epilepsy Detection},
	la = {en},
	number = {1},
	publisher = {Springer Science+Business Media},
	title = {EEG-based human emotion recognition using entropy as a feature extraction measure},
	url = {https://doi.org/10.1186/s40708-021-00141-5},
	volume = {8},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1186/s40708-021-00141-5}}

@article{samaragalway_2017,
	abstract = {The advancement in technology indicates that there is an opportunity to enhance human--computer interaction by way of affective state recognition. Affective state recognition is typically based on passive stimuli such as watching video clips, which does not reflect genuine interaction. This paper presents a study on affective state recognition using active stimuli, i.e. facial expressions of users when they attempt computerised tasks, particularly across typical usage of computer systems. A data collection experiment is presented for acquiring data from normal users whilst they interact with software, attempting to complete a set of predefined tasks. In addition, a hierarchical machine learning approach is presented for facial expression-based affective state recognition, which employs an Euclidean distance-based feature representation, conjointly with a customised encoding for users' self-reported affective states. Consequently, the aim is to find the potential relationship between the facial expressions, as defined by Paul Ekman, and the self-reported emotional states specified by users using Russells Circumplex model, in relation to the actual feelings and affective states. The main findings of this study suggest that facial expressions cannot precisely reveal the actual feelings of users whilst interacting with common computerised tasks. Moreover, during active interaction tasks more variation occurs within the facial expressions of participants than occurs within passive interaction.},
	author = {Samara, Anas and Galway, Leo and Bond, Raymond and Wang, Hui},
	c1 = {School of Computing, Ulster University, Belfast, BT37 0QB, UK; School of Computing, Ulster University, Belfast, BT37 0QB, UK; School of Computing, Ulster University, Belfast, BT37 0QB, UK; School of Computing, Ulster University, Belfast, BT37 0QB, UK},
	date = {2017-12-04},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1007/s12652-017-0636-8},
	isbn = {1868-5137},
	journal = {Journal of ambient intelligence \& humanized computing/Journal of ambient intelligence and humanized computing},
	keywords = {Affective Computing; Emotion Recognition; Facial Expression; Human-Computer Interaction; Face Perception},
	la = {en},
	number = {6},
	pages = {2175--2184},
	publisher = {Springer Science+Business Media},
	title = {Affective state detection via facial expression analysis within a human--computer interaction context},
	url = {https://doi.org/10.1007/s12652-017-0636-8},
	volume = {10},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1007/s12652-017-0636-8}}

@article{galvaoalarcao_2021,
	abstract = {Recognition of emotions from physiological signals, and in particular from electroencephalography (EEG), is a field within affective computing gaining increasing relevance. Although researchers have used these signals to recognize emotions, most of them only identify a limited set of emotional states (e.g., happiness, sadness, anger, etc.) and have not attempted to predict exact values for valence and arousal, which would provide a wider range of emotional states. This paper describes our proposed model for predicting the exact values of valence and arousal in a subject-independent scenario. To create it, we studied the best features, brain waves, and machine learning models that are currently in use for emotion classification. This systematic analysis revealed that the best prediction model uses a KNN regressor (K = 1) with Manhattan distance, features from the alpha, beta and gamma bands, and the differential asymmetry from the alpha band. Results, using the DEAP, AMIGOS and DREAMER datasets, show that our model can predict valence and arousal values with a low error (MAE {$<$} 0.06, RMSE {$<$} 0.16) and a strong correlation between predicted and expected values (PCC {$>$} 0.80), and can identify four emotional classes with an accuracy of 84.4{\%}. The findings of this work show that the features, brain waves and machine learning models, typically used in emotion classification tasks, can be used in more challenging situations, such as the prediction of exact values for valence and arousal.},
	author = {Galv{\~a}o, Filipe and Alarc{\~a}o, Soraia and Fonseca, Manuel},
	c1 = {LASIGE, Faculdade de Ci{\^e}ncias, Universidade de Lisboa, 1749-016 Lisboa, Portugal; LASIGE, Faculdade de Ci{\^e}ncias, Universidade de Lisboa, 1749-016 Lisboa, Portugal; LASIGE, Faculdade de Ci{\^e}ncias, Universidade de Lisboa, 1749-016 Lisboa, Portugal},
	date = {2021-05-14},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/s21103414},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Emotion Recognition; Affective Computing; Deep Learning for EEG; EEG Analysis; Epilepsy Detection},
	la = {en},
	number = {10},
	pages = {3414--3414},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Predicting Exact Valence and Arousal Values from EEG},
	url = {https://doi.org/10.3390/s21103414},
	volume = {21},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.3390/s21103414}}

@article{zhuangzeng_2018,
	abstract = {Most current approaches to emotion recognition are based on neural signals elicited by affective materials such as images, sounds and videos. However, the application of neural patterns in the recognition of self-induced emotions remains uninvestigated. In this study we inferred the patterns and neural signatures of self-induced emotions from electroencephalogram (EEG) signals. The EEG signals of 30 participants were recorded while they watched 18 Chinese movie clips which were intended to elicit six discrete emotions, including joy, neutrality, sadness, disgust, anger and fear. After watching each movie clip the participants were asked to self-induce emotions by recalling a specific scene from each movie. We analyzed the important features, electrode distribution and average neural patterns of different self-induced emotions. Results demonstrated that features related to high-frequency rhythm of EEG signals from electrodes distributed in the bilateral temporal, prefrontal and occipital lobes have outstanding performance in the discrimination of emotions. Moreover, the six discrete categories of self-induced emotion exhibit specific neural patterns and brain topography distributions. We achieved an average accuracy of 87.36{\%} in the discrimination of positive from negative self-induced emotions and 54.52{\%} in the classification of emotions into six discrete categories. Our research will help promote the development of comprehensive endogenous emotion recognition methods.},
	author = {Zhuang, Ning and Zeng, Ying and Yang, Kai and Zhang, Chi and Li, Tong and Yan, Bin},
	c1 = {China National Digital Switching System Engineering and Technological Research Center, Zhengzhou 450002, China; China National Digital Switching System Engineering and Technological Research Center, Zhengzhou 450002, China; Key Laboratory for NeuroInformation of Ministry of Education, School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu 611731, China; China National Digital Switching System Engineering and Technological Research Center, Zhengzhou 450002, China; China National Digital Switching System Engineering and Technological Research Center, Zhengzhou 450002, China; China National Digital Switching System Engineering and Technological Research Center, Zhengzhou 450002, China; China National Digital Switching System Engineering and Technological Research Center, Zhengzhou 450002, China},
	date = {2018-03-12},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3390/s18030841},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Speech Emotion; Affective Computing},
	la = {en},
	number = {3},
	pages = {841--841},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Investigating Patterns for Self-Induced Emotion Recognition from EEG Signals},
	url = {https://doi.org/10.3390/s18030841},
	volume = {18},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.3390/s18030841}}

@article{arnau-gonzlezarevalillo-herrez_2017,
	abstract = {In this paper, a novel method for affect detection is presented. The method combines both connectivity-based and channel-based features with a selection method that considerably reduces the dimensionality of the data and allows for an efficient classification. In particular, the Relative Energy (RE) and its logarithm in the spacial domain, and the Spectral Power (SP) in the frequency domain are computed for the four typical frequency bands (α, β, γand θ), and complemented with the Mutual Information measured over all channel pairs. The resulting features are then reduced by using a hybrid method that combines supervised and unsupervised feature selection. First, Welch's t-test is used to select the features that best separate the classes, and discard the ones that are less useful for classification. To this end, all features where the t-test yields a p-value above a threshold are eliminated. The remaining ones are further reduced by using Principal Component Analysis. Detection results are compared to state-of-the-art methods on DEAP, a database for emotion analysis composed of labeled recordings from 32 subjects while watching 40 music videos. The effect of using different classifiers is also evaluated, and a significant improvement is observed in all cases.},
	author = {Arnau-Gonzlez, Pablo and Arevalillo-Herrez, Miguel and Ramzan, Naeem},
	c1 = {School of Engineering and Computing, University of West of Scotland, High Street, Paisley PA1 2BE, Scotland, UK; Departament d'Inform{\`a}tica, Avda de la Universitat S/N, Universitat de Val{\`e}ncia, 46100 Burjassot, Spain; School of Engineering and Computing, University of West of Scotland, High Street, Paisley PA1 2BE, Scotland, UK},
	date = {2017-06-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1016/j.neucom.2017.03.027},
	isbn = {0925-2312},
	journal = {Neurocomputing},
	keywords = {Affective Computing; Audio Event Detection; Epilepsy Detection; Emotion Recognition; Feature Extraction},
	la = {en},
	pages = {81--89},
	publisher = {Elsevier BV},
	title = {Fusing highly dimensional energy and connectivity features to identify affective states from EEG signals},
	url = {https://doi.org/10.1016/j.neucom.2017.03.027},
	volume = {244},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1016/j.neucom.2017.03.027}}

@article{zhaoyao_2022,
	abstract = {Images can convey rich semantics and induce various emotions in viewers. Recently, with the rapid advancement of emotional intelligence and the explosive growth of visual data, extensive research efforts have been dedicated to affective image content analysis (AICA). In this survey, we will comprehensively review the development of AICA in the recent two decades, especially focusing on the state-of-the-art methods with respect to three main challenges - the affective gap, perception subjectivity, and label noise and absence. We begin with an introduction to the key emotion representation models that have been widely employed in AICA and description of available datasets for performing evaluation with quantitative comparison of label noise and dataset bias. We then summarize and compare the representative approaches on (1) emotion feature extraction, including both handcrafted and deep features, (2) learning methods on dominant emotion recognition, personalized emotion prediction, emotion distribution learning, and learning from noisy data or few labels, and (3) AICA based applications. Finally, we discuss some challenges and promising research directions in the future, such as image content and context understanding, group emotion clustering, and viewer-image interaction.},
	author = {Zhao, Sicheng and Yao, Xingxu and Yang, Jufeng and Jia, Guoli and Ding, Guiguang and Chua, Tat-Seng and Schuller, Bj{\"o}rn and Keutzer, Kurt},
	c1 = {BNRist, Tsinghua University, Beijing, China; College of Computer Science, Nankai University, Tianjin, China; College of Computer Science, Nankai University, Tianjin, China; College of Computer Science, Nankai University, Tianjin, China; BNRist, Tsinghua University, Beijing, China; School of Computing, National University of Singapore, Singapore, Singapore; Department of Computing, Imperial College London, London, U.K.; Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, Berkeley, CA, USA},
	date = {2022-10-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1109/tpami.2021.3094362},
	isbn = {0162-8828},
	journal = {IEEE transactions on pattern analysis and machine intelligence},
	keywords = {Affective Computing; Emotion Recognition},
	la = {en},
	number = {10},
	pages = {6729--6751},
	publisher = {IEEE Computer Society},
	title = {Affective Image Content Analysis: Two Decades Review and New Perspectives},
	url = {https://doi.org/10.1109/tpami.2021.3094362},
	volume = {44},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/tpami.2021.3094362}}

@article{banzigerhosoya_2015,
	abstract = {We propose to use a comprehensive path model of vocal emotion communication, encompassing encoding, transmission, and decoding processes, to empirically model data sets on emotion expression and recognition. The utility of the approach is demonstrated for two data sets from two different cultures and languages, based on corpora of vocal emotion enactment by professional actors and emotion inference by na{\"\i}ve listeners. Lens model equations, hierarchical regression, and multivariate path analysis are used to compare the relative contributions of objectively measured acoustic cues in the enacted expressions and subjective voice cues as perceived by listeners to the variance in emotion inference from vocal expressions for four emotion families (fear, anger, happiness, and sadness). While the results confirm the central role of arousal in vocal emotion communication, the utility of applying an extended path modeling framework is demonstrated by the identification of unique combinations of distal cues and proximal percepts carrying information about specific emotion families, independent of arousal. The statistical models generated show that more sophisticated acoustic parameters need to be developed to explain the distal underpinnings of subjective voice quality percepts that account for much of the variance in emotion inference, in particular voice instability and roughness. The general approach advocated here, as well as the specific results, open up new research strategies for work in psychology (specifically emotion and social perception research) and engineering and computer science (specifically research and development in the domain of affective computing, particularly on automatic emotion detection and synthetic emotion expression in avatars).},
	author = {B{\"a}nziger, Tanja and Hosoya, Georg and Scherer, Klaus},
	c1 = {Department of Psychology, Mid Sweden University, {\"O}stersund, Sweden; Department of Educational Science and Psychology, Freie Universit{\"a}t, Berlin, Germany; Swiss Centre for Affective Sciences, University of Geneva, Geneva, Switzerland},
	date = {2015-09-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1371/journal.pone.0136675},
	isbn = {1932-6203},
	journal = {PloS one},
	keywords = {Emotion Recognition; Speech Emotion; Affective Computing; Audio-Visual Speech Recognition; Facial Expression},
	la = {en},
	number = {9},
	pages = {e0136675--e0136675},
	publisher = {Public Library of Science},
	title = {Path Models of Vocal Emotion Communication},
	url = {https://doi.org/10.1371/journal.pone.0136675},
	volume = {10},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1371/journal.pone.0136675}}

@article{tarnowskikoodziej_2020,
	abstract = {This article reports the results of the study related to emotion recognition by using eye-tracking. Emotions were evoked by presenting a dynamic movie material in the form of 21 video fragments. Eye-tracking signals recorded from 30 participants were used to calculate 18 features associated with eye movements (fixations and saccades) and pupil diameter. To ensure that the features were related to emotions, we investigated the influence of luminance and the dynamics of the presented movies. Three classes of emotions were considered: high arousal and low valence, low arousal and moderate valence, and high arousal and high valence. A maximum of 80{\%} classification accuracy was obtained using the support vector machine (SVM) classifier and leave-one-subject-out validation method.},
	author = {Tarnowski, Pawe{\l} and Ko{\l}odziej, Marcin and Majkowski, Andrzej and Rak, Remigiusz},
	c1 = {Institute of Theory of Electrical Engineering, Measurement, and Information Systems, Warsaw University of Technology, Warsaw 00-662, Poland; Institute of Theory of Electrical Engineering, Measurement, and Information Systems, Warsaw University of Technology, Warsaw 00-662, Poland; Institute of Theory of Electrical Engineering, Measurement, and Information Systems, Warsaw University of Technology, Warsaw 00-662, Poland; Institute of Theory of Electrical Engineering, Measurement, and Information Systems, Warsaw University of Technology, Warsaw 00-662, Poland},
	date = {2020-09-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1155/2020/2909267},
	isbn = {1687-5265},
	journal = {Computational intelligence and neuroscience},
	keywords = {Emotion Recognition; Eye Movement Analysis; Eye Tracking; Affective Computing; Head Gesture Recognition},
	la = {en},
	pages = {1--13},
	publisher = {Hindawi Publishing Corporation},
	title = {Eye-Tracking Analysis for Emotion Recognition},
	url = {https://doi.org/10.1155/2020/2909267},
	volume = {2020},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1155/2020/2909267}}

@article{rukavinagruss_2016,
	abstract = {Affective computing aims at the detection of users' mental states, in particular, emotions and dispositions during human-computer interactions. Detection can be achieved by measuring multimodal signals, namely, speech, facial expressions and/or psychobiology. Over the past years, one major approach was to identify the best features for each signal using different classification methods. Although this is of high priority, other subject-specific variables should not be neglected. In our study, we analyzed the effect of gender, age, personality and gender roles on the extracted psychobiological features (derived from skin conductance level, facial electromyography and heart rate variability) as well as the influence on the classification results. In an experimental human-computer interaction, five different affective states with picture material from the International Affective Picture System and ULM pictures were induced. A total of 127 subjects participated in the study. Among all potentially influencing variables (gender has been reported to be influential), age was the only variable that correlated significantly with psychobiological responses. In summary, the conducted classification processes resulted in 20{\%} classification accuracy differences according to age and gender, especially when comparing the neutral condition with four other affective states. We suggest taking age and gender specifically into account for future studies in affective computing, as these may lead to an improvement of emotion recognition accuracy.},
	author = {Rukavina, Stefanie and Gruss, Sascha and Hoffmann, Holger and Tan, Jun-Wen and Walter, Steffen and Traue, Harald},
	c1 = {Department of Psychosomatic Medicine and Psychotherapy, Medical Psychology, Ulm University, Ulm, Germany; Department of Psychosomatic Medicine and Psychotherapy, Medical Psychology, Ulm University, Ulm, Germany; Department of Psychosomatic Medicine and Psychotherapy, Medical Psychology, Ulm University, Ulm, Germany; College of Teacher Education, Lishui University, Lishui, P.R. China; Department of Psychosomatic Medicine and Psychotherapy, Medical Psychology, Ulm University, Ulm, Germany; Department of Psychosomatic Medicine and Psychotherapy, Medical Psychology, Ulm University, Ulm, Germany},
	date = {2016-03-03},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1371/journal.pone.0150584},
	isbn = {1932-6203},
	journal = {PloS one},
	keywords = {Affective Computing; Emotion Recognition; Affective Design; Human-Computer Interaction; Color Psychology},
	la = {en},
	number = {3},
	pages = {e0150584--e0150584},
	publisher = {Public Library of Science},
	title = {Affective Computing and the Impact of Gender and Age},
	url = {https://doi.org/10.1371/journal.pone.0150584},
	volume = {11},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1371/journal.pone.0150584}}

@article{hanzhang_2017,
	abstract = {Automatic continuous affect recognition from audiovisual cues is arguably one of the most active research areas in machine learning. In addressing this regression problem, the advantages of the models, such as the global-optimisation capability of Support Vector Machine for Regression and the context-sensitive capability of memory-enhanced neural networks, have been frequently explored, but in an isolated way. Motivated to leverage the individual advantages of these techniques, this paper proposes and explores a novel framework, Strength Modelling, where two models are concatenated in a hierarchical framework. In doing this, the strength information of the first model, as represented by its predictions, is joined with the original features, and this expanded feature space is then utilised as the input by the successive model. A major advantage of Strength Modelling, besides its ability to hierarchically explore the strength of different machine learning algorithms, is that it can work together with the conventional feature- and decision-level fusion strategies for multimodal affect recognition. To highlight the effectiveness and robustness of the proposed approach, extensive experiments have been carried out on two time- and value-continuous spontaneous emotion databases (RECOLA and SEMAINE) using audio and video signals. The experimental results indicate that employing Strength Modelling can deliver a significant performance improvement for both arousal and valence in the unimodal and bimodal settings. The results further show that the proposed systems is competitive or outperform the other state-of-the-art approaches, but being with a simple implementation.},
	author = {Han, Jing and Zhang, Zixing and Cummins, Nicholas and Ringeval, Fabien and Schuller, Bj{\"o}rn},
	c1 = {Chair of Complex and Intelligent Systems, University of Passau, Innstr. 41, Passau 94032, Germany; Chair of Complex and Intelligent Systems, University of Passau, Innstr. 41, Passau 94032, Germany; Chair of Complex and Intelligent Systems, University of Passau, Innstr. 41, Passau 94032, Germany; Chair of Complex and Intelligent Systems, University of Passau, Innstr. 41, Passau 94032, Germany; Laboratoire d'Informatique de Grenoble, Universit{\'e}Grenoble Alpes, 700 Avenue Centrale, Grenoble 38058, France; Chair of Complex and Intelligent Systems, University of Passau, Innstr. 41, Passau 94032, Germany; Department of Computing, Imperial College London, 180 Queens' Gate, London SW7 2AZ, UK},
	date = {2017-09-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1016/j.imavis.2016.11.020},
	isbn = {0262-8856},
	journal = {Image and vision computing},
	keywords = {Feature Extraction; Affective Computing; Environmental Sound Recognition; Emotion Recognition},
	la = {en},
	pages = {76--86},
	publisher = {Elsevier BV},
	title = {Strength modelling for real-worldautomatic continuous affect recognition from audiovisual signals},
	url = {https://doi.org/10.1016/j.imavis.2016.11.020},
	volume = {65},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1016/j.imavis.2016.11.020}}

@article{aydinkaya_2016,
	abstract = {This paper illustrates the wavelet-based feature extraction for emotion assessment using electroencephalogram (EEG) signal through graphical coding design. Two-dimensional (valence--arousal) emotion model was studied. Different emotions (happy, joy, melancholy, and disgust) were studied for assessment. These emotions were stimulated by video clips. EEG signals obtained from four subjects were decomposed into five frequency bands (gamma, beta, alpha, theta, and delta) using ``db5''wavelet function. Relative features were calculated to obtain further information. Impact of the emotions according to valence value was observed to be optimal on power spectral density of gamma band. The main objective of this work is not only to investigate the influence of the emotions on different frequency bands but also to overcome the difficulties in the text-based program. This work offers an alternative approach for emotion evaluation through EEG processing. There are a number of methods for emotion recognition such as wavelet transform-based, Fourier transform-based, and Hilbert--Huang transform-based methods. However, the majority of these methods have been applied with the text-based programming languages. In this study, we proposed and implemented an experimental feature extraction with graphics-based language, which provides great convenience in bioelectrical signal processing.},
	author = {Aydin, Seda and Kaya, Turgay and G{\"u}ler, Hasan},
	c1 = {Department of Electrical-Electronics Engineering, Faculty of Engineering, University of Firat, Elazig, Turkey; Department of Electrical-Electronics Engineering, Faculty of Engineering, University of Firat, Elazig, Turkey; Department of Electrical-Electronics Engineering, Faculty of Engineering, University of Firat, Elazig, Turkey},
	date = {2016-01-21},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1007/s40708-016-0031-9},
	isbn = {2198-4026},
	journal = {Brain informatics},
	keywords = {Emotion Recognition; Affective Computing; EEG Analysis; Deep Learning for EEG; Feature Extraction},
	la = {en},
	number = {2},
	pages = {109--117},
	publisher = {Springer Science+Business Media},
	title = {Wavelet-based study of valence--arousal model of emotions on EEG signals with LabVIEW},
	url = {https://doi.org/10.1007/s40708-016-0031-9},
	volume = {3},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1007/s40708-016-0031-9}}

@article{liufang_2018,
	abstract = {Automatic emotion recognition based on electroencephalo-graphic (EEG) signals has received increasing attention in recent years. The Deep Residual Networks (ResNets) can solve vanishing gradient problem and exploding gradient problem well in computer vision and can learn more profound semantic information. And for traditional methods, frequency features often play important role in signal processing area. Thus, in this paper, we use the pre-trained ResNets to extract deep semantic information and the linear-frequency cepstral coefficients (LFCC) as features from raw EEG signals. Then the two features are fused to improve the emotion classification performance of our approach. Moreover, several classifiers are used for our fused features to evaluate the performance and it shows that the proposed approach is effective for emotion classification. We find that the best performance is achieved when use k-nearst neighbor (KNN) as classifier, and we provide a detailed discussion for the reason.},
	author = {Liu, Ningjie and Fang, Yuchun and Li, Ling and Hou, Limin and Yang, Fenglei and Guo, Yike},
	c1 = {School of Computer Engineering and Science, University of Kent; School of Computer Engineering and Science, University of Kent; School of Computer Engineering and Science, University of Kent; School of Computer Engineering and Science, University of Kent; School of Computer Engineering and Science, University of Kent; School of Computer Engineering and Science, University of Kent},
	date = {2018-04-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/icassp.2018.8462518},
	keywords = {Emotion Recognition; Deep Learning for EEG; Affective Computing; Head Gesture Recognition; EEG Analysis},
	la = {en},
	title = {Multiple Feature Fusion for Automatic Emotion Recognition Using EEG Signals},
	url = {https://doi.org/10.1109/icassp.2018.8462518},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/icassp.2018.8462518}}

@article{alial-machot_2018,
	abstract = {Machine learning approaches for human emotion recognition have recently demonstrated high performance. However, only/mostly for subject-dependent approaches, in a variety of applications like advanced driver assisted systems, smart homes and medical environments. Therefore, now the focus is shifted more towards subject-independent approaches, which are more universal and where the emotion recognition system is trained using a specific group of subjects and then tested on totally new persons and thereby possibly while using other sensors of same physiological signals in order to recognize their emotions. In this paper, we explore a novel robust subject-independent human emotion recognition system, which consists of two major models. The first one is an automatic feature calibration model and the second one is a classification model based on Cellular Neural Networks (CNN). The proposed system produces state-of-the-art results with an accuracy rate between 80{\%} and 89{\%} when using the same elicitation materials and physiological sensors brands for both training and testing and an accuracy rate of 71.05{\%} when the elicitation materials and physiological sensors brands used in training are different from those used in training. Here, the following physiological signals are involved: ECG (Electrocardiogram), EDA (Electrodermal activity) and ST (Skin-Temperature).},
	author = {Ali, Mouhannad and Al Machot, Fadi and Mosa, Ahmad and Jdeed, Midhat and Al Machot, Elyan and Kyamakya, Kyandoghere},
	c1 = {Department of Smart Systems Technologies, Alpen-Adira University, Klagenfurt 9020, Austria;; Research Center Borstel-Leibniz Center for Medicine and Biosciences, Borstel 23845, Germany;; Department of Smart Systems Technologies, Alpen-Adira University, Klagenfurt 9020, Austria;; Department of Smart Systems Technologies, Alpen-Adira University, Klagenfurt 9020, Austria;; Carl Gustav Carus Faculty of Medicine, Dresden University of Technology, Dresden 01069, Germany;; Department of Smart Systems Technologies, Alpen-Adira University, Klagenfurt 9020, Austria;},
	date = {2018-06-11},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/s18061905},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Emotion Recognition; Affective Computing; Speech Emotion},
	la = {en},
	number = {6},
	pages = {1905--1905},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {A Globally Generalized Emotion Recognition System Involving Different Physiological Signals},
	url = {https://doi.org/10.3390/s18061905},
	volume = {18},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.3390/s18061905}}

@article{nezamilou_2018,
	abstract = {This paper introduces a large-scale, validated database for Persian called Sharif Emotional Speech Database (ShEMO). The database includes 3000 semi-natural utterances, equivalent to 3 h and 25 min of speech data extracted from online radio plays. The ShEMO covers speech samples of 87 native-Persian speakers for five basic emotions including anger, fear, happiness, sadness and surprise, as well as neutral state. Twelve annotators label the underlying emotional state of utterances and majority voting is used to decide on the final labels. According to the kappa measure, the inter-annotator agreement is 64{\%} which is interpreted as ``substantial agreement''. We also present benchmark results based on common classification methods in speech emotion detection task. According to the experiments, support vector machine achieves the best results for both gender-independent (58.2{\%}) and gender-dependent models (female = 59.4{\%}, male = 57.6{\%}). The ShEMO will be available for academic purposes free of charge to provide a baseline for further research on Persian emotional speech.},
	author = {Nezami, Omid and Lou, Paria and Karami, Mansoureh},
	c1 = {Bijar Branch, Islamic Azad University, Bijar, Iran; Sharif University of Technology, Tehran, Iran; Sharif University of Technology, Tehran, Iran},
	date = {2018-10-08},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1007/s10579-018-9427-x},
	isbn = {1574-020X},
	journal = {Language resources and evaluation},
	keywords = {Speech Emotion; Affective Computing; Emotion Recognition},
	la = {en},
	number = {1},
	pages = {1--16},
	publisher = {Springer Science+Business Media},
	title = {ShEMO: a large-scale validated database for Persian speech emotion detection},
	url = {https://doi.org/10.1007/s10579-018-9427-x},
	volume = {53},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1007/s10579-018-9427-x}}

@article{nematirohani_2019,
	abstract = {Multimodal emotion recognition is an emerging interdisciplinary field of research in the area of affective computing and sentiment analysis. It aims at exploiting the information carried by signals of different nature to make emotion recognition systems more accurate. This is achieved by employing a powerful multimodal fusion method. In this study, a hybrid multimodal data fusion method is proposed in which the audio and visual modalities are fused using a latent space linear map and then, their projected features into the cross-modal space are fused with the textual modality using a Dempster-Shafer (DS) theory-based evidential fusion method. The evaluation of the proposed method on the videos of the DEAP dataset shows its superiority over both decision-level and non-latent space fusion methods. Furthermore, the results reveal that employing Marginal Fisher Analysis (MFA) for feature-level audio-visual fusion results in higher improvement in comparison to cross-modal factor analysis (CFA) and canonical correlation analysis (CCA). Also, the implementation results show that exploiting textual users' comments with the audiovisual content of movies improves the performance of the system.},
	author = {Nemati, Shahla and Rohani, Reza and Basiri, Mohammad and Abdar, Moloud and Yen, Neil and Makarenkov, Vladimir},
	c1 = {Department of Computer Engineering, Shahrekord University, Shahrekord, Iran; Department of Computer Engineering, Shahrekord University, Shahrekord, Iran; Department of Computer Engineering, Shahrekord University, Shahrekord, Iran; Department of Computer Science, Universit{\'e}du Qu{\'e}bec {\`a}Montr{\'e}al, Canada; School of Computer Science and Engineering, University of Aizu, Aizu, Japan; Department of Computer Science, Universit{\'e}du Qu{\'e}bec {\`a}Montr{\'e}al, Canada},
	date = {2019-01-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1109/access.2019.2955637},
	isbn = {2169-3536},
	journal = {IEEE access},
	keywords = {Emotion Recognition; Multisensory Integration; Multimodal Data; Affective Computing; Crossmodal Processing},
	la = {en},
	pages = {172948--172964},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {A Hybrid Latent Space Data Fusion Method for Multimodal Emotion Recognition},
	url = {https://doi.org/10.1109/access.2019.2955637},
	volume = {7},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/access.2019.2955637}}

@article{makantasisliapis_2023,
	abstract = {What if emotion could be captured in a general and subject-agnostic fashion? Is it possible, for instance, to design general-purpose representations that detect affect solely from the pixels and audio of a human-computer interaction video? In this paper we address the above questions by evaluating the capacity of deep learned representations to predict affect by relying only on audiovisual information of videos. We assume that the pixels and audio of an interactive session embed the necessary information required to detect affect. We test our hypothesis in the domain of digital games and evaluate the degree to which deep classifiers and deep preference learning algorithms can learn to predict the arousal of players based only on the video footage of their gameplay. Our results from four dissimilar games suggest that general-purpose representations can be built across games as the arousal models obtain average accuracies as high as 85{\%} using the challenging leave-one-video-out cross-validation scheme. The dissimilar audiovisual characteristics of the tested games showcase the strengths and limitations of the proposed method.},
	author = {Makantasis, Konstantinos and Liapis, Antonios and Yannakakis, Georgios},
	c1 = {Institute of Digital Games, University of Malta, Msida, Malta; Institute of Digital Games, University of Malta, Msida, Malta; Institute of Digital Games, University of Malta, Msida, Malta},
	date = {2023-01-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/taffc.2021.3060877},
	isbn = {1949-3045},
	journal = {IEEE transactions on affective computing},
	keywords = {Affective Computing; Emotion Recognition; General Game Playing; Player Modeling; Deep Learning},
	la = {en},
	number = {1},
	pages = {680--693},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {The Pixels and Sounds of Emotion: General-Purpose Representations of Arousal in Games},
	url = {https://doi.org/10.1109/taffc.2021.3060877},
	volume = {14},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/taffc.2021.3060877}}

@article{zhangali_2020,
	abstract = {Collecting accurate and precise emotion ground truth labels for mobile video watching is essential for ensuring meaningful predictions. However, video-based emotion annotation techniques either rely on post-stimulus discrete self-reports, or allow real-time, continuous emotion annotations (RCEA) only for desktop settings. Following a user-centric approach, we designed an RCEA technique for mobile video watching, and validated its usability and reliability in a controlled, indoor (N=12) and later outdoor (N=20) study. Drawing on physiological measures, interaction logs, and subjective workload reports, we show that (1) RCEA is perceived to be usable for annotating emotions while mobile video watching, without increasing users' mental workload (2) the resulting time-variant annotations are comparable with intended emotion attributes of the video stimuli (classification error for valence: 8.3{\%}; arousal: 25{\%}). We contribute a validated annotation technique and associated annotation fusion method, that is suitable for collecting fine-grained emotion annotations while users watch mobile videos.},
	author = {Zhang, Tianyi and Ali, Abdallah and Wang, Chen and Hanjalic, Alan and C{\'e}sar, Pablo},
	c1 = {Centrum Wiskunde \& Informatica and Delft University of Technology, Amsterdam \& Delft, Netherlands; Centrum Wiskunde \& Informatica (CWI), Amsterdam, Netherlands; Xinhuanet, Beijing, China; Delft University of Technology, Delft, Netherlands; Centrum Wiskunde \& Informatica and Delft University of Technology, Amsterdam \& Delft, Netherlands},
	date = {2020-04-21},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1145/3313831.3376808},
	keywords = {Emotion Recognition; Affective Computing},
	la = {en},
	title = {RCEA: Real-time, Continuous Emotion Annotation for Collecting Precise Mobile Video Ground Truth Labels},
	url = {https://doi.org/10.1145/3313831.3376808},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3313831.3376808}}

@article{candrayuwono_2015,
	abstract = {Objectively recognizing emotions is a particularly important task to ensure that patients with emotional symptoms are given the appropriate treatments. The aim of this study was to develop an emotion recognition system using Electroencephalogram (EEG) signals to identify four emotions including happy, sad, angry, and relaxed. We approached this objective by firstly investigating the relevant EEG frequency band followed by deciding the appropriate feature extraction method. Two features were considered namely: 1. Wavelet Energy, and 2. Wavelet Entropy. EEG Channels reduction was then implemented to reduce the complexity of the features. The ground truth emotional states of each subject were inferred using Russel's circumplex model of emotion, that is, by mapping the subjectively reported degrees of valence (pleasure) and arousal to the appropriate emotions - for example, an emotion with high valence and high arousal is equivalent to a `happy' emotional state, while low valence and low arousal is equivalent to a `sad' emotional state. The Support Vector Machine (SVM) classifier was then used for mapping each feature vector into corresponding discrete emotions. The results presented in this study indicated thatWavelet features extracted from alpha, beta and gamma bands seem to provide the necessary information for describing the aforementioned emotions. Using the DEAP (Dataset for Emotion Analysis using electroencephalogram, Physiological and Video Signals), our proposed method achieved an average sensitivity and specificity of 77.4{\%} $\pm$14.1{\%} and 69.1{\%} $\pm$12.8{\%}, respectively.},
	author = {Candra, Henry and Yuwono, Mitchell and Handojoseno, A. and Chai, Rifai and Su, Steven and Nguyen, Hung},
	c1 = {Centre for Health Technologies, Faculty of Engineering and Information Technology, University of Technology, Sydney, New South Wales, Australia; Centre for Health Technologies, Faculty of Engineering and Information Technology, University of Technology, Sydney, New South Wales, Australia; Centre for Health Technologies, Faculty of Engineering and Information Technology, University of Technology, Sydney, New South Wales, Australia; Centre for Health Technologies, Faculty of Engineering and Information Technology, University of Technology, Sydney, New South Wales, Australia; Centre for Health Technologies, Faculty of Engineering and Information Technology, University of Technology, Sydney, New South Wales, Australia; Centre for Health Technologies, Faculty of Engineering and Information Technology, University of Technology, Sydney, New South Wales, Australia},
	date = {2015-08-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/embc.2015.7319766},
	keywords = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Epilepsy Detection; Affective Computing},
	la = {en},
	title = {Recognizing emotions from EEG subbands using wavelet analysis},
	url = {https://doi.org/10.1109/embc.2015.7319766},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/embc.2015.7319766}}

@article{panwei_2021,
	abstract = {Emotion recognition plays an important role in the field of human-computer interaction (HCI). Automatic emotion recognition based on EEG is an important topic in brain-computer interface (BCI) applications. Currently, deep learning has been widely used in the field of EEG emotion recognition and has achieved remarkable results. However, due to the cost of data collection, most EEG datasets have only a small amount of EEG data, and the sample categories are unbalanced in these datasets. These problems will make it difficult for the deep learning model to predict the emotional state. In this paper, we propose a new sample generation method using generative adversarial networks to solve the problem of EEG sample shortage and sample category imbalance. In experiments, we explore the performance of emotion recognition with the frequency band correlation and frequency band separation computational models before and after data augmentation on standard EEG-based emotion datasets. Our experimental results show that the method of generative adversarial networks for data augmentation can effectively improve the performance of emotion recognition based on the deep learning model. And we find that the frequency band correlation deep learning model is more conducive to emotion recognition.},
	author = {Pan, Bo and Wei, Zheng},
	c1 = {School of Electronics and Information, Jiangsu University of Science and Technology, Zhenjiang 212100, China; School of Electronics and Information, Jiangsu University of Science and Technology, Zhenjiang 212100, China},
	date = {2021-10-11},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1155/2021/2520394},
	isbn = {1748-670X},
	journal = {Computational and mathematical methods in medicine},
	keywords = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing},
	la = {en},
	pages = {1--11},
	publisher = {Hindawi Publishing Corporation},
	title = {Emotion Recognition Based on EEG Using Generative Adversarial Nets and Convolutional Neural Network},
	url = {https://doi.org/10.1155/2021/2520394},
	volume = {2021},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1155/2021/2520394}}

@article{romeocavallo_2022,
	abstract = {The problem of continuous emotion recognition has been the subject of several studies. The proposed affective computing approaches employ sequential machine learning algorithms for improving the classification stage, accounting for the time ambiguity of emotional responses. Modeling and predicting the affective state over time is not a trivial problem because continuous data labeling is costly and not always feasible. This is a crucial issue in real-life applications, where data labeling is sparse and possibly captures only the most important events rather than the typical continuous subtle affective changes that occur. In this work, we introduce a framework from the machine learning literature called Multiple Instance Learning, which is able to model time intervals by capturing the presence or absence of relevant states, without the need to label the affective responses continuously (as required by standard sequential learning approaches). This choice offers a viable and natural solution for learning in a weakly supervised setting, taking into account the ambiguity of affective responses. We demonstrate the reliability of the proposed approach in a gold-standard scenario and towards real-world usage by employing an existing dataset (DEAP) and a purposely built one (Consumer). We also outline the advantages of this method with respect to standard supervised machine learning algorithms.},
	author = {Romeo, Luca and Cavallo, Andrea and Pepa, Lucia and Bianchi‐Berthouze, Nadia and Pontil, Massimiliano},
	c1 = {Cognition, Motion and Neuroscience and Computational Statistics and Machine Learning, Fondazione Istituto Italiano di Tecnologia, Genova, Italy; Department of Information Engineering, Universit{\`a}Politecnica delle Marche, Ancona, Italy; Dipartimento di Psicologia, Fondazione Istituto Italiano di Tecnologia, Universit{\`a}di Torino, and C'MoN Unit, Genoa, Italy; Department of Information Engineering, Universit{\`a}Politecnica delle Marche, Ancona, Italy; UCL Interaction Centre, University College London, London, United Kingdom; Department of Computer Science, University College London, London, United Kingdom; Fondazione Istituto Italiano di Tecnologia, Genova, Italy},
	date = {2022-01-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1109/taffc.2019.2954118},
	isbn = {1949-3045},
	journal = {IEEE transactions on affective computing},
	keywords = {Emotion Recognition; Affective Computing; Sensory Processing; Multimodal Data},
	la = {en},
	number = {1},
	pages = {389--407},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {Multiple Instance Learning for Emotion Recognition Using Physiological Signals},
	url = {https://doi.org/10.1109/taffc.2019.2954118},
	volume = {13},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/taffc.2019.2954118}}

@article{abadicorrea_2015b,
	abstract = {This paper presents a method for inferring the Positive and Negative Affect Schedule (PANAS) and the BigFive personality traits of 35 participants through the analysis of their implicit responses to 16 emotional videos. The employed modalities to record the implicit responses are (i) EEG, (ii) peripheral physiological signals (ECG, GSR), and (iii) facial landmark trajectories. The predictions of personality traits/PANAS are done using linear regression models that are trained independently on each modality. The main findings of this study are that: (i) PANAS and personality traits of individuals can be predicted based on the users' implicit responses to affective video content, (ii) ECG+GSR signals yield 70{\%}$\pm$8{\%} F1-score on the distinction between extroverts/introverts, (iii) EEG signals yield 69{\%}$\pm$6{\%} F1-score on the distinction between creative/non creative people, and finally (iv) for the prediction of agreeableness, emotional stability, and baseline affective states we achieved significantly higher than chance-level results.},
	author = {Abadi, Mojtaba and Correa, Juan and Wache, Julia and Yang, Heng and Patras, Ioannis and Sebe, Nicu},
	c1 = {Semantic, Knowledge, and Innovation Lab(SKIL), Telecom Italia; University of Trento, Italy; Queen Mary University of London, London, London, GB; University of Trento, Italy; Queen Mary University of London, UK; Queen Mary University of London, UK; University of Trento, Italy},
	date = {2015-05-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:33 +0100},
	doi = {10.1109/fg.2015.7163100},
	keywords = {Personality Data; Affective Computing; Emotion Recognition; Color Psychology; Cognitive Performance},
	la = {en},
	title = {Inference of personality traits and affect schedule by analysis of spontaneous reactions to affective videos},
	url = {https://doi.org/10.1109/fg.2015.7163100},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/fg.2015.7163100}}

@article{samaramenezes_2016,
	abstract = {The ubiquitous computing paradigm is becoming a reality; we are reaching a level of automation and computing in which people and devices interact seamlessly. However, one of the main challenges is the difficulty users have in interacting with these increasingly complex systems. Ultimately, endowing machines with the ability to perceive users' emotions will enable a more intuitive and reliable interaction. Consequently, using the electroencephalogram (EEG) as a bio-signal sensor, the affective state of a user can be modelled and subsequently utilised in order to achieve a system that can recognise and react to the users emotions. In this context, this paper investigates feature vector generation from EEG signals for the purpose of affective state modelling based on Russells Circumplex Model. Investigations are presented that aim to provide the foundation for future work in modelling user affect and interaction experiences through exploitation of different input modalities. The DEAP dataset was used within this work, along with a Support Vector Machine, which yielded reasonable classification accuracies for Valence and Arousal using feature vectors based on statistical measurements, band power from the , β, δand θwaves, and High Order Crossing of the EEG signal.},
	author = {Samara, Anas and Menezes, Maria and Galway, Leo},
	c1 = {School of Computing and Mathematics, Ulster University, Belfast, United Kingdom},
	date = {2016-12-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1109/iucc-css.2016.027},
	keywords = {Emotion Recognition; Affective Computing; Deep Learning for EEG; Feature Extraction; EEG Analysis},
	la = {en},
	title = {Feature Extraction for Emotion Recognition and Modelling Using Neurophysiological Data},
	url = {https://doi.org/10.1109/iucc-css.2016.027},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/iucc-css.2016.027}}

@article{yanchen_2019,
	abstract = {As an advanced function of the human brain, emotion has a significant influence on human studies, works, and other aspects of life. Artificial Intelligence has played an important role in recognizing human emotion correctly. EEG-based emotion recognition (ER), one application of Brain Computer Interface (BCI), is becoming more popular in recent years. However, due to the ambiguity of human emotions and the complexity of EEG signals, the EEG-ER system which can recognize emotions with high accuracy is not easy to achieve. Based on the time scale, this paper chooses the recurrent neural network as the breakthrough point of the screening model. According to the rhythmic characteristics and temporal memory characteristics of EEG, this research proposes a Rhythmic Time EEG Emotion Recognition Model (RT-ERM) based on the valence and arousal of Long-Short-Term Memory Network (LSTM). By applying this model, the classification results of different rhythms and time scales are different. The optimal rhythm and time scale of the RT-ERM model are obtained through the results of the classification accuracy of different rhythms and different time scales. Then, the classification of emotional EEG is carried out by the best time scales corresponding to different rhythms. Finally, by comparing with other existing emotional EEG classification methods, it is found that the rhythm and time scale of the model can contribute to the accuracy of RT-ERM.},
	author = {Yan, Jianzhuo and Chen, Shangbin and Deng, Sinuo},
	c1 = {Faculty of Information Technology, Beijing University of Technology, Beijing, 100124, China; Faculty of Information Technology, Beijing University of Technology, Beijing, 100124, China; Faculty of Information Technology, Beijing University of Technology, Beijing, 100124, China},
	date = {2019-09-23},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1186/s40708-019-0100-y},
	isbn = {2198-4026},
	journal = {Brain informatics},
	keywords = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Neuroimaging Data Analysis},
	la = {en},
	number = {1},
	publisher = {Springer Science+Business Media},
	title = {A EEG-based emotion recognition model with rhythm and time characteristics},
	url = {https://doi.org/10.1186/s40708-019-0100-y},
	volume = {6},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1186/s40708-019-0100-y}}

@article{nandixhafa_2021,
	abstract = {In face-to-face and online learning, emotions and emotional intelligence have an influence and play an essential role. Learners'emotions are crucial for e-learning system because they promote or restrain the learning. Many researchers have investigated the impacts of emotions in enhancing and maximizing e-learning outcomes. Several machine learning and deep learning approaches have also been proposed to achieve this goal. All such approaches are suitable for an offline mode, where the data for emotion classification are stored and can be accessed infinitely. However, these offline mode approaches are inappropriate for real-time emotion classification when the data are coming in a continuous stream and data can be seen to the model at once only. We also need real-time responses according to the emotional state. For this, we propose a real-time emotion classification system (RECS)-based Logistic Regression (LR) trained in an online fashion using the Stochastic Gradient Descent (SGD) algorithm. The proposed RECS is capable of classifying emotions in real-time by training the model in an online fashion using an EEG signal stream. To validate the performance of RECS, we have used the DEAP data set, which is the most widely used benchmark data set for emotion classification. The results show that the proposed approach can effectively classify emotions in real-time from the EEG data stream, which achieved a better accuracy and F1-score than other offline and online approaches. The developed real-time emotion classification system is analyzed in an e-learning context scenario.},
	author = {Nandi, Arijit and Xhafa, Fatos and Subirats, Laia and Fort, Santi},
	c1 = {Department of Computer Science, Universitat Polit{\`e}cnica de Catalunya (BarcelonaTech), 08034 Barcelona, Spain;; Eurecat, Centre Tecnol{\`o}gic de Catalunya, 08005 Barcelona, Spain;; Department of Computer Science, Universitat Polit{\`e}cnica de Catalunya (BarcelonaTech), 08034 Barcelona, Spain;; ADaS Lab, Universitat Oberta de Catalunya, 08018 Barcelona, Spain; Eurecat, Centre Tecnol{\`o}gic de Catalunya, 08005 Barcelona, Spain;; Eurecat, Centre Tecnol{\`o}gic de Catalunya, 08005 Barcelona, Spain;},
	date = {2021-02-25},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3390/s21051589},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Emotion Recognition; Affective Computing; Online Learning; Deep Learning for EEG; Speech Emotion},
	la = {en},
	number = {5},
	pages = {1589--1589},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Real-Time Emotion Classification Using EEG Data Stream in E-Learning Contexts},
	url = {https://doi.org/10.3390/s21051589},
	volume = {21},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.3390/s21051589}}

@article{balanmoise_2020,
	abstract = {In this paper, we investigate various machine learning classifiers used in our Virtual Reality (VR) system for treating acrophobia. The system automatically estimates fear level based on multimodal sensory data and a self-reported emotion assessment. There are two modalities of expressing fear ratings: the 2-choice scale, where 0 represents relaxation and 1 stands for fear; and the 4-choice scale, with the following correspondence: 0-relaxation, 1-low fear, 2-medium fear and 3-high fear. A set of features was extracted from the sensory signals using various metrics that quantify brain (electroencephalogram-EEG) and physiological linear and non-linear dynamics (Heart Rate-HR and Galvanic Skin Response-GSR). The novelty consists in the automatic adaptation of exposure scenario according to the subject's affective state. We acquired data from acrophobic subjects who had undergone an in vivo pre-therapy exposure session, followed by a Virtual Reality therapy and an in vivo evaluation procedure. Various machine and deep learning classifiers were implemented and tested, with and without feature selection, in both a user-dependent and user-independent fashion. The results showed a very high cross-validation accuracy on the training set and good test accuracies, ranging from 42.5{\%} to 89.5{\%}. The most important features of fear level classification were GSR, HR and the values of the EEG in the beta frequency range. For determining the next exposure scenario, a dominant role was played by the target fear level, a parameter computed by taking into account the patient's estimated fear level.},
	author = {B{\u a}lan, Oana and Moise, Gabriela and Moldoveanu, Alin and Leordeanu, Marius and Moldoveanu, Florica},
	c1 = {Faculty of Automatic Control and Computers, University POLITEHNICA of Bucharest, Bucharest 060042, Romania;; Department of Computer Science, Information Technology, Mathematics and Physics, Petroleum-Gas University of Ploiesti, Ploiesti 100680, Romania;; Faculty of Automatic Control and Computers, University POLITEHNICA of Bucharest, Bucharest 060042, Romania;; Faculty of Automatic Control and Computers, University POLITEHNICA of Bucharest, Bucharest 060042, Romania;; Faculty of Automatic Control and Computers, University POLITEHNICA of Bucharest, Bucharest 060042, Romania;},
	date = {2020-01-15},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/s20020496},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Emotion Recognition; Affective Computing; Deep Learning},
	la = {en},
	number = {2},
	pages = {496--496},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {An Investigation of Various Machine and Deep Learning Techniques Applied in Automatic Fear Level Detection and Acrophobia Virtual Therapy},
	url = {https://doi.org/10.3390/s20020496},
	volume = {20},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.3390/s20020496}}

@article{costarincon_2019,
	abstract = {This paper presents the Emotional Smart Wristband and its integration with the iGenda. The aim is to detect emotional states of a group of entities through the wristband and send the social emotion value to the iGenda so it may change the home environment and notify the caregivers. This project is advantageous to communities of elderly people, like retirement homes, where a harmonious environment is imperative and where the number of inhabitants keeps increasing. The iGenda provides the visual interface and the information center, receiving the information from the Emotional Smart Wristband and tries achieve a specific emotion (such as calm or excitement). Thus, the goal is to provide an affective system that directly interacts with humans by discreetly improving their lifestyle. In this paper, it is described the wristband in depth and the data models, and is provided an evaluation of them performed by real individuals and the validation of this evaluation.},
	author = {Costa, {\^A}ngelo and Rincon, J. and Carrascosa, Carlos and Juli{\'a}n, Vicente and Nov{\'a}is, Paulo},
	c1 = {Centro ALGORITMI, University of Minho, Braga, Portugal; D. Sistemas Inform{\'a}ticos y Computaci{\'o}n, Universitat Polit{\`e}cnica de Val{\`e}ncia, Valencia, Spain; D. Sistemas Inform{\'a}ticos y Computaci{\'o}n, Universitat Polit{\`e}cnica de Val{\`e}ncia, Valencia, Spain; D. Sistemas Inform{\'a}ticos y Computaci{\'o}n, Universitat Polit{\`e}cnica de Val{\`e}ncia, Valencia, Spain; Centro ALGORITMI, University of Minho, Braga, Portugal},
	date = {2019-03-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1016/j.future.2018.03.038},
	isbn = {0167-739X},
	journal = {Future generation computer systems},
	keywords = {Emotion Recognition; Emotions; Affective Computing},
	la = {en},
	pages = {479--489},
	publisher = {Elsevier BV},
	title = {Emotions detection on an ambient intelligent system using wearable devices},
	url = {https://doi.org/10.1016/j.future.2018.03.038},
	volume = {92},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1016/j.future.2018.03.038}}

@article{zamanianfarsi_2018,
	abstract = {Since emotion plays an important role in human life, demand and importance of automatic emotion detection have grown with increasing role of human computer interface applications. In this research, the focus is on the emotion detection from the electroencephalogram (EEG) signals. The system derives a mechanism of quantification of basic emotions using. So far, several methods have been reported, which generally use different processing algorithms, evolutionary algorithms, neural networks and classification algorithms. The aim of this paper is to develop a smart method to improve the accuracy of emotion detection by discrete signal processing techniques and applying optimized support vector machine classifier with genetic evolutionary algorithm. The obtained results show that the proposed method provides the accuracy of 93.86{\%} in detection of 4 emotions which is higher than state-of-the-art methods.},
	author = {Zamanian, Hanieh and Farsi, Hassan},
	c1 = {University of Birjand; University of Birjand},
	date = {2018-11-07},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.5565/rev/elcvia.1045},
	isbn = {1577-5097},
	journal = {ELCVIA. Electronic letters on computer vision and image analysis},
	keywords = {Emotion Recognition; Deep Learning for EEG; Affective Computing; EEG Analysis; Epilepsy Detection},
	la = {en},
	number = {1},
	pages = {29--29},
	publisher = {Computer Vision Center Press},
	title = {A New feature extraction method to Improve Emotion Detection Using EEG Signals},
	url = {https://doi.org/10.5565/rev/elcvia.1045},
	volume = {17},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.5565/rev/elcvia.1045}}

@article{arevalilloherraezcobos_2019,
	abstract = {Existing correlations between features extracted from Electroencephalography (EEG) signals and emotional aspects have motivated the development of a diversity of EEG-based affect detection methods. Both intra-subject and inter-subject approaches have been used in this context. Intra-subject approaches generally suffer from the small sample problem, and require the collection of exhaustive data for each new user before the detection system is usable. On the contrary, inter-subject models do not account for the personality and physiological influence of how the individual is feeling and expressing emotions. In this paper, we analyze both modeling approaches, using three public repositories. The results show that the subject's influence on the EEG signals is substantially higher than that of the emotion and hence it is necessary to account for the subject's influence on the EEG signals. To do this, we propose a data transformation that seamlessly integrates individual traits into an inter-subject approach, improving classification results.},
	author = {Arevalillo‐Herr{\'a}ez, Miguel and Cobos, M{\'a}ximo and Roger, Sandra and Garc{\'\i}a-Pineda, Miguel},
	c1 = {Departament d'Inform{\`a}tica, Universitat de Val{\`e}ncia, Avda. de la Universidad, s/n, 46100-Burjasot, Spain. miguel.arevalillo{\char64}uv.es.; Departament d'Inform{\`a}tica, Universitat de Val{\`e}ncia, Avda. de la Universidad, s/n, 46100-Burjasot, Spain.; Departament d'Inform{\`a}tica, Universitat de Val{\`e}ncia, Avda. de la Universidad, s/n, 46100-Burjasot, Spain.; Departament d'Inform{\`a}tica, Universitat de Val{\`e}ncia, Avda. de la Universidad, s/n, 46100-Burjasot, Spain.},
	date = {2019-07-08},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/s19132999},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Emotion Regulation; Affective Computing},
	la = {en},
	number = {13},
	pages = {2999--2999},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Combining Inter-Subject Modeling with a Subject-Based Data Transformation to Improve Affect Recognition from EEG Signals},
	url = {https://doi.org/10.3390/s19132999},
	volume = {19},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.3390/s19132999}}

@article{zhaoding_2018,
	abstract = {Emotion recognition methodologies from physiological signals are increasingly becoming personalized, due to the subjective responses of different subjects to physical stimuli. Existing works mainly focused on modelling the involved physiological corpus of each subject, without considering the psychological factors. The latent correlation among different subjects has also been rarely examined. We propose to investigate the influence of personality on emotional behavior in a hypergraph learning framework. Assuming that each vertex is a compound tuple (subject, stimuli), multi-modal hypergraphs can be constructed based on the personality correlation among different subjects and on the physiological correlation among corresponding stimuli. To reveal the different importance of vertices, hyperedges, and modalities, we assign each of them with weights. The emotion relevance learned on the vertex-weighted multi-modal multi-task hypergraphs is employed for emotion recognition. We carry out extensive experiments on the ASCERTAIN dataset and the results demonstrate the superiority of the proposed method.},
	author = {Zhao, Sicheng and Ding, Guiguang and Han, Jungong and Gao, Yue},
	c1 = {Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, USA; School of Computing and Communications, Lancaster University, UK; School of Software, Tsinghua University, China; School of Software, Tsinghua University, China; School of Computing and Communications, Lancaster University, UK; School of Software, Tsinghua University, China},
	date = {2018-07-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.24963/ijcai.2018/230},
	keywords = {Emotion Recognition; Affective Computing; Physiological Signals; Speech Emotion; Aspect-based Sentiment Analysis},
	la = {en},
	title = {Personality-Aware Personalized Emotion Recognition from Physiological Signals},
	url = {https://doi.org/10.24963/ijcai.2018/230},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.24963/ijcai.2018/230}}

@article{apicellaarpaia_2021,
	abstract = {Abstract A methodological contribution to a reproducible Measurement of Emotions for an EEG-based system is proposed. Emotional Valence detection is the suggested use case. Valence detection occurs along the interval scale theorized by the Circumplex Model of emotions. The binary choice, positive valence vs negative valence, represents a first step towards the adoption of a metric scale with a finer resolution. EEG signals were acquired through a 8-channel dry electrode cap. An implicit-more controlled EEG paradigm was employed to elicit emotional valence through the passive view of standardized visual stimuli (i.e., Oasis dataset) in 25 volunteers without depressive disorders. Results from the Self Assessment Manikin questionnaire confirmed the compatibility of the experimental sample with that of Oasis . Two different strategies for feature extraction were compared: (i) based on a-priory knowledge (i.e., Hemispheric Asymmetry Theories), and (ii) automated (i.e., a pipeline of a custom 12-band Filter Bank and Common Spatial Pattern). An average within-subject accuracy of 96.1 {\%}, was obtained by a shallow Artificial Neural Network, while k -Nearest Neighbors allowed to obtain a cross-subject accuracy equal to 80.2{\%}.},
	author = {Apicella, Andrea and Arpa{\"\i}a, Pasquale and Mastrati, Giovanna and Moccaldi, Nicola},
	c1 = {Laboratory of Augmented Reality for Health Monitoring (ARHeMLab), Department of Electrical Engineering and Information Technology, University of Naples Federico II, Naples, Italy; Laboratory of Augmented Reality for Health Monitoring (ARHeMLab), Department of Electrical Engineering and Information Technology, University of Naples Federico II, Naples, Italy; Laboratory of Augmented Reality for Health Monitoring (ARHeMLab), Department of Electrical Engineering and Information Technology, University of Naples Federico II, Naples, Italy; Laboratory of Augmented Reality for Health Monitoring (ARHeMLab), Department of Electrical Engineering and Information Technology, University of Naples Federico II, Naples, Italy},
	date = {2021-11-03},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1038/s41598-021-00812-7},
	isbn = {2045-2322},
	journal = {Scientific reports},
	keywords = {Emotion Recognition; EEG Analysis; Affective Computing; Deep Learning for EEG; Epilepsy Detection},
	la = {en},
	number = {1},
	publisher = {Nature Portfolio},
	title = {EEG-based detection of emotional valence towards a reproducible measurement of emotions},
	url = {https://doi.org/10.1038/s41598-021-00812-7},
	volume = {11},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1038/s41598-021-00812-7}}

@article{clericotiwari_2018,
	abstract = {The quantity of music content is rapidly increasing and automated affective tagging of music video clips can enable the development of intelligent retrieval, music recommendation, automatic playlist generators, and music browsing interfaces tuned to the users' current desires, preferences, or affective states. To achieve this goal, the field of affective computing has emerged, in particular the development of so-called affective brain-computer interfaces, which measure the user's affective state directly from measured brain waves using non-invasive tools, such as electroencephalography (EEG). Typically, conventional features extracted from the EEG signal have been used, such as frequency subband powers and/or inter-hemispheric power asymmetry indices. More recently, the coupling between EEG and peripheral physiological signals, such as the galvanic skin response (GSR), have also been proposed. Here, we show the importance of EEG amplitude modulations and propose several new features that measure the amplitude-amplitude cross-frequency coupling per EEG electrode, as well as linear and non-linear connections between multiple electrode pairs. When tested on a publicly available dataset of music video clips tagged with subjective affective ratings, support vector classifiers trained on the proposed features were shown to outperform those trained on conventional benchmark EEG features by as much as 6, 20, 8, and 7{\%} for arousal, valence, dominance and liking, respectively. Moreover, fusion of the proposed features with EEG-GSR coupling features showed to be particularly useful for arousal (feature-level fusion) and liking (decision-level fusion) prediction. Together, these findings show the importance of the proposed features to characterize human affective states during music clip watching.},
	author = {Clerico, Andrea and Tiwari, Abhishek and Gupta, Rishabh and Jayaraman, Srinivasan and Falk, Tiago},
	c1 = {Centre Energie, Materiaux, Telecommunications, Institut National de la Recherche Scientifique, University of Quebec, Canada; Centre Energie, Materiaux, Telecommunications, Institut National de la Recherche Scientifique, University of Quebec, Canada; Centre Energie, Materiaux, Telecommunications, Institut National de la Recherche Scientifique, University of Quebec, Canada; Centre Energie, Materiaux, Telecommunications, Institut National de la Recherche Scientifique, University of Quebec, Canada; Centre Energie, Materiaux, Telecommunications, Institut National de la Recherche Scientifique, University of Quebec, Canada},
	date = {2018-01-10},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3389/fncom.2017.00115},
	isbn = {1662-5188},
	journal = {Frontiers in computational neuroscience},
	keywords = {Affective Computing; Emotion Recognition; EEG Analysis; Deep Learning for EEG; Speech Emotion},
	la = {en},
	publisher = {Frontiers Media},
	title = {Electroencephalography Amplitude Modulation Analysis for Automated Affective Tagging of Music Video Clips},
	url = {https://doi.org/10.3389/fncom.2017.00115},
	volume = {11},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.3389/fncom.2017.00115}}

@article{keelawatthammasan_2021,
	abstract = {Emotion recognition based on electroencephalograms has become an active research area. Yet, identifying emotions using only brainwaves is still very challenging, especially the subject-independent task. Numerous studies have tried to propose methods to recognize emotions, including machine learning techniques like convolutional neural network (CNN). Since CNN has shown its potential in generalization to unseen subjects, manipulating CNN hyperparameters like the window size and electrode order might be beneficial. To our knowledge, this is the first work that extensively observed the parameter selection effect on the CNN. The temporal information in distinct window sizes was found to significantly affect the recognition performance, and CNN was found to be more responsive to changing window sizes than the support vector machine. Classifying the arousal achieved the best performance with a window size of ten seconds, obtaining 56.85{\%} accuracy and a Matthews correlation coefficient (MCC) of 0.1369. Valence recognition had the best performance with a window length of eight seconds at 73.34{\%} accuracy and an MCC value of 0.4669. Spatial information from varying the electrode orders had a small effect on the classification. Overall, valence results had a much more superior performance than arousal results, which were, perhaps, influenced by features related to brain activity asymmetry between the left and right hemispheres.},
	author = {Keelawat, Panayu and Thammasan, Nattapong and Numao, Masayuki and Kijsirikul, Boonserm},
	c1 = {Department of Computer Engineering, Chulalongkorn University, Pathum Wan, Bangkok 10330, Thailand; Department of Computer Science and Engineering, University of California San Diego, La Jolla, CA 92093-0404, USA; Human Media Interaction, Faculty of Electrical Engineering, Mathematics and Computer Science, University of Twente, 7522 NB Enschede, The Netherlands; The Institute of Scientific and Industrial Research, Osaka University, Mihogaoka, Ibaraki, Osaka 567-0047, Japan; Department of Computer Engineering, Chulalongkorn University, Pathum Wan, Bangkok 10330, Thailand},
	date = {2021-03-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/s21051678},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Emotion Recognition; Deep Learning for EEG; Affective Computing; Sensory Processing; EEG Analysis},
	la = {en},
	number = {5},
	pages = {1678--1678},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {A Comparative Study of Window Size and Channel Arrangement on EEG-Emotion Recognition Using Deep CNN},
	url = {https://doi.org/10.3390/s21051678},
	volume = {21},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.3390/s21051678}}

@article{guocandra_2017,
	abstract = {Emotion classification is one of the state-of-the-art topics in biomedical signal research, and yet a significant portion remains unknown. This paper offers a novel approach with a combined classifier to recognise human emotion states based on electroencephalogram (EEG) signal. The objective is to achieve high accuracy using the combined classifier designed, which categorises the extracted features calculated from time domain features and Discrete Wavelet Transform (DWT). Two innovative designs are involved in this project: a novel variable is established as a new feature and a combined SVM and HMM classifier is developed. The result shows that the joined features raise the accuracy by 5{\%} on valence axis and 1.5{\%} on arousal axis. The combined classifier can improve the accuracy by 3{\%} comparing with SVM classifier. One of the important applications for high accuracy emotion classification system is offering a powerful tool for psychologists to diagnose emotion related mental diseases and the system developed in this project has the potential to serve such purpose.},
	author = {Guo, Kairui and Candra, Henry and Yu, Hairong and Li, Huiqi and Nguyen, Hung and Su, Steven},
	c1 = {Faculty of Engineering and Information Technology, University of Technology, Sydney, New South Wales, Australia; Faculty of Engineering and Information Technology, University of Technology, Sydney, New South Wales, Australia; Faculty of Engineering and Information Technology, University of Technology, Sydney, New South Wales, Australia; School of Information and Electronics, Beijing Institute of Technology; Faculty of Engineering and Information Technology, University of Technology, Sydney, New South Wales, Australia; Faculty of Engineering and Information Technology, University of Technology, Sydney, New South Wales, Australia},
	date = {2017-07-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/embc.2017.8036868},
	keywords = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Epilepsy Detection},
	la = {en},
	title = {EEG-based emotion classification using innovative features and combined SVM and HMM classifier},
	url = {https://doi.org/10.1109/embc.2017.8036868},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/embc.2017.8036868}}

@article{schullerweninger_2019,
	abstract = {In this article, we review the INTERSPEECH 2013 Computational Paralinguistics ChallengE (ComParE) --the first of its kind--in light of the recent developments in affective and behavioural computing. The impact of the first ComParE instalment is manifold: first, it featured various new recognition tasks including social signals such as laughter and fillers, conflict in dyadic group discussions, and atypical communication due to pervasive developmental disorders, as well as enacted emotion; second, it marked the onset of the ComParE, subsuming all tasks investigated hitherto within the realm of computational paralinguistics; finally, besides providing a unified test-bed under well-defined and strictly comparable conditions, we present the definite feature vector used for computation of the baselines, thus laying the foundation for a successful series of follow-up Challenges. Starting with a review of the preceding INTERSPEECH Challenges, we present the four Sub-Challenges of ComParE 2013. In particular, we provide details of the Challenge databases and a meta-analysis by conducting experiments of logistic regression on single features and evaluating the performances achieved by the participants.},
	author = {Schuller, Bj{\"o}rn and Weninger, Felix and Zhang, Yue and Ringeval, Fabien and Batliner, Anton and Steidl, Stefan and Eyben, Florian and Marchi, Erik and Vinciarelli, Alessandro and Scherer, Klaus and Ch{\'e}touani, Mohamed and Mortillaro, Marcello},
	c1 = {Imperial College London; University of Augsburg; Universit{\'e}de Gen{\`e}ve = University of Geneva; Technische Universit{\"a}t Munchen - Universit{\'e}Technique de Munich {$[$}Munich, Allemagne{$]$}; Imperial College London; Technische Universit{\"a}t Munchen - Universit{\'e}Technique de Munich {$[$}Munich, Allemagne{$]$}; Groupe d'{\'E}tude en Traduction Automatique/Traitement Automatis{\'e}des Langues et de la Parole; audEERING GmbH; Friedrich-Alexander Universit{\"a}t Erlangen-N{\"u}rnberg; University of Augsburg; Friedrich-Alexander Universit{\"a}t Erlangen-N{\"u}rnberg; Associate Institute for Signal Processing {$[$}Munich{$]$}; audEERING GmbH; Technische Universit{\"a}t Munchen - Universit{\'e}Technique de Munich {$[$}Munich, Allemagne{$]$}; audEERING GmbH; University of Glasgow; Department of Psychology; Institut des Syst{\`e}mes Intelligents et de Robotique; Laboratoire des Instruments et Syst{\`e}mes d'Ile de France; Sorbonne Universit{\'e}; Universit{\'e}de Gen{\`e}ve = University of Geneva},
	date = {2019-01-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1016/j.csl.2018.02.004},
	isbn = {0885-2308},
	journal = {Computer speech \& language},
	keywords = {Affective Computing; Emotion Recognition; Human-Computer Interaction; Coping Mechanisms; Speech Emotion},
	la = {en},
	pages = {156--180},
	publisher = {Elsevier BV},
	title = {Affective and behavioural computing: Lessons learnt from the First Computational Paralinguistics Challenge},
	url = {https://doi.org/10.1016/j.csl.2018.02.004},
	volume = {53},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1016/j.csl.2018.02.004}}

@article{dziezycgjoreski_2020,
	abstract = {To further extend the applicability of wearable sensors in various domains such as mobile health systems and the automotive industry, new methods for accurately extracting subtle physiological information from these wearable sensors are required. However, the extraction of valuable information from physiological signals is still challenging---smartphones can count steps and compute heart rate, but they cannot recognize emotions and related affective states. This study analyzes the possibility of using end-to-end multimodal deep learning (DL) methods for affect recognition. Ten end-to-end DL architectures are compared on four different datasets with diverse raw physiological signals used for affect recognition, including emotional and stress states. The DL architectures specialized for time-series classification were enhanced to simultaneously facilitate learning from multiple sensors, each having their own sampling frequency. To enable fair comparison among the different DL architectures, Bayesian optimization was used for hyperparameter tuning. The experimental results showed that the performance of the models depends on the intensity of the physiological response induced by the affective stimuli, i.e., the DL models recognize stress induced by the Trier Social Stress Test more successfully than they recognize emotional changes induced by watching affective content, e.g., funny videos. Additionally, the results showed that the CNN-based architectures might be more suitable than LSTM-based architectures for affect recognition from physiological sensors.},
	author = {Dzie{\.z}yc, Maciej and Gjoreski, Martin and Kazienko, Przemys{\l}aw and Saganowski, Stanis{\l}aw and Gams, Matja{\v z}},
	c1 = {Faculty of Computer Science and Management, Wroc{\l}aw University of Science and Technology, 50-370 Wroc{\l}aw, Poland; Jo{\v z}ef Stefan Institute, 1000 Ljubljana, Slovenia; Jo{\v z}ef Stefan Postgraduate School, 1000 Ljubljana, Slovenia; Faculty of Computer Science and Management, Wroc{\l}aw University of Science and Technology, 50-370 Wroc{\l}aw, Poland; Faculty of Computer Science and Management, Wroc{\l}aw University of Science and Technology, 50-370 Wroc{\l}aw, Poland; Jo{\v z}ef Stefan Institute, 1000 Ljubljana, Slovenia; Jo{\v z}ef Stefan Postgraduate School, 1000 Ljubljana, Slovenia},
	date = {2020-11-16},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/s20226535},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Affective Computing; Emotion Recognition; Deep Learning; Pattern Discovery; Feature Extraction},
	la = {en},
	number = {22},
	pages = {6535--6535},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Can We Ditch Feature Engineering? End-to-End Deep Learning for Affect Recognition from Physiological Sensor Data},
	url = {https://doi.org/10.3390/s20226535},
	volume = {20},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.3390/s20226535}}

@article{lopesliapis_2019,
	abstract = {The feeling of horror within movies or games relies on the audience's perception of a tense atmosphere-often achieved through sound accompanied by the on-screen drama-guiding its emotional experience throughout the scene or game-play sequence. These progressions are often crafted through an a priori knowledge of how a scene or game-play sequence will playout, and the intended emotional patterns a game director wants to transmit. The appropriate design of sound becomes even more challenging once the scenery and the general context is autonomously generated by an algorithm. Towards realizing sound-based affective interaction in games this paper explores the creation of computational models capable of ranking short audio pieces based on crowdsourced annotations of tension, arousal and valence. Affect models are trained via preference learning on over a thousand annotations with the use of support vector machines, whose inputs are low-level features extracted from the audio assets of a comprehensive sound library. The models constructed in this work are able to predict the tension, arousal and valence elicited by sound, respectively, with an accuracy of approximately 65{\%}, 66{\%} and 72{\%}.},
	author = {Lopes, Phil and Liapis, Antonios and Yannakakis, Georgios},
	c1 = {Institute of Digital Games, University of Malta, Msida, MSD, Malta; Institute of Digital Games, University of Malta, Msida, MSD, Malta; Institute of Digital Games, University of Malta, Msida, MSD, Malta},
	date = {2019-04-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/taffc.2017.2695460},
	isbn = {1949-3045},
	journal = {IEEE transactions on affective computing},
	keywords = {Affective Computing; Emotion Recognition; Audio Event Detection; Speech Emotion; Acoustic Scene Classification},
	la = {en},
	number = {2},
	pages = {209--222},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {Modelling Affect for Horror Soundscapes},
	url = {https://doi.org/10.1109/taffc.2017.2695460},
	volume = {10},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/taffc.2017.2695460}}

@article{andersonhsiao_2017,
	abstract = {In the study of emotion recognition, relatively few efforts have been made to compare classification results across different emotion induction methods. In this study, we attempt to classify emotional arousal using physiological signals collected across three stimulus types -- music, videos, and games. Subjects were exposed to relaxing and exciting music and videos and then asked to play Tetris and Minesweeper. Data from GSR, ECG, EOG, EEG, and PPG signals were analyzed using machine learning algorithms. We were able to successfully detect emotion arousal over a set of contiguous multimedia activities. Furthermore, we found that the patterns of physiological response to each multimedia stimuli are varying enough, that we can guess the stimulus type just by looking at the biosignals.},
	author = {Anderson, Adam and Hsiao, Thomas and Metsis, Vangelis},
	c1 = {Dept. of Computer Science, University of Maryland, MD, USA; Dept. of Statistics Rice, University Houston, TX, USA; Dept. of Computer Science, Texas State University, TX, USA},
	date = {2017-06-21},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1145/3056540.3064956},
	keywords = {Emotion Recognition; Affective Computing; Physiological Signals; Sensory Processing; Speech Emotion},
	la = {en},
	title = {Classification of Emotional Arousal During Multimedia Exposure},
	url = {https://doi.org/10.1145/3056540.3064956},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3056540.3064956}}

@article{buitelaarwood_2018,
	abstract = {Recently, there is an increasing tendency to embed functionalities for recognizing emotions from user-generated media content in automated systems such as call-centre operations, recommendations, and assistive technologies, providing richer and more informative user and content profiles. However, to date, adding these functionalities was a tedious, costly, and time-consuming effort, requiring identification and integration of diverse tools with diverse interfaces as required by the use case at hand. The MixedEmotions Toolbox leverages the need for such functionalities by providing tools for text, audio, video, and linked data processing within an easily integrable plug-and-play platform. These functionalities include: 1) for text processing: emotion and sentiment recognition; 2) for audio processing: emotion, age, and gender recognition; 3) for video processing: face detection and tracking, emotion recognition, facial landmark localization, head pose estimation, face alignment, and body pose estimation; and 4) for linked data: knowledge graph integration. Moreover, the MixedEmotions Toolbox is open-source and free. In this paper, we present this toolbox in the context of the existing landscape, and provide a range of detailed benchmarks on standard test-beds showing its state-of-the-art performance. Furthermore, three real-world use cases show its effectiveness, namely, emotion-driven smart TV, call center monitoring, and brand reputation analysis.},
	author = {Buitelaar, Paul and Wood, I. and Negi, Sapna and Ar{\v c}an, Mihael and McCrae, John and Abele, Andrejs and Robin, C{\'e}cile and Andryushechkin, Vladimir and Ziad, Housam and Sagha, Hesam and Schmitt, Maximilian and Schuller, Bj{\"o}rn and S{\'a}nchez-Rada, J. and Iglesias, Carlos and Navarro, Carlos and Giefer, Andreas and Heise, Nicolaus and Masucci, Vincenzo and Danza, Francesco and Caterino, Ciro and Smr{\v z}, Pavel and Hradi{\v s}, Michal and Povoln{\'y}, Filip and Klimes, Marek and Mat{\v e}jka, Pavel and Tummarello, Giovanni},
	c1 = {National University of Ireland Galway, Galway, Ireland; National University of Ireland Galway, Galway, Ireland; National University of Ireland Galway, Galway, Ireland; National University of Ireland Galway, Galway, Ireland; National University of Ireland Galway, Galway, Ireland; National University of Ireland Galway, Galway, Ireland; National University of Ireland Galway, Galway, Ireland; National University of Ireland Galway, Galway, Ireland; National University of Ireland Galway, Galway, Ireland; Chair of Complex \& Intelligent Systems, University of Passau, Passau, Germany; Chair of Complex \& Intelligent Systems, University of Passau, Passau, Germany; Chair of Complex \& Intelligent Systems, University of Passau, Passau, Germany; GSI Universidad Polit-{\'e}cnica de Madrid, Madrid, Spain; GSI Universidad Polit-{\'e}cnica de Madrid, Madrid, Spain; Paradigma Digital, Madrid, Spain; Deutsche Welle, Bonn, Germany; Deutsche Welle, Bonn, Germany; Expert Systems, Modena, Italy; Expert Systems, Modena, Italy; Expert Systems, Modena, Italy; Brno University of Technology, Brno-st{\v r}ed, Czech Republic; Brno University of Technology, Brno-st{\v r}ed, Czech Republic; Phonexia, Brno-Krlovo Pole, Czech Republic; Phonexia, Brno-Krlovo Pole, Czech Republic; Phonexia, Brno-Krlovo Pole, Czech Republic; Siren Solutions, Dublin, Ireland},
	date = {2018-09-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/tmm.2018.2798287},
	isbn = {1520-9210},
	journal = {IEEE transactions on multimedia},
	keywords = {Emotion Recognition; Emotion Perception; Affective Computing; Multimodal Data; Speech Emotion},
	la = {en},
	number = {9},
	pages = {2454--2465},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {MixedEmotions: An Open-Source Toolbox for Multimodal Emotion Analysis},
	url = {https://doi.org/10.1109/tmm.2018.2798287},
	volume = {20},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/tmm.2018.2798287}}

@article{kortelainenvayrynen_2015,
	abstract = {Recent findings suggest that specific neural correlates for the key elements of basic emotions do exist and can be identified by neuroimaging techniques. In this paper, electroencephalogram (EEG) is used to explore the markers for video-induced emotions. The problem is approached from a classifier perspective: the features that perform best in classifying person's valence and arousal while watching video clips with audiovisual emotional content are searched from a large feature set constructed from the EEG spectral powers of single channels as well as power differences between specific channel pairs. The feature selection is carried out using a sequential forward floating search method and is done separately for the classification of valence and arousal, both derived from the emotional keyword that the subject had chosen after seeing the clips. The proposed classifier-based approach reveals a clear association between the increased high-frequency (15-32 Hz) activity in the left temporal area and the clips described as "pleasant" in the valence and "medium arousal" in the arousal scale. These clips represent the emotional keywords amusement and joy/happiness. The finding suggests the occurrence of a specific neural activation during video-induced pleasant emotion and the possibility to detect this from the left temporal area using EEG.},
	author = {Kortelainen, Jukka and V{\"a}yrynen, Eero and Sepp{\"a}nen, Tapio},
	c1 = {Department of Computer Science and Engineering, University of Oulu, P.O. Box 4500, 90014 Oulu, Finland; Department of Computer Science and Engineering, University of Oulu, P.O. Box 4500, 90014 Oulu, Finland; Department of Computer Science and Engineering, University of Oulu, P.O. Box 4500, 90014 Oulu, Finland},
	date = {2015-01-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1155/2015/762769},
	isbn = {1687-5265},
	journal = {Computational intelligence and neuroscience},
	keywords = {Emotion Recognition; Affective Computing; Emotion Regulation; Speech Emotion; Neural Activity},
	la = {en},
	pages = {1--14},
	publisher = {Hindawi Publishing Corporation},
	title = {High-Frequency Electroencephalographic Activity in Left Temporal Area Is Associated with Pleasant Emotion Induced by Video Clips},
	url = {https://doi.org/10.1155/2015/762769},
	volume = {2015},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1155/2015/762769}}

@article{soroushmaghooli_2018,
	abstract = {Emotion recognition is an increasingly important field of research in brain computer interactions.With the advance of technology, automatic emotion recognition systems no longer seem far-fetched. Be that as it may, detecting neural correlates of emotion has remained a substantial bottleneck. Settling this issue will be a breakthrough of significance in the literature.The current study aims to identify the correlations between different emotions and brain regions with the help of suitable electrodes. Initially, independent component analysis algorithm is employed to remove artifacts and extract the independent components. The informative channels are then selected based on the thresholded average activity value for obtained components. Afterwards, effective features are extracted from selected channels common between all emotion classes. Features are reduced using the local subset feature selection method and then fed to a new classification model using modified Dempster-Shafer theory of evidence.The presented method is employed to DEAP dataset and the results are compared to those of previous studies, which highlights the significant ability of this method to recognize emotions through electroencephalography, by the accuracy of about 91{\%}. Finally, the obtained results are discussed and new aspects are introduced.The present study addresses the long-standing challenge of finding neural correlates between human emotions and the activated brain regions. Also, we managed to solve uncertainty problem in emotion classification which is one of the most challenging issues in this field. The proposed method could be employed in other practical applications in future.},
	author = {Soroush, Morteza and Maghooli, Keivan and Setarehdan, Seyed and Nasrabadi, Ali},
	c1 = {Department of Biomedical Engineering, Science and Research Branch, Islamic Azad University, Tehran, Iran; Department of Biomedical Engineering, Science and Research Branch, Islamic Azad University, Tehran, Iran; Control and Intelligent Processing Centre of Excellence, School of Electrical and Computer Engineering, College of Engineering, University of Tehran, Tehran, Iran; Department of Biomedical Engineering, Faculty of Engineering, Shahed University, Tehran, Iran},
	date = {2018-10-31},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1186/s12993-018-0149-4},
	isbn = {1744-9081},
	journal = {Behavioral and brain functions},
	keywords = {Emotion Recognition; Affective Computing; Feature Extraction; Epilepsy Detection; Deep Learning for EEG},
	la = {en},
	number = {1},
	publisher = {BioMed Central},
	title = {A novel approach to emotion recognition using local subset feature selection and modified Dempster-Shafer theory},
	url = {https://doi.org/10.1186/s12993-018-0149-4},
	volume = {14},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1186/s12993-018-0149-4}}

@article{lisong_2017,
	abstract = {How to fuse multi-channel neurophysiological signals for emotion recognition is emerging as a hot research topic in community of Computational Psychophysiology. Nevertheless, prior feature engineering based approaches require extracting various domain knowledge related features at a high time cost. Moreover, traditional fusion method cannot fully utilise correlation information between different channels and frequency components. In this paper, we design a hybrid deep learning model, in which the 'Convolutional Neural Network (CNN)' is utilised for extracting task-related features, as well as mining inter-channel and inter-frequency correlation, besides, the 'Recurrent Neural Network (RNN)' is concatenated for integrating contextual information from the frame cube sequence. Experiments are carried out in a trial-level emotion recognition task, on the DEAP benchmarking dataset. Experimental results demonstrate that the proposed framework outperforms the classical methods, with regard to both of the emotional dimensions of Valence and Arousal.},
	author = {Li, Xiang and Song, Dawei and Zhang, Peng and Hou, Yuexian and Hu, Bin},
	c1 = {Tianjin Key Laboratory of Cognitive Computing and Application, School of Computer Science and Technology, Tianjin University, Tianjin 300350, China; School of Computing and Communications, The Open University, Milton Keynes MK76AA, UK; Tianjin Key Laboratory of Cognitive Computing and Application, School of Computer Science and Technology, Tianjin University, Tianjin 300350, China; Tianjin Key Laboratory of Cognitive Computing and Application, School of Computer Science and Technology, Tianjin University, Tianjin 300350, China; Tianjin Key Laboratory of Cognitive Computing and Application, School of Computer Science and Technology, Tianjin University, Tianjin 300350, China; The Ubiquitous Awareness and Intelligent Solutions Lab, School of Information Science and Engineering, Lanzhou University, Lanzhou 730000, China},
	date = {2017-01-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1504/ijdmb.2017.086097},
	isbn = {1748-5673},
	journal = {International journal of data mining and bioinformatics},
	keywords = {Emotion Recognition; Affective Computing; Deep Learning for EEG; Deep Learning; Signal Processing},
	la = {en},
	number = {1},
	pages = {1--1},
	publisher = {Inderscience Publishers},
	title = {Deep fusion of multi-channel neurophysiological signal for emotion recognition and monitoring},
	url = {https://doi.org/10.1504/ijdmb.2017.086097},
	volume = {18},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1504/ijdmb.2017.086097}}

@article{lanliu_2020,
	abstract = {An affective brain-computer interface (aBCI) is a direct communication pathway between human brain and computer, via which the computer tries to recognize the affective states of its user and respond accordingly. As aBCI introduces personal affective factors into human-computer interaction, it could potentially enrich the user's experience during the interaction. Successful emotion recognition plays a key role in such a system. The state-of-the-art aBCIs leverage machine learning techniques which consist in acquiring affective electroencephalogram (EEG) signals from the user and calibrating the classifier to the affective patterns of the user. Many studies have reported satisfactory recognition accuracy using this paradigm. However, affective neural patterns are volatile over time even for the same subject. The recognition accuracy cannot be maintained if the usage of aBCI prolongs without recalibration. Existing studies have overlooked the performance evaluation of aBCI during long-term use. In this paper, we propose SAFE---an EEG dataset for stable affective feature selection. The dataset includes multiple recording sessions spanning across several days for each subject. Multiple sessions across different days were recorded so that the long-term recognition performance of aBCI can be evaluated. Based on this dataset, we demonstrate that the recognition accuracy of aBCIs deteriorates when re-calibration is ruled out during long-term usage. Then, we propose a stable feature selection method to choose the most stable affective features, for mitigating the accuracy deterioration to a lesser extent and maximizing the aBCI performance in the long run. We invite other researchers to test the performance of their aBCI algorithms on this dataset, and especially to evaluate the long-term performance of their methods.},
	author = {Lan, Zirui and Liu, Yisi and Sourina, Olga and Wang, Lipo and Scherer, Reinhold and M{\"u}ller-Putz, Gernot},
	c1 = {Fraunhofer Singapore, Singapore; Nanyang Technological University, Singapore; Fraunhofer Singapore, Singapore; Fraunhofer Singapore, Singapore; Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Computer Science and Electronic Engineering, University of Essex, UK; Institute of Neural Engineering, Graz University of Technology, Graz, Austria},
	date = {2020-04-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1016/j.aei.2020.101047},
	isbn = {1474-0346},
	journal = {Advanced engineering informatics},
	keywords = {Affective Computing; Brain-Computer Interfaces; Emotion Recognition; Human-Computer Interaction; BCI Technology},
	la = {en},
	pages = {101047--101047},
	publisher = {Elsevier BV},
	title = {SAFE: An EEG dataset for stable affective feature selection},
	url = {https://doi.org/10.1016/j.aei.2020.101047},
	volume = {44},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1016/j.aei.2020.101047}}

@article{zhangtjondronegoro_2014,
	abstract = {Representation of facial expressions using continuous dimensions has shown to be inherently more expressive and psychologically meaningful than using categorized emotions, and thus has gained increasing attention over recent years.Many sub-problems have arisen in this new field that remain only partially understood.A comparison of the regression performance of different texture and geometric features and investigation of the correlations between continuous dimensional axes and basic categorized emotions are two of these.This paper presents empirical studies addressing these problems, and it reports results from an evaluation of different methods for detecting spontaneous facial expressions within the arousal-valence dimensional space (AV).The evaluation compares the performance of texture features (SIFT, Gabor, LBP) against geometric features (FAP-based distances), and the fusion of the two.It also compares the prediction of arousal and valence, obtained using the best fusion method, to the corresponding ground truths.Spatial distribution, shift, similarity, and correlation are considered for the six basic categorized emotions (i.e.anger, disgust, fear, happiness, sadness, surprise).Using the NVIE database, results show that the fusion of LBP and FAP features performs the best.The results from the NVIE and FEEDTUM databases reveal novel findings about the correlations of arousal and valence dimensions to each of six basic emotion categories.},
	author = {Zhang, Ligang and Tjondronegoro, Dian and Chandran, Vinod},
	c1 = {Faculty of Computer Science and Engineering, Xi'an University of Technology, 5 South Jinhua Road, Xi'an 710048, China; Science and Engineering Faculty, Queensland University of Technology, 2 George St, Brisbane 4000, Australia; Science and Engineering Faculty, Queensland University of Technology, 2 George St, Brisbane 4000, Australia; Science and Engineering Faculty, Queensland University of Technology, 2 George St, Brisbane 4000, Australia},
	date = {2014-12-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1016/j.imavis.2014.09.005},
	isbn = {0262-8856},
	journal = {Image and vision computing},
	keywords = {Facial Expression; Emotion Recognition; Affective Computing; Face Perception; Emotional Expressions},
	la = {en},
	number = {12},
	pages = {1067--1079},
	publisher = {Elsevier BV},
	title = {Representation of facial expression categories in continuous arousal--valence space: Feature and correlation},
	url = {https://doi.org/10.1016/j.imavis.2014.09.005},
	volume = {32},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1016/j.imavis.2014.09.005}}

@article{guptasahu_2018,
	abstract = {Often people might not be able to express themselves properly on social media, like not being able to think of appropriate words representative of their emotional state. In this paper, we propose an end to end system which aims to enhance user-input sentence according to his/her current emotional state. It works by a) detecting the emotion of the user and b) enhancing the input sentence by inserting emotive words to make the sentence more representative of the emotional state of the user. The emotional state of the user is recognized by analyzing the Electroencephalogram (EEG) signals from the brain. For text enhancement, we modify the words corresponding to the detected emotion using correlation finder scheme. Next, the verification of the sentence correctness has been performed using Long Short Term Memory (LSTM) Networks based Language Modeling framework. An accuracy of 74.95{\%} has been recorded for the classification of five emotional states in a dataset of 25 participants using EEG signals. Similarly, promising results have been obtained for the task text enhancement and overall end-to-end system. To the best of our knowledge, this work is the first of its kind to enhance text according to the emotional state detected by EEG brainwaves. The system also releases an individual from thinking and typing words, which might be a complicated procedure sometimes.},
	author = {Gupta, Akash and Sahu, Harsh and Nanecha, Nihal and Kumar, Pradeep and Roy, Partha and Chang, Victor},
	c1 = {Department of Computer Science \& Engineering, IIT Roorkee, Roorkee, India; Department of Computer Science \& Engineering, IIT Roorkee, Roorkee, India; Department of Computer Science \& Engineering, IIT Roorkee, Roorkee, India; Department of Computer Science \& Engineering, IIT Roorkee, Roorkee, India; Department of Computer Science \& Engineering, IIT Roorkee, Roorkee, India; International Business School Suzhou and Research Institute of Big Data Analytics, Xi'an Jiaotong-Liverpool University, Suzhou, China},
	date = {2018-08-08},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1007/s10723-018-9462-2},
	isbn = {1570-7873},
	journal = {Journal of grid computing},
	keywords = {Emotion Recognition; Speech Emotion; EEG Analysis; Deep Learning for EEG; Affective Computing},
	la = {en},
	number = {2},
	pages = {325--340},
	publisher = {Springer Science+Business Media},
	title = {Enhancing Text Using Emotion Detected from EEG Signals},
	url = {https://doi.org/10.1007/s10723-018-9462-2},
	volume = {17},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1007/s10723-018-9462-2}}

@article{arnau-gonzalezarevalilloherraez_2021,
	abstract = {Biometric signals have been extensively used for user identification and authentication due to their inherent characteristics that are unique to each person. The variation exhibited between the brain signals (EEG) of different people makes such signals especially suitable for biometric user identification. However, the characteristics of these signals are also influenced by the user's current condition, including his/her affective state. In this paper, we analyze the significance of the affect-related component of brain signals within the subject identification context. Consistent results are obtained across three different public datasets, suggesting that the dominant component of the signal is subject-related, but the affective state also has a contribution that affects identification accuracy. Results show that identification accuracy increases when the system has been trained with EEG recordings that refer to similar affective states as the sample that is to be identified. This improvement holds independently of the features and classification algorithm used, and it is generally above 10 percent under a rigorous setting, when the training and validation datasets do not share data from the same recording days. This finding emphasizes the potential benefits of considering affective information in applications that require subject identification, such as user authentication.},
	author = {Arnau-Gonz{\'a}lez, Pablo and Arevalillo‐Herr{\'a}ez, Miguel and Katsigiannis, Stamos and Ramzan, Naeem},
	c1 = {School of Engineering and Computing, University of the West of Scotland, Paisley, United Kingdom; Departament d'Inform{\`a}tica, Universitat de Val{\`e}ncia. Avda. de la Universidad s/n., Burjasot, Spain; School of Engineering and Computing, University of the West of Scotland, Paisley, United Kingdom; School of Engineering and Computing, University of the West of Scotland, Paisley, United Kingdom},
	date = {2021-04-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/taffc.2018.2877986},
	isbn = {1949-3045},
	journal = {IEEE transactions on affective computing},
	keywords = {Emotion Recognition; Affective Computing; Biometrics; EEG Analysis; Speech Emotion},
	la = {en},
	number = {2},
	pages = {391--401},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {On the Influence of Affect in EEG-Based Subject Identification},
	url = {https://doi.org/10.1109/taffc.2018.2877986},
	volume = {12},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/taffc.2018.2877986}}

@article{panyin_2020,
	abstract = {Emotion recognition realizing human inner perception has a very important application prospect in human-computer interaction. In order to improve the accuracy of emotion recognition, a novel method combining fused nonlinear features and team-collaboration identification strategy was proposed for emotion recognition using physiological signals. Four nonlinear features, namely approximate entropy (ApEn), sample entropy (SaEn), fuzzy entropy (FuEn) and wavelet packet entropy (WpEn) are employed to reflect emotional states deeply with each type of physiological signal. Then the features of different physiological signals are fused to represent the emotional states from multiple perspectives. Each classifier has its own advantages and disadvantages. In order to make full use of the advantages of other classifiers and avoid the limitation of single classifier, the team-collaboration model is built and the team-collaboration decision-making mechanism is designed according to the proposed team-collaboration identification strategy which is based on the fusion of support vector machine (SVM), decision tree (DT) and extreme learning machine (ELM). Through analysis, SVM is selected as the main classifier with DT and ELM as auxiliary classifiers. According to the designed decision-making mechanism, the proposed team-collaboration identification strategy can effectively employ different classification methods to make decision based on the characteristics of the samples through SVM classification. For samples which are easy to be identified by SVM, SVM directly determines the identification results, whereas SVM-DT-ELM collaboratively determines the identification results, which can effectively utilize the characteristics of each classifier and improve the classification accuracy. The effectiveness and universality of the proposed method are verified by Augsburg database and database for emotion analysis using physiological (DEAP) signals. The experimental results uniformly indicated that the proposed method combining fused nonlinear features and team-collaboration identification strategy presents better performance than the existing methods.},
	author = {Pan, Lizheng and Yin, Zeming and She, Shigang and Song, Aiguo},
	c1 = {Remote Measurement and Control Key Lab of Jiangsu Province, School of Instrument Science and Engineering, Southeast University, Nanjing 210096, China; School of Mechanical Engineering, Changzhou University, Changzhou 213164, China; School of Mechanical Engineering, Changzhou University, Changzhou 213164, China; School of Mechanical Engineering, Changzhou University, Changzhou 213164, China; Remote Measurement and Control Key Lab of Jiangsu Province, School of Instrument Science and Engineering, Southeast University, Nanjing 210096, China},
	date = {2020-04-30},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3390/e22050511},
	isbn = {1099-4300},
	journal = {Entropy},
	keywords = {Emotion Recognition; Affective Computing; Head Gesture Recognition; Physiological Signals; Neural Ensemble Physiology},
	la = {en},
	number = {5},
	pages = {511--511},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Emotional State Recognition from Peripheral Physiological Signals Using Fused Nonlinear Features and Team-Collaboration Identification Strategy},
	url = {https://doi.org/10.3390/e22050511},
	volume = {22},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.3390/e22050511}}

@article{chenhu_2017,
	abstract = {Collaboration between humans and computers has become pervasive and ubiquitous, however current computer systems are limited in that they fail to address the emotional component. An accurate understanding of human emotions is necessary for these computers to trigger proper feedback. Among multiple emotional channels, physiological signals are synchronous with emotional responses; therefore, analyzing physiological changes is a recognized way to estimate human emotions. In this paper, a three-stage decision method is proposed to recognize four emotions based on physiological signals in the multi-subject context. Emotion detection is achieved by using a stage-divided strategy in which each stage deals with a fine-grained goal.The decision method consists of three stages. During the training process, the initial stage transforms mixed training subjects to separate groups, thus eliminating the effect of individual differences. The second stage categorizes four emotions into two emotion pools in order to reduce recognition complexity. The third stage trains a classifier based on emotions in each emotion pool. During the testing process, a test case or test trial will be initially classified to a group followed by classification into an emotion pool in the second stage. An emotion will be assigned to the test trial in the final stage. In this paper we consider two different ways of allocating four emotions into two emotion pools. A comparative analysis is also carried out between the proposal and other methods.An average recognition accuracy of 77.57{\%} was achieved on the recognition of four emotions with the best accuracy of 86.67{\%} to recognize the positive and excited emotion. Using differing ways of allocating four emotions into two emotion pools, we found there is a difference in the effectiveness of a classifier on learning each emotion. When compared to other methods, the proposed method demonstrates a significant improvement in recognizing four emotions in the multi-subject context.The proposed three-stage decision method solves a crucial issue which is 'individual differences' in multi-subject emotion recognition and overcomes the suboptimal performance with respect to direct classification of multiple emotions. Our study supports the observation that the proposed method represents a promising methodology for recognizing multiple emotions in the multi-subject context.},
	author = {Chen, Jing and Hu, Bin and Wang, Yue and Moore, Philip and Dai, Yongqiang and Feng, Lei and Ding, Zhijie},
	c1 = {F. Joseph Halcomb III, M.D. Department of Biomedical Engineering, University of Kentucky, Lexington, 40506, USA; School of Information Science and Engineering, Lanzhou University, Lanzhou, 730000, China; School of Information Science and Engineering, Lanzhou University, Lanzhou, 730000, China; School of Information Science and Engineering, Lanzhou University, Lanzhou, 730000, China; School of Information Science and Engineering, Lanzhou University, Lanzhou, 730000, China; Beijing Anding Hospital of Capital Medical University, Beijing, 100088, China; The Third People's Hospital of Tianshui, Tianshui, 741020, China},
	date = {2017-12-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1186/s12911-017-0562-x},
	isbn = {1472-6947},
	journal = {BMC medical informatics and decision making},
	keywords = {Emotion Recognition; Affective Computing; Physiological Signals; Signal Processing; Human-Computer Interaction},
	la = {en},
	number = {S3},
	publisher = {BioMed Central},
	title = {Subject-independent emotion recognition based on physiological signals: a three-stage decision method},
	url = {https://doi.org/10.1186/s12911-017-0562-x},
	volume = {17},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1186/s12911-017-0562-x}}

@article{mendoza-palechormenezes_2018,
	abstract = {Emotions play an important role in human communication, interaction, and decision making processes. Therefore, considerable efforts have been made towards the automatic identification of human emotions, in particular electroencephalogram (EEG) signals and Data Mining (DM) techniques have been then used to create models recognizing the affective states of users. However, most previous works have used clinical grade EEG systems with at least 32 electrodes. These systems are expensive and cumbersome, and therefore unsuitable for usage during normal daily activities. Smaller EEG headsets such as the Emotiv are now available and can be used during daily activities. This paper investigates the accuracy and applicability of previous affective recognition methods on data collected with an Emotiv headset while participants used a personal computer to fulfill several tasks. Several features were extracted from four channels only (AF3, AF4, F3 and F4 in accordance with the 10--20 system). Both Support Vector Machine and Na{\"\i}ve Bayes were used for emotion classification. Results demonstrate that such methods can be used to accurately detect emotions using a small EEG headset during a normal daily activity.},
	author = {Mendoza-Palechor, Fabio and Menezes, Maria and Sant'Anna, Anita and Ort{\'\i}z‐Barrios, Miguel and Samara, Anas and Galway, Leo},
	c1 = {Department of Electronic and Systems Engineering, Universidad de la Costa CUC, Barranquilla, Colombia; Center for Applied Intelligent Systems Research, Halmstad University, Halmstad, Sweden; Center for Applied Intelligent Systems Research, Halmstad University, Halmstad, Sweden; Department of Industrial Management, Agroindustry and Operations, Universidad de la Costa CUC, Barranquilla, Colombia; School of Computing, Computer Science Research Institute, Ulster University, Belfast, UK; School of Computing, Computer Science Research Institute, Ulster University, Belfast, UK},
	date = {2018-09-29},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1007/s12652-018-1065-z},
	isbn = {1868-5137},
	journal = {Journal of ambient intelligence \& humanized computing/Journal of ambient intelligence and humanized computing},
	keywords = {Emotion Recognition; Deep Learning for EEG; Affective Computing; EEG Analysis; Head Gesture Recognition},
	la = {en},
	number = {10},
	pages = {3955--3974},
	publisher = {Springer Science+Business Media},
	title = {Affective recognition from EEG signals: an integrated data-mining approach},
	url = {https://doi.org/10.1007/s12652-018-1065-z},
	volume = {10},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1007/s12652-018-1065-z}}

@article{shenpeng_2021,
	abstract = {Emotion recognition has a wide range of potential applications in the real world. Among the emotion recognition data sources, electroencephalography (EEG) signals can record the neural activities across the human brain, providing us a reliable way to recognize the emotional states. Most of existing EEG-based emotion recognition studies directly concatenated features extracted from all EEG frequency bands for emotion classification. This way assumes that all frequency bands share the same importance by default; however, it cannot always obtain the optimal performance. In this paper, we present a novel multi-scale frequency bands ensemble learning (MSFBEL) method to perform emotion recognition from EEG signals. Concretely, we first re-organize all frequency bands into several local scales and one global scale. Then we train a base classifier on each scale. Finally we fuse the results of all scales by designing an adaptive weight learning method which automatically assigns larger weights to more important scales to further improve the performance. The proposed method is validated on two public data sets. For the ``SEED IV''data set, MSFBEL achieves average accuracies of 82.75{\%}, 87.87{\%}, and 78.27{\%} on the three sessions under the within-session experimental paradigm. For the ``DEAP''data set, it obtains average accuracy of 74.22{\%} for four-category classification under 5-fold cross validation. The experimental results demonstrate that the scale of frequency bands influences the emotion recognition rate, while the global scale that directly concatenating all frequency bands cannot always guarantee to obtain the best emotion recognition performance. Different scales provide complementary information to each other, and the proposed adaptive weight learning method can effectively fuse them to further enhance the performance.},
	author = {Shen, Fangyao and Peng, Yong and Kong, Wanzeng and Dai, Guojun},
	c1 = {School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou 310018, China; MoE Key Laboratory of Advanced Perception and Intelligent Control of High-End Equipment, Anhui Polytechnic University, Wuhu 241000, China; School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou 310018, China; Key Laboratory of Brain Machine Collaborative Intelligence of Zhejiang Province, Hangzhou 310018, China; School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou 310018, China; Key Laboratory of Brain Machine Collaborative Intelligence of Zhejiang Province, Hangzhou 310018, China; School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou 310018, China},
	date = {2021-02-10},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3390/s21041262},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Neural Ensemble Physiology},
	la = {en},
	number = {4},
	pages = {1262--1262},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Multi-Scale Frequency Bands Ensemble Learning for EEG-Based Emotion Recognition},
	url = {https://doi.org/10.3390/s21041262},
	volume = {21},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.3390/s21041262}}

@article{nandixhafa_2022,
	abstract = {Emotional and physical health are strongly connected and should be taken care of simultaneously to ensure completely healthy persons. A person's emotional health can be determined by detecting emotional states from various physiological measurements (EDA, RB, EEG, etc.). Affective Computing has become the field of interest, which uses software and hardware to detect emotional states. In the IoT era, wearable sensor-based real-time multi-modal emotion state classification has become one of the hottest topics. In such setting, a data stream is generated from wearable-sensor devices, data accessibility is restricted to those devices only and usually a high data generation rate should be processed to achieve real-time emotion state responses. Additionally, protecting the users' data privacy makes the processing of such data even more challenging. Traditional classifiers have limitations to achieve high accuracy of emotional state detection under demanding requirements of decentralized data and protecting users' privacy of sensitive information as such classifiers need to see all data. Here comes the federated learning, whose main idea is to create a global classifier without accessing the users' local data. Therefore, we have developed a federated learning framework for real-time emotion state classification using multi-modal physiological data streams from wearable sensors, called Fed-ReMECS. The main findings of our Fed-ReMECS framework are the development of an efficient and scalable real-time emotion classification system from distributed multimodal physiological data streams, where the global classifier is built without accessing (privacy protection) the users' data in an IoT environment. The experimental study is conducted using the popularly used multi-modal benchmark DEAP dataset for emotion classification. The results show the effectiveness of our developed approach in terms of accuracy, efficiency, scalability and users' data privacy protection.},
	author = {Nandi, Arijit and Xhafa, Fatos},
	c1 = {Eurecat, Centre Tecnol{\`o}gic de Catalunya, 08005 Barcelona, Spain; Universitat Polit{\`e}cnica de Catalunya (BarcelonaTech), 08034 Barcelona, Spain; Universitat Polit{\`e}cnica de Catalunya (BarcelonaTech), 08034 Barcelona, Spain},
	date = {2022-08-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1016/j.ymeth.2022.03.005},
	isbn = {1046-2023},
	journal = {Methods},
	keywords = {Affective Computing; Emotion Recognition; Emotion Regulation; Speech Emotion; Multimodal Data},
	la = {en},
	pages = {340--347},
	publisher = {Elsevier BV},
	title = {A federated learning method for real-time emotion state classification from multi-modal streaming},
	url = {https://doi.org/10.1016/j.ymeth.2022.03.005},
	volume = {204},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1016/j.ymeth.2022.03.005}}

@article{kapucuklc_2018,
	abstract = {The present study combined dimensional and categorical approaches to emotion to develop normative ratings for a large set of Turkish words on two major dimensions of emotion: arousal and valence, as well as on five basic emotion categories of happiness, sadness, anger, fear, and disgust. A set of 2031 Turkish words obtained by translating Affective Norms for English Words to Turkish and pooling from the Turkish Word Norms were rated by a large sample of 1527 participants. This is the first comprehensive and standardized word set in Turkish offering discrete emotional ratings in addition to dimensional ratings along with concreteness judgments. Consistent with Affective Norms for English Words and word databases in several other languages, arousal increased as valence became more positive or more negative. As expected, negative emotions (anger, sadness, fear, and disgust) were positively correlated with each other, whereas the positive emotion, happiness, was negatively correlated with the negative emotion categories. Data further showed that the valence dimension was strongly correlated with happiness, and the arousal dimension was mostly correlated with fear. These findings show highly similar and consistent patterns with word sets provided in other languages in terms of the relationships between arousal and valence dimensions, relationships between dimensions and specific emotion categories, relationships among specific emotions, and further support the stability of the relationship between basic discrete emotions at the word level across different cultures.},
	author = {Kapucu, Aycan and Kılı{\c c}, Aslı and {\"O}zkılı{\c c}, Yıldız and Sarıbaz, Bengisu},
	c1 = {Department of Psychology, Ege University, {\.I}zmir, Turkey; Department of Psychology, Middle East Technical University, Ankara, Turkey; Department of Psychology, Uludag University, Bursa, Turkey; Department of Psychology, Ege University, {\.I}zmir, Turkey},
	date = {2018-12-04},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1177/0033294118814722},
	isbn = {0033-2941},
	journal = {Psychological reports},
	keywords = {Emotion Recognition; Emotion Regulation; Affective Computing},
	la = {en},
	number = {1},
	pages = {188--209},
	publisher = {SAGE Publishing},
	title = {Turkish Emotional Word Norms for Arousal, Valence, and Discrete Emotion Categories},
	url = {https://doi.org/10.1177/0033294118814722},
	volume = {124},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1177/0033294118814722}}

@article{mougunes_2016,
	abstract = {Automatic affect analysis and understanding has become a well established research area in the last two decades. However, little attention has been paid to the analysis of the affect expressed in group settings, either in the form of affect expressed by the whole group collectively or affect expressed by each individual member of the group. This paper presents a framework which, in group settings automatically classifies the affect expressed by each individual group member along both arousal and valence dimensions. We first introduce a novel Volume Quantised Local Zernike Moments Fisher Vectors (vQLZM-FV) descriptor to represent the facial behaviours of individuals in the spatio-temporal domain and then propose a method to recognize the group membership of each individual (i.e., which group the individual in question is part of) by using their face and body behavioural cues. We conduct a set of experiments on a newly collected dataset that contains fourteen recordings of four groups, each consisting of four people watching affective movie stimuli. Our experimental results show that (1) the proposed vQLZM-FV outperforms the other feature representations in affect recognition, and (2) group membership can be recognized using the non-verbal face and body features, indicating that individuals influence each other's behaviours within a group setting.},
	author = {Mou, Wenxuan and G{\"u}ne{\c s}, Hatice and Patras, Ioannis},
	c1 = {Queen Mary University of London, UK; University of Cambridge, Cambridge, UK; Queen Mary University of London, UK},
	date = {2016-06-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1109/cvprw.2016.185},
	keywords = {Facial Expression Analysis; Affective Computing; Emotion Recognition; Feature Learning; Facial Landmark Detection},
	la = {en},
	title = {Automatic Recognition of Emotions and Membership in Group Videos},
	url = {https://doi.org/10.1109/cvprw.2016.185},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/cvprw.2016.185}}

@article{shaosnyder_2020,
	abstract = {To effectively communicate with people, social robots must be capable of detecting, interpreting, and responding to human affect during human--robot interactions (HRIs). In order to accurately detect user affect during HRIs, affect elicitation techniques need to be developed to create and train appropriate affect detection models. In this paper, we present such a novel affect elicitation and detection method for social robots in HRIs. Non-verbal emotional behaviors of the social robot were designed to elicit user affect, which was directly measured through electroencephalography (EEG) signals. HRI experiments with both younger and older adults were conducted to evaluate our affect elicitation technique and compare the two types of affect detection models we developed and trained utilizing multilayer perceptron neural networks (NNs) and support vector machines (SVMs). The results showed that; on average, the self-reported valence and arousal were consistent with the intended elicited affect. Furthermore, it was also noted that the EEG data obtained could be used to train affect detection models with the NN models achieving higher classification rates},
	author = {Shao, Mingyang and Snyder, Matt and Nejat, Goldie and Benhabib, B.},
	c1 = {Department of Mechanical and Industrial Engineering, University of Toronto, Toronto, ON M5S 3G8, Canada;; Yee Hong Centre for Geriatric Care, Mississauga, ON L5V 2X5, Canada;; Department of Mechanical and Industrial Engineering, University of Toronto, Toronto, ON M5S 3G8, Canada;; Department of Mechanical and Industrial Engineering, University of Toronto, Toronto, ON M5S 3G8, Canada;},
	date = {2020-06-03},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3390/robotics9020044},
	isbn = {2218-6581},
	journal = {Robotics},
	keywords = {Emotion Recognition; Human Perception of Robots; Affective Computing; Human-Robot Interaction; Emotion Perception},
	la = {en},
	number = {2},
	pages = {44--44},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {User Affect Elicitation with a Socially Emotional Robot},
	url = {https://doi.org/10.3390/robotics9020044},
	volume = {9},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.3390/robotics9020044}}

@article{phankim_2021,
	abstract = {Besides facial or gesture-based emotion recognition, Electroencephalogram (EEG) data have been drawing attention thanks to their capability in countering the effect of deceptive external expressions of humans, like faces or speeches. Emotion recognition based on EEG signals heavily relies on the features and their delineation, which requires the selection of feature categories converted from the raw signals and types of expressions that could display the intrinsic properties of an individual signal or a group of them. Moreover, the correlation or interaction among channels and frequency bands also contain crucial information for emotional state prediction, and it is commonly disregarded in conventional approaches. Therefore, in our method, the correlation between 32 channels and frequency bands were put into use to enhance the emotion prediction performance. The extracted features chosen from the time domain were arranged into feature-homogeneous matrices, with their positions following the corresponding electrodes placed on the scalp. Based on this 3D representation of EEG signals, the model must have the ability to learn the local and global patterns that describe the short and long-range relations of EEG channels, along with the embedded features. To deal with this problem, we proposed the 2D CNN with different kernel-size of convolutional layers assembled into a convolution block, combining features that were distributed in small and large regions. Ten-fold cross validation was conducted on the DEAP dataset to prove the effectiveness of our approach. We achieved the average accuracies of 98.27{\%} and 98.36{\%} for arousal and valence binary classification, respectively.},
	author = {Phan, Tran-Dac-Thinh and Kim, Soo-Hyung and Yang, Hyung-Jeong and Lee, Guee-Sang},
	c1 = {Department of Artificial Intelligence Convergence, Chonnam National University, 77 Yongbong-ro, Gwangju 500-757, Korea;; Department of Artificial Intelligence Convergence, Chonnam National University, 77 Yongbong-ro, Gwangju 500-757, Korea;; Department of Artificial Intelligence Convergence, Chonnam National University, 77 Yongbong-ro, Gwangju 500-757, Korea;; Department of Artificial Intelligence Convergence, Chonnam National University, 77 Yongbong-ro, Gwangju 500-757, Korea;},
	date = {2021-07-27},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3390/s21155092},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Affective Computing; Head Gesture Recognition},
	la = {en},
	number = {15},
	pages = {5092--5092},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {EEG-Based Emotion Recognition by Convolutional Neural Network with Multi-Scale Kernels},
	url = {https://doi.org/10.3390/s21155092},
	volume = {21},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.3390/s21155092}}

@article{tiwarifalk_2019,
	abstract = {Emotion recognition is a burgeoning field allowing for more natural human-machine interactions and interfaces. Electroencephalography (EEG) has shown to be a useful modality with which user emotional states can be measured and monitored, particularly primitives such as valence and arousal. In this paper, we propose the use of ordinal pattern analysis, also called motifs, for improved EEG-based emotion recognition. Motifs capture recurring structures in time series and are inherently robust to noise, thus are well suited for the task at hand. Several connectivity, asymmetry, and graph-theoretic features are proposed and extracted from the motifs to be used for affective state recognition. Experiments with a widely used public database are conducted, and results show the proposed features outperforming benchmark spectrum-based features, as well as other more recent nonmotif-based graph-theoretic features and amplitude modulation-based connectivity/asymmetry measures. Feature and score-level fusion suggest complementarity between the proposed and benchmark spectrum-based measures. When combined, the fused models can provide up to 9{\%} improvement relative to benchmark features alone and up to 16{\%} to nonmotif-based graph-theoretic features.},
	author = {Tiwari, Abhishek and Falk, Tiago},
	c1 = {Institut National de la Research Scientifique, Universit{\'e}du Qu{\'e}bec, Montr{\'e}al, Qu{\'e}bec, Canada; Institut National de la Research Scientifique, Universit{\'e}du Qu{\'e}bec, Montr{\'e}al, Qu{\'e}bec, Canada},
	date = {2019-01-17},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1155/2019/3076324},
	isbn = {1687-5265},
	journal = {Computational intelligence and neuroscience},
	keywords = {Emotion Recognition; EEG Analysis; Emotion Regulation; Affective Computing; Deep Learning for EEG},
	la = {en},
	pages = {1--14},
	publisher = {Hindawi Publishing Corporation},
	title = {Fusion of Motif- and Spectrum-Related Features for Improved EEG-Based Emotion Recognition},
	url = {https://doi.org/10.1155/2019/3076324},
	volume = {2019},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1155/2019/3076324}}

@article{zhangwen_2020,
	abstract = {The increasing interest in the effects of emotion on cognitive, social, and neural processes creates a constant need for efficient and reliable techniques for emotion elicitation. Emotions are important in many areas, especially in advertising design and video production. The impact of emotions on the audience plays an important role. This paper analyzes the physical elements in a two-dimensional emotion map by extracting the physical elements of a video (color, light intensity, sound, etc.). We used k-nearest neighbors (K-NN), support vector machine (SVM), and multilayer perceptron (MLP) classifiers in the machine learning method to accurately predict the four dimensions that express emotions, as well as summarize the relationship between the two-dimensional emotion space and physical elements when designing and producing video.},
	author = {Zhang, Jing and Wen, Xingyu and Whang, Mincheol},
	c1 = {Department of Emotion Engineering, University of Sangmyung, Seoul 03016, Korea.; Department of Emotion Engineering, University of Sangmyung, Seoul 03016, Korea.; Department of Human-Centered Artificial Intelligence, University of Sangmyung, Seoul 03016, Korea.},
	date = {2020-01-24},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3390/s20030649},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Emotion Recognition; Affective Computing; Emotions; Feature Extraction; Speech Emotion},
	la = {en},
	number = {3},
	pages = {649--649},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Recognition of Emotion According to the Physical Elements of the Video},
	url = {https://doi.org/10.3390/s20030649},
	volume = {20},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.3390/s20030649}}

@article{saganowskikomoszynska_2022,
	abstract = {Abstract The Emognition dataset is dedicated to testing methods for emotion recognition (ER) from physiological responses and facial expressions. We collected data from 43 participants who watched short film clips eliciting nine discrete emotions: amusement, awe, enthusiasm, liking, surprise, anger, disgust, fear, and sadness. Three wearables were used to record physiological data: EEG, BVP (2x), HR, EDA, SKT, ACC (3x), and GYRO (2x); in parallel with the upper-body videos. After each film clip, participants completed two types of self-reports: (1) related to nine discrete emotions and (2) three affective dimensions: valence, arousal, and motivation. The obtained data facilitates various ER approaches, e.g., multimodal ER, EEG- vs. cardiovascular-based ER, discrete to dimensional representation transitions. The technical validation indicated that watching film clips elicited the targeted emotions. It also supported signals'high quality.},
	author = {Saganowski, Stanis{\l}aw and Komoszy{\'n}ska, Joanna and Behnke, Maciej and Perz, Bartosz and Kunc, Dominika and Klich, Bart{\l}omiej and Kaczmarek, {\L}ukasz and Kazienko, Przemys{\l}aw},
	c1 = {Wroclaw University of Science and Technology, Faculty of Information and Communication Technology, Department of Artificial Intelligence, Wroc{\l}aw, 50-370, Poland; Wroclaw University of Science and Technology, Faculty of Information and Communication Technology, Department of Artificial Intelligence, Wroc{\l}aw, 50-370, Poland; Wroclaw University of Science and Technology, Faculty of Information and Communication Technology, Department of Artificial Intelligence, Wroc{\l}aw, 50-370, Poland; Wroclaw University of Science and Technology, Faculty of Information and Communication Technology, Department of Artificial Intelligence, Wroc{\l}aw, 50-370, Poland; Wroclaw University of Science and Technology, Faculty of Information and Communication Technology, Department of Artificial Intelligence, Wroc{\l}aw, 50-370, Poland; Wroclaw University of Science and Technology, Faculty of Information and Communication Technology, Department of Artificial Intelligence, Wroc{\l}aw, 50-370, Poland; Adam Mickiewicz University, Faculty of Psychology and Cognitive Science, Poznan, 61-664, Poland; Wroclaw University of Science and Technology, Faculty of Information and Communication Technology, Department of Artificial Intelligence, Wroc{\l}aw, 50-370, Poland},
	date = {2022-04-07},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1038/s41597-022-01262-0},
	isbn = {2052-4463},
	journal = {Scientific data},
	keywords = {Emotion Recognition; Affective Computing; Emotion Regulation; Speech Emotion; EEG Analysis},
	la = {en},
	number = {1},
	publisher = {Nature Portfolio},
	title = {Emognition dataset: emotion recognition with self-reports, facial expressions, and physiology using wearables},
	url = {https://doi.org/10.1038/s41597-022-01262-0},
	volume = {9},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1038/s41597-022-01262-0}}

@article{liyan_2017,
	abstract = {With the rapid development of computer technology, pervasive computing and wearable devices, EEG-based emotion recognition has gradually attracted much attention in affecting computing (AC) domain. In this paper, we propose an approach of emotion recognition using EEG signals based on the weighted fusion of multiple base classifiers. These base classifiers based on SVM are constructed using a channel division mechanism according to the neuropsychological theory that different brain areas are differ in processing intensity of emotional information. The outputs of channel base classifiers are integrated by a weighted fusion strategy which is based on the confidence estimation on each emotional label by each base classifier. The evaluation on the DEAP dataset shows that our proposed multiple classifiers fusion method outperforms individual channel base classifiers and the feature fusion method for EEG-based emotion recognition.},
	author = {Li, Xian and Yan, Jianzhuo and Chen, Jianhui},
	c1 = {Faculty of Information Technology, Beijing University of Technology, Beijing 100124, China; Faculty of Information Technology, Beijing University of Technology, Beijing 100124, China; Faculty of Information Technology, Beijing University of Technology, Beijing 100124, China},
	date = {2017-01-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1051/itmconf/20171107006},
	isbn = {2271-2097},
	journal = {ITM web of conferences},
	keywords = {Emotion Recognition; Deep Learning for EEG; Affective Computing; EEG Analysis; Head Gesture Recognition},
	la = {en},
	pages = {07006--07006},
	publisher = {EDP Sciences},
	title = {Channel Division Based Multiple Classifiers Fusion for Emotion Recognition Using EEG signals},
	url = {https://doi.org/10.1051/itmconf/20171107006},
	volume = {11},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1051/itmconf/20171107006}}

@article{valderasbolea_2019,
	abstract = {Interest in emotion recognition has increased in recent years as a useful tool for diagnosing psycho-neural illnesses. In this study, the auto-mutual and the cross-mutual information function, AMIF and CMIF respectively, are used for human emotion recognition.The AMIF technique was applied to heart rate variability (HRV) signals to study complex interdependencies, and the CMIF technique was considered to quantify the complex coupling between HRV and respiratory signals. Both algorithms were adapted to short-term RR time series. Traditional band pass filtering was applied to the RR series at low frequency (LF) and high frequency (HF) bands, and a respiration-based filter bandwidth was also investigated ({$[$}Formula: see text{$]$}). Both the AMIF and the CMIF algorithms were calculated with regard to different time scales as specific complexity measures. The ability of the parameters derived from the AMIF and the CMIF to discriminate emotions was evaluated on a database of video-induced emotion elicitation. Five elicited states i.e. relax (neutral), joy (positive valence), as well as fear, sadness and anger (negative valences) were considered.The results revealed that the AMIF applied to the RR time series filtered in the {$[$}Formula: see text{$]$} band was able to discriminate between the following: relax and joy and fear, joy and each negative valence conditions, and finally fear and sadness and anger, all with a statistical significance level p -value {$[$}Formula: see text{$]$} 0.05, sensitivity, specificity and accuracy higher than 70{\%} and area under the receiver operating characteristic curve index AUC {$[$}Formula: see text{$]$}0.70. Furthermore, the parameters derived from the AMIF and the CMIF allowed the low signal complexity presented during fear to be characterized in front of any of the studied elicited states.Based on these results, human emotion manifested in the HRV and respiratory signal responses could be characterized by means of the information-content complexity.},
	author = {Valderas, Mar{\'\i}a and Bolea, Juan and Laguna, Pablo and Bail{\'o}n, Raquel and Vallverd{\'u}, Montserrat},
	c1 = {Arag{\'o}n Institute for Engineering Research (I3A), IIS Arag{\'o}n, University of Zaragoza, 50018 Zaragoza, Spain; Biomedical Signal Interpretation and Computational Simulation (BSICoS), Arag{\'o}n Institute for Engineering Research (I3A), IIS Arag{\'o}n, University of Zaragoza, Spain, Mar{\'\i}a de Luna, 1, 50015 Zaragoza, Spain; Department ESAII, Centre for Biomedical Engineering Research, Universitat Polit{\`e}cnica de Catalunya, Barcelona, 08028, Spain; Biomedical Signal Interpretation and Computational Simulation (BSICoS), Arag{\'o}n Institute for Engineering Research (I3A), IIS Arag{\'o}n, University of Zaragoza, Spain, Mar{\'\i}a de Luna, 1, 50015 Zaragoza, Spain; CIBER de Bioingenier{\'\i}a, Biomateriales y Nanomedicina (CIBER-BBN), Madrid, Spain; Biomedical Signal Interpretation and Computational Simulation (BSICoS), Arag{\'o}n Institute for Engineering Research (I3A), IIS Arag{\'o}n, University of Zaragoza, Spain, Mar{\'\i}a de Luna, 1, 50015 Zaragoza, Spain; CIBER de Bioingenier{\'\i}a, Biomateriales y Nanomedicina (CIBER-BBN), Madrid, Spain; Biomedical Signal Interpretation and Computational Simulation (BSICoS), Arag{\'o}n Institute for Engineering Research (I3A), IIS Arag{\'o}n, University of Zaragoza, Spain, Mar{\'\i}a de Luna, 1, 50015 Zaragoza, Spain; CIBER de Bioingenier{\'\i}a, Biomateriales y Nanomedicina (CIBER-BBN), Madrid, Spain; CIBER de Bioingenier{\'\i}a, Biomateriales y Nanomedicina (CIBER-BBN), Madrid, Spain; Department ESAII, Centre for Biomedical Engineering Research, Universitat Polit{\`e}cnica de Catalunya, Barcelona, 08028, Spain},
	date = {2019-09-03},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1088/1361-6579/ab310a},
	isbn = {0967-3334},
	journal = {Physiological measurement},
	keywords = {Heart Rate Variability; Emotion Recognition; Affective Computing; Emotion Regulation; Human-Computer Interaction},
	la = {en},
	number = {8},
	pages = {084001--084001},
	publisher = {IOP Publishing},
	title = {Mutual information between heart rate variability and respiration for emotion characterization},
	url = {https://doi.org/10.1088/1361-6579/ab310a},
	volume = {40},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1088/1361-6579/ab310a}}

@article{kongshao_2021,
	abstract = {Emotion recognition, as a challenging and active research area, has received considerable awareness in recent years. In this study, an attempt was made to extract complex network features from electroencephalogram (EEG) signals for emotion recognition. We proposed a novel method of constructing forward weighted horizontal visibility graphs (FWHVG) and backward weighted horizontal visibility graphs (BWHVG) based on angle measurement. The two types of complex networks were used to extract network features. Then, the two feature matrices were fused into a single feature matrix to classify EEG signals. The average emotion recognition accuracies based on complex network features of proposed method in the valence and arousal dimension were 97.53{\%} and 97.75{\%}. The proposed method achieved classification accuracies of 98.12{\%} and 98.06{\%} for valence and arousal when combined with time-domain features.},
	author = {Kong, Tianjiao and Shao, Jie and Hu, Jiuyuan and Yang, Xin and Yang, Shiyiling and Malekian, Reza},
	c1 = {College of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing 210016, China; Key Laboratory of Underwater Acoustic Signal Processing, Ministry of Education, Southeast University, Nanjing 210096, China; College of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing 210016, China; Key Laboratory of Underwater Acoustic Signal Processing, Ministry of Education, Southeast University, Nanjing 210096, China; College of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing 210016, China; Key Laboratory of Underwater Acoustic Signal Processing, Ministry of Education, Southeast University, Nanjing 210096, China; College of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing 210016, China; Key Laboratory of Underwater Acoustic Signal Processing, Ministry of Education, Southeast University, Nanjing 210096, China; College of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing 210016, China; Key Laboratory of Underwater Acoustic Signal Processing, Ministry of Education, Southeast University, Nanjing 210096, China; Department of Computer Science and Media Technology, Malm{\"o}University, 20506 Malm{\"o}, Sweden},
	date = {2021-03-07},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/s21051870},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Head Gesture Recognition; Affective Computing},
	la = {en},
	number = {5},
	pages = {1870--1870},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {EEG-Based Emotion Recognition Using an Improved Weighted Horizontal Visibility Graph},
	url = {https://doi.org/10.3390/s21051870},
	volume = {21},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.3390/s21051870}}

@article{abaalkhailguthier_2018,
	abstract = {Human behavior is impacted by emotion, mood, personality, needs and subjective well-being. Emotion and mood are human affective states while personality, needs and subjective well-being are influences on those affective states. Ontologies are a method of representing real-world knowledge, such as h uman affective states and their influences, in a format that a computer can process. They allow researchers to build systems that harness affective states. By unifying terms and meanings, ontologies enable these systems to communicate and share knowledge with each other. In this paper, we survey existing ontologies on affective states and their influences. We also provide the psychological background of affective states, their influences and representational models. The paper discusses a total of 20 ontologies on emotion, one ontology on mood, one ontology on needs, and 11 general purpose ontologies and lexicons. Based on the analysis of existing ontologies, we summarize and discuss the current state of the art in the field.},
	author = {Abaalkhail, Rana and Guthier, Benjamin and Alharthi, Rajwa and Saddik, Abdulmotaleb},
	c1 = {College of Computer Science, Information System Department, King Saud University, Kingdom of Saudi Arabia; Multimedia Communications Research Laboratory, University of Ottawa, 800 King Edward Ave, K1N 6N5, Ottawa, ON, Canada. E-mails: rabaa006{\char64}uottawa.ca, ralha081{\char64}uottawa.ca, elsaddik{\char64}uottawa.ca; Department of Computer Science IV, University of Mannheim, Germany. E-mail: guthier{\char64}informatik.uni-mannheim.de; College of Computer and Information Technology, Taif University, Kingdom of Saudi Arabia; Multimedia Communications Research Laboratory, University of Ottawa, 800 King Edward Ave, K1N 6N5, Ottawa, ON, Canada. E-mails: rabaa006{\char64}uottawa.ca, ralha081{\char64}uottawa.ca, elsaddik{\char64}uottawa.ca; Multimedia Communications Research Laboratory, University of Ottawa, 800 King Edward Ave, K1N 6N5, Ottawa, ON, Canada. E-mails: rabaa006{\char64}uottawa.ca, ralha081{\char64}uottawa.ca, elsaddik{\char64}uottawa.ca},
	date = {2018-06-29},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:24 +0100},
	doi = {10.3233/sw-170270},
	isbn = {1570-0844},
	journal = {Semantic web},
	keywords = {Affective Computing; Emotion Recognition; Emotion Dynamics; Speech Emotion; Personality Data},
	la = {en},
	number = {4},
	pages = {441--458},
	publisher = {IOS Press},
	title = {Survey on ontologies for affective states and their influences},
	url = {https://doi.org/10.3233/sw-170270},
	volume = {9},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.3233/sw-170270}}

@article{chenro_2022,
	abstract = {This paper describes a new posed multimodal emotional dataset and compares human emotion classification based on four different modalities -audio, video, electromyography (EMG), and electroencephalography (EEG).The results are reported with several baseline approaches using various feature extraction techniques and machine-learning algorithms.First, we collected a dataset from 11 human subjects expressing six basic emotions and one neutral emotion.We then extracted features from each modality using principal component analysis, autoencoder, convolution network, and mel-frequency cepstral coefficient (MFCC), some unique to individual modalities.A number of baseline models have been applied to compare the classification performance in emotion recognition, including k-nearest neighbors (KNN), support vector machines (SVM), random forest, multilayer perceptron (MLP), long short-term memory (LSTM) model, and convolutional neural network (CNN).Our results show that bootstrapping the biosensor signals (i.e., EMG and EEG) can greatly increase emotion classification performance by reducing noise.In contrast, the best classification results were obtained by a traditional KNN, whereas audio and image sequences of human emotions could be better classified using LSTM.},
	author = {Chen, J. and Ro, Tony and Zhu, Zhigang},
	c1 = {Computer Science Department, The City College of New York (CUNY), New York, NY 10031, USA; Programs in Psychology, Biology, and Cognitive Neuroscience, The Graduate Center, CUNY, New York, NY 10016, USA; Computer Science Department, The City College of New York (CUNY), New York, NY 10031, USA; Doctoral Program in Computer Science, The Graduate Center, CUNY, New York, NY 10016, USA},
	date = {2022-01-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/access.2022.3146729},
	isbn = {2169-3536},
	journal = {IEEE access},
	keywords = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Affective Computing; Environmental Sound Recognition},
	la = {en},
	pages = {13229--13242},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {Emotion Recognition With Audio, Video, EEG, and EMG: A Dataset and Baseline Approaches},
	url = {https://doi.org/10.1109/access.2022.3146729},
	volume = {10},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/access.2022.3146729}}

@article{tangkuppens_2021,
	abstract = {Amongst the various characteristics of a speech signal, the expression of emotion is one of the characteristics that exhibits the slowest temporal dynamics. Hence, a performant speech emotion recognition (SER) system requires a predictive model that is capable of learning sufficiently long temporal dependencies in the analysed speech signal. Therefore, in this work, we propose a novel end-to-end neural network architecture based on the concept of dilated causal convolution with context stacking. Firstly, the proposed model consists only of parallelisable layers and is hence suitable for parallel processing, while avoiding the inherent lack of parallelisability occurring with recurrent neural network (RNN) layers. Secondly, the design of a dedicated dilated causal convolution block allows the model to have a receptive field as large as the input sequence length, while maintaining a reasonably low computational cost. Thirdly, by introducing a context stacking structure, the proposed model is capable of exploiting long-term temporal dependencies hence providing an alternative to the use of RNN layers. We evaluate the proposed model in SER regression and classification tasks and provide a comparison with a state-of-the-art end-to-end SER model. Experimental results indicate that the proposed model requires only 1/3 of the number of model parameters used in the state-of-the-art model, while also significantly improving SER performance. Further experiments are reported to understand the impact of using various types of input representations (i.e. raw audio samples vs log mel-spectrograms) and to illustrate the benefits of an end-to-end approach over the use of hand-crafted audio features. Moreover, we show that the proposed model can efficiently learn intermediate embeddings preserving speech emotion information.},
	author = {Tang, Duowei and Kuppens, Peter and Geurts, Luc and van Waterschoot, Toon},
	c1 = {Department of Electrical Engineering (ESAT), STADIUS Center for Dynamical Systems, Signal Processing, and Data Analytics, KU Leuven, Leuven, Belgium; Faculty of Psychology and Educational Sciences, KU Leuven, Leuven, Belgium; e-Media Research Lab, KU Leuven, Leuven, Belgium; Department of Electrical Engineering (ESAT), STADIUS Center for Dynamical Systems, Signal Processing, and Data Analytics, KU Leuven, Leuven, Belgium},
	date = {2021-05-12},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1186/s13636-021-00208-5},
	isbn = {1687-4714},
	journal = {EURASIP Journal on Audio, Speech and Music Processing},
	keywords = {Emotion Recognition; Speech Emotion; Audio-Visual Speech Recognition; Affective Computing; Audio Event Detection},
	la = {en},
	number = {1},
	publisher = {Springer Nature},
	title = {End-to-end speech emotion recognition using a novel context-stacking dilated convolution neural network},
	url = {https://doi.org/10.1186/s13636-021-00208-5},
	volume = {2021},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1186/s13636-021-00208-5}}

@article{amiripariancummins_2017,
	abstract = {We test the suitability of our novel deep spectrum feature representation for performing speech-based sentiment analysis. Deep spectrum features are formed by passing spectrograms through a pre-trained image convolutional neural network (CNN) and have been shown to capture useful emotion information in speech; however, their usefulness for sentiment analysis is yet to be investigated. Using a data set of movie reviews collected from YouTube, we compare deep spectrum features combined with the bag-of-audio-words (BoAW) paradigm with a state-of-the-art Mel Frequency Cepstral Coefficients (MFCC) based BoAW system when performing a binary sentiment classification task. Key results presented indicate the suitability of both features for the proposed task. The deep spectrum features achieve an unweighted average recall of 74.5 {\%}. The results provide further evidence for the effectiveness of deep spectrum features as a robust feature representation for speech analysis.},
	author = {Amiriparian, Shahin and Cummins, Nicholas and Ottl, Sandra and Gerczuk, Maurice and Schuller, Bj{\"o}rn},
	c1 = {Chair of Complex \& Intelligent Systems, Universit{\"a}t Passau, Germany; Chair of Embedded Intelligence for Health Care \& Wellbeing, Augsburg University, Augsburg, Germany; Machine Intelligence \& Signal Processing Group, Technische Universit{\"a}at M{\"u}unchen, Germany; Chair of Complex \& Intelligent Systems, Universit{\"a}t Passau, Germany; Chair of Embedded Intelligence for Health Care \& Wellbeing, Augsburg University, Augsburg, Germany; Chair of Complex \& Intelligent Systems, Universit{\"a}t Passau, Germany{\#}TAB{\#}; Chair of Complex \& Intelligent Systems, Universit{\"a}t Passau, Germany{\#}TAB{\#}; Chair of Complex \& Intelligent Systems, Universit{\"a}t Passau, Germany; Imperial College London, GLAM--Group on Language, Audio \& Music, London, UK},
	date = {2017-10-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/aciiw.2017.8272618},
	keywords = {Emotion Recognition; Affective Computing; Feature Extraction; Aspect-based Sentiment Analysis; Speech Emotion},
	la = {en},
	title = {Sentiment analysis using image-based deep spectrum features},
	url = {https://doi.org/10.1109/aciiw.2017.8272618},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/aciiw.2017.8272618}}

@article{hazer-raumeudt_2020,
	abstract = {In this paper, we present a multimodal dataset for affective computing research acquired in a human-computer interaction (HCI) setting. An experimental mobile and interactive scenario was designed and implemented based on a gamified generic paradigm for the induction of dialog-based HCI relevant emotional and cognitive load states. It consists of six experimental sequences, inducing Interest, Overload, Normal, Easy, Underload, and Frustration. Each sequence is followed by subjective feedbacks to validate the induction, a respiration baseline to level off the physiological reactions, and a summary of results. Further, prior to the experiment, three questionnaires related to emotion regulation (ERQ), emotional control (TEIQue-SF), and personality traits (TIPI) were collected from each subject to evaluate the stability of the induction paradigm. Based on this HCI scenario, the University of Ulm Multimodal Affective Corpus (uulmMAC), consisting of two homogenous samples of 60 participants and 100 recording sessions was generated. We recorded 16 sensor modalities including 4 ×video, 3 ×audio, and 7 ×biophysiological, depth, and pose streams. Further, additional labels and annotations were also collected. After recording, all data were post-processed and checked for technical and signal quality, resulting in the final uulmMAC dataset of 57 subjects and 95 recording sessions. The evaluation of the reported subjective feedbacks shows significant differences between the sequences, well consistent with the induced states, and the analysis of the questionnaires shows stable results. In summary, our uulmMAC database is a valuable contribution for the field of affective computing and multimodal data analysis: Acquired in a mobile interactive scenario close to real HCI, it consists of a large number of subjects and allows transtemporal investigations. Validated via subjective feedbacks and checked for quality issues, it can be used for affective computing and machine learning applications.},
	author = {Hazer-Rau, Dilana and Meudt, Sascha and Daucher, Andreas and Spohrs, Jennifer and Hoffmann, Holger and Schwenker, Friedhelm and Traue, Harald},
	c1 = {Section Medical Psychology, University of Ulm, Frauensteige 6, 89075 Ulm, Germany; Institute of Neural Information Processing, University of Ulm, James-Frank-Ring, 89081 Ulm, Germany; Section Medical Psychology, University of Ulm, Frauensteige 6, 89075 Ulm, Germany; Section Medical Psychology, University of Ulm, Frauensteige 6, 89075 Ulm, Germany; Section Medical Psychology, University of Ulm, Frauensteige 6, 89075 Ulm, Germany; Institute of Neural Information Processing, University of Ulm, James-Frank-Ring, 89081 Ulm, Germany; Section Medical Psychology, University of Ulm, Frauensteige 6, 89075 Ulm, Germany},
	date = {2020-04-17},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/s20082308},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Affective Computing; Human-Computer Interaction; Emotion Recognition; Affective Design; Multimodal Data},
	la = {en},
	number = {8},
	pages = {2308--2308},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {The uulmMAC Database---A Multimodal Affective Corpus for Affective Computing in Human-Computer Interaction},
	url = {https://doi.org/10.3390/s20082308},
	volume = {20},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.3390/s20082308}}

@article{cavallosemeraro_2019,
	abstract = {Future smart agents, like robots, should produce personalized behaviours based on user emotions and moods to fit more in ordinary users'activities. Besides, the emotions are also linked to human cognitive systems, thus their monitoring could be extremely useful in the case of neurodegenerative diseases such as dementia and Alzheimer. Literature works propose the use of music tracks and videos to stimulate emotions, and cameras to recorder the evoked reactions in human beings. However, these approaches may not be effective in everyday life, due to camera obstructions and different types of stimulation which can be related also with the interaction with other human beings. In this work, we investigate the Electrocardiogram, the ElectroDermal Activity and the Brain Activity signals as main informative channels, acquired through a wireless wearable sensor network. An experimental methodology was built to induce three different emotional states through social interaction. Collected data were classified with three supervised machine learning approaches with different kernels (Support Vector Machine, Decision Tree and k-nearest neighbour) considering the valence dimension and a combination of valence and arousal dimension evoked during the interaction. 34 healthy young participants were involved in the study and a total of 239 instances were analyzed. The supervised algorithms achieve an accuracy of 0.877 in the best case.},
	author = {Cavallo, Filippo and Semeraro, Francesco and Mancioppi, Gianmaria and Betti, Stefano and Fiorini, Laura},
	c1 = {The BioRobotics Institute, Scuola Superiore Sant'Anna, Viale Rinaldo Piaggio 34, 56025, Pontedera, PI, Italy; The BioRobotics Institute, Scuola Superiore Sant'Anna, Viale Rinaldo Piaggio 34, 56025, Pontedera, PI, Italy; The BioRobotics Institute, Scuola Superiore Sant'Anna, Viale Rinaldo Piaggio 34, 56025, Pontedera, PI, Italy; The BioRobotics Institute, Scuola Superiore Sant'Anna, Viale Rinaldo Piaggio 34, 56025, Pontedera, PI, Italy; The BioRobotics Institute, Scuola Superiore Sant'Anna, Viale Rinaldo Piaggio 34, 56025, Pontedera, PI, Italy},
	date = {2019-12-07},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1007/s12652-019-01595-6},
	isbn = {1868-5137},
	journal = {Journal of ambient intelligence \& humanized computing/Journal of ambient intelligence and humanized computing},
	keywords = {Emotion Recognition; Affective Computing; Physiological Signals; Neuroimaging Data Analysis; Speech Emotion},
	la = {en},
	number = {4},
	pages = {4471--4484},
	publisher = {Springer Science+Business Media},
	title = {Mood classification through physiological parameters},
	url = {https://doi.org/10.1007/s12652-019-01595-6},
	volume = {12},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1007/s12652-019-01595-6}}

@article{wangzhang_2022,
	abstract = {Emotion recognition is receiving significant attention in research on health care and Human-Computer Interaction (HCI). Due to the high correlation with emotion and the capability to affect deceptive external expressions such as voices and faces, Electroencephalogram (EEG) based emotion recognition methods have been globally accepted and widely applied. Recently, great improvements have been made in the development of machine learning for EEG-based emotion detection. However, there are still some major disadvantages in previous studies. Firstly, traditional machine learning methods require extracting features manually which is time-consuming and rely heavily on human experts. Secondly, to improve the model accuracies, many researchers used user-dependent models that lack generalization and universality. Moreover, there is still room for improvement in the recognition accuracies in most studies. Therefore, to overcome these shortcomings, an EEG-based novel deep neural network is proposed for emotion classification in this article. The proposed 2D CNN uses two convolutional kernels of different sizes to extract emotion-related features along both the time direction and the spatial direction. To verify the feasibility of the proposed model, the pubic emotion dataset DEAP is used in experiments. The results show accuracies of up to 99.99{\%} and 99.98 for arousal and valence binary classification, respectively, which are encouraging for research and applications in the emotion recognition field.},
	author = {Wang, Yuqi and Zhang, Lijun and Xia, Pan and Wang, Peng and Chen, Xianxiang and Du, Lidong and Fang, Zhen and Du, Mingyan},
	c1 = {Institute of Microelectronics of Chinese Academy of Sciences, Beijing 100029, China;; University of Chinese Academy of Sciences, Beijing 100049, China;; Institute of Microelectronics of Chinese Academy of Sciences, Beijing 100029, China;; University of Chinese Academy of Sciences, Beijing 100049, China;; Aerospace Information Research Institute, Chinese Academy of Sciences (AIRCAS), Beijing 100190, China; University of Chinese Academy of Sciences, Beijing 100049, China;; Aerospace Information Research Institute, Chinese Academy of Sciences (AIRCAS), Beijing 100190, China; Aerospace Information Research Institute, Chinese Academy of Sciences (AIRCAS), Beijing 100190, China; University of Chinese Academy of Sciences, Beijing 100049, China;; Aerospace Information Research Institute, Chinese Academy of Sciences (AIRCAS), Beijing 100190, China; University of Chinese Academy of Sciences, Beijing 100049, China;; Aerospace Information Research Institute, Chinese Academy of Sciences (AIRCAS), Beijing 100190, China; Personalized Management of Chronic Respiratory Disease, Chinese Academy of Medical Sciences, Beijing 100190, China; University of Chinese Academy of Sciences, Beijing 100049, China;; China Beijing Luhe Hospital, Capital Medical University, Beijing 101199, China; University of Chinese Academy of Sciences, Beijing 100049, China;},
	date = {2022-05-26},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3390/bioengineering9060231},
	isbn = {2306-5354},
	journal = {Bioengineering},
	keywords = {Emotion Recognition; Deep Learning for EEG; Affective Computing; EEG Analysis; Speech Emotion},
	la = {en},
	number = {6},
	pages = {231--231},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {EEG-Based Emotion Recognition Using a 2D CNN with Different Kernels},
	url = {https://doi.org/10.3390/bioengineering9060231},
	volume = {9},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.3390/bioengineering9060231}}

@article{marimpisdimitriadis_2020,
	abstract = {A high number of studies have already demonstrated an electroencephalography (EEG)-based emotion recognition system with moderate results. Emotions are classified into discrete and dimensional models. We focused on the latter that incorporates valence and arousal dimensions. The mainstream methodology is the extraction of univariate measures derived from EEG activity from various frequencies classifying trials into low/high valence and arousal levels. Here, we evaluated brain connectivity within and between brain frequencies under the multiplexity framework. We analyzed an EEG database called DEAP that contains EEG responses to video stimuli and users' emotional self-assessments. We adopted a dynamic functional connectivity analysis under the notion of our dominant coupling model (DoCM). DoCM detects the dominant coupling mode per pair of EEG sensors, which can be either within frequencies coupling (intra) or between frequencies coupling (cross-frequency). DoCM revealed an integrated dynamic functional connectivity graph (IDFCG) that keeps both the strength and the preferred dominant coupling mode. We aimed to create a connectomic mapping of valence-arousal map via employing features derive from IDFCG. Our results outperformed previous findings succeeding to predict in a high accuracy participants' ratings in valence and arousal dimensions based on a flexibility index of dominant coupling modes.},
	author = {Marimpis, Avraam and Dimitriadis, Stavros and Goebel, Rainer},
	c1 = {Brain Innovation B.V., Maastricht, The Netherlands; Cardiff University Brain Research Imaging Center (CUBRIC), Neuroinformatics Group, School of Psychology, Cardiff University, Cardiff, U.K.; Faculty of Psychology and Neuroscience, Maastricht University, Maastricht, The Netherlands; Cardiff University Brain Research Imaging Center (CUBRIC), Neuroinformatics Group, School of Psychology, Cardiff University, Cardiff, U.K.; Neuroscience and Mental Health Research Institute, Cardiff University, Cardiff, U.K.; Brain Innovation B.V., Maastricht, The Netherlands; Faculty of Psychology and Neuroscience, Maastricht University, Maastricht, The Netherlands},
	date = {2020-01-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/access.2020.3025370},
	isbn = {2169-3536},
	journal = {IEEE access},
	keywords = {Emotion Recognition; Affective Computing; Deep Learning for EEG; Epilepsy Detection; EEG Analysis},
	la = {en},
	pages = {170928--170938},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {A Multiplex Connectivity Map of Valence-Arousal Emotional Model},
	url = {https://doi.org/10.1109/access.2020.3025370},
	volume = {8},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/access.2020.3025370}}

@article{tufu_2020,
	abstract = {Emotional content is a crucial ingredient in user-generated videos. However, the sparsity of emotional expressions in the videos poses an obstacle to visual emotion analysis. In this paper, we propose a new neural approach, Bi-stream Emotion Attribution-Classification Network (BEAC-Net), to solve three related emotion analysis tasks: emotion recognition, emotion attribution, and emotion-oriented summarization, in a single integrated framework. BEAC-Net has two major constituents, an attribution network and a classification network. The attribution network extracts the main emotional segment that classification should focus on in order to mitigate the sparsity issue. The classification network utilizes both the extracted segment and the original video in a bi-stream architecture. We contribute a new dataset for the emotion attribution task with human-annotated ground-truth labels for emotion segments. Experiments on two video datasets demonstrate superior performance of the proposed framework and the complementary nature of the dual classification streams.},
	author = {Tu, Guoyun and Fu, Yanwei and Li, Boyang and Gao, Jiarui and Jiang, Yu-Gang and Xue, Xiangyang},
	c1 = {Fudan University, Shanghai, China; School of Data Science, Fudan University, Shanghai, China; Big Data Laboratory, Baidu Research, Sunnyvale, US; Fudan University, Shanghai, China; Jilian Technology Group (Video++), Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; Fudan University, Shanghai, China},
	date = {2020-01-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1109/tmm.2019.2922129},
	isbn = {1520-9210},
	journal = {IEEE transactions on multimedia},
	keywords = {Affective Computing; Emotion Recognition; Video Summarization; Action Recognition; Feature Extraction},
	la = {en},
	number = {1},
	pages = {148--159},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {A Multi-Task Neural Approach for Emotion Attribution, Classification, and Summarization},
	url = {https://doi.org/10.1109/tmm.2019.2922129},
	volume = {22},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/tmm.2019.2922129}}

@article{suli_2019,
	abstract = {Giving a robot the ability to perceive emotion in its environment can improve human-robot interaction (HRI), thereby facilitating more human-like communication. To achieve emotion recognition in different built environments for adolescents, we propose a multi-modal emotion intensity perception method using an integration of electroencephalography (EEG) and eye movement information. Specifically, we first develop a new stimulus video selection method based on computation of normalized arousal and valence scores according to subjective feedback from participants. Then, we establish a valence perception sub-model and an arousal sub-model by collecting and analyzing emotional EEG and eye movement signals, respectively. We employ this dual recognition method to perceive emotional intensities synchronously in two dimensions. In the laboratory environment, the best recognition accuracies of the modality fusion for the arousal and valence dimensions are 72.8{\%} and 69.3{\%}. The experimental results validate the feasibility of the proposed multi-modal emotion recognition method for environment emotion intensity perception. This promising tool not only achieves more accurate emotion perception for HRI systems but also provides an alternative approach to quantitatively assess environmental psychology.},
	author = {Su, Yuanyuan and Li, Wenchao and Bi, Ning and Lv, Zhao},
	c1 = {College of Design, Iowa State University, Ames, IA, United States; Department of Design, Anhui University, Hefei, China; School of Computer Science and Technology, Anhui University, Hefei, China; School of Computer Science, Georgia Institute of Technology, Atlanta, GA, United States; School of Computer Science and Technology, Anhui University, Hefei, China},
	date = {2019-06-26},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3389/fnbot.2019.00046},
	isbn = {1662-5218},
	journal = {Frontiers in neurorobotics},
	keywords = {Emotion Recognition; Eye Movement Analysis; Deep Learning for EEG; EEG Analysis; Affective Computing},
	la = {en},
	publisher = {Frontiers Media},
	title = {Adolescents Environmental Emotion Perception by Integrating EEG and Eye Movements},
	url = {https://doi.org/10.3389/fnbot.2019.00046},
	volume = {13},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.3389/fnbot.2019.00046}}

@article{maengkang_2020,
	abstract = {Emotional awareness is vital for advanced interactions between humans and computer systems. This paper introduces a new multimodal dataset called MERTI-Apps based on Asian physiological signals and proposes a genetic algorithm (GA)---long short-term memory (LSTM) deep learning model to derive the active feature groups for emotion recognition. This study developed an annotation labeling program for observers to tag the emotions of subjects by their arousal and valence during dataset creation. In the learning phase, a GA was used to select effective LSTM model parameters and determine the active feature group from 37 features and 25 brain lateralization features extracted from the electroencephalogram (EEG) time, frequency, and time--frequency domains. The proposed model achieved a root-mean-square error (RMSE) of 0.0156 in terms of the valence regression performance in the MAHNOB-HCI dataset, and RMSE performances of 0.0579 and 0.0287 in terms of valence and arousal regression performance, and 65.7{\%} and 88.3{\%} in terms of valence and arousal accuracy in the in-house MERTI-Apps dataset, which uses Asian-population-specific 12-channel EEG data and adds an additional brain lateralization (BL) feature. The results revealed 91.3{\%} and 94.8{\%} accuracy in the valence and arousal domain in the DEAP dataset owing to the effective model selection of a GA.},
	author = {Maeng, Junho and Kang, Dong and Kim, Deok‐Hwan},
	c1 = {Department of Electronic Engineering, Inha University, Incheon 22212, Korea;; Department of Electronic Engineering, Inha University, Incheon 22212, Korea;; Department of Electronic Engineering, Inha University, Incheon 22212, Korea;},
	date = {2020-11-24},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/electronics9121988},
	isbn = {2079-9292},
	journal = {Electronics},
	keywords = {Emotion Recognition; Affective Computing; Head Gesture Recognition; Deep Learning for EEG; Deep Learning},
	la = {en},
	number = {12},
	pages = {1988--1988},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Deep Learning Method for Selecting Effective Models and Feature Groups in Emotion Recognition Using an Asian Multimodal Database},
	url = {https://doi.org/10.3390/electronics9121988},
	volume = {9},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.3390/electronics9121988}}

@article{marinmoraleshiguera-trujillo_2021,
	abstract = {Many affective computing studies have developed automatic emotion recognition models, mostly using emotional images, audio and videos. In recent years, virtual reality (VR) has been also used as a method to elicit emotions in laboratory environments. However, there is still a need to analyse the validity of VR in order to extrapolate the results it produces and to assess the similarities and differences in physiological responses provoked by real and virtual environments. We investigated the cardiovascular oscillations of 60 participants during a free exploration of a real museum and its virtualisation viewed through a head-mounted display. The differences between the heart rate variability features in the high and low arousal stimuli conditions were analysed through statistical hypothesis testing; and automatic arousal recognition models were developed across the real and the virtual conditions using a support vector machine algorithm with recursive feature selection. The subjects' self-assessments suggested that both museums elicited low and high arousal levels. In addition, the real museum showed differences in terms of cardiovascular responses, differences in vagal activity, while arousal recognition reached 72.92{\%} accuracy. However, we did not find the same arousal-based autonomic nervous system change pattern during the virtual museum exploration. The results showed that, while the direct virtualisation of a real environment might be self-reported as evoking psychological arousal, it does not necessarily evoke the same cardiovascular changes as a real arousing elicitation. These contribute to the understanding of the use of VR in emotion recognition research; future research is needed to study arousal and emotion elicitation in immersive VR.},
	author = {Mar{\'\i}n‐Morales, Javier and Higuera-Trujillo, Juan and Guixeres, Jaime and Llinares, Carmen and Alca{\~n}{\'\i}z, Mariano and Valenza, Gaetano},
	c1 = {Instituto de Investigaci{\'o}n e Innovaci{\'o}n en Bioingenier{\'\i}a, Universitat Polit{\`e}cnica de Val{\`e}ncia, Val{\`e}ncia, Spain; Instituto de Investigaci{\'o}n e Innovaci{\'o}n en Bioingenier{\'\i}a, Universitat Polit{\`e}cnica de Val{\`e}ncia, Val{\`e}ncia, Spain; Instituto de Investigaci{\'o}n e Innovaci{\'o}n en Bioingenier{\'\i}a, Universitat Polit{\`e}cnica de Val{\`e}ncia, Val{\`e}ncia, Spain; Instituto de Investigaci{\'o}n e Innovaci{\'o}n en Bioingenier{\'\i}a, Universitat Polit{\`e}cnica de Val{\`e}ncia, Val{\`e}ncia, Spain; Instituto de Investigaci{\'o}n e Innovaci{\'o}n en Bioingenier{\'\i}a, Universitat Polit{\`e}cnica de Val{\`e}ncia, Val{\`e}ncia, Spain; Bioengineering and Robotics Research Centre E Piaggio \& Department of Information Engineering, University of Pisa, Pisa, Italy},
	date = {2021-07-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1371/journal.pone.0254098},
	isbn = {1932-6203},
	journal = {PloS one},
	keywords = {Heart Rate Variability; Affective Computing; Emotion Recognition; Emotion Regulation},
	la = {en},
	number = {7},
	pages = {e0254098--e0254098},
	publisher = {Public Library of Science},
	title = {Heart rate variability analysis for the assessment of immersive emotional arousal using virtual reality: Comparing real and virtual scenarios},
	url = {https://doi.org/10.1371/journal.pone.0254098},
	volume = {16},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1371/journal.pone.0254098}}

@article{zhangxu_2018,
	abstract = {Multichannel physiological datasets are usually nonlinear and separable in the field of emotion recognition. Many researchers have applied linear or partial nonlinear processing in feature reduction and classification, but these applications did not work well. Therefore, this paper proposed a comprehensive nonlinear method to solve this problem. On the one hand, as traditional feature reduction may cause the loss of significant amounts of feature information, Kernel Principal Component Analysis (KPCA) based on radial basis function (RBF) was introduced to map the data into a high-dimensional space, extract the nonlinear information of the features, and then reduce the dimension. This method can provide many features carrying information about the structure in the physiological dataset. On the other hand, considering its advantages of predictive power and feature selection from a large number of features, Gradient Boosting Decision Tree (GBDT) was used as a nonlinear ensemble classifier to improve the recognition accuracy. The comprehensive nonlinear processing method had a great performance on our physiological dataset. Classification accuracy of four emotions in 29 participants achieved 93.42{\%}.},
	author = {Zhang, Xingxing and Xu, Chao and Xue, Wanli and Hu, Jing and He, Yongchuan and Gao, Mengxin},
	c1 = {College of Intelligence and Computing, Tianjin University, Tianjin 300350, China;; College of Intelligence and Computing, Tianjin University, Tianjin 300350, China;; College of Intelligence and Computing, Tianjin University, Tianjin 300350, China;; School of Computer Science and Engineering, Tianjin University of Technology, Tianjin 300384, China; College of Intelligence and Computing, Tianjin University, Tianjin 300350, China;; Shenzhen Graduate School, Peking University, Shenzhen 518055, China;; Department of Economics, Pennsylvania State University, State College, PA 16803, USA;},
	date = {2018-11-11},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3390/s18113886},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Emotion Recognition; Affective Computing; Feature Extraction; Multimodal Data},
	la = {en},
	number = {11},
	pages = {3886--3886},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Emotion Recognition Based on Multichannel Physiological Signals with Comprehensive Nonlinear Processing},
	url = {https://doi.org/10.3390/s18113886},
	volume = {18},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.3390/s18113886}}

@article{rinconpoza_2016,
	abstract = {This article proposes an application of a social emotional model, which allows to extract, analyse, represent and manage the social emotion of a group of entities. Specifically, the application is based on how music can influence in a positive or negative way over emotional states. The proposed approach employs the JaCalIVE framework, which facilitates the development of this kind of environments. A physical device called smart resource offers to agents processed sensor data as a service. So that, agents obtain real data from a smart resource. MAS uses the smart resource as an artifact by means of a specific communications protocol. The framework includes a design method and a physical simulator. In this way, the social emotional model allows the creation of simulations over JaCalIVE, in which the emotional states are used in the decision-making of the agents.},
	author = {Rincon, J. and Poza, J. L. and Posadas, Ju{\'a}n and Juli{\'a}n, Vicente and Carrascosa, Carlos},
	c1 = {Valencia Polytechnic University; Valencia Polytechnic University; Valencia Polytechnic University; Valencia Polytechnic University; Valencia Polytechnic University},
	date = {2016-11-15},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.14201/adcaij2016548592},
	isbn = {2255-2863},
	journal = {Advances in distributed computing and artificial intelligence journal},
	keywords = {Affective Computing; Emotion Recognition},
	la = {en},
	number = {4},
	pages = {85--92},
	publisher = {Ediciones Universidad de Salamanca},
	title = {Adding real data to detect emotions by means of smart resource artifacts in MAS},
	url = {https://doi.org/10.14201/adcaij2016548592},
	volume = {5},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.14201/adcaij2016548592}}

@article{kalashamipedram_2022,
	abstract = {Emotion recognition is a challenging problem in Brain-Computer Interaction (BCI). Electroencephalogram (EEG) gives unique information about brain activities that are created due to emotional stimuli. This is one of the most substantial advantages of brain signals in comparison to facial expression, tone of voice, or speech in emotion recognition tasks. However, the lack of EEG data and high dimensional EEG recordings lead to difficulties in building effective classifiers with high accuracy. In this study, data augmentation and feature extraction techniques are proposed to solve the lack of data problem and high dimensionality of data, respectively. In this study, the proposed method is based on deep generative models and a data augmentation strategy called Conditional Wasserstein GAN (CWGAN), which is applied to the extracted features to regenerate additional EEG features. DEAP dataset is used to evaluate the effectiveness of the proposed method. Finally, a standard support vector machine and a deep neural network with different tunes were implemented to build effective models. Experimental results show that using the additional augmented data enhances the performance of EEG-based emotion recognition models. Furthermore, the mean accuracy of classification after data augmentation is increased 6.5{\%} for valence and 3.0{\%} for arousal, respectively.},
	author = {Kalashami, Mahsa and Pedram, Mir and Sadr, Hossein},
	c1 = {Department of Electrical and Computer Engineering, Faculty of Engineering, Kharazmi University, Tehran 15719-14911, Iran; Department of Electrical and Computer Engineering, Faculty of Engineering, Kharazmi University, Tehran 15719-14911, Iran; Department of Computer Engineering, Rahbord Shomal Institute of Higher Education, Rasht, Iran},
	date = {2022-03-28},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1155/2022/7028517},
	isbn = {1687-5265},
	journal = {Computational intelligence and neuroscience},
	keywords = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Feature Extraction; Affective Computing},
	la = {en},
	pages = {1--16},
	publisher = {Hindawi Publishing Corporation},
	title = {EEG Feature Extraction and Data Augmentation in Emotion Recognition},
	url = {https://doi.org/10.1155/2022/7028517},
	volume = {2022},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1155/2022/7028517}}

@article{aranogloor_2021,
	abstract = {Abstract Speech is one of the most natural communication channels for expressing human emotions. Therefore, speech emotion recognition (SER) has been an active area of research with an extensive range of applications that can be found in several domains, such as biomedical diagnostics in healthcare and human--machine interactions. Recent works in SER have been focused on end-to-end deep neural networks (DNNs). However, the scarcity of emotion-labeled speech datasets inhibits the full potential of training a deep network from scratch. In this paper, we propose new approaches for classifying emotions from speech by combining conventional mel-frequency cepstral coefficients (MFCCs) with image features extracted from spectrograms by a pretrained convolutional neural network (CNN). Unlike prior studies that employ end-to-end DNNs, our methods eliminate the resource-intensive network training process. By using the best prediction model obtained, we also build an SER application that predicts emotions in real time. Among the proposed methods, the hybrid feature set fed into a support vector machine (SVM) achieves an accuracy of 0.713 in a 6-class prediction problem evaluated on the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) dataset, which is higher than the previously published results. Interestingly, MFCCs taken as unique input into a long short-term memory (LSTM) network achieve a slightly higher accuracy of 0.735. Our results reveal that the proposed approaches lead to an improvement in prediction accuracy. The empirical findings also demonstrate the effectiveness of using a pretrained CNN as an automatic feature extractor for the task of emotion prediction. Moreover, the success of the MFCC-LSTM model is evidence that, despite being conventional features, MFCCs can still outperform more sophisticated deep-learning feature sets.},
	author = {Ara{\~n}o, Keith and Gloor, Peter and Orsenigo, Carlotta and Vercellis, Carlo},
	c1 = {Department of Management, Economics and Industrial Engineering, Politecnico di Milano, Milan, 20156, Italy; Center for Collective Intelligence, Massachusetts Institute of Technology, Boston, 02139, USA; Department of Management, Economics and Industrial Engineering, Politecnico di Milano, Milan, 20156, Italy; Department of Management, Economics and Industrial Engineering, Politecnico di Milano, Milan, 20156, Italy},
	date = {2021-04-19},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1007/s12559-021-09865-2},
	isbn = {1866-9956},
	journal = {Cognitive computation},
	keywords = {Emotion Recognition; Speech Emotion; Affective Computing; Audio-Visual Speech Recognition; Environmental Sound Recognition},
	la = {en},
	number = {3},
	pages = {771--783},
	publisher = {Springer Science+Business Media},
	title = {When Old Meets New: Emotion Recognition from Speech Signals},
	url = {https://doi.org/10.1007/s12559-021-09865-2},
	volume = {13},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1007/s12559-021-09865-2}}

@article{yuvarajthangavel_2023,
	abstract = {Advances in signal processing and machine learning have expedited electroencephalogram (EEG)-based emotion recognition research, and numerous EEG signal features have been investigated to detect or characterize human emotions. However, most studies in this area have used relatively small monocentric data and focused on a limited range of EEG features, making it difficult to compare the utility of different sets of EEG features for emotion recognition. This study addressed that by comparing the classification accuracy (performance) of a comprehensive range of EEG feature sets for identifying emotional states, in terms of valence and arousal. The classification accuracy of five EEG feature sets were investigated, including statistical features, fractal dimension (FD), Hjorth parameters, higher order spectra (HOS), and those derived using wavelet analysis. Performance was evaluated using two classifier methods, support vector machine (SVM) and classification and regression tree (CART), across five independent and publicly available datasets linking EEG to emotional states: MAHNOB-HCI, DEAP, SEED, AMIGOS, and DREAMER. The FD-CART feature-classification method attained the best mean classification accuracy for valence (85.06{\%}) and arousal (84.55{\%}) across the five datasets. The stability of these findings across the five different datasets also indicate that FD features derived from EEG data are reliable for emotion recognition. The results may lead to the possible development of an online feature extraction framework, thereby enabling the development of an EEG-based emotion recognition system in real time.},
	author = {Yuvaraj, Rajamanickam and Thangavel, Prasanth and Thomas, John and Fogarty, Jack and Ali, Farhan},
	c1 = {National Institute of Education, Nanyang Technological University, Singapore 637616, Singapore; Interdisciplinary Graduate School, Nanyang Technological University, Singapore 639798, Singapore; Montreal Neurological Institute, McGill University, Montreal, QC H3A 2B4, Canada; National Institute of Education, Nanyang Technological University, Singapore 637616, Singapore; National Institute of Education, Nanyang Technological University, Singapore 637616, Singapore},
	date = {2023-01-12},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3390/s23020915},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Feature Extraction; Affective Computing},
	la = {en},
	number = {2},
	pages = {915--915},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Comprehensive Analysis of Feature Extraction Methods for Emotion Recognition from Multichannel EEG Recordings},
	url = {https://doi.org/10.3390/s23020915},
	volume = {23},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.3390/s23020915}}

@article{martinez-tejadapuertas-gonzalez_2021,
	abstract = {In this article we present the study of electroencephalography (EEG) traits for emotion recognition process using a videogame as a stimuli tool, and considering two different kind of information related to emotions: arousal--valence self-assesses answers from participants, and game events that represented positive and negative emotional experiences under the videogame context. We performed a statistical analysis using Spearman's correlation between the EEG traits and the emotional information. We found that EEG traits had strong correlation with arousal and valence scores; also, common EEG traits with strong correlations, belonged to the theta band of the central channels. Then, we implemented a regression algorithm with feature selection to predict arousal and valence scores using EEG traits. We achieved better result for arousal regression, than for valence regression. EEG traits selected for arousal and valence regression belonged to time domain (standard deviation, complexity, mobility, kurtosis, skewness), and frequency domain (power spectral density---PDS, and differential entropy---DE from theta, alpha, beta, gamma, and all EEG frequency spectrum). Addressing game events, we found that EEG traits related with the theta, alpha and beta band had strong correlations. In addition, distinctive event-related potentials where identified in the presence of both types of game events. Finally, we implemented a classification algorithm to discriminate between positive and negative events using EEG traits to identify emotional information. We obtained good classification performance using only two traits related with frequency domain on the theta band and on the full EEG spectrum.},
	author = {Mart{\'\i}nez-Tejada, Laura and Puertas-Gonz{\'a}lez, Alex and Yoshimura, Natsue and Koike, Yasuharu},
	c1 = {FIRST Institute of Innovative Research, Tokyo Institute of Technology, Yokohama, Kanagawa 226-8503, Japan; System Engineering and Computation School, Universidad Pedag{\'o}gica y Tecnol{\'o}gica de Colombia, Santiago de Tunja 150007, Colombia; Department of Advanced Neuroimaging, Integrative Brain Imaging Center, National Center of Neurology and Psychiatry, Kodaira, Tokyo 187-8551, Japan; FIRST Institute of Innovative Research, Tokyo Institute of Technology, Yokohama, Kanagawa 226-8503, Japan; Neural Information Analysis Laboratories, ATR, Kyoto 619-0288, Japan; PRESTO, JST, Kawaguchi, Saitama 332-0012, Japan; Department of Advanced Neuroimaging, Integrative Brain Imaging Center, National Center of Neurology and Psychiatry, Kodaira, Tokyo 187-8551, Japan; FIRST Institute of Innovative Research, Tokyo Institute of Technology, Yokohama, Kanagawa 226-8503, Japan},
	date = {2021-03-16},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/brainsci11030378},
	isbn = {2076-3425},
	journal = {Brain sciences},
	keywords = {EEG Analysis; Emotion Recognition; Deep Learning for EEG; Emotion Regulation; Affective Computing},
	la = {en},
	number = {3},
	pages = {378--378},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Exploring EEG Characteristics to Identify Emotional Reactions under Videogame Scenarios},
	url = {https://doi.org/10.3390/brainsci11030378},
	volume = {11},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.3390/brainsci11030378}}

@article{quispeutyiama_2022,
	abstract = {The use of machine learning (ML) techniques in affective computing applications focuses on improving the user experience in emotion recognition. The collection of input data (e.g., physiological signals), together with expert annotations are part of the established standard supervised learning methodology used to train human emotion recognition models. However, these models generally require large amounts of labeled data, which is expensive and impractical in the healthcare context, in which data annotation requires even more expert knowledge. To address this problem, this paper explores the use of the self-supervised learning (SSL) paradigm in the development of emotion recognition methods. This approach makes it possible to learn representations directly from unlabeled signals and subsequently use them to classify affective states. This paper presents the key concepts of emotions and how SSL methods can be applied to recognize affective states. We experimentally analyze and compare self-supervised and fully supervised training of a convolutional neural network designed to recognize emotions. The experimental results using three emotion datasets demonstrate that self-supervised representations can learn widely useful features that improve data efficiency, are widely transferable, are competitive when compared to their fully supervised counterparts, and do not require the data to be labeled for learning.},
	author = {Quispe, Kevin and Utyiama, Daniel and dos Santos, Eulanda and Oliveira, Hor{\'a}cio and Souto, Eduardo},
	c1 = {Computer Institute, Federal University of Amazonas, Manaus 69080-900, Brazil; Computer Institute, Federal University of Amazonas, Manaus 69080-900, Brazil; Computer Institute, Federal University of Amazonas, Manaus 69080-900, Brazil; Computer Institute, Federal University of Amazonas, Manaus 69080-900, Brazil; Computer Institute, Federal University of Amazonas, Manaus 69080-900, Brazil},
	date = {2022-11-23},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3390/s22239102},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Emotion Recognition; Affective Computing; Sensory Processing; Speech Emotion; Physiological Signals},
	la = {en},
	number = {23},
	pages = {9102--9102},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Applying Self-Supervised Representation Learning for Emotion Recognition Using Physiological Signals},
	url = {https://doi.org/10.3390/s22239102},
	volume = {22},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.3390/s22239102}}

@article{barathiproulx_2020,
	abstract = {User experience estimation of VR exergame players by recognising their affective state could enable us to personalise and optimise their experience. Affect recognition based on psychophysiological measurements has been successful for moderate intensity activities. High intensity VR exergames pose challenges as the effects of exercise and VR headsets interfere with those measurements. We present two experiments that investigate the use of different sensors for affect recognition in a VR exergame. The first experiment compares the impact of physical exertion and gamification on psychophysiological measurements during rest, conventional exercise, VR exergaming, and sedentary VR gaming. The second experiment compares underwhelming, overwhelming and optimal VR exergaming scenarios. We identify gaze fixations, eye blinks, pupil diameter and skin conductivity as psychophysiological measures suitable for affect recognition in VR exergaming and analyse their utility in determining affective valence and arousal. Our findings provide guidelines for researchers of affective VR exergames.},
	author = {Barathi, Soumya and Proulx, Michael and O'Neill, Eamonn and Lutteroth, Christof},
	c1 = {University of Bath, Bath, United Kingdom; University of Bath, Bath, United Kingdom; University of Bath, Bath, United Kingdom; University of Bath, Bath, United Kingdom},
	date = {2020-04-21},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1145/3313831.3376596},
	keywords = {Emotion Recognition; Affective Computing; Attention Lapses},
	la = {en},
	title = {Affect Recognition using Psychophysiological Correlates in High Intensity VR Exergaming},
	url = {https://doi.org/10.1145/3313831.3376596},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3313831.3376596}}

@article{thaobalamurali_2021,
	abstract = {In this paper, we tackle the problem of predicting the affective responses of movie viewers, based on the content of the movies. Current studies on this topic focus on video representation learning and fusion techniques to combine the extracted features for predicting affect. Yet, these typically, while ignoring the correlation between multiple modality inputs, ignore the correlation between temporal inputs (i.e., sequential features). To explore these correlations, a neural network architecture-namely AttendAffectNet (AAN)-uses the self-attention mechanism for predicting the emotions of movie viewers from different input modalities. Particularly, visual, audio, and text features are considered for predicting emotions (and expressed in terms of valence and arousal). We analyze three variants of our proposed AAN: Feature AAN, Temporal AAN, and Mixed AAN. The Feature AAN applies the self-attention mechanism in an innovative way on the features extracted from the different modalities (including video, audio, and movie subtitles) of a whole movie to, thereby, capture the relationships between them. The Temporal AAN takes the time domain of the movies and the sequential dependency of affective responses into account. In the Temporal AAN, self-attention is applied on the concatenated (multimodal) feature vectors representing different subsequent movie segments. In the Mixed AAN, we combine the strong points of the Feature AAN and the Temporal AAN, by applying self-attention first on vectors of features obtained from different modalities in each movie segment and then on the feature representations of all subsequent (temporal) movie segments. We extensively trained and validated our proposed AAN on both the MediaEval 2016 dataset for the Emotional Impact of Movies Task and the extended COGNIMUSE dataset. Our experiments demonstrate that audio features play a more influential role than those extracted from video and movie subtitles when predicting the emotions of movie viewers on these datasets. The models that use all visual, audio, and text features simultaneously as their inputs performed better than those using features extracted from each modality separately. In addition, the Feature AAN outperformed other AAN variants on the above-mentioned datasets, highlighting the importance of taking different features as context to one another when fusing them. The Feature AAN also performed better than the baseline models when predicting the valence dimension.},
	author = {Thao, Ha and Balamurali, B and Roig, Gemma and Herremans, Dorien},
	c1 = {Information Systems Technology and Design, Singapore University of Technology and Design, 8 Somapah Rd, Singapore 48737, Singapore; Science, Mathematics and Technology, Singapore University of Technology and Design, 8 Somapah Rd, Singapore 48737, Singapore; Computer Science Department, Goethe University Frankfurt, 60323 Frankfurt, Germany; Information Systems Technology and Design, Singapore University of Technology and Design, 8 Somapah Rd, Singapore 48737, Singapore},
	date = {2021-12-14},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3390/s21248356},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Affective Computing; Emotion Recognition; Speech Emotion; Action Recognition; Media Enjoyment},
	la = {en},
	number = {24},
	pages = {8356--8356},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {AttendAffectNet--Emotion Prediction of Movie Viewers Using Multimodal Fusion with Self-Attention},
	url = {https://doi.org/10.3390/s21248356},
	volume = {21},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.3390/s21248356}}

@article{pinillagarcia_2021,
	abstract = {A cluster of research in Affective Computing suggests that it is possible to infer some characteristics of users'affective states by analyzing their electrophysiological activity in real-time. However, it is not clear how to use the information extracted from electrophysiological signals to create visual representations of the affective states of Virtual Reality (VR) users. Visualization of users'affective states in VR can lead to biofeedback therapies for mental health care. Understanding how to visualize affective states in VR requires an interdisciplinary approach that integrates psychology, electrophysiology, and audio-visual design. Therefore, this review aims to integrate previous studies from these fields to understand how to develop virtual environments that can automatically create visual representations of users'affective states. The manuscript addresses this challenge in four sections: First, theories related to emotion and affect are summarized. Second, evidence suggesting that visual and sound cues tend to be associated with affective states are discussed. Third, some of the available methods for assessing affect are described. The fourth and final section contains five practical considerations for the development of virtual reality environments for affect visualization.},
	author = {Pinilla, Andr{\'e}s and Garc{\'\i}a, Jaime and Raffe, William and Voigt-Antons, Jan‐Niklas and Spang, Robert and M{\"o}ller, Sebastian},
	c1 = {Quality and Usability Lab, Institute for Software Technology and Theoretical Computer Science, Faculty of Electrical Engineering and Computer Science, Technische Universit?t Berlin, Germany; UTS Games Studio, Faculty of Engineering and IT, University of Technology Sydney UTS, Australia; UTS Games Studio, Faculty of Engineering and IT, University of Technology Sydney UTS, Australia; UTS Games Studio, Faculty of Engineering and IT, University of Technology Sydney UTS, Australia; German Research Center for Artificial Intelligence (DFKI), Germany; Quality and Usability Lab, Institute for Software Technology and Theoretical Computer Science, Faculty of Electrical Engineering and Computer Science, Technische Universit?t Berlin, Germany; Quality and Usability Lab, Institute for Software Technology and Theoretical Computer Science, Faculty of Electrical Engineering and Computer Science, Technische Universit?t Berlin, Germany; German Research Center for Artificial Intelligence (DFKI), Germany; Quality and Usability Lab, Institute for Software Technology and Theoretical Computer Science, Faculty of Electrical Engineering and Computer Science, Technische Universit?t Berlin, Germany},
	date = {2021-08-06},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3389/frvir.2021.630731},
	isbn = {2673-4192},
	journal = {Frontiers in virtual reality},
	keywords = {Affective Computing; Emotion Recognition; Multisensory Integration; Affective Design},
	la = {en},
	publisher = {Frontiers Media},
	title = {Affective Visualization in Virtual Reality: An Integrative Review},
	url = {https://doi.org/10.3389/frvir.2021.630731},
	volume = {2},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.3389/frvir.2021.630731}}

@article{shumailovgunes_2017,
	abstract = {Progress in the affective computing field has led to the creation of affect-aware games that aim to adapt to the emotions experienced by the players. In this paper we focus on affect recognition in virtual reality (VR) gaming, a problem that to the best of our knowledge has not yet been sufficiently explored. We aim to answer two research questions: (i) Is it possible to reliably capture and recognize the affective state of a person based on EMG sensors placed on their lower arms, while they interact with the virtual environment? and (ii) Is EMG signal from one arm sufficient for detecting affect? We conducted a study in which 8 people were playing a set of VR games with two EMG sensors placed on their arms. We analysed the EMG signals and extracted a number of features to infer the affective states of the players. Our experimental results show that the EMG measures from left and right arms provide sufficient information to detect emotions experienced by a player of a VR game. Our results also show that classifying a DWT-dbl signal with Support Vector Machine (SVM) yields F1=0.91 for predicting low/high arousal and F1=0.85 for predicting positive/negative valence when using just the left-arm EMG signal. To the best of our knowledge, this is the first work that uses EMG data from arm movements as a single source of affective information and addresses affect recognition in VR gaming.},
	author = {Shumailov, Ilia and G{\"u}ne{\c s}, Hatice},
	c1 = {Department of Computer Science \& Technology, University of Cambridge; Department of Computer Science \& Technology, University of Cambridge},
	date = {2017-10-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1109/acii.2017.8273595},
	keywords = {Affective Computing; Emotion Recognition; Sensory Feedback; EEG Analysis},
	la = {en},
	title = {Computational analysis of valence and arousal in virtual reality gaming using lower arm electromyograms},
	url = {https://doi.org/10.1109/acii.2017.8273595},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/acii.2017.8273595}}

@article{petrescupetrescu_2021,
	abstract = {This paper focuses on the binary classification of the emotion of fear, based on the physiological data and subjective responses stored in the DEAP dataset. We performed a mapping between the discrete and dimensional emotional information considering the participants'ratings and extracted a substantial set of 40 types of features from the physiological data, which represented the input to various machine learning algorithms---Decision Trees, k-Nearest Neighbors, Support Vector Machine and artificial networks---accompanied by dimensionality reduction, feature selection and the tuning of the most relevant hyperparameters, boosting classification accuracy. The methodology we approached included tackling different situations, such as resolving the problem of having an imbalanced dataset through data augmentation, reducing overfitting, computing various metrics in order to obtain the most reliable classification scores and applying the Local Interpretable Model-Agnostic Explanations method for interpretation and for explaining predictions in a human-understandable manner. The results show that fear can be predicted very well (accuracies ranging from 91.7{\%} using Gradient Boosting Trees to 93.5{\%} using dimensionality reduction and Support Vector Machine) by extracting the most relevant features from the physiological data and by searching for the best parameters which maximize the machine learning algorithms'classification scores.},
	author = {Petrescu, Livia and Petrescu, C{\u a}t{\u a}lin and Oprea, Ana and Mitruț, Oana and Moise, Gabriela and Moldoveanu, Alin and Moldoveanu, Florica},
	c1 = {Faculty of Biology, University of Bucharest, 050095 Bucharest, Romania; Faculty of Automatic Control and Computers, University Politehnica of Bucharest, 060042 Bucharest, Romania; Faculty of Automatic Control and Computers, University Politehnica of Bucharest, 060042 Bucharest, Romania; Faculty of Automatic Control and Computers, University Politehnica of Bucharest, 060042 Bucharest, Romania; Faculty of Letters and Sciences, Petroleum-Gas University of Ploiesti, 100680 Ploiesti, Romania; Faculty of Automatic Control and Computers, University Politehnica of Bucharest, 060042 Bucharest, Romania; Faculty of Automatic Control and Computers, University Politehnica of Bucharest, 060042 Bucharest, Romania},
	date = {2021-07-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3390/s21134519},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Affective Computing; Emotion Recognition; Feature Extraction; Emotion Regulation},
	la = {en},
	number = {13},
	pages = {4519--4519},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Machine Learning Methods for Fear Classification Based on Physiological Features},
	url = {https://doi.org/10.3390/s21134519},
	volume = {21},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.3390/s21134519}}

@article{kumanootsuka_2015,
	abstract = {This paper presents a research framework for understanding the empathy that arises between people while they are conversing. By focusing on the process by which empathy is perceived by other people, this paper aims to develop a computational model that automatically infers perceived empathy from participant behavior. To describe such perceived empathy objectively, we introduce the idea of using the collective impressions of external observers. In particular, we focus on the fact that the perception of other's empathy varies from person to person, and take the standpoint that this individual difference itself is an essential attribute of human communication for building, for example, successful human relationships and consensus. This paper describes a probabilistic model of the process that we built based on the Bayesian network, and that relates the empathy perceived by observers to how the gaze and facial expressions of participants co-occur between a pair. In this model, the probability distribution represents the diversity of observers' impression, which reflects the individual differences in the schema when perceiving others' empathy from their behaviors, and the ambiguity of the behaviors. Comprehensive experiments demonstrate that the inferred distributions are similar to those made by observers.},
	author = {Kumano, Shiro and Otsuka, Kazuhiro and Mikami, Dan and Matsuda, Masafumi and Yamato, Junji},
	c1 = {{$[$}Nippon Telegraph and Telephone Corporation, Kanagawa, Japan{$]$}; {$[$}Nippon Telegraph and Telephone Corporation, Kanagawa, Japan{$]$}; {$[$}Nippon Telegraph and Telephone Corporation, Kanagawa, Japan{$]$}; {$[$}Nippon Telegraph and Telephone Corporation, Kanagawa, Japan{$]$}; {$[$}Nippon Telegraph and Telephone Corporation, Kanagawa, Japan{$]$}},
	date = {2015-10-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/taffc.2015.2417561},
	isbn = {1949-3045},
	journal = {IEEE transactions on affective computing},
	keywords = {Affective Computing; Emotion Perception; Intention Understanding; Emotion Recognition; Interpersonal Synchrony},
	la = {en},
	number = {4},
	pages = {324--336},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {Analyzing Interpersonal Empathy via Collective Impressions},
	url = {https://doi.org/10.1109/taffc.2015.2417561},
	volume = {6},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/taffc.2015.2417561}}

@article{chaudharibhatt_2023,
	abstract = {Emotion recognition is a very challenging research field due to its complexity, as individual differences in cognitive--emotional cues involve a wide variety of ways, including language, expressions, and speech. If we use video as the input, we can acquire a plethora of data for analyzing human emotions. In this research, we use features derived from separately pretrained self-supervised learning models to combine text, audio (speech), and visual data modalities. The fusion of features and representation is the biggest challenge in multimodal emotion classification research. Because of the large dimensionality of self-supervised learning characteristics, we present a unique transformer and attention-based fusion method for incorporating multimodal self-supervised learning features that achieved an accuracy of 86.40{\%} for multimodal emotion classification.},
	author = {Chaudhari, Aayushi and Bhatt, Chintan and Krishna, Achyut and Gonz{\'a}lez, Carmelo},
	c1 = {CHARUSAT Campus, Charotar University of Science and Technology, Changa 388421, India; U \& P U. Patel Department of Computer Engineering, Chandubhai S Patel Institute of Technology (CSPIT),; Department of Computer Science and Engineering, School of Technology, Pandit Deendayal Energy University, Gandhinagar 382007, India; CHARUSAT Campus, Charotar University of Science and Technology, Changa 388421, India; U \& P U. Patel Department of Computer Engineering, Chandubhai S Patel Institute of Technology (CSPIT),; Signals and Communications Department, IDeTIC, University of Las Palmas de Gran Canaria, 35001 Las Palmas, Spain},
	date = {2023-01-05},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/electronics12020288},
	isbn = {2079-9292},
	journal = {Electronics},
	keywords = {Emotion Recognition; Audio-Visual Speech Recognition; Affective Computing; Speech Emotion; Multimodal Data},
	la = {en},
	number = {2},
	pages = {288--288},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Facial Emotion Recognition with Inter-Modality-Attention-Transformer-Based Self-Supervised Learning},
	url = {https://doi.org/10.3390/electronics12020288},
	volume = {12},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.3390/electronics12020288}}

@article{wosiakdura_2020,
	abstract = {Based on the growing interest in encephalography to enhance human--computer interaction (HCI) and develop brain--computer interfaces (BCIs) for control and monitoring applications, efficient information retrieval from EEG sensors is of great importance. It is difficult due to noise from the internal and external artifacts and physiological interferences. The enhancement of the EEG-based emotion recognition processes can be achieved by selecting features that should be taken into account in further analysis. Therefore, the automatic feature selection of EEG signals is an important research area. We propose a multistep hybrid approach incorporating the Reversed Correlation Algorithm for automated frequency band---electrode combinations selection. Our method is simple to use and significantly reduces the number of sensors to only three channels. The proposed method has been verified by experiments performed on the DEAP dataset. The obtained effects have been evaluated regarding the accuracy of two emotions---valence and arousal. In comparison to other research studies, our method achieved classification results that were 4.20--8.44{\%} greater. Moreover, it can be perceived as a universal EEG signal classification technique, as it belongs to unsupervised methods.},
	author = {Wosiak, Agnieszka and Dura, Aleksandra},
	c1 = {Institute of Information Technology, Lodz University of Technology, W{\'o}lcza {\'n}ska 215, 90-924 {\L}{\'o}d{\'z}, Poland;; Institute of Information Technology, Lodz University of Technology, W{\'o}lcza {\'n}ska 215, 90-924 {\L}{\'o}d{\'z}, Poland;},
	date = {2020-12-10},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3390/s20247083},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {EEG Analysis; Emotion Recognition; Deep Learning for EEG; Affective Computing; Sensory Processing},
	la = {en},
	number = {24},
	pages = {7083--7083},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Hybrid Method of Automated EEG Signals'Selection Using Reversed Correlation Algorithm for Improved Classification of Emotions},
	url = {https://doi.org/10.3390/s20247083},
	volume = {20},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.3390/s20247083}}

@article{zhangali_2023,
	abstract = {Instead of predicting just one emotion for one activity (e.g., video watching), fine-grained emotion recognition enables more temporally precise recognition. Previous works on fine-grained emotion recognition require segment-by-segment, fine-grained emotion labels to train the recognition algorithm. However, experiments to collect these labels are costly and time-consuming compared with only collecting one emotion label after the user watched that stimulus (i.e., the post-stimuli emotion labels). To recognize emotions at a finer granularity level when trained with only post-stimuli labels, we propose an emotion recognition algorithm based on Deep Multiple Instance Learning ( <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">EDMIL</i> ) using physiological signals. <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">EDMIL</i> recognizes fine-grained valence and arousal (V-A) labels by identifying which instances represent the post-stimuli V-A annotated by users after watching the videos. Instead of fully-supervised training, the instances are weakly-supervised by the post-stimuli labels in the training stage. The V-A of instances are estimated by the instance gains, which indicate the probability of instances to predict the post-stimuli labels. We tested <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">EDMIL</i> on three different datasets, <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">CASE</i> , <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">MERCA</i> and <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">CEAP-360VR</i> , collected in three different environments: desktop, mobile and HMD-based Virtual Reality, respectively. Recognition results validated with the fine-grained V-A self-reports show that for subject-independent 3-class classification (high/neutral/low), <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">EDMIL</i> obtains promising recognition accuracies: 75.63{\%} and 79.73{\%} for V-A on <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">CASE</i> , 70.51{\%} and 67.62{\%} for V-A on <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">MERCA</i> and 65.04{\%} and 67.05{\%} for V-A on <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">CEAP-360VR</i> . Our ablation study shows that all components of <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">EDMIL</i> contribute to both the classification and regression tasks. Our experiments also show that (1) compared with fully-supervised learning, weakly-supervised learning can reduce the problem of overfitting caused by the temporal mismatch between fine-grained annotations and physiological signals, (2) instance segment lengths between 1-2 s result in the highest recognition accuracies and (3) <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">EDMIL</i> performs best if post-stimuli annotations consist of less than 30{\%} or more than 60{\%} of the entire video watching.},
	author = {Zhang, Tianyi and Ali, Abdallah and Wang, Chen and Hanjalic, Alan and C{\'e}sar, Pablo},
	c1 = {Distributed and Interactive Systems, Centrum Wiskunde \&{\#}x0026; Informatica (CWI), Amsterdam, The Netherlands; Multimedia Computing Group, Delft University of Technology, Delft, The Netherlands; Distributed and Interactive Systems, Centrum Wiskunde \&{\#}x0026; Informatica (CWI), Amsterdam, The Netherlands; State Key Laboratory of Media Convergence Production Technology and Systems, Xinhua News Agency \&{\#}x0026; Future Media and Convergence Institute, Beijing, China; Multimedia Computing Group, Delft University of Technology, Delft, The Netherlands; Distributed and Interactive Systems, Centrum Wiskunde \&{\#}x0026; Informatica (CWI), Amsterdam, The Netherlands; Multimedia Computing Group, Delft University of Technology, Delft, The Netherlands},
	date = {2023-07-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1109/taffc.2022.3158234},
	isbn = {1949-3045},
	journal = {IEEE transactions on affective computing},
	keywords = {Emotion Recognition; Affective Computing; Deep Learning for EEG; Deep Learning; Physiological Signals},
	la = {en},
	number = {3},
	pages = {2304--2322},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {Weakly-Supervised Learning for Fine-Grained Emotion Recognition Using Physiological Signals},
	url = {https://doi.org/10.1109/taffc.2022.3158234},
	volume = {14},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/taffc.2022.3158234}}

@article{vujictong_2020,
	abstract = {A hard challenge for wearable systems is to measure differences in emotional valence, i.e. positive and negative affect via physiology. However, the stomach or gastric signal is an unexplored modality that could offer new affective information. We created a wearable device and software to record gastric signals, known as electrogastrography (EGG). An in-laboratory study was conducted to compare EGG with electrodermal activity (EDA) in 33 individuals viewing affective stimuli. We found that negative stimuli attenuate EGG's indicators of parasympathetic activation, or "rest and digest" activity. We compare EGG to the remaining physiological signals and describe implications for affect detection. Further, we introduce how wearable EGG may support future applications in areas as diverse as reducing nausea in virtual reality and helping treat emotion-related eating disorders.},
	author = {Vujic, Angela and Tong, Stephanie and Picard, Rosalind and Maes, Pattie},
	c1 = {Massachusetts Institute of Technology, Cambridge, MA, USA; Harvard University, Cambridge, MA, USA; Massachusetts Institute of Technology, Cambridge, MA, USA; Massachusetts Institute of Technology, Cambridge, MA, USA},
	date = {2020-10-21},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1145/3382507.3418882},
	keywords = {Sensory Expectations; Emotion Recognition; Affective Computing; Physiological Signals; Multisensory Integration},
	la = {en},
	title = {Going with our Guts},
	url = {https://doi.org/10.1145/3382507.3418882},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3382507.3418882}}

@article{glowinskimortillaro_2015,
	abstract = {How efficiently decoding affective information when computational resources and sensor systems are limited?This paper presents a framework for analysis of affective behavior starting with a reduced amount of visual information related to human upper-body movements.The main goal is to individuate a minimal representation of emotional displays based on non-verbal gesture features.The GEMEP (Geneva multimodal emotion portrayals) corpus was used to validate this framework.Twelve emotions expressed by ten actors form the selected data set of emotion portrayals.Visual tracking of trajectories of head and hands was performed from a frontal and a lateral view.Postural/shape and dynamic expressive gesture features were identified and analyzed.A feature reduction procedure was carried out, resulting in a four-dimensional model of emotion expression, that effectively classified/grouped emotions according to their valence (positive, negative) and arousal (high, low).These results show that emotionally relevant information can be detected/measured/obtained from the dynamic qualities of gesture.The framework was implemented as software modules (plug-ins) extending the EyesWeb XMI Expressive Gesture Processing Library and was tested as a component for a multimodal search engine in collaboration with Google within the EU-ICT I-SEARCH project.},
	author = {Glowinski, Donald and Mortillaro, Marcello and Scherer, Klaus and Dael, Nele and Camurri, Antonio},
	c1 = {Swiss Center for Affective Sciences University of Geneva Switzerland CH-1205; Swiss Center for Affective Sciences University of Geneva Switzerland CH-1205; Swiss Center for Affective Sciences University of Geneva Switzerland CH-1205; Institute of Psychology University of Lausanne Lausanne, CH 1015; Casa Paganini -InfoMus (DIBRIS) University of Genoa Genoa, IT-16145},
	date = {2015-09-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/acii.2015.7344616},
	keywords = {Affective Computing; Emotion Recognition; Gesture; Facial Expression; Feature Extraction},
	la = {en},
	title = {Towards a minimal representation of affective gestures (Extended abstract)},
	url = {https://doi.org/10.1109/acii.2015.7344616},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/acii.2015.7344616}}

@article{cunninghamridley_2019,
	abstract = {In recent years, the field of Music Emotion Recognition has become established. Less attention has been directed towards the counterpart domain of Audio Emotion Recognition, which focuses upon detection of emotional stimuli resulting from non-musical sound. By better understanding how sounds provoke emotional responses in an audience it may be possible to enhance the work of sound designers.},
	author = {Cunningham, Stuart and Ridley, Harrison and Weinel, Jonathan and Picking, Richard},
	c1 = {Manchester Metropolitan University, Manchester, UK; Manchester Metropolitan University, Manchester, UK; Coventry University, Coventry, UK; Wrexham Glynd{\^w}r University, Wrexham, UK},
	date = {2019-09-18},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1145/3356590.3356609},
	keywords = {Emotion Recognition; Environmental Sound Recognition; Audio Event Detection; Speech Emotion; Affective Computing},
	la = {en},
	title = {Audio Emotion Recognition using Machine Learning to support Sound Design},
	url = {https://doi.org/10.1145/3356590.3356609},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1145/3356590.3356609}}

@article{wangren_2023,
	abstract = {Automatic electroencephalogram (EEG) emotion recognition is a challenging component of human-computer interaction (HCI). Inspired by the powerful feature learning ability of recently-emerged deep learning techniques, various advanced deep learning models have been employed increasingly to learn high-level feature representations for EEG emotion recognition. This paper aims to provide an up-to-date and comprehensive survey of EEG emotion recognition, especially for various deep learning techniques in this area. We provide the preliminaries and basic knowledge in the literature. We review EEG emotion recognition benchmark data sets briefly. We review deep learning techniques in details, including deep belief networks, convolutional neural networks, and recurrent neural networks. We describe the state-of-the-art applications of deep learning techniques for EEG emotion recognition in detail. We analyze the challenges and opportunities in this field and point out its future directions.},
	author = {Wang, Xiaohu and Ren, Yongmei and Luo, Ze and He, Wei and Hong, Jun and Huang, Yue},
	c1 = {School of Intelligent Manufacturing and Mechanical Engineering, Hunan Institute of Technology, Hengyang, China; School of Electrical and Information Engineering, Hunan Institute of Technology, Hengyang, China; School of Intelligent Manufacturing and Mechanical Engineering, Hunan Institute of Technology, Hengyang, China; School of Electrical and Information Engineering, Hunan Institute of Technology, Hengyang, China; School of Intelligent Manufacturing and Mechanical Engineering, Hunan Institute of Technology, Hengyang, China; School of Computer and Information Engineering, Hunan Institute of Technology, Hengyang, China},
	date = {2023-02-27},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3389/fpsyg.2023.1126994},
	isbn = {1664-1078},
	journal = {Frontiers in psychology},
	keywords = {Deep Learning for EEG; Emotion Recognition; EEG Analysis; Affective Computing; Head Gesture Recognition},
	la = {en},
	publisher = {Frontiers Media},
	title = {Deep learning-based EEG emotion recognition: Current trends and future perspectives},
	url = {https://doi.org/10.3389/fpsyg.2023.1126994},
	volume = {14},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.3389/fpsyg.2023.1126994}}

@article{kimkim_2022,
	abstract = {Deep learning-based emotion recognition using EEG has received increasing attention in recent years. The existing studies on emotion recognition show great variability in their employed methods including the choice of deep learning approaches and the type of input features. Although deep learning models for EEG-based emotion recognition can deliver superior accuracy, it comes at the cost of high computational complexity. Here, we propose a novel 3D convolutional neural network with a channel bottleneck module (CNN-BN) model for EEG-based emotion recognition, with the aim of accelerating the CNN computation without a significant loss in classification accuracy. To this end, we constructed a 3D spatiotemporal representation of EEG signals as the input of our proposed model. Our CNN-BN model extracts spatiotemporal EEG features, which effectively utilize the spatial and temporal information in EEG. We evaluated the performance of the CNN-BN model in the valence and arousal classification tasks. Our proposed CNN-BN model achieved an average accuracy of 99.1{\%} and 99.5{\%} for valence and arousal, respectively, on the DEAP dataset, while significantly reducing the number of parameters by 93.08{\%} and FLOPs by 94.94{\%}. The CNN-BN model with fewer parameters based on 3D EEG spatiotemporal representation outperforms the state-of-the-art models. Our proposed CNN-BN model with a better parameter efficiency has excellent potential for accelerating CNN-based emotion recognition without losing classification performance.},
	author = {Kim, Sungkyu and Kim, Tae‐Seong and Lee, Won},
	c1 = {Department of Software Convergence, Kyung Hee University, Yongin 17104, Korea; Department of Biomedical Engineering, Kyung Hee University, Yongin 17104, Korea; Department of Software Convergence, Kyung Hee University, Yongin 17104, Korea},
	date = {2022-09-08},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/s22186813},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Emotion Recognition; Deep Learning for EEG; Affective Computing; EEG Analysis; Deep Learning},
	la = {en},
	number = {18},
	pages = {6813--6813},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Accelerating 3D Convolutional Neural Network with Channel Bottleneck Module for EEG-Based Emotion Recognition},
	url = {https://doi.org/10.3390/s22186813},
	volume = {22},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.3390/s22186813}}

@article{aunghassan_2022,
	abstract = {Humans experience a variety of emotions throughout the course of their daily lives, including happiness, sadness, and rage. As a result, an effective emotion identification system is essential for electroencephalography (EEG) data to accurately reflect emotion in real-time. Although recent studies on this problem can provide acceptable performance measures, it is still not adequate for the implementation of a complete emotion recognition system. In this research work, we propose a new approach for an emotion recognition system, using multichannel EEG calculation with our developed entropy known as multivariate multiscale modified-distribution entropy (MM-mDistEn) which is combined with a model based on an artificial neural network (ANN) to attain a better outcome over existing methods. The proposed system has been tested with two different datasets and achieved better accuracy than existing methods. For the GAMEEMO dataset, we achieved an average accuracy $\pm$standard deviation of 95.73{\%} $\pm$0.67 for valence and 96.78{\%} $\pm$0.25 for arousal. Moreover, the average accuracy percentage for the DEAP dataset reached 92.57{\%} $\pm$1.51 in valence and 80.23{\%} $\pm$1.83 in arousal.},
	author = {Aung, Si and Hassan, Mehedi and Brady, Mark and Mannan, Zubaer and Azam, Sami and Karim, Asif and Zaman, Sadika and Wongsawat, Yodchanan},
	c1 = {Department of Biomedical Engineering, Faculty of Engineering, Mahidol University, Salaya, Thailand; Computer Science and Engineering, North Western University, Khulna, Bangladesh; Asia Pacific College of Business and Law, Charles Darwin University, Casuarina, NT, Australia; Department of Smart Computing, Kyungdong University, Global Campus, Goseong-Gun, Republic of Korea; College of Engineering IT and Environment, Charles Darwin University, Casuarina, NT, Australia; College of Engineering IT and Environment, Charles Darwin University, Casuarina, NT, Australia; Computer Science and Engineering, North Western University, Khulna, Bangladesh; Department of Biomedical Engineering, Faculty of Engineering, Mahidol University, Salaya, Thailand},
	date = {2022-10-13},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1155/2022/6000989},
	isbn = {1687-5265},
	journal = {Computational intelligence and neuroscience},
	keywords = {Emotion Recognition; Affective Computing; Deep Learning for EEG; EEG Analysis; Speech Emotion},
	la = {en},
	pages = {1--13},
	publisher = {Hindawi Publishing Corporation},
	title = {Entropy-Based Emotion Recognition from Multichannel EEG Signals Using Artificial Neural Network},
	url = {https://doi.org/10.1155/2022/6000989},
	volume = {2022},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1155/2022/6000989}}

@article{lara-cabreracamacho_2019,
	abstract = {Affective games are a sub-field of affective computing that tries to study how to design videogames that are able to react to the emotions expressed by the player, as well as provoking desired emotions to them. To achieve those goals it is necessary to research on how to measure and detect human emotions using a computer, and how to adapt videogames to the perceived emotions to finally provoke them to the players. This work presents a taxonomy for research on affective games centring on the aforementioned issues. Here we devise as well a revision of the most relevant published works known to the authors on this area. Finally, we analyse and discuss which important research problem are yet open and might be tackled by future investigations in the area of affective games.},
	author = {Lara-Cabrera, Ra{\'u}l and Camacho, David},
	c1 = {Computer Science Department, Universidad Aut{\'o}noma de Madrid, Spain; Computer Science Department, Universidad Aut{\'o}noma de Madrid, Spain},
	date = {2019-03-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1016/j.future.2017.12.056},
	isbn = {0167-739X},
	journal = {Future generation computer systems},
	keywords = {Affective Computing; Emotion Recognition; Emotion Perception},
	la = {en},
	pages = {516--525},
	publisher = {Elsevier BV},
	title = {A taxonomy and state of the art revision on affective games},
	url = {https://doi.org/10.1016/j.future.2017.12.056},
	volume = {92},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1016/j.future.2017.12.056}}

@article{wampflerklingler_2022,
	abstract = {Knowledge of users'affective states can improve their interaction with smartphones by providing more personalized experiences (e.g., search results and news articles). We present an affective state classification model based on data gathered on smartphones in real-world environments. From touch events during keystrokes and the signals from the inertial sensors, we extracted two-dimensional heat maps as input into a convolutional neural network to predict the affective states of smartphone users. For evaluation, we conducted a data collection in the wild with 82 participants over 10 weeks. Our model accurately predicts three levels (low, medium, high) of valence (AUC up to 0.83), arousal (AUC up to 0.85), and dominance (AUC up to 0.84). We also show that using the inertial sensor data alone, our model achieves a similar performance (AUC up to 0.83), making our approach less privacy-invasive. By personalizing our model to the user, we show that performance increases by an additional 0.07 AUC.},
	author = {Wampfler, Rafael and Klingler, Severin and Solenthaler, Barbara and Schinazi, Victor and Gro{\ss}, Markus and Holz, Christian},
	c1 = {Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland; Department of Psychology, Bond University, Australia and Future Health Technologies, Singapore-ETH Centre, Singapore; Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland},
	date = {2022-04-29},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1145/3491102.3501835},
	journal = {CHI Conference on Human Factors in Computing Systems},
	keywords = {Affective Computing; Emotion Recognition; Emotional Design},
	la = {en},
	title = {Affective State Prediction from Smartphone Touch and Sensor Data in the Wild},
	url = {https://doi.org/10.1145/3491102.3501835},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1145/3491102.3501835}}

@article{chenmin_2022,
	abstract = {In electroencephalograph (EEG) emotion recognition research, obtaining high-level emotional features with more discriminative information has become the key to improving the classification performance. This study proposes a new end-to-end emotion recognition method based on brain connectivity (BC) features and domain adaptive residual convolutional network (short for BC-DA-RCNN), which could effectively extract the spatial connectivity information related to the emotional state of the human brain and introduce domain adaptation to achieve accurate emotion recognition within and across the subject's EEG signals. The BC information is represented by the global brain network connectivity matrix. The DA-RCNN is used to extract high-level emotional features between different dimensions of EEG signals, reduce the domain offset between different subjects, and strengthen the common features between different subjects. The experimental results on the large public DEAP data set show that the accuracy of the subject-dependent and subject-independent binary emotion classification in valence reaches 95.15 and 88.28{\%}, respectively, which outperforms all the benchmark methods. The proposed method is proven to have lower complexity, better generalization ability, and domain robustness that help to lay a solid foundation for the development of high-performance affective brain-computer interface applications.},
	author = {Chen, Jingxia and Min, Chongdan and Wang, Changhao and Tang, Zhezhe and Liu, Yang and Hu, Xiuwen},
	c1 = {School of Electronic Information and Artificial Intelligence, Shaanxi University of Science and Technology, China; School of Electronic Information and Artificial Intelligence, Shaanxi University of Science and Technology, China; School of Electronic Information and Artificial Intelligence, Shaanxi University of Science and Technology, China; School of Electronic Information and Artificial Intelligence, Shaanxi University of Science and Technology, China; School of Electronic Information and Artificial Intelligence, Shaanxi University of Science and Technology, China; School of Electronic Information and Artificial Intelligence, Shaanxi University of Science and Technology, China},
	date = {2022-06-22},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3389/fnins.2022.878146},
	isbn = {1662-453X},
	journal = {Frontiers in neuroscience},
	keywords = {Emotion Recognition; Affective Computing; Deep Learning for EEG; EEG Analysis; Epilepsy Detection},
	la = {en},
	publisher = {Frontiers Media},
	title = {Electroencephalograph-Based Emotion Recognition Using Brain Connectivity Feature and Domain Adaptive Residual Convolution Model},
	url = {https://doi.org/10.3389/fnins.2022.878146},
	volume = {16},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.3389/fnins.2022.878146}}

@article{moontahaschumann_2023,
	abstract = {Giving emotional intelligence to machines can facilitate the early detection and prediction of mental diseases and symptoms. Electroencephalography (EEG)-based emotion recognition is widely applied because it measures electrical correlates directly from the brain rather than indirect measurement of other physiological responses initiated by the brain. Therefore, we used non-invasive and portable EEG sensors to develop a real-time emotion classification pipeline. The pipeline trains different binary classifiers for Valence and Arousal dimensions from an incoming EEG data stream achieving a 23.9{\%} (Arousal) and 25.8{\%} (Valence) higher F1-Score on the state-of-art AMIGOS dataset than previous work. Afterward, the pipeline was applied to the curated dataset from 15 participants using two consumer-grade EEG devices while watching 16 short emotional videos in a controlled environment. Mean F1-Scores of 87{\%} (Arousal) and 82{\%} (Valence) were achieved for an immediate label setting. Additionally, the pipeline proved to be fast enough to achieve predictions in real-time in a live scenario with delayed labels while continuously being updated. The significant discrepancy from the readily available labels on the classification scores leads to future work to include more data. Thereafter, the pipeline is ready to be used for real-time applications of emotion classification.},
	author = {Moontaha, Sidratul and Schumann, Franziska and Arnrich, Bert},
	c1 = {Digital Health---Connected Healthcare, Hasso Plattner Institute, University of Potsdam, 14482 Potsdam, Germany; Digital Health---Connected Healthcare, Hasso Plattner Institute, University of Potsdam, 14482 Potsdam, Germany; Digital Health---Connected Healthcare, Hasso Plattner Institute, University of Potsdam, 14482 Potsdam, Germany},
	date = {2023-02-21},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3390/s23052387},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Epilepsy Detection},
	la = {en},
	number = {5},
	pages = {2387--2387},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Online Learning for Wearable EEG-Based Emotion Classification},
	url = {https://doi.org/10.3390/s23052387},
	volume = {23},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.3390/s23052387}}

@article{elalamyfanourakis_2021,
	abstract = {In this paper we propose to use Recurrence Plots (RP) to generate 2D representations of physiological activity which should be less subject dependent and better suited for non-stationary signals such as EDA. The performance of spectrograms and RPs are compared on two publicly available datasets: AMIGOS and DEAP. Transfer learning is employed by using a pre-trained ResNet-50 model to recognize emotional states (high vs low arousal and high vs low valence) from the two types of representations. Results show that RPs reach a similar performance to spectrograms on periodic signals such as ECG and plethysmography (F1 of 0.76 for valence and 0.74 for arousal on the AMIGOS dataset) while they outperform spectrograms on EDA (F1 of 0.74 for valence and 0.75 for arousal). By combining the two sources of information we were able to reach a F1 of 0.76 for valence and 0.75 for arousal.},
	author = {Elalamy, Rayan and Fanourakis, Marios and Chanel, Guillaume},
	c1 = {University of Geneva, Geneva, Switzerland; University of Geneva, Geneva, Switzerland; University of Geneva, Geneva, Switzerland},
	date = {2021-09-28},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/acii52823.2021.9597442},
	keywords = {Emotion Recognition; Affective Computing; Deep Learning for EEG; Signal Processing; Neural Ensemble Physiology},
	la = {en},
	title = {Multi-modal emotion recognition using recurrence plots and transfer learning on physiological signals},
	url = {https://doi.org/10.1109/acii52823.2021.9597442},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/acii52823.2021.9597442}}

@article{saffaryazdiwasim_2022,
	abstract = {Emotions are multimodal processes that play a crucial role in our everyday lives. Recognizing emotions is becoming more critical in a wide range of application domains such as healthcare, education, human-computer interaction, Virtual Reality, intelligent agents, entertainment, and more. Facial macro-expressions or intense facial expressions are the most common modalities in recognizing emotional states. However, since facial expressions can be voluntarily controlled, they may not accurately represent emotional states. Earlier studies have shown that facial micro-expressions are more reliable than facial macro-expressions for revealing emotions. They are subtle, involuntary movements responding to external stimuli that cannot be controlled. This paper proposes using facial micro-expressions combined with brain and physiological signals to more reliably detect underlying emotions. We describe our models for measuring arousal and valence levels from a combination of facial micro-expressions, Electroencephalography (EEG) signals, galvanic skin responses (GSR), and Photoplethysmography (PPG) signals. We then evaluate our model using the DEAP dataset and our own dataset based on a subject-independent approach. Lastly, we discuss our results, the limitations of our work, and how these limitations could be overcome. We also discuss future directions for using facial micro-expressions and physiological signals in emotion recognition.},
	author = {Saffaryazdi, Nastaran and Wasim, Syed and Dileep, Kuldeep and Nia, Alireza and Nanayakkara, Suranga and Broadbent, Elizabeth and Billinghurst, Mark},
	c1 = {Empathic Computing Laboratory, Auckland Bioengineering Institute, The University of Auckland, Auckland, New Zealand; Empathic Computing Laboratory, Auckland Bioengineering Institute, The University of Auckland, Auckland, New Zealand; Empathic Computing Laboratory, Auckland Bioengineering Institute, The University of Auckland, Auckland, New Zealand; Empathic Computing Laboratory, Auckland Bioengineering Institute, The University of Auckland, Auckland, New Zealand; Augmented Human Laboratory, Auckland Bioengineering Institute, The University of Auckland, Auckland, New Zealand; Department of Psychological Medicine, The University of Auckland, Auckland, New Zealand; Empathic Computing Laboratory, Auckland Bioengineering Institute, The University of Auckland, Auckland, New Zealand},
	date = {2022-06-28},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3389/fpsyg.2022.864047},
	isbn = {1664-1078},
	journal = {Frontiers in psychology},
	keywords = {Emotion Recognition; Facial Expression; Affective Computing; Speech Emotion; Head Gesture Recognition},
	la = {en},
	publisher = {Frontiers Media},
	title = {Using Facial Micro-Expressions in Combination With EEG and Physiological Signals for Emotion Recognition},
	url = {https://doi.org/10.3389/fpsyg.2022.864047},
	volume = {13},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.3389/fpsyg.2022.864047}}

@article{jiazhang_2022,
	abstract = {Electroencephalography (EEG) is recorded by electrodes from different areas of the brain and is commonly used to measure neuronal activity. EEG-based methods have been widely used for emotion recognition recently. However, most current methods for EEG-based emotion recognition do not fully exploit the relationship of EEG channels, which affects the precision of emotion recognition. To address the issue, in this paper, we propose a novel method for EEG-based emotion recognition called CR-GCN: Channel-Relationships-based Graph Convolutional Network. Specifically, topological structure of EEG channels is distance-based and tends to capture local relationships, and brain functional connectivity tends to capture global relationships among EEG channels. Therefore, in this paper, we construct EEG channel relationships using an adjacency matrix in graph convolutional network where the adjacency matrix captures both local and global relationships among different EEG channels. Extensive experiments demonstrate that CR-GCN method significantly outperforms the state-of-the-art methods. In subject-dependent experiments, the average classification accuracies of 94.69{\%} and 93.95{\%} are achieved for valence and arousal. In subject-independent experiments, the average classification accuracies of 94.78{\%} and 93.46{\%} are obtained for valence and arousal.},
	author = {Jia, Jingjing and Zhang, Bofeng and Lv, Hehe and Xu, Zhikang and Hu, Shengxiang and Li, Haiyan},
	c1 = {School of Computer Engineering and Science, Shanghai University, Shanghai 200444, China; School of Computer Science and Technology, Kashi University, Kashi 844008, China; School of Computer and Communication Engineering, Shanghai Polytechnic University, Shanghai 201209, China; School of Computer Engineering and Science, Shanghai University, Shanghai 200444, China; School of Computer Engineering and Science, Shanghai University, Shanghai 200444, China; School of Computer Engineering and Science, Shanghai University, Shanghai 200444, China; School of Computer Science and Technology, Kashi University, Kashi 844008, China},
	date = {2022-07-26},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/brainsci12080987},
	isbn = {2076-3425},
	journal = {Brain sciences},
	keywords = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Brain-Computer Interfaces},
	la = {en},
	number = {8},
	pages = {987--987},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {CR-GCN: Channel-Relationships-Based Graph Convolutional Network for EEG Emotion Recognition},
	url = {https://doi.org/10.3390/brainsci12080987},
	volume = {12},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.3390/brainsci12080987}}

@article{wirawanwardoyo_2022,
	abstract = {Electroencephalogram (EEG) signals in recognizing emotions have several advantages. Still, the success of this study, however, is strongly influenced by: i) the distribution of the data used, ii) consider of differences in participant characteristics, and iii) consider the characteristics of the EEG signals. In response to these issues, this study will examine three important points that affect the success of emotion recognition packaged in several research questions: i) What factors need to be considered to generate and distribute EEG data?, ii) How can EEG signals be generated with consideration of differences in participant characteristics?, and iii) How do EEG signals with characteristics exist among its features for emotion recognition? The results, therefore, indicate some important challenges to be studied further in EEG signals-based emotion recognition research. These include i) determine robust methods for imbalanced EEG signals data, ii) determine the appropriate smoothing method to eliminate disturbances on the baseline signals, iii) determine the best baseline reduction methods to reduce the differences in the characteristics of the participants on the EEG signals, iv) determine the robust architecture of the capsule network method to overcome the loss of knowledge information and apply it in more diverse data set.},
	author = {Wirawan, I and Wardoyo, Retantyo and Lelono, Danang},
	c1 = {Universitas Gadjah Mada; Universitas Gadjah Mada; Universitas Gadjah Mada},
	date = {2022-04-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.11591/ijece.v12i2.pp1508-1519},
	isbn = {2722-256X},
	journal = {International Journal of Power Electronics and Drive Systems/International Journal of Electrical and Computer Engineering},
	keywords = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Affective Computing; ECG Signal},
	la = {en},
	number = {2},
	pages = {1508--1508},
	publisher = {Institute of Advanced Engineering and Science (IAES)},
	title = {The challenges of emotion recognition methods based on electroencephalogram signals: a literature review},
	url = {https://doi.org/10.11591/ijece.v12i2.pp1508-1519},
	volume = {12},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.11591/ijece.v12i2.pp1508-1519}}

@article{guyu_2021,
	abstract = {Electroencephalogram (EEG)-based emotion recognition (ER) has drawn increasing attention in the brain--computer interface (BCI) due to its great potentials in human--machine interaction applications. According to the characteristics of rhythms, EEG signals usually can be divided into several different frequency bands. Most existing methods concatenate multiple frequency band features together and treat them as a single feature vector. However, it is often difficult to utilize band-specific information in this way. In this study, an optimized projection and Fisher discriminative dictionary learning (OPFDDL) model is proposed to efficiently exploit the specific discriminative information of each frequency band. Using subspace projection technology, EEG signals of all frequency bands are projected into a subspace. The shared dictionary is learned in the projection subspace such that the specific discriminative information of each frequency band can be utilized efficiently, and simultaneously, the shared discriminative information among multiple bands can be preserved. In particular, the Fisher discrimination criterion is imposed on the atoms to minimize within-class sparse reconstruction error and maximize between-class sparse reconstruction error. Then, an alternating optimization algorithm is developed to obtain the optimal solution for the projection matrix and the dictionary. Experimental results on two EEG-based ER datasets show that this model can achieve remarkable results and demonstrate its effectiveness.},
	author = {Gu, Xiaoqing and Yu, Fan and Zhou, Jie and Zhu, Jiaqun},
	c1 = {School of Computer Science and Artificial Intelligence, Changzhou University, China; Viterbi School of Engineering, University of Southern California, United States; School of Electrical and Mechanical Engineering, Shaoxing University, China; School of Computer Science and Artificial Intelligence, Changzhou University, China},
	date = {2021-06-28},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3389/fpsyg.2021.705528},
	isbn = {1664-1078},
	journal = {Frontiers in psychology},
	keywords = {Deep Learning for EEG; Emotion Recognition; EEG Analysis; Epilepsy Detection; Affective Computing},
	la = {en},
	publisher = {Frontiers Media},
	title = {Optimized Projection and Fisher Discriminative Dictionary Learning for EEG Emotion Recognition},
	url = {https://doi.org/10.3389/fpsyg.2021.705528},
	volume = {12},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.3389/fpsyg.2021.705528}}

@article{canmahesh_2023,
	abstract = {An automatic emotion recognition system can serve as a fundamental framework for various applications in daily life from monitoring emotional well-being to improving the quality of life through better emotion regulation. Understanding the process of emotion manifestation becomes crucial for building emotion recognition systems. An emotional experience results in changes not only in interpersonal behavior but also in physiological responses. Physiological signals are one of the most reliable means for recognizing emotions since individuals cannot consciously manipulate them for a long duration. These signals can be captured by medical-grade wearable devices, as well as commercial smart watches and smart bands. With the shift in research direction from laboratory to unrestricted daily life, commercial devices have been employed ubiquitously. However, this shift has introduced several challenges, such as low data quality, dependency on subjective self-reports, unlimited movement-related changes, and artifacts in physiological signals. This tutorial provides an overview of practical aspects of emotion recognition, such as experiment design, properties of different physiological modalities, existing datasets, suitable machine learning algorithms for physiological data, and several applications. It aims to provide the necessary psychological and physiological backgrounds through various emotion theories and the physiological manifestation of emotions, thereby laying a foundation for emotion recognition. Finally, the tutorial discusses open research directions and possible solutions.},
	author = {Can, Yekta and Mahesh, Bhargavi and Andr{\'e}, Elisabeth},
	c1 = {Chair for Human-Centered Artificial Intelligence, University of Augsburg, Augsburg, Germany; Chair for Human-Centered Artificial Intelligence, University of Augsburg, Augsburg, Germany; Chair for Human-Centered Artificial Intelligence, University of Augsburg, Augsburg, Germany},
	date = {2023-10-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/jproc.2023.3286445},
	isbn = {0018-9219},
	journal = {Proceedings of the IEEE},
	keywords = {Emotion Recognition; Physiological Signals; Affective Computing},
	la = {en},
	number = {10},
	pages = {1287--1313},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {Approaches, Applications, and Challenges in Physiological Emotion Recognition---A Tutorial Overview},
	url = {https://doi.org/10.1109/jproc.2023.3286445},
	volume = {111},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/jproc.2023.3286445}}

@article{baradaranfarzan_2023,
	abstract = {Automatic emotion recognition from electroencephalogram (EEG) signals can be considered as the main component of brain--computer interface (BCI) systems. In the previous years, many researchers in this direction have presented various algorithms for the automatic classification of emotions from EEG signals, and they have achieved promising results; however, lack of stability, high error, and low accuracy are still considered as the central gaps in this research. For this purpose, obtaining a model with the precondition of stability, high accuracy, and low error is considered essential for the automatic classification of emotions. In this research, a model based on Deep Convolutional Neural Networks (DCNNs) is presented, which can classify three positive, negative, and neutral emotions from EEG signals based on musical stimuli with high reliability. For this purpose, a comprehensive database of EEG signals has been collected while volunteers were listening to positive and negative music in order to stimulate the emotional state. The architecture of the proposed model consists of a combination of six convolutional layers and two fully connected layers. In this research, different feature learning and hand-crafted feature selection/extraction algorithms were investigated and compared with each other in order to classify emotions. The proposed model for the classification of two classes (positive and negative) and three classes (positive, neutral, and negative) of emotions had 98{\%} and 96{\%} accuracy, respectively, which is very promising compared with the results of previous research. In order to evaluate more fully, the proposed model was also investigated in noisy environments; with a wide range of different SNRs, the classification accuracy was still greater than 90{\%}. Due to the high performance of the proposed model, it can be used in brain--computer user environments.},
	author = {Baradaran, Farzad and Farzan, Ali and Daneshvar, Sabalan and Sheykhivand, Sobhan},
	c1 = {Department of Computer Engineering, Shabestar Branch, Islamic Azad University, Shabestar 53816-37181, Iran; Department of Computer Engineering, Shabestar Branch, Islamic Azad University, Shabestar 53816-37181, Iran; College of Engineering, Design and Physical Sciences, Brunel University London, Uxbridge UB8 3PH, UK; Department of Biomedical Engineering, University of Bonab, Bonab 55517-61167, Iran},
	date = {2023-05-14},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/electronics12102232},
	isbn = {2079-9292},
	journal = {Electronics},
	keywords = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; ECG Signal},
	la = {en},
	number = {10},
	pages = {2232--2232},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Customized 2D CNN Model for the Automatic Emotion Recognition Based on EEG Signals},
	url = {https://doi.org/10.3390/electronics12102232},
	volume = {12},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.3390/electronics12102232}}

@article{lilyu_2021,
	abstract = {Emotional brain-computer interface based on electroencephalogram (EEG) is a hot issue in the field of human-computer interaction, and is also an important part of the field of emotional computing. Among them, the recognition of EEG induced by emotion is a key problem. Firstly, the preprocessed EEG is decomposed by tunable-Q wavelet transform. Secondly, the sample entropy, second-order differential mean, normalized second-order differential mean, and Hjorth parameter (mobility and complexity) of each sub-band are extracted. Then, the binary gray wolf optimization algorithm is used to optimize the feature matrix. Finally, support vector machine is used to train the classifier. The five types of emotion signal samples of 32 subjects in the database for emotion analysis using physiological signal dataset is identified by the proposed algorithm. After 6-fold cross-validation, the maximum recognition accuracy is 90.48{\%}, the sensitivity is 70.25{\%}, the specificity is 82.01{\%}, and the Kappa coefficient is 0.603. The results show that the proposed method has good performance indicators in the recognition of multiple types of EEG emotion signals, and has a better performance improvement compared with the traditional methods.},
	author = {Li, Siyu and Lyu, Xiaotong and Zhao, Lei and Chen, Zhuangfei and Gong, Anmin and Fu, Yunfa},
	c1 = {School of Information Engineering and Automation, Kunming University of Science and Technology, Kunming, China; School of Information Engineering and Automation, Kunming University of Science and Technology, Kunming, China; Brain Cognition and Brain-Computer Intelligence Integration Group, Kunming University of Science and Technology, Kunming, China; Brain Cognition and Brain-Computer Intelligence Integration Group, Kunming University of Science and Technology, Kunming, China; College of Information Engineering, Engineering University of PAP, Xi'an, China; Computer Technology Application Key Lab of Yunnan Province, Kunming University of Science and Technology, Kunming, China},
	date = {2021-09-08},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3389/fncom.2021.732763},
	isbn = {1662-5188},
	journal = {Frontiers in computational neuroscience},
	keywords = {Emotion Recognition; Affective Computing; Deep Learning for EEG; EEG Analysis; Speech Emotion},
	la = {en},
	publisher = {Frontiers Media},
	title = {Identification of Emotion Using Electroencephalogram by Tunable Q-Factor Wavelet Transform and Binary Gray Wolf Optimization},
	url = {https://doi.org/10.3389/fncom.2021.732763},
	volume = {15},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.3389/fncom.2021.732763}}

@article{santamaria-granadosmendoza-moreno_2021,
	abstract = {The collection of physiological data from people has been facilitated due to the mass use of cheap wearable devices. Although the accuracy is low compared to specialized healthcare devices, these can be widely applied in other contexts. This study proposes the architecture for a tourist experiences recommender system (TERS) based on the user's emotional states who wear these devices. The issue lies in detecting emotion from Heart Rate (HR) measurements obtained from these wearables. Unlike most state-of-the-art studies, which have elicited emotions in controlled experiments and with high-accuracy sensors, this research's challenge consisted of emotion recognition (ER) in the daily life context of users based on the gathering of HR data. Furthermore, an objective was to generate the tourist recommendation considering the emotional state of the device wearer. The method used comprises three main phases: The first was the collection of HR measurements and labeling emotions through mobile applications. The second was emotional detection using deep learning algorithms. The final phase was the design and validation of the TERS-ER. In this way, a dataset of HR measurements labeled with emotions was obtained as results. Among the different algorithms tested for ER, the hybrid model of Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) networks had promising results. Moreover, concerning TERS, Collaborative Filtering (CF) using CNN showed better performance.},
	author = {Santamar{\'\i}a-Granados, Luz and Mendoza-Moreno, Juan and Chantre-Astaiza, {\'A}ngela and Mu{\~n}oz-Organero, Mario and Ram{\'\i}rez-Gonz{\'a}lez, Gustavo},
	c1 = {GIDINT, Faculty of Systems Engineering, Universidad Santo Tom{\'a}s Seccional Tunja, Calle 19, No. 11-64, Tunja 150001, Colombia.; GIDINT, Faculty of Systems Engineering, Universidad Santo Tom{\'a}s Seccional Tunja, Calle 19, No. 11-64, Tunja 150001, Colombia.; SysT{\'e}mico Research Group, Department of Tourism Sciences, Universidad del Cauca, Calle 5, No. 4-70, Popay{\'a}n 190002, Colombia.; GAST, Telematics Engineering Department, Universidad Carlos III de Madrid, Av. de la Universidad, 30, 28911 Madrid, Spain.; GIT, Telematics Department, Universidad del Cauca, Calle 5, No. 4-70, Popay{\'a}n 190002, Colombia.},
	date = {2021-11-25},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3390/s21237854},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Emotion Recognition; Affective Computing; Emotion Regulation; Epilepsy Detection},
	la = {en},
	number = {23},
	pages = {7854--7854},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Tourist Experiences Recommender System Based on Emotion Recognition with Wearable Data},
	url = {https://doi.org/10.3390/s21237854},
	volume = {21},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.3390/s21237854}}

@article{costantiniparada-cabaleiro_2022,
	abstract = {Machine Learning (ML) algorithms within a human--computer framework are the leading force in speech emotion recognition (SER). However, few studies explore cross-corpora aspects of SER; this work aims to explore the feasibility and characteristics of a cross-linguistic, cross-gender SER. Three ML classifiers (SVM, Na{\"\i}ve Bayes and MLP) are applied to acoustic features, obtained through a procedure based on Kononenko's discretization and correlation-based feature selection. The system encompasses five emotions (disgust, fear, happiness, anger and sadness), using the Emofilm database, comprised of short clips of English movies and the respective Italian and Spanish dubbed versions, for a total of 1115 annotated utterances. The results see MLP as the most effective classifier, with accuracies higher than 90{\%} for single-language approaches, while the cross-language classifier still yields accuracies higher than 80{\%}. The results show cross-gender tasks to be more difficult than those involving two languages, suggesting greater differences between emotions expressed by male versus female subjects than between different languages. Four feature domains, namely, RASTA, F0, MFCC and spectral energy, are algorithmically assessed as the most effective, refining existing literature and approaches based on standard sets. To our knowledge, this is one of the first studies encompassing cross-gender and cross-linguistic assessments on SER.},
	author = {Costantini, Giovanni and Parada-Cabaleiro, Emilia and Casali, Daniele and Cesarini, Valerio},
	c1 = {Department of Electronic Engineering, University of Rome Tor Vergata, 00133 Rome, Italy; Institute of Computational Perception, Johannes Kepler University, 4040 Linz, Austria; Department of Electronic Engineering, University of Rome Tor Vergata, 00133 Rome, Italy; Department of Electronic Engineering, University of Rome Tor Vergata, 00133 Rome, Italy},
	date = {2022-03-23},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/s22072461},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Emotion Recognition; Speech Emotion; End-to-End Speech Recognition; Affective Computing; Audio-Visual Speech Recognition},
	la = {en},
	number = {7},
	pages = {2461--2461},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {The Emotion Probe: On the Universality of Cross-Linguistic and Cross-Gender Speech Emotion Recognition via Machine Learning},
	url = {https://doi.org/10.3390/s22072461},
	volume = {22},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.3390/s22072461}}

@article{vempalarusso_2024,
	abstract = {{$<$}p{$>$}We describe our implementation of two neural networks: a static feedforward network, and an Elman network, for predicting mean valence/arousal ratings of participants for musical excerpts based on audio features. Thirteen audio features were extracted from 12 classical music excerpts (3 from each emotion quadrant). Valence/arousal ratings were collected from 45 participants for the static network, and 9 participants for the Elman network. For the Elman network, each excerpt was temporally segmented into four, sequential chunks of equal duration. Networks were trained on eight of the 12 excerpts and tested on the remaining four. The static network predicted values that closely matched mean participant ratings of valence and arousal. The Elman network did a good job of predicting the arousal trend but not the valence trend. Our study indicates that neural networks can be trained to identify statistical consistencies across audio features to predict valence/arousal values.{$<$}/p{$>$}},
	author = {Vempala, Naresh and Russo, Frank and Lab, Smart},
	c1 = {<div class=page title="Page 1><div class="layoutArea><div class="column><p><span>Ryerson University </span></p></div></div></div><span style="white-space: pre;> </span>},
	date = {2024-03-21},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.32920/25413184},
	keywords = {Audio Event Detection; Melody Extraction; Emotion Recognition; Affective Computing; Speech Emotion},
	la = {en},
	title = {Predicting Emotion from Music Audio Features Using Neural Networks},
	url = {https://doi.org/10.32920/25413184},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.32920/25413184}}

@article{morgan_2019,
	abstract = {Purpose Emotion classification for auditory stimuli typically employs 1 of 2 approaches (discrete categories or emotional dimensions). This work presents a new emotional speech set, compares these 2 classification methods for emotional speech stimuli, and emphasizes the need to consider the entire communication model (i.e., the talker, message, and listener) when studying auditory emotion portrayal and perception. Method Emotional speech from male and female talkers was evaluated using both categorical and dimensional rating methods. Ten young adult listeners (ages 19-28 years) evaluated stimuli recorded in 4 emotional speaking styles (Angry, Calm, Happy, and Sad). Talker and listener factors were examined for potential influences on emotional ratings using categorical and dimensional rating methods. Listeners rated stimuli by selecting an emotion category, rating the activation and pleasantness, and indicating goodness of category fit. Results Discrete ratings were generally consistent with dimensional ratings for speech, with accuracy for emotion recognition well above chance. As stimuli approached dimensional extremes of activation and pleasantness, listeners were more confident in their category selection, indicative of a hybrid approach to emotion classification. Female talkers were rated as more activated than male talkers, and female listeners gave higher ratings of activation compared to male listeners, confirming gender differences in emotion perception. Conclusion A hybrid model for auditory emotion classification is supported by the data. Talker and listener factors, such as gender, were found to impact the ratings of emotional speech and must be considered alongside stimulus factors in the design of future studies of emotion.},
	author = {Morgan, Shae},
	c1 = {Department of Communication Sciences and Disorders, University of Utah, Salt Lake City; Program in Audiology, Department of Otolaryngology Head and Neck Surgery and Communicative Disorders, School of Medicine, University of Louisville, KY},
	date = {2019-11-22},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1044/2019{\_}jslhr-s-19-0144},
	isbn = {1092-4388},
	journal = {Journal of speech, language, and hearing research},
	keywords = {Speech Emotion; Emotion Recognition; Affective Computing; Sensory Expectations},
	la = {en},
	number = {11},
	pages = {4015--4029},
	publisher = {American Speech--Language--Hearing Association},
	title = {Categorical and Dimensional Ratings of Emotional Speech: Behavioral Findings From the Morgan Emotional Speech Set},
	url = {https://doi.org/10.1044/2019_jslhr-s-19-0144},
	volume = {62},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1044/2019_jslhr-s-19-0144},
	bdsk-url-2 = {https://doi.org/10.1044/2019%7B%5C_%7Djslhr-s-19-0144}}

@article{cuili_2023,
	abstract = {Electroencephalography (EEG)-based emotion recognition is an important technology for human-computer interactions. In the field of neuromarketing, emotion recognition based on group EEG can be used to analyze the emotional states of multiple users. Previous emotion recognition experiments have been based on individual EEGs; therefore, it is difficult to use them for estimating the emotional states of multiple users. The purpose of this study is to find a data processing method that can improve the efficiency of emotion recognition. In this study, the DEAP dataset was used, which comprises EEG signals of 32 participants that were recorded as they watched 40 videos with different emotional themes. This study compared emotion recognition accuracy based on individual and group EEGs using the proposed convolutional neural network model. Based on this study, we can see that the differences of phase locking value (PLV) exist in different EEG frequency bands when subjects are in different emotional states. The results showed that an emotion recognition accuracy of up to 85{\%} can be obtained for group EEG data by using the proposed model. It means that using group EEG data can effectively improve the efficiency of emotion recognition. Moreover, the significant emotion recognition accuracy for multiple users achieved in this study can contribute to research on handling group human emotional states.},
	author = {Cui, Gaochao and Li, Xueyuan and Touyama, Hideaki},
	c1 = {Graduate School of Engineering, Toyama Prefectural University, Imizu, 9390398, Japan; Graduate School of Engineering, Toyama Prefectural University, Imizu, 9390398, Japan; Graduate School of Engineering, Toyama Prefectural University, Imizu, 9390398, Japan},
	date = {2023-03-07},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1038/s41598-023-30458-6},
	isbn = {2045-2322},
	journal = {Scientific reports},
	keywords = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Emotions},
	la = {en},
	number = {1},
	publisher = {Nature Portfolio},
	title = {Emotion recognition based on group phase locking value using convolutional neural network},
	url = {https://doi.org/10.1038/s41598-023-30458-6},
	volume = {13},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1038/s41598-023-30458-6}}

@article{sripiananuardi_2021,
	abstract = {Recently, robot services have been widely applied in many fields. To provide optimum service, it is essential to maintain good acceptance of the robot for more effective interaction with users. Previously, we attempted to implement facial expressions by synchronizing an estimated human emotion on the face of a robot. The results revealed that the robot could present different perceptions according to individual preferences. In this study, we considered individual differences to improve the acceptance of the robot by changing the robot's expression according to the emotion of its interacting partner. The emotion was estimated using biological signals, and the robot changed its expression according to three conditions: synchronized with the estimated emotion, inversely synchronized, and a funny expression. During the experiment, the participants provided feedback regarding the robot's expression by choosing whether they ``like''or ``dislike''the expression. We investigated individual differences in the acceptance of the robot expression using the Semantic Differential scale method. In addition, logistic regression was used to create a classification model by considering individual differences based on the biological data and feedback from each participant. We found that the robot expression based on inverse synchronization when the participants felt a negative emotion could result in impression differences among individuals. Then, the robot's expression was determined based on the classification model, and the Semantic Differential scale on the impression of the robot was compared with the three conditions. Overall, we found that the participants were most accepting when the robot expression was calculated using the proposed personalized method.},
	author = {Sripian, Peeraya and Anuardi, Muhammad and Yu, Jirong and Sugaya, Midori},
	c1 = {College of Engineering, Shibaura Institute of Technology, Tokyo 135-8548, Japan; College of Engineering, Shibaura Institute of Technology, Tokyo 135-8548, Japan; College of Engineering, Shibaura Institute of Technology, Tokyo 135-8548, Japan; College of Engineering, Shibaura Institute of Technology, Tokyo 135-8548, Japan},
	date = {2021-09-21},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3390/s21186322},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Emotion Recognition; Emotional Expressions; Affective Computing; Facial Expression},
	la = {en},
	number = {18},
	pages = {6322--6322},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {The Implementation and Evaluation of Individual Preference in Robot Facial Expression Based on Emotion Estimation Using Biological Signals},
	url = {https://doi.org/10.3390/s21186322},
	volume = {21},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.3390/s21186322}}

@article{liliu_2022,
	abstract = {As a major daily task for the popularization of artificial intelligence technology, more and more attention has been paid to the scientific research of mental state electroencephalogram (EEG) in recent years. To retain the spatial information of EEG signals and fully mine the EEG timing-related information, this paper proposes a novel EEG emotion recognition method. First, to obtain the frequency, spatial, and temporal information of multichannel EEG signals more comprehensively, we choose the multidimensional feature structure as the input of the artificial neural network. Then, a neural network model based on depthwise separable convolution is proposed, extracting the input structure's frequency and spatial features. The network can effectively reduce the computational parameters. Finally, we modeled using the ordered neuronal long short-term memory (ON-LSTM) network, which can automatically learn hierarchical information to extract deep emotional features hidden in EEG time series. The experimental results show that the proposed model can reasonably learn the correlation and temporal dimension information content between EEG multi-channel and improve emotion classification performance. We performed the experimental validation of this paper in two publicly available EEG emotional datasets. In the experiments on the DEAP dataset (a dataset for emotion analysis using EEG, physiological, and video signals), the mean accuracy of emotion recognition for arousal and valence is 95.02{\%} and 94.61{\%}, respectively. In the experiments on the SEED dataset (a dataset collection for various purposes using EEG signals), the average accuracy of emotion recognition is 95.49{\%}.},
	author = {Li, Qi and Liu, Yunqing and Liu, Quanyang and Zhang, Qiong and Yan, Fei and Ma, Yanhui and Zhang, Xinyu},
	c1 = {Department of Electronics and Information Engineering, Changchun University of Science and Technology, Changchun 130000, China; Department of Electronics and Information Engineering, Changchun University of Science and Technology, Changchun 130000, China; Department of Electronics and Information Engineering, Changchun University of Science and Technology, Changchun 130000, China; Department of Electronics and Information Engineering, Changchun University of Science and Technology, Changchun 130000, China; Department of Electronics and Information Engineering, Changchun University of Science and Technology, Changchun 130000, China; Department of Electronics and Information Engineering, Changchun University of Science and Technology, Changchun 130000, China; Department of Electronics and Information Engineering, Changchun University of Science and Technology, Changchun 130000, China; Economics School, Jilin University, Changchun 130000, China},
	date = {2022-12-15},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/e24121830},
	isbn = {1099-4300},
	journal = {Entropy},
	keywords = {Deep Learning for EEG; Emotion Recognition; EEG Analysis; Affective Computing; Emotion Regulation},
	la = {en},
	number = {12},
	pages = {1830--1830},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Multidimensional Feature in Emotion Recognition Based on Multi-Channel EEG Signals},
	url = {https://doi.org/10.3390/e24121830},
	volume = {24},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.3390/e24121830}}

@article{velascojusto_2019,
	abstract = {Decoding emotional states from multimodal signals is an increasingly active domain, within the framework of affective computing, which aims to a better understanding of Human-Human Communication as well as to improve Human-Computer Interaction.But the automatic recognition of spontaneous emotions from speech is a very complex task due to the lack of a certainty of the speaker states as well as to the difficulty to identify a variety of emotions in real scenarios.In this work we explore the extent to which emotional states can be decoded from speech signals extracted from TV political debates.The labelling procedure was supported by perception experiments where only a small set of emotions has been identified.In addition, some scaled judgements of valence, arousal and dominance were also provided.In this framework the paper shows meaningful comparisons between both, the dimensional and the categorical models of emotions, which is a new contribution when dealing with spontaneous emotions.To this end Support Vector Machines (SVM) as well as Feedforward Neural Networks (FNN) have been proposed to develop classifiers and predictors.The experimental evaluation over a Spanish corpus has shown the ability of both models to be identified in speech segments by the proposed artificial systems.},
	author = {de Velasco, Mikel and Justo, Raquel and Zorrilla, Asier and Torres, M.},
	c1 = {Universidad del Pa{\'\i}s Vasco UPV/EHU Leioa, Spain 48940; Universidad del Pa{\'\i}s Vasco UPV/EHU Leioa, Spain 48940; Universidad del Pa{\'\i}s Vasco UPV/EHU Leioa, Spain 48940; Universidad del Pa{\'\i}s Vasco UPV/EHU Leioa, Spain 48940},
	date = {2019-10-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1109/coginfocom47531.2019.9089948},
	keywords = {Speech Emotion; Emotion Recognition; Affective Computing},
	la = {en},
	title = {Can Spontaneous Emotions be Detected from Speech on TV Political Debates?},
	url = {https://doi.org/10.1109/coginfocom47531.2019.9089948},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/coginfocom47531.2019.9089948}}

@article{mirandarituerto-gonzalez_2022,
	abstract = {The main research motivation of this article is the fight against gender-based violence and achieving gender equality from a technological perspective. The solution proposed in this work goes beyond currently existing panic buttons, needing to be manually operated by the victims under difficult circumstances. Instead, Bindi, our end-to-end autonomous multimodal system, relies on artificial intelligence methods to automatically identify violent situations, based on detecting fear-related emotions, and trigger a protection protocol, if necessary. To this end, Bindi integrates modern state-of-the-art technologies, such as the Internet of Bodies, affective computing, and cyber--physical systems, leveraging: 1) affective Internet of Things (IoT) with auditory and physiological commercial off-the-shelf smart sensors embedded in wearable devices; 2) hierarchical multisensorial information fusion; and 3) the edge-fog-cloud IoT architecture. This solution is evaluated using our own data set named WEMAC, a very recently collected and freely available collection of data comprising the auditory and physiological responses of 47 women to several emotions elicited by using a virtual reality environment. On this basis, this work provides an analysis of multimodal late fusion strategies to combine the physiological and speech data processing pipelines to identify the best intelligence engine strategy for Bindi. In particular, the best data fusion strategy reports an overall fear classification accuracy of 63.61{\%} for a subject-independent approach. Both a power consumption study and an audio data processing pipeline to detect violent acoustic events complement this analysis. This research is intended as an initial multimodal baseline that facilitates further work with real-life elicited fear in women.},
	author = {Miranda, Jos{\'e} and Rituerto-Gonz{\'a}lez, Esther and Luis-Mingueza, Clara and Canabal, Manuel and Barcenas, Alberto and Lanza-Guti{\'e}rrez, Jos{\'e} and Pel{\'a}ez-Moreno, Carmen and L{\'o}pez-Ongil, C.},
	c1 = {Department of Electronics, Universidad Carlos III of Madrid, Legan\&{\#}x00E9;s, Spain; Department of Signal Theory and Communications, Universidad Carlos III of Madrid, Legan\&{\#}x00E9;s, Spain; Department of Signal Theory and Communications, Universidad Carlos III of Madrid, Legan\&{\#}x00E9;s, Spain; Department of Electronics, Universidad Carlos III of Madrid, Legan\&{\#}x00E9;s, Spain; Department of Electronics, Universidad Carlos III of Madrid, Legan\&{\#}x00E9;s, Spain; Department of Computer Science, Universidad de Alcal\&{\#}x00E1;, Alcal\&{\#}x00E1; de Henares, Spain; Department of Signal Theory and Communications, Universidad Carlos III of Madrid, Legan\&{\#}x00E9;s, Spain; Department of Electronics, Universidad Carlos III of Madrid, Legan\&{\#}x00E9;s, Spain},
	date = {2022-11-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/jiot.2022.3177256},
	isbn = {2327-4662},
	journal = {IEEE internet of things journal},
	keywords = {Affective Computing; Emotion Recognition; Internet-based Interventions},
	la = {en},
	number = {21},
	pages = {21174--21193},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {Bindi: Affective Internet of Things to Combat Gender-Based Violence},
	url = {https://doi.org/10.1109/jiot.2022.3177256},
	volume = {9},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/jiot.2022.3177256}}

@article{mirandarituerto-gonzalez_2022,
	abstract = {The main research motivation of this article is the fight against gender-based violence and achieving gender equality from a technological perspective. The solution proposed in this work goes beyond currently existing panic buttons, needing to be manually operated by the victims under difficult circumstances. Instead, Bindi, our end-to-end autonomous multimodal system, relies on artificial intelligence methods to automatically identify violent situations, based on detecting fear-related emotions, and trigger a protection protocol, if necessary. To this end, Bindi integrates modern state-of-the-art technologies, such as the Internet of Bodies, affective computing, and cyber--physical systems, leveraging: 1) affective Internet of Things (IoT) with auditory and physiological commercial off-the-shelf smart sensors embedded in wearable devices; 2) hierarchical multisensorial information fusion; and 3) the edge-fog-cloud IoT architecture. This solution is evaluated using our own data set named WEMAC, a very recently collected and freely available collection of data comprising the auditory and physiological responses of 47 women to several emotions elicited by using a virtual reality environment. On this basis, this work provides an analysis of multimodal late fusion strategies to combine the physiological and speech data processing pipelines to identify the best intelligence engine strategy for Bindi. In particular, the best data fusion strategy reports an overall fear classification accuracy of 63.61{\%} for a subject-independent approach. Both a power consumption study and an audio data processing pipeline to detect violent acoustic events complement this analysis. This research is intended as an initial multimodal baseline that facilitates further work with real-life elicited fear in women.},
	author = {Miranda, Jos{\'e} and Rituerto-Gonz{\'a}lez, Esther and Luis-Mingueza, Clara and Canabal, Manuel and Barcenas, Alberto and Lanza-Guti{\'e}rrez, Jos{\'e} and Pel{\'a}ez-Moreno, Carmen and L{\'o}pez-Ongil, C.},
	c1 = {Department of Electronics, Universidad Carlos III of Madrid, Legan\&{\#}x00E9;s, Spain; Department of Signal Theory and Communications, Universidad Carlos III of Madrid, Legan\&{\#}x00E9;s, Spain; Department of Signal Theory and Communications, Universidad Carlos III of Madrid, Legan\&{\#}x00E9;s, Spain; Department of Electronics, Universidad Carlos III of Madrid, Legan\&{\#}x00E9;s, Spain; Department of Electronics, Universidad Carlos III of Madrid, Legan\&{\#}x00E9;s, Spain; Department of Computer Science, Universidad de Alcal\&{\#}x00E1;, Alcal\&{\#}x00E1; de Henares, Spain; Department of Signal Theory and Communications, Universidad Carlos III of Madrid, Legan\&{\#}x00E9;s, Spain; Department of Electronics, Universidad Carlos III of Madrid, Legan\&{\#}x00E9;s, Spain},
	date = {2022-11-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/jiot.2022.3177256},
	isbn = {2327-4662},
	journal = {IEEE internet of things journal},
	keywords = {Affective Computing; Emotion Recognition; Internet-based Interventions},
	la = {en},
	number = {21},
	pages = {21174--21193},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {Bindi: Affective Internet of Things to Combat Gender-Based Violence},
	url = {https://doi.org/10.1109/jiot.2022.3177256},
	volume = {9},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/jiot.2022.3177256}}

@article{ghafoorshi_2022,
	abstract = {Microblogs generate a vast amount of data in which users express their emotions regarding almost all aspects of everyday life. Capturing affective content from these context-dependent and subjective texts is a challenging task. We propose an intelligent probabilistic model for textual emotion recognition in multidimensional space (TERMS) that captures the subjective emotional boundaries and contextual information embedded in a text for robust emotion recognition. It is implausible with discrete label assignment;therefore, the model employs a soft assignment by mapping varying emotional perceptions in a multidimensional space and generates them as distributions via the Gaussian mixture model (GMM). To strengthen emotion distributions, TERMS integrates a probabilistic emotion classifier that captures the contextual and linguistic information from texts. The integration of these aspects, the context-aware emotion classifier and the learned GMM parameters provide a complete coverage for accurate emotion recognition. The large-scale experimentation shows that compared to baseline and state-of-the-art models, TERMS achieved better performance in terms of distinguishability, prediction, and classification performance. In addition, TERMS provide insights on emotion classes, the annotation patterns, and the models application in different scenarios.},
	author = {Ghafoor, Yusra and Shi, Jianxin and Calder{\'o}n, Fernando and Huang, Yen-Hao and Chen, Kuan-Ta and Chen, Yi-Shin},
	c1 = {Social Networks and Human-Centered Computing, Taiwan International Graduate Program, Institute of Information Science, Academia Sinica. Institute of Information Systems and Applications, National Tsing Hua University, Hsinchu City, Taiwan; Institute of Information Systems and Applications, National Tsing Hua University, Hsinchu City, Taiwan; Social Networks and Human-Centered Computing, Taiwan International Graduate Program, Institute of Information Science, Academia Sinica. Institute of Information Systems and Applications, National Tsing Hua University, Hsinchu City, Taiwan; Institute of Information Systems and Applications, National Tsing Hua University, Hsinchu City, Taiwan; Institute of Information Science, Academia Sinica, Taipei, Taiwan; Department of Computer Science, National Tsing Hua University, Hsinchu City, Taiwan},
	date = {2022-05-11},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1007/s10489-022-03567-4},
	isbn = {0924-669X},
	journal = {Applied intelligence},
	keywords = {Affective Computing; Emotion Recognition; Textual Data; Speech Emotion; Aspect-based Sentiment Analysis},
	la = {en},
	number = {3},
	pages = {2673--2693},
	publisher = {Springer Science+Business Media},
	title = {TERMS: textual emotion recognition in multidimensional space},
	url = {https://doi.org/10.1007/s10489-022-03567-4},
	volume = {53},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1007/s10489-022-03567-4}}

@article{kuttdrazyk_2022,
	abstract = {Abstract Generic emotion prediction models based on physiological data developed in the field of affective computing apparently are not robust enough. To improve their effectiveness, one needs to personalize them to specific individuals and incorporate broader contextual information. To address the lack of relevant datasets, we propose the 2nd Study in Bio-Reactions and Faces for Emotion-based Personalization for AI Systems (BIRAFFE2) dataset. In addition to the classical procedure in the stimulus-appraisal paradigm, it also contains data from an affective gaming session in which a range of contextual data was collected from the game environment. This is complemented by accelerometer, ECG and EDA signals, participants'facial expression data, together with personality and game engagement questionnaires. The dataset was collected on 102 participants. Its potential usefulness is presented by validating the correctness of the contextual data and indicating the relationships between personality and participants'emotions and between personality and physiological signals.},
	author = {Kutt, Krzysztof and Dr{\k a}{\.z}yk, Dominika and {\.Z}uchowska, Laura and Szel{\k a}{\.z}ek, Maciej and Bobek, Szymon and Nalepa, Grzegorz},
	c1 = {Jagiellonian Human-Centered Artificial Intelligence Laboratory (JAHCAI) and Institute of Applied Computer Science, Jagiellonian University, Krak{\'o}w, Poland; Institute of Neuroscience, Universit{\'e}Catholique de Louvain, Louvain-la-Neuve, Belgium; Department of Applied Computer Science, AGH University of Science and Technology, Krak{\'o}w, Poland; Department of Applied Computer Science, AGH University of Science and Technology, Krak{\'o}w, Poland; Jagiellonian Human-Centered Artificial Intelligence Laboratory (JAHCAI) and Institute of Applied Computer Science, Jagiellonian University, Krak{\'o}w, Poland; Jagiellonian Human-Centered Artificial Intelligence Laboratory (JAHCAI) and Institute of Applied Computer Science, Jagiellonian University, Krak{\'o}w, Poland},
	date = {2022-06-07},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1038/s41597-022-01402-6},
	isbn = {2052-4463},
	journal = {Scientific data},
	keywords = {Affective Computing; Emotion Recognition; Personality Data; Emotion Regulation; Multimodal Data},
	la = {en},
	number = {1},
	publisher = {Nature Portfolio},
	title = {BIRAFFE2, a multimodal dataset for emotion-based personalization in rich affective game environments},
	url = {https://doi.org/10.1038/s41597-022-01402-6},
	volume = {9},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1038/s41597-022-01402-6}}

@article{liliu_2022,
	abstract = {Recently, emotional electroencephalography (EEG) has been of great importance in brain-computer interfaces, and it is more urgent to realize automatic emotion recognition. The EEG signal has the disadvantages of being non-smooth, non-linear, stochastic, and susceptible to background noise. Additionally, EEG signal processing network models have the disadvantages of a large number of parameters and long training time. To address the above issues, a novel model is presented in this paper. Initially, a deep sparse autoencoder network (DSAE) was used to remove redundant information from the EEG signal and reconstruct its underlying features. Further, combining a convolutional neural network (CNN) with long short-term memory (LSTM) can extract relevant features from task-related features, mine the correlation between the 32 channels of the EEG signal, and integrate contextual information from these frames. The proposed DSAE + CNN + LSTM (DCRNN) model was experimented with on the public dataset DEAP. The classification accuracies of valence and arousal reached 76.70{\%} and 81.43{\%}, respectively. Meanwhile, we conducted experiments with other comparative methods to further demonstrate the effectiveness of the DCRNN method.},
	author = {Li, Qi and Liu, Yunqing and Shang, Yujie and Zhang, Qiong and Yan, Fei},
	c1 = {Department of Electronics and Information Engineering, Changchun University of Science and Technology, Changchun 130012, China; Department of Electronics and Information Engineering, Changchun University of Science and Technology, Changchun 130012, China; Department of Electronics and Information Engineering, Changchun University of Science and Technology, Changchun 130012, China; Department of Electronics and Information Engineering, Changchun University of Science and Technology, Changchun 130012, China; Department of Electronics and Information Engineering, Changchun University of Science and Technology, Changchun 130012, China},
	date = {2022-08-25},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/e24091187},
	isbn = {1099-4300},
	journal = {Entropy},
	keywords = {Deep Learning for EEG; Emotion Recognition; EEG Analysis; Neural Ensemble Physiology; Affective Computing},
	la = {en},
	number = {9},
	pages = {1187--1187},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Deep Sparse Autoencoder and Recursive Neural Network for EEG Emotion Recognition},
	url = {https://doi.org/10.3390/e24091187},
	volume = {24},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.3390/e24091187}}

@article{almarrirajasekaran_2021,
	abstract = {The dimensionality of the spatially distributed channels and the temporal resolution of electroencephalogram (EEG) based brain-computer interfaces (BCI) undermine emotion recognition models. Thus, prior to modeling such data, as the final stage of the learning pipeline, adequate preprocessing, transforming, and extracting temporal (i.e., time-series signals) and spatial (i.e., electrode channels) features are essential phases to recognize underlying human emotions. Conventionally, inter-subject variations are dealt with by avoiding the sources of variation (e.g., outliers) or turning the problem into a subject-deponent. We address this issue by preserving and learning from individual particularities in response to affective stimuli. This paper investigates and proposes a subject-independent emotion recognition framework that mitigates the subject-to-subject variability in such systems. Using an unsupervised feature selection algorithm, we reduce the feature space that is extracted from time-series signals. For the spatial features, we propose a subject-specific unsupervised learning algorithm that learns from inter-channel co-activation online. We tested this framework on real EEG benchmarks, namely DEAP, MAHNOB-HCI, and DREAMER. We train and test the selection outcomes using nested cross-validation and a support vector machine (SVM). We compared our results with the state-of-the-art subject-independent algorithms. Our results show an enhanced performance by accurately classifying human affection (i.e., based on valence and arousal) by 16{\%}--27{\%} compared to other studies. This work not only outperforms other subject-independent studies reported in the literature but also proposes an online analysis solution to affection recognition.},
	author = {Almarri, Badar and Rajasekaran, Sanguthevar and Huang, Chun-Hsi},
	c1 = {Dept. of Computer Science and Engineering, University of Connecticut, Storrs, CT, United States of America; Dept. of Computer Science, King Faisal University, Al-Ahsa, Saudi Arabia; Dept. of Computer Science and Engineering, University of Connecticut, Storrs, CT, United States of America; The School of Computing, Southern Illinois University, Carbondale, IL, United States of America},
	date = {2021-08-26},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1371/journal.pone.0253383},
	isbn = {1932-6203},
	journal = {PloS one},
	keywords = {Emotion Recognition; Affective Computing; Brain-Computer Interfaces; Feature Extraction; Sensory Processing},
	la = {en},
	number = {8},
	pages = {e0253383--e0253383},
	publisher = {Public Library of Science},
	title = {Automatic subject-specific spatiotemporal feature selection for subject-independent affective BCI},
	url = {https://doi.org/10.1371/journal.pone.0253383},
	volume = {16},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1371/journal.pone.0253383}}

@article{haoliu_2019,
	abstract = {Hyperspectral imaging (HSI) technology can be used to detect human emotions based on the power of material discrimination from their faces. In this paper, HSI is used to remotely sense and distinguish blood chromophores in facial tissues and acquire an evaluation indicator (tissue oxygen saturation, StO2) using an optical absorption model. This study explored facial analysis while people were showing spontaneous expressions of happiness during social interaction. Happiness, as a psychological emotion, has been shown to be strongly linked to other activities such as physiological reaction and facial expression. Moreover, facial expression as a communicative motor behavior likely arises from musculoskeletal anatomy, neuromuscular activity, and individual personality. This paper quantified the neuromotor movements of tissues surrounding some regions of interest (ROIs) on smiling happily. Next, we selected six regions-the forehead, eye, nose, cheek, mouth, and chin-according to a facial action coding system (FACS). Nineteen segments were subsequently partitioned from the above ROIs. The affective data (StO2) of 23 young adults were acquired by HSI while the participants expressed emotions (calm or happy), and these were used to compare the significant differences in the variations of StO2 between the different ROIs through repeated measures analysis of variance. Results demonstrate that happiness causes different distributions in the variations of StO2 for the above ROIs; these are explained in depth in the article. This study establishes that facial tissue oxygen saturation is a valid and reliable physiological indicator of happiness and merits further research.},
	author = {Hao, Min and Liu, Guangyuan and Gokhale, Anu and Xu, Yanwu and Chen, Rui},
	c1 = {School of Electronic and Information Engineering, Southwest University, Chongqing, China; School of Electronic and Information Engineering, Southwest University, Chongqing, China; Illinois State University, Normal, IL, USA; Center of Technical Support for Network Security, Chongqing Municipal Public Security Bureau, Chongqing, China; College of Computer and Information Science, Southwest University, Chongqing, China},
	date = {2019-01-15},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1155/2019/1965789},
	isbn = {1687-5265},
	journal = {Computational intelligence and neuroscience},
	keywords = {Emotion Recognition; Affective Computing; Face Perception; Facial Expression; Emotional Expressions},
	la = {en},
	pages = {1--16},
	publisher = {Hindawi Publishing Corporation},
	title = {Detecting Happiness Using Hyperspectral Imaging Technology},
	url = {https://doi.org/10.1155/2019/1965789},
	volume = {2019},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1155/2019/1965789}}

@article{panditcummins_2018,
	abstract = {This first-of-its-kind study aims to track authentic affect representations in-the-wild. We use the `Graz Real-life Affect in the Street and Supermarket (GRAS2)' corpus featuring audiovisual recordings of random participants in non-laboratory conditions. The participants were initially unaware of being recorded. This paradigm enabled us to use a collection of a wide range of authentic, spontaneous and natural affective behaviours. Six raters annotated twenty-eight conversations averaging 2.5 minutes in duration, tracking the arousal and valence levels of the participants. We generate the gold standards through a novel robust Evaluator Weighted Estimator (EWE) formulation. We train Support Vector Regressors (SVR) and Recurrent Neural Networks (RNN) with the low-level-descriptors (LLDs) of the ComParE feature-set in different derived representations including bag-of-audio-words. Despite the challenging nature of this database, a fusion system achieved a highly promising concordance correlation coefficient (CCC) of.372 for arousal dimension, while RNNs achieved a top CCC of.223 in predicting valence, using a bag-of-features representation.},
	author = {Pandit, Vedhas and Cummins, Nicholas and Schmitt, Maximilian and Hantke, Simone and Graf, Franz and Paletta, Lucas and Schuller, Bj{\"o}rn},
	c1 = {University of Augsburg, Augsburg, Germany; University of Augsburg, Augsburg, Germany; University of Augsburg, Augsburg, Germany; University of Augsburg, Augsburg, Germany; Joanneum Research Forschungsgesellschaft mbH, Graz, Austria; Joanneum Research Forschungsgesellschaft mbH, Graz, Austria; Joanneum Research Forschungsgesellschaft mbH, Graz, Austria},
	date = {2018-05-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1109/aciiasia.2018.8470340},
	keywords = {Affective Computing; Emotion Recognition; Speech Emotion; Environmental Sound Recognition},
	la = {en},
	title = {Tracking Authentic and In-the-wild Emotions Using Speech},
	url = {https://doi.org/10.1109/aciiasia.2018.8470340},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/aciiasia.2018.8470340}}

@article{fouladgaralirezaie_2021,
	abstract = {Abstract Affective computing solutions, in the literature, mainly rely on machine learning methods designed to accurately detect human affective states. Nevertheless, many of the proposed methods are based on handcrafted features, requiring sufficient expert knowledge in the realm of signal processing. With the advent of deep learning methods, attention has turned toward reduced feature engineering and more end-to-end machine learning. However, most of the proposed models rely on late fusion in a multimodal context. Meanwhile, addressing interrelations between modalities for intermediate-level data representation has been largely neglected. In this paper, we propose a novel deep convolutional neural network, called CN-Waterfall, consisting of two modules: Base and General . While the Base module focuses on the low-level representation of data from each single modality, the General module provides further information, indicating relations between modalities in the intermediate- and high-level data representations. The latter module has been designed based on theoretically grounded concepts in the Explainable AI (XAI) domain, consisting of four different fusions. These fusions are mainly tailored to correlation - and non-correlation -based modalities. To validate our model, we conduct an exhaustive experiment on WESAD and MAHNOB-HCI, two publicly and academically available datasets in the context of multimodal affective computing. We demonstrate that our proposed model significantly improves the performance of physiological-based multimodal affect detection.},
	author = {Fouladgar, Nazanin and Alirezaie, Marjan and Fr{\"a}mling, Kary},
	c1 = {Department of Computing Science, Ume{\aa}University, Ume{\aa}, Sweden; Centre for Applied Autonomous Sensor Systems (AASS), {\"O}rebro University, {\"O}rebro, Sweden; Department of Computing Science, Ume{\aa}University, Ume{\aa}, Sweden},
	date = {2021-09-24},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1007/s00521-021-06516-3},
	isbn = {0941-0643},
	journal = {Neural computing \& applications},
	keywords = {Affective Computing; Emotion Recognition; Deep Learning; Multimodal Data},
	la = {en},
	number = {3},
	pages = {2157--2176},
	publisher = {Springer Science+Business Media},
	title = {CN-waterfall: a deep convolutional neural network for multimodal physiological affect detection},
	url = {https://doi.org/10.1007/s00521-021-06516-3},
	volume = {34},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1007/s00521-021-06516-3}}

@article{al-qazzazsabir_2021,
	abstract = {Investigating gender differences based on emotional changes becomes essential to understand various human behaviors in our daily life. Ten students from the University of Vienna have been recruited by recording the electroencephalogram (EEG) dataset while watching four short emotional video clips (anger, happiness, sadness, and neutral) of audiovisual stimuli. In this study, conventional filter and wavelet (WT) denoising techniques were applied as a preprocessing stage and Hurst exponent (Hur) and amplitude-aware permutation entropy (AAPE) features were extracted from the EEG dataset. k-nearest neighbors (kNN) and support vector machine (SVM) classification techniques were considered for automatic gender recognition from emotional-based EEGs. The main novelty of this paper is twofold: first, to investigate Hur as a complexity feature and AAPE as an irregularity parameter for the emotional-based EEGs using two-way analysis of variance (ANOVA) and then integrating these features to propose a new CompEn hybrid feature fusion method towards developing the novel WT{\_}CompEn gender recognition framework as a core for an automated gender recognition model to be sensitive for identifying gender roles in the brain-emotion relationship for females and males. The results illustrated the effectiveness of Hur and AAPE features as remarkable indices for investigating gender-based anger, sadness, happiness, and neutral emotional state. Moreover, the proposed WT{\_}CompEn framework achieved significant enhancement in SVM classification accuracy of 100{\%}, indicating that the novel WT{\_}CompEn may offer a useful way for reliable enhancement of gender recognition of different emotional states. Therefore, the novel WT{\_}CompEn framework is a crucial goal for improving the process of automatic gender recognition from emotional-based EEG signals allowing for more comprehensive insights to understand various gender differences and human behavior effects of an intervention on the brain.},
	author = {Al-Qazzaz, Noor and Sabir, Mohannad and Bin Mohd Ali, Sawal and Ahmad, Siti and Grammer, Karl},
	c1 = {Department of Biomedical Engineering, Al-Khwarizmi College of Engineering, University of Baghdad, Baghdad 47146, Iraq; Department of Electrical Electronic \& Systems Engineering, Faculty of Engineering \& Built Environment, Universiti Kebangsaan Malaysia, UKM, Bangi, Selangor 43600, Malaysia; Department of Biomedical Engineering, Al-Khwarizmi College of Engineering, University of Baghdad, Baghdad 47146, Iraq; Department of Electrical Electronic \& Systems Engineering, Faculty of Engineering \& Built Environment, Universiti Kebangsaan Malaysia, UKM, Bangi, Selangor 43600, Malaysia; Department of Electrical and Electronic Engineering, Faculty of Engineering, Universiti Putra Malaysia, UPM, Serdang, Selangor 43400, Malaysia; Department of Evolutionary Anthropology, University of Vienna, Althan Strasse 14, A-1090 Vienna, Vienna, Austria},
	date = {2021-09-21},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:06:52 +0100},
	doi = {10.1155/2021/8537000},
	isbn = {2040-2295},
	journal = {Journal of healthcare engineering},
	keywords = {EEG Analysis; Deep Learning for EEG; Emotion Recognition; Affective Computing; Epilepsy Detection},
	la = {en},
	pages = {1--17},
	publisher = {Hindawi Publishing Corporation},
	title = {Complexity and Entropy Analysis to Improve Gender Identification from Emotional-Based EEGs},
	url = {https://doi.org/10.1155/2021/8537000},
	volume = {2021},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1155/2021/8537000}}

@article{vafaeirahatabad_2023,
	abstract = {Emotion recognition based on brain signals has increasingly become attractive to evaluate human's internal emotional states. Conventional emotion recognition studies focus on developing machine learning and classifiers. However, most of these methods do not provide information on the involvement of different areas of the brain in emotions. Brain mapping is considered as one of the most distinguishing methods of showing the involvement of different areas of the brain in performing an activity. Most mapping techniques rely on projection and visualization of only one of the electroencephalogram (EEG) subband features onto brain regions. The present study aims to develop a new EEG-based brain mapping, which combines several features to provide more complete and useful information on a single map instead of common maps. In this study, the optimal combination of EEG features for each channel was extracted using a stacked autoencoder (SAE) network and visualizing a topographic map. Based on the research hypothesis, autoencoders can extract optimal features for quantitative EEG (QEEG) brain mapping. The DEAP EEG database was employed to extract topographic maps. The accuracy of image classifiers using the convolutional neural network (CNN) was used as a criterion for evaluating the distinction of the obtained maps from a stacked autoencoder topographic map (SAETM) method for different emotions. The average classification accuracy was obtained 0.8173 and 0.8037 in the valence and arousal dimensions, respectively. The extracted maps were also ranked by a team of experts compared to common maps. The results of quantitative and qualitative evaluation showed that the obtained map by SAETM has more information than conventional maps.},
	author = {Vafaei, Elnaz and Rahatabad, Fereidoun and Setarehdan, Seyed and Azadfallah, Parviz},
	c1 = {Department of Biomedical Engineering, Science and Research Branch, Islamic Azad University, Tehran, Iran; Department of Biomedical Engineering, Science and Research Branch, Islamic Azad University, Tehran, Iran; School of Electrical and Computer Engineering, College of Engineering, University of Tehran, Tehran, Iran; Tarbiat Modares University, Tehran, Iran},
	date = {2023-01-19},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1155/2023/9223599},
	isbn = {2040-2295},
	journal = {Journal of healthcare engineering},
	keywords = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Affective Computing; Speech Emotion},
	la = {en},
	pages = {1--19},
	publisher = {Hindawi Publishing Corporation},
	title = {Extracting a Novel Emotional EEG Topographic Map Based on a Stacked Autoencoder Network},
	url = {https://doi.org/10.1155/2023/9223599},
	volume = {2023},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1155/2023/9223599}}

@article{konethanh_2018,
	abstract = {Emotica (EMOTIon CApture) system is a multimodal emotion recognition system that uses physiological signals.A DLF (Decision Level Fusion) approach with a voting method is used in this system to merge monomodal decisions for a multimodal detection.In this document, on the one hand, we describe how from a physiological signal Emotica can detect an emotional activity and distinguish one emotional activity from others.On the other hand, we present a study about two classification algorithms, KNN and SVM.These algorithms have been implemented on the Emotica system in order to see which one is the best.The experiments show that KNN and SVM allow a high accuracy in emotion recognition, but SVM is more accurate than KNN on the data that was used.Indeed, we obtain a recognition rate of 81.69{\%} and 84{\%} respectively with KNN and SVM algorithms under certain conditions.},
	author = {Kon{\'e}, Chaka and Th{\`a}nh, Nh{\^a}n and Flamary, R{\'e}mi and Belleudy, C{\'e}cile},
	c1 = {Universit´e Cˆote d'Azur, Laboratoire LEAT CNRS UMR 7248, Sophia-Antipolis, France; Universit´e Cˆote d'Azur, Laboratoire I3S CNRS UMR 7271, Sophia-Antipolis, France; Universit´e Cˆote d'Azur, Observatoire de la Cˆote d'Azur, CNRS, Laboratoire Lagrange, Nice, France; Universit´e Cˆote d'Azur, Laboratoire LEAT CNRS UMR 7248, Sophia-Antipolis, France},
	date = {2018-01-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.4172/2090-4886.1000153},
	isbn = {2090-4878},
	journal = {Internatinoal Journal of Sensor Networks and Data Communications/Internatinoal Journal of Sensor Networks and Data Communications},
	keywords = {Emotion Recognition; Affective Computing; Speech Emotion},
	la = {en},
	number = {01},
	publisher = {OMICS Publishing Group},
	title = {Performance Comparison of the KNN and SVM Classification Algorithms in the Emotion Detection System EMOTICA},
	url = {https://doi.org/10.4172/2090-4886.1000153},
	volume = {07},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.4172/2090-4886.1000153}}

@article{zhangzhang_2022,
	abstract = {Graph convolutional neural networks (GCN) have attracted much attention in the task of electroencephalogram (EEG) emotion recognition. However, most features of current GCNs do not take full advantage of the causal connection between the EEG signals in different frequency bands during the process of constructing the adjacency matrix. Based on the causal connectivity between the EEG channels obtained by Granger causality (GC) analysis, this paper proposes a multi-frequency band EEG graph feature extraction and fusion method for EEG emotion recognition. First, the original GC matrices between the EEG signals at each frequency band are calculated via GC analysis, and then they are adaptively converted to asymmetric binary GC matrices through an optimal threshold. Then, a kind of novel GC-based GCN feature (GC-GCN) is constructed by using differential entropy features and the binary GC matrices as the node values and adjacency matrices, respectively. Finally, on the basis of the GC-GCN features, a new multi-frequency band feature fusion method (GC-F-GCN) is proposed, which integrates the graph information of the EEG signals at different frequency bands for the same node. The experimental results demonstrate that the proposed GC-F-GCN method achieves better recognition performance than the state-of-the-art GCN methods, for which average accuracies of 97.91{\%}, 98.46{\%}, and 98.15{\%} were achieved for the arousal, valence, and arousal-valence classifications, respectively.},
	author = {Zhang, Jing and Zhang, Xueying and Chen, Guijun and Zhao, Qing},
	c1 = {College of Information and Computer, Taiyuan University of Technology, Taiyuan 030024, China; College of Information and Computer, Taiyuan University of Technology, Taiyuan 030024, China; College of Information and Computer, Taiyuan University of Technology, Taiyuan 030024, China; College of Information and Computer, Taiyuan University of Technology, Taiyuan 030024, China},
	date = {2022-12-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3390/brainsci12121649},
	isbn = {2076-3425},
	journal = {Brain sciences},
	keywords = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Affective Computing; Feature Extraction},
	la = {en},
	number = {12},
	pages = {1649--1649},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Granger-Causality-Based Multi-Frequency Band EEG Graph Feature Extraction and Fusion for Emotion Recognition},
	url = {https://doi.org/10.3390/brainsci12121649},
	volume = {12},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.3390/brainsci12121649}}

@article{huynhlee_2016,
	abstract = {This paper aims to develop a system that evaluates the emotional experience of gamers based on physiological changes. A within-subject experiment with 22 participants has been designed to investigate the effects of difficulty level and social playing mode on player emotions and to examine the correlation between each emotion and the physiological changes. We demonstrate the feasibility of using commodity wearable physiological sensing devices to recognize mobile gamer's emotion. Specifically, our system performs 3-level excitement classification at an accuracy of 77.38{\%} and binary classification of happiness state at an accuracy of 73.21{\%}. These classification results show the potential of using commodity wearable sensing devices as a valuable evaluation tool for game designers to gauge user emotions and develop personalized gaming experience.},
	author = {Huynh, Sinh and Lee, Eun and Park, Taiwoo and Balan, Rajesh},
	c1 = {Singapore Management University, Singapore, Singapore; Singapore Management University, Singapore, Singapore; Michigan State University, East Lansing, Michigan, USA; Singapore Management University, Singapore, Singapore},
	date = {2016-06-30},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1145/2934646.2934648},
	keywords = {Affective Computing; Emotion Recognition; User Satisfaction; Emotions; Affective Design},
	la = {en},
	title = {Jasper},
	url = {https://doi.org/10.1145/2934646.2934648},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1145/2934646.2934648}}

@article{abdel-ghaffarwu_2022,
	abstract = {Emotion recognition plays an important role in human computer interaction systems as it helps the computer in understanding human behavior and their decision making process.Using Electroencephalographic (EEG) signals in emotion recognition offers a direct assessment on the inner state of human mind.This study aims to build a subject dependent emotion recognition system that differentiate between high and low levels of valance and arousal, using multidimensional EEG signals.Our system offers a transfer learning-minimum distance to Riemannian mean (TL-MDRM) framework.In this work, we perform two pre-processing stages.In the first stage, we analyze the EEG signals to investigate their non-Gaussianity and determine the most appropriate signal distribution.Using several statistical and goodness of fit tests, T-distribution was found to be the most appropriate distribution.Covariance matrix estimations plays a crucial step in manifold learning technique, based on the most suitable signal distribution the covariance matrix estimation technique is chosen.In the second stage, we perform transfer learning to deal with cross-session variability by generating a unique reference point for each participant and performing affine transformation for the covariance matrices on the symmetric positive definite (SPD) manifold around this point.The results show that, TL process improved the performance even when assuming Gaussian distribution, while assuming T-distribution with TL improved the performance further.},
	author = {Abdel-Ghaffar, Eman and Wu, Yujin and Daoudi, Mohamed},
	c1 = {Engineering Shoubra, Benha Univ; Centre de Recherche en Informatique, Signal et Automatique de Lille - UMR 9189; Centre de Recherche en Informatique, Signal et Automatique de Lille - UMR 9189; Ecole nationale sup{\'e}rieure Mines-T{\'e}l{\'e}com Lille Douai},
	date = {2022-01-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:42 +0100},
	doi = {10.1109/access.2022.3147461},
	isbn = {2169-3536},
	journal = {IEEE access},
	keywords = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Speech Emotion},
	la = {en},
	pages = {14993--15006},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {Subject-Dependent Emotion Recognition System Based on Multidimensional Electroencephalographic Signals: A Riemannian Geometry Approach},
	url = {https://doi.org/10.1109/access.2022.3147461},
	volume = {10},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/access.2022.3147461}}

@article{romaniszyn-kaniapollak_2021,
	abstract = {Invasive or uncomfortable procedures especially during healthcare trigger emotions. Technological development of the equipment and systems for monitoring and recording psychophysiological functions enables continuous observation of changes to a situation responding to a situation. The presented study aimed to focus on the analysis of the individual's affective state. The results reflect the excitation expressed by the subjects'statements collected with psychological questionnaires. The research group consisted of 49 participants (22 women and 25 men). The measurement protocol included acquiring the electrodermal activity signal, cardiac signals, and accelerometric signals in three axes. Subjective measurements were acquired for affective state using the JAWS questionnaires, for cognitive skills the DST, and for verbal fluency the VFT. The physiological and psychological data were subjected to statistical analysis and then to a machine learning process using different features selection methods (JMI or PCA). The highest accuracy of the kNN classifier was achieved in combination with the JMI method (81.63{\%}) concerning the division complying with the JAWS test results. The classification sensitivity and specificity were 85.71{\%} and 71.43{\%}.},
	author = {Romaniszyn-Kania, Patrycja and Pollak, Anita and Bugdo{\l}, Marcin and Bugdol, Monika and Kania, Damian and Ma{\'n}ka, Anna and Danch--Wierzchowska, Marta and Mitas, Andrzej},
	c1 = {Faculty of Biomedical Engineering, Silesian University of Technology, Roosevelta 40, 41-800 Zabrze, Poland; Institute of Psychology, University of Silesia in Katowice, Bankowa 12, 40-007 Katowice, Poland; Faculty of Biomedical Engineering, Silesian University of Technology, Roosevelta 40, 41-800 Zabrze, Poland; Faculty of Biomedical Engineering, Silesian University of Technology, Roosevelta 40, 41-800 Zabrze, Poland; Institute of Physiotherapy and Health Sciences, The Jerzy Kukuczka Academy of Physical Education in Katowice, Miko{\l}owska 72A, 40-065 Katowice, Poland; Faculty of Biomedical Engineering, Silesian University of Technology, Roosevelta 40, 41-800 Zabrze, Poland; Faculty of Biomedical Engineering, Silesian University of Technology, Roosevelta 40, 41-800 Zabrze, Poland; Faculty of Biomedical Engineering, Silesian University of Technology, Roosevelta 40, 41-800 Zabrze, Poland},
	date = {2021-07-16},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3390/s21144853},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Affective Computing; Emotion Regulation; Emotion Recognition; Speech Emotion; Baroreflex Sensitivity},
	la = {en},
	number = {14},
	pages = {4853--4853},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Affective State during Physiotherapy and Its Analysis Using Machine Learning Methods},
	url = {https://doi.org/10.3390/s21144853},
	volume = {21},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.3390/s21144853}}

@article{brownhoward_2014,
	abstract = {The role of emotions in social scenarios is to provide an inherent mode of communication between two parties. When emotions are properly employed and understood, people are able to respond appropriately, which further enhances the social interaction. Ultimately, effective emotion execution in social settings has the capability to build rapport, improve engagement, optimize learning, provide comfort, and increase overall likability. In this paper, we discuss associating dominant emotions of effective social interaction to gestural behaviors on a humanoid robotic platform. Studies with 13 participants interacting with the robot show that by integrating key principles related to the characteristics of happy and sad emotions, the intended emotion is perceived across all participants with 95.19{\%} and 94.23{\%} sensitivity, respectively.},
	author = {Brown, LaVonda and Howard, Ayanna},
	c1 = {{$[$}School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, 30332, USA{$]$}; {$[$}School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, 30332, USA{$]$}},
	date = {2014-08-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/roman.2014.6926297},
	keywords = {Human Perception of Robots; Emotion Recognition; Emotion Perception; Affective Computing; Human-Robot Interaction},
	la = {en},
	title = {Gestural behavioral implementation on a humanoid robotic platform for effective social interaction},
	url = {https://doi.org/10.1109/roman.2014.6926297},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1109/roman.2014.6926297}}

@article{manojmurali_2015,
	abstract = {Detection and classification of human emotions from multiple bio-signals has a wide variety of applications. Though electronic devices are available in the market today that acquire multiple body signals, the classification of human emotions in real-time, adapted to the tight energy budgets of wearable embedded systems is a big challenge. In this paper we present an embedded classifier for real-time emotion classification. We propose a system that operates at different energy budgeted modes, depending on the available energy, where each mode is constrained by an operating energy bound. The classifier has an offline training phase where feature selection is performed for each operating mode, with an energy-budget aware algorithm that we propose. Across the different operating modes, the classification accuracy ranges from 95{\%} - 75{\%} and 89{\%} - 70{\%} for arousal and valence respectively. The accuracy is traded off for less power consumption, which results in an increased battery life of up to 7.7 times (from 146.1 to 1126.9 hours).},
	author = {Manoj, Padmanabhan and Murali, Srinivasan and Rincon, Francisco and Atienza, David},
	c1 = {Embedded Systems Lab. (ESL), EPFL, Switzerland; SmartCardia Inc. Ltd, Switzerland; SmartCardia Inc. Ltd, Switzerland; Embedded Systems Lab. (ESL), EPFL, Switzerland},
	date = {2015-08-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/embc.2015.7318846},
	keywords = {Emotion Recognition; Affective Computing; Deep Learning for EEG; Sensory Processing},
	la = {en},
	title = {Energy-aware embedded classifier design for real-time emotion analysis},
	url = {https://doi.org/10.1109/embc.2015.7318846},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/embc.2015.7318846}}

@article{bhattsethi_2023,
	abstract = {Abstract Human behaviour reflects cognitive abilities. Human cognition is fundamentally linked to the different experiences or characteristics of consciousness/emotions, such as joy, grief, anger, etc., which assists in effective communication with others. Detection and differentiation between thoughts, feelings, and behaviours are paramount in learning to control our emotions and respond more effectively in stressful circumstances. The ability to perceive, analyse, process, interpret, remember, and retrieve information while making judgments to respond correctly is referred to as Cognitive Behavior. After making a significant mark in emotion analysis, deception detection is one of the key areas to connect human behaviour, mainly in the forensic domain. Detection of lies, deception, malicious intent, abnormal behaviour, emotions, stress, etc., have significant roles in advanced stages of behavioral science. Artificial Intelligence and Machine learning (AI/ML) has helped a great deal in pattern recognition, data extraction and analysis, and interpretations. The goal of using AI and ML in behavioral sciences is to infer human behaviour, mainly for mental health or forensic investigations. The presented work provides an extensive review of the research on cognitive behaviour analysis. A parametric study is presented based on different physical characteristics, emotional behaviours, data collection sensing mechanisms, unimodal and multimodal datasets, modelling AI/ML methods, challenges, and future research directions.},
	author = {Bhatt, Priya and Sethi, Ajay and Tasgaonkar, Vaibhav and Shroff, Jugal and Pendharkar, Isha and Desai, Aditya and Sinha, Pratyush and Deshpande, Aditya and Joshi, Gargi and Rahate, Anil and Jain, Priyanka and Walambe, Rahee and Kotecha, Ketan and Jain, Nidhi},
	c1 = {Symbiosis Institute of Technology, Symbiosis International Deemed University, Pune, India; Symbiosis Institute of Technology, Symbiosis International Deemed University, Pune, India; Symbiosis Institute of Technology, Symbiosis International Deemed University, Pune, India; Symbiosis Institute of Technology, Symbiosis International Deemed University, Pune, India; Symbiosis Institute of Technology, Symbiosis International Deemed University, Pune, India; Symbiosis Institute of Technology, Symbiosis International Deemed University, Pune, India; Symbiosis Institute of Technology, Symbiosis International Deemed University, Pune, India; Symbiosis Institute of Technology, Symbiosis International Deemed University, Pune, India; Symbiosis Institute of Technology, Symbiosis International Deemed University, Pune, India; Symbiosis Institute of Technology, Symbiosis International Deemed University, Pune, India; Centre for Development of Advanced Computing (C-DAC), Delhi, India; Symbiosis Centre for Applied Artificial Intelligence, Symbiosis International Deemed University, Pune, India; Symbiosis Centre for Applied Artificial Intelligence, Symbiosis International Deemed University, Pune, India; Centre for Development of Advanced Computing (C-DAC), Delhi, India},
	date = {2023-07-31},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1186/s40708-023-00196-6},
	isbn = {2198-4026},
	journal = {Brain informatics},
	keywords = {Affective Computing; Emotion Recognition; Detection; Cognitive Processes; Nonverbal Behavior},
	la = {en},
	number = {1},
	publisher = {Springer Science+Business Media},
	title = {Machine learning for cognitive behavioral analysis: datasets, methods, paradigms, and research directions},
	url = {https://doi.org/10.1186/s40708-023-00196-6},
	volume = {10},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1186/s40708-023-00196-6}}

@article{fanliu_2022,
	abstract = {Emotion recognition means the automatic identification of a human's emotional state by obtaining his/her physiological or nonphysiological signals. The EEG-based method is an effective mechanism, which is commonly used for the recognition of emotions in real environments. In this paper, the convolutional neural network is used to classify the EEG signal into three and four emotional states under the DEAP dataset, which is defined as a Database for Emotion Analysis using physiological signals. For this purpose, a high-order cross-feature sample is extracted to recognize the emotional state with a single channel. A seven-layer convolutional neural network is used to classify the 32-channel EEG signal, and the average accuracy of four and three emotional states is 65{\%} and 58.62{\%}. The single-channel high-order cross-sample is classified with convolutional neural networks, and the average accuracy of four emotional states is 43.5{\%}. Among all the channels related to emotion recognition, the F4 channel gets the best classification accuracy of 44.25{\%}, and the average accuracy of the even number channel is higher than the odd number channel. The proposed method provides a basis for the real-time application of EEG-based emotion recognition.},
	author = {Fan, Chengcheng and Liu, Xiang and Gu, Xiaochen and Zhou, Liang and Li, Xiaoou},
	c1 = {Department of Automation, School of Mechatronics Engineering and Automation, Key Laboratory of Power Station Automation Technology, Shanghai University, 333 Nanchen Road, Shanghai 200072, China; Shanghai University of Medicine \& Health Science, School of Medical Instrument, 257 Tianxiong Road, Pudong New District, Shanghai 201318, China; Weihai Municipal Hospital, 70 Heping Road, Huanchui District, Weihai 264200, China; Department of Automation, School of Mechatronics Engineering and Automation, Key Laboratory of Power Station Automation Technology, Shanghai University, 333 Nanchen Road, Shanghai 200072, China; Shanghai University of Medicine \& Health Science, School of Medical Instrument, 257 Tianxiong Road, Pudong New District, Shanghai 201318, China; Shanghai University of Medicine \& Health Science, School of Medical Instrument, 257 Tianxiong Road, Pudong New District, Shanghai 201318, China; Shanghai University of Medicine \& Health Science, School of Medical Instrument, 257 Tianxiong Road, Pudong New District, Shanghai 201318, China},
	date = {2022-02-15},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1155/2022/6238172},
	isbn = {2040-2295},
	journal = {Journal of healthcare engineering},
	keywords = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Signal Processing},
	la = {en},
	pages = {1--11},
	publisher = {Hindawi Publishing Corporation},
	title = {Research on Emotion Recognition of EEG Signal Based on Convolutional Neural Networks and High-Order Cross-Analysis},
	url = {https://doi.org/10.1155/2022/6238172},
	volume = {2022},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1155/2022/6238172}}

@article{yldrmakbulut_2023,
	abstract = {Eating is experienced as an emotional social activity in any culture. There are factors that influence the emotions felt during food consumption. The emotion felt while eating has a significant impact on our lives and affects different health conditions such as obesity. In addition, investigating the emotion during food consumption is considered a multidisciplinary problem ranging from neuroscience to anatomy. In this study, we focus on evaluating the emotional experience of different participants during eating activities and aim to analyze them automatically using deep learning models. We propose a facial expression-based prediction model to eliminate user bias in questionnaire-based assessment systems and to minimize false entries to the system. We measured the neural, behavioral, and physical manifestations of emotions with a mobile app and recognize emotional experiences from facial expressions. In this research, we used three different situations to test whether there could be any factor other than the food that could affect a person's mood. We asked users to watch videos, listen to music or do nothing while eating. This way we found out that not only food but also external factors play a role in emotional change. We employed three Convolutional Neural Network (CNN) architectures, fine-tuned VGG16, and Deepface to recognize emotional responses during eating. The experimental results demonstrated that the fine-tuned VGG16 provides remarkable results with an overall accuracy of 77.68{\%} for recognizing the four emotions. This system is an alternative to today's survey-based restaurant and food evaluation systems.},
	author = {Yıldırım, Elif and Akbulut, Fatma and {\c C}atal, {\c C}a{\u g}atay},
	c1 = {Department of Computer Engineering, Istanbul K{\"u}lt{\"u}r University, Istanbul, Turkey; Department of Computer Engineering, Istanbul K{\"u}lt{\"u}r University, Istanbul, Turkey; Department of Computer Science and Engineering, Qatar University, Doha, Qatar},
	date = {2023-03-22},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1007/s11042-023-15008-6},
	isbn = {1380-7501},
	journal = {Multimedia tools and applications},
	keywords = {Emotion Recognition; Affective Computing; Facial Expression; Emotional Responses; Sensory Analysis},
	la = {en},
	number = {20},
	pages = {31659--31671},
	publisher = {Springer Science+Business Media},
	title = {Analysis of facial emotion expression in eating occasions using deep learning},
	url = {https://doi.org/10.1007/s11042-023-15008-6},
	volume = {82},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1007/s11042-023-15008-6}}

@article{wangzhao_2023,
	abstract = {The emergence of artificial emotional intelligence technology is revolutionizing the fields of computers and robotics, allowing for a new level of communication and understanding of human behavior that was once thought impossible. While recent advancements in deep learning have transformed the field of computer vision, automated understanding of evoked or expressed emotions in visual media remains in its infancy. This foundering stems from the absence of a universally accepted definition of "emotion", coupled with the inherently subjective nature of emotions and their intricate nuances. In this article, we provide a comprehensive, multidisciplinary overview of the field of emotion analysis in visual media, drawing on insights from psychology, engineering, and the arts. We begin by exploring the psychological foundations of emotion and the computational principles that underpin the understanding of emotions from images and videos. We then review the latest research and systems within the field, accentuating the most promising approaches. We also discuss the current technological challenges and limitations of emotion analysis, underscoring the necessity for continued investigation and innovation. We contend that this represents a "Holy Grail" research problem in computing and delineate pivotal directions for future inquiry. Finally, we examine the ethical ramifications of emotion-understanding technologies and contemplate their potential societal impacts. Overall, this article endeavors to equip readers with a deeper understanding of the domain of emotion analysis in visual media and to inspire further research and development in this captivating and rapidly evolving field.},
	author = {Wang, James and Zhao, Sicheng and Wu, Chenyan and Adams, Reginald and Newman, Michelle and Shafir, Tal and Tsachor, Rachelle},
	c1 = {College of Information Sciences and Technology, The Pennsylvania State University, University Park, PA, USA; Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China; College of Information Sciences and Technology, The Pennsylvania State University, University Park, PA, USA; Department of Psychology, The Pennsylvania State University, University Park, PA, USA; Department of Psychology, The Pennsylvania State University, University Park, PA, USA; Emily Sagol Creative Arts Therapies Research Center, University of Haifa, Haifa, Israel; School of Theatre and Music, University of Illinois at Chicago, Chicago, IL, USA},
	date = {2023-10-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1109/jproc.2023.3273517},
	isbn = {0018-9219},
	journal = {Proceedings of the IEEE},
	keywords = {Emotion Recognition; Affective Computing; Aesthetic Emotions; Visual Attention; Emotional Responses},
	la = {en},
	number = {10},
	pages = {1236--1286},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {Unlocking the Emotional World of Visual Media: An Overview of the Science, Research, and Impact of Understanding Emotion},
	url = {https://doi.org/10.1109/jproc.2023.3273517},
	volume = {111},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/jproc.2023.3273517}}

@article{chavez-martinezruiz-correa_2015,
	abstract = {The mobile and ubiquitous nature of conversational social video has placed video blogs among the most popular forms of online video. For this reason, there has been an increasing interest in conducting studies of human behavior from video blogs in affective and social computing. In this context, we consider the problem of mood and personality trait impression inference using verbal and nonverbal audio-visual features. Under a multi-label classification framework, we show that for both mood and personality trait binary label sets, not only the simultaneous inference of multiple labels is feasible, but also that classification accuracy increases moderately for several labels, compared to a single-label approach. The multi-label method we consider naturally exploits label correlations, which motivate our approach, and our results are consistent with models proposed in psychology to define human emotional states and personality. Our approach points to the automatic specification of co-occurring emotional states and personality, by inferring several labels at once, compared to single-label approaches. We also propose a new set of facial features, based on emotion valence from facial expressions, and analyze their suitability in the multi-label framework.},
	author = {Ch{\'a}vez-Mart{\'\i}nez, Gilberto and Ru{\'\i}z-Correa, Salvador and G{\'a}tica-P{\'e}rez, Daniel},
	c1 = {Idiap Research Institute, Switzerland; Instituto Potosino de Investigati{\'o}n Cient{\'\i}fica y Tecnol{\'o}gica, Mexico; Idiap Research Institute, {\'E}cole Polytechnique F{\'e}d{\'e}rale de Lausanne, Switzerland},
	date = {2015-11-30},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1145/2836041.2836051},
	keywords = {Multi-label Learning; Affective Computing; Emotion Recognition; Aspect-based Sentiment Analysis},
	la = {en},
	title = {Happy and agreeable?},
	url = {https://doi.org/10.1145/2836041.2836051},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1145/2836041.2836051}}

@article{jemioostorman_2022,
	abstract = {Our review aimed to assess the current state and quality of publicly available datasets used for automated affect and emotion recognition (AAER) with artificial intelligence (AI), and emphasising cardiovascular (CV) signals. The quality of such datasets is essential to create replicable systems for future work to grow. We investigated nine sources up to 31 August 2020, using a developed search strategy, including studies considering the use of AI in AAER based on CV signals. Two independent reviewers performed the screening of identified records, full-text assessment, data extraction, and credibility. All discrepancies were resolved by discussion. We descriptively synthesised the results and assessed their credibility. The protocol was registered on the Open Science Framework (OSF) platform. Eighteen records out of 195 were selected from 4649 records, focusing on datasets containing CV signals for AAER. Included papers analysed and shared data of 812 participants aged 17 to 47. Electrocardiography was the most explored signal (83.33{\%} of datasets). Authors utilised video stimulation most frequently (52.38{\%} of experiments). Despite these results, much information was not reported by researchers. The quality of the analysed papers was mainly low. Researchers in the field should concentrate more on methodology.},
	author = {Jemio{\l}o, Pawe{\l} and Storman, Dawid and Mamica, Maria and Szymkowski, Mateusz and {\.Z}abicka, Wioletta and Wojtaszek-G{\l}{\'o}wka, Magdalena and Lig{\k e}za, Antoni},
	c1 = {AGH University of Science and Technology, Faculty of Electrical Engineering, Automatics, Computer Science and Biomedical Engineering, al. A. Mickiewicza 30, 30-059 Krakow, Poland; Chair of Epidemiology and Preventive Medicine, Department of Hygiene and Dietetics, Jagiellonian University Medical College, ul. M. Kopernika 7, 31-034 Krakow, Poland; AGH University of Science and Technology, Faculty of Electrical Engineering, Automatics, Computer Science and Biomedical Engineering, al. A. Mickiewicza 30, 30-059 Krakow, Poland; AGH University of Science and Technology, Faculty of Electrical Engineering, Automatics, Computer Science and Biomedical Engineering, al. A. Mickiewicza 30, 30-059 Krakow, Poland; Students'Scientific Research Group of Systematic Reviews, Jagiellonian University Medical College, ul. M. Kopernika 7, 31-034 Krakow, Poland; Students'Scientific Research Group of Systematic Reviews, Jagiellonian University Medical College, ul. M. Kopernika 7, 31-034 Krakow, Poland; AGH University of Science and Technology, Faculty of Electrical Engineering, Automatics, Computer Science and Biomedical Engineering, al. A. Mickiewicza 30, 30-059 Krakow, Poland},
	date = {2022-03-25},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/s22072538},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Affective Computing; Emotion Recognition; Heartbeat Classification; Signal Processing; Arrhythmia Detection},
	la = {en},
	number = {7},
	pages = {2538--2538},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Datasets for Automated Affect and Emotion Recognition from Cardiovascular Signals Using Artificial Intelligence---A Systematic Review},
	url = {https://doi.org/10.3390/s22072538},
	volume = {22},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.3390/s22072538}}

@article{chakrabortytjondronegoro_2018,
	abstract = {Automatic detection of viewer interest while watching video contents can enable multimedia applications, such as online video streaming, to recommend contents in real time. However, there is yet a generic model for detecting viewer interest that is independent of subject and content while using noninvasive sensors in near-natural settings. This paper is the first attempt at solving this issue by investigating the feasibility of a generic model for detecting viewer interest based on facial expression and heart rate features. The proposed model adopts deep learning features, which are trained and tested using multisubjects' data across different video stimuli domains. The experimental results show that the generic model can reach a similar accuracy to a domain-specific model.},
	author = {Chakraborty, Prithwi and Tjondronegoro, Dian and Zhang, Ligang and Chandran, Vinod},
	c1 = {IT Discipline, School of Business and Tourism, Southern Cross University, Bilinga, QLD, Australia; IT Discipline, School of Business and Tourism, Southern Cross University, Bilinga, QLD, Australia; School of Engineering and Technology, Central Queensland University, Brisbane, QLD, Australia; School of Electrical, Engineering and Computer Science, Queensland University of Technology, Brisbane, QLD, Australia},
	date = {2018-01-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/access.2018.2874892},
	isbn = {2169-3536},
	journal = {IEEE access},
	keywords = {Visual Attention; Emotion Recognition; Feature Extraction; Video Object Segmentation; Affective Computing},
	la = {en},
	pages = {62490--62502},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {Towards Generic Modelling of Viewer Interest Using Facial Expression and Heart Rate Features},
	url = {https://doi.org/10.1109/access.2018.2874892},
	volume = {6},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/access.2018.2874892}}

@article{garcia-faurahernandez-garcia_2019,
	abstract = {The electrodermal activity (EDA) is a psychophysiological indicator which can be considered a somatic marker of the emotional and attentional reaction of subjects towards stimuli. EDA measurements are not biased by the cognitive process of giving an opinion or a score to characterize the subjective perception, and group-level EDA recordings integrate the reaction of the whole audience, thus reducing the signal noise. This paper contributes to the field of affective video content analysis, extending previous novel work on the use of EDA as ground truth for prediction algorithms. Here, we label short video clips according to the audience's emotion (high vs. low) and attention (increasing vs. decreasing), derived from EDA records. Then, we propose a set of low-level audiovisual descriptors and train binary classifiers that predict the emotion and attention with 75{\%} and 80{\%} accuracy, respectively. These results, along with those of previous works, reinforce the usefulness of such low-level audiovisual descriptors to model video in terms of the induced affective response.},
	author = {Garc{\'\i}a-Faura, {\'A}lvaro and Hern{\'a}ndez-Garc{\'\i}a, A. and Fern{\'a}ndez-Mart{\'\i}nez, Fernando and D{\'\i}az-de-Mar{\'\i}a, Fernando and San-Segundo, Rub{\'e}n},
	c1 = {Department of Electrical Engineering, Universidad Polit{\'e}cnica de Madrid, Madrid, Spain. E-mails: agfaura{\char64}die.upm.es, fernando.fernandezm{\char64}upm.es, ruben.sansegundo{\char64}upm.es; Institute of Cognitive Science, Universit{\"a}t Osnabr{\"u}ck, Osnabr{\"u}ck, Germany. E-mail: ahernandez{\char64}uos.de; Department of Electrical Engineering, Universidad Polit{\'e}cnica de Madrid, Madrid, Spain. E-mails: agfaura{\char64}die.upm.es, fernando.fernandezm{\char64}upm.es, ruben.sansegundo{\char64}upm.es; Signal Theory and Communications, Universidad Carlos III de Madrid, Madrid, Spain. E-mail: fdiaz{\char64}tsc.uc3m.es; Department of Electrical Engineering, Universidad Polit{\'e}cnica de Madrid, Madrid, Spain. E-mails: agfaura{\char64}die.upm.es, fernando.fernandezm{\char64}upm.es, ruben.sansegundo{\char64}upm.es},
	date = {2019-02-22},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3233/web-190398},
	isbn = {2405-6456},
	journal = {Web intelligence},
	keywords = {Emotion Recognition; Audio Event Detection; Speech Emotion; Affective Computing; Facial Expression},
	la = {en},
	number = {1},
	pages = {29--40},
	publisher = {IOS Press},
	title = {Emotion and attention: Audiovisual models for group-level skin response recognition in short movies},
	url = {https://doi.org/10.3233/web-190398},
	volume = {17},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.3233/web-190398}}

@article{jalilifardrastegarnia_2020,
	abstract = {It has been observed from recent studies that corticolimbic Theta rhythm from EEG recordings perceived as fear or threatening scene during neural processing of visual stimuli. In additions, neural oscillations' patterns in Theta, Alpha and Beta sub-bands also play important role in brain's emotional processing. Inspired from these findings, in this paper we attempt to classify two different emotional states by analyzing single-channel EEG recordings. A video clip that can evoke 3 different emotional states: neutral, relaxation and scary is shown to 19 college-aged subjects and they were asked to score their emotional outcome by giving a number between 0 to 10 (where 0 means not scary at all and 10 means the most scary). First, recorded EEG data were preprocessed by stationary wavelet transform (SWT) based artifact removal algorithm. Then power distribution in simultaneous time-frequency domain was analyzed using short-time Fourier transform (STFT) followed by calculating the average power during each 0.2s time-segment for each brain sub-band. Finally, 46 features, as the mean power of frequency bands between 4 and 50 Hz during each time-segment, containing 689 instances{$\backslash$}textemdash for each subject {$\backslash$}textemdash were collected for classification. We found that relaxation and fear emotions evoked during watching scary and relaxing movies can be classified with average classification rate of 94.208{$\backslash$}{\%} using K-NN by applying methods and materials proposed in this paper. We also classified the dataset using SVM and we found out that K-NN classifier (when {$\backslash$}begin{\{}math{\}} k=1 {$\backslash$}end{\{}math{\}}) outperforms SVM in classifying EEG dynamics induced by horror and relaxing movies, however, for {$\backslash$}begin{\{}math{\}} K \&gt;1 {$\backslash$}end{\{}math{\}} in K-NN, SVM has better average classification rate.},
	author = {Jalilifard, Amir and Rastegarnia, Amir and Pizzolato, Ednaldo and Islam, Kafiul},
	c1 = {Federal University of Sao Carlos; Malayer University; Federal University of Sao Carlos; Independent University},
	date = {2020-08-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.11591/ijece.v10i4.pp3826-3838},
	isbn = {2722-256X},
	journal = {International Journal of Power Electronics and Drive Systems/International Journal of Electrical and Computer Engineering},
	keywords = {EEG Analysis; Emotion Recognition; Speech Emotion; Affective Computing; Deep Learning for EEG},
	la = {en},
	number = {4},
	pages = {3826--3826},
	publisher = {Institute of Advanced Engineering and Science (IAES)},
	title = {Classification of emotions induced by horror and relaxing movies using single-channel EEG recordings},
	url = {https://doi.org/10.11591/ijece.v10i4.pp3826-3838},
	volume = {10},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.11591/ijece.v10i4.pp3826-3838}}

@article{masudayairi_2023,
	abstract = {Objective and accurate classification of fear levels is a socially important task that contributes to developing treatments for Anxiety Disorder, Obsessive-compulsive Disorder, Post-Traumatic Stress Disorder (PTSD), and Phobia. This study examines a deep learning model to automatically estimate human fear levels with high accuracy using multichannel EEG signals and multimodal peripheral physiological signals in the DEAP dataset. The Multi-Input CNN-LSTM classification model combining Convolutional Neural Network (CNN) and Long Sort-Term Memory (LSTM) estimated four fear levels with an accuracy of 98.79{\%} and an F1 score of 99.01{\%} in a 10-fold cross-validation. This study contributes to the following; (1) to present the possibility of recognizing fear emotion with high accuracy using a deep learning model from physiological signals without arbitrary feature extraction or feature selection, (2) to investigate effective deep learning model structures for high-accuracy fear recognition and to propose Multi-Input CNN-LSTM, and (3) to examine the model's tolerance to individual differences in physiological signals and the possibility of improving accuracy through additional learning.},
	author = {Masuda, Nagisa and Yairi, Ikuko},
	c1 = {Graduate School of Science and Engineering, Sophia University, Japan; Graduate School of Science and Engineering, Sophia University, Japan},
	date = {2023-06-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3389/fpsyg.2023.1141801},
	isbn = {1664-1078},
	journal = {Frontiers in psychology},
	keywords = {Deep Learning for EEG; Emotion Recognition; Deep Learning; Affective Computing; Epilepsy Detection},
	la = {en},
	publisher = {Frontiers Media},
	title = {Multi-Input CNN-LSTM deep learning model for fear level classification based on EEG and peripheral physiological signals},
	url = {https://doi.org/10.3389/fpsyg.2023.1141801},
	volume = {14},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.3389/fpsyg.2023.1141801}}

@article{sridharbusso_2022,
	abstract = {The prediction of valence from speech is an important, but challenging problem. The expression of valence in speech has speaker-dependent cues, which contribute to performances that are often significantly lower than the prediction of other emotional attributes such as arousal and dominance. A practical approach to improve valence prediction from speech is to adapt the models to the target speakers in the test set. Adapting a <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">speech emotion recognition</i> (SER) system to a particular speaker is a hard problem, especially with <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">deep neural networks</i> (DNNs), since it requires optimizing millions of parameters. This study proposes an unsupervised approach to address this problem by searching for speakers in the train set with similar acoustic patterns as the speaker in the test set. Speech samples from the selected speakers are used to create the adaptation set. This approach leverages transfer learning using pre-trained models, which are adapted with these speech samples. We propose three alternative adaptation strategies: unique speaker, oversampling and weighting approaches. These methods differ on the use of the adaptation set in the personalization of the valence models. The results demonstrate that a valence prediction model can be efficiently personalized with these unsupervised approaches, leading to relative improvements as high as 13.52{\%}.},
	author = {Sridhar, Kusha and Busso, Carlos},
	c1 = {Erik Jonsson School of Engineering and Computer Science, The University of Texas at Dallas, Richardson, TX, USA; Erik Jonsson School of Engineering and Computer Science, The University of Texas at Dallas, Richardson, TX, USA},
	date = {2022-10-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1109/taffc.2022.3187336},
	isbn = {1949-3045},
	journal = {IEEE transactions on affective computing},
	keywords = {Speech Emotion; Speech Enhancement; Emotion Recognition; Audio-Visual Speech Recognition; Affective Computing},
	la = {en},
	number = {4},
	pages = {1959--1972},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {Unsupervised Personalization of an Emotion Recognition System: The Unique Properties of the Externalization of Valence in Speech},
	url = {https://doi.org/10.1109/taffc.2022.3187336},
	volume = {13},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/taffc.2022.3187336}}

@article{brancoehteshami_2022,
	abstract = {Affective studies provide essential insights to address emotion recognition and tracking. In traditional open-loop structures, a lack of knowledge about the internal emotional state makes the system incapable of adjusting stimuli parameters and automatically responding to changes in the brain. To address this issue, we propose to use facial electromyogram measurements as biomarkers to infer the internal hidden brain state as feedback to close the loop. In this research, we develop a systematic way to track and control emotional valence, which codes emotions as being pleasant or obstructive. Hence, we conduct a simulation study by modeling and tracking the subject's emotional valence dynamics using state-space approaches. We employ Bayesian filtering to estimate the person-specific model parameters along with the hidden valence state, using continuous and binary features extracted from experimental electromyogram measurements. Moreover, we utilize a mixed-filter estimator to infer the secluded brain state in a real-time simulation environment. We close the loop with a fuzzy logic controller in two categories of regulation: inhibition and excitation. By designing a control action, we aim to automatically reflect any required adjustments within the simulation and reach the desired emotional state levels. Final results demonstrate that, by making use of physiological data, the proposed controller could effectively regulate the estimated valence state. Ultimately, we envision future outcomes of this research to support alternative forms of self-therapy by using wearable machine interface architectures capable of mitigating periods of pervasive emotions and maintaining daily well-being and welfare.},
	author = {Branco, Luciano and Ehteshami, A. and Azgomi, Hamid and Faghih, Rose},
	c1 = {Department of Electrical and Computer Engineering, United States; Department of Electrical and Computer Engineering, United States; Department of Electrical and Computer Engineering, United States; Department of Neurological Surgery, United States; Department of Biomedical Engineering, United States; Department of Electrical and Computer Engineering, United States},
	date = {2022-03-25},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3389/fncom.2022.747735},
	isbn = {1662-5188},
	journal = {Frontiers in computational neuroscience},
	keywords = {Affective Computing; Emotion Recognition; EEG Analysis; Speech Emotion},
	la = {en},
	publisher = {Frontiers Media},
	title = {Closed-Loop Tracking and Regulation of Emotional Valence State From Facial Electromyogram Measurements},
	url = {https://doi.org/10.3389/fncom.2022.747735},
	volume = {16},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.3389/fncom.2022.747735}}

@article{chenzhao_2022,
	abstract = {Recognizing emotion from Electroencephalography (EEG) is a promising and valuable research issue in the field of affective brain-computer interfaces (aBCI). To improve the accuracy of emotion recognition, an emotional feature extraction method is proposed based on the temporal information in the EEG signal. This study adopts microstate analysis as a spatio-temporal analysis for EEG signals. Microstates are defined as a series of momentary quasi-stable scalp electric potential topographies. Brain electrical activity could be modeled as being composed of a time sequence of microstates. Microstate sequences provide an ideal macroscopic window for observing the temporal dynamics of spontaneous brain activity. To further analyze the fine structure of the microstate sequence, we propose a feature extraction method based on k-mer. K-mer is a k-length substring of a given sequence. It has been widely used in computational genomics and sequence analysis. We extract features that are based on the D2∗statistic of k-mer. In addition, we also extract four parameters (duration, occurrence, time coverage, GEV) of each microstate class as features at the coarse level. We conducted experiments on the DEAP dataset to evaluate the performance of the proposed features. The experimental results demonstrate that the fusion of features in fine and coarse levels can effectively improve classification accuracy.},
	author = {Chen, Jing and Zhao, Zexian and Shu, Qin‐Fen and Cai, Guoen},
	c1 = {Faculty of Computing, Harbin Institute of Technology, Harbin, China; School of Computer Science and Engineering, Xi'an University of Technology, Xi'an, China; Department of Neurology, Zhejiang Hospital, Hangzhou, China; Department of Neurology, Zhejiang Hospital, Hangzhou, China; Department of Critical Care Medicine, Zhejiang Hospital, Hangzhou, China},
	date = {2022-12-23},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3389/fpsyg.2022.1065196},
	isbn = {1664-1078},
	journal = {Frontiers in psychology},
	keywords = {Emotion Recognition; EEG Analysis; Feature Extraction; Deep Learning for EEG; Affective Computing},
	la = {en},
	publisher = {Frontiers Media},
	title = {Feature extraction based on microstate sequences for EEG--based emotion recognition},
	url = {https://doi.org/10.3389/fpsyg.2022.1065196},
	volume = {13},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.3389/fpsyg.2022.1065196}}

@article{chakrabortyli-gang_2015,
	abstract = {Viewer interest, evoked by video content, can potentially identify the highlights of the video. This paper explores the use of facial expressions (FE) and heart rate (HR) of viewers captured using camera and non-strapped sensor for identifying interesting video segments. The data from ten subjects with three videos showed that these signals are viewer dependent and not synchronized with the video contents. To address this issue, new algorithms are proposed to effectively combine FE and HR signals for identifying the time when viewer interest is potentially high. The results show that, compared with subjective annotation and match report highlights, 'non-neutral' FE and 'relatively higher and faster' HR is able to capture 60{\%}-80{\%} of goal, foul, and shot-on-goal soccer video events. FE is found to be more indicative than HR of viewer interest, but the fusion of these two modalities outperforms each of them.},
	author = {Chakraborty, Prithwi and Li-gang, Zhang and Tjondronegoro, Dian and Chandran, Vinod},
	c1 = {Queensland University of Technology, Brisbane, Australia; Xi'an University of Technology, Xi'an, China; Queensland University of Technology, Brisbane, Australia; Queensland University of Technology, Brisbane, Australia},
	date = {2015-06-22},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1145/2671188.2749361},
	keywords = {Event Detection; Feature Extraction; Emotion Recognition; Video Summarization; Affective Computing},
	la = {en},
	title = {Using Viewer's Facial Expression and Heart Rate for Sports Video Highlights Detection},
	url = {https://doi.org/10.1145/2671188.2749361},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1145/2671188.2749361}}

@article{niewu_2017,
	abstract = {With the continuous development of the modern intelligent household, people found that brain wave can be used for controlling household appliances.EEG signal, as a typical brain wave signal carrying the brain state information, has walked into the researcher's view.Brain wave carries detailed information about the state of the brain.As a result, using computer to collect and analyze EEG signal plays a great role in smart home.This paper uses positive and negative emotional EEG signal as the research object, begins with introducing the research status of brain waves, and then uses the Chinese Affective Picture System (CAPS {$[$}12{$]$}) of Chinese Academy of Sciences, designs the watch pictures to experiment, utilizes machine learning algorithm of support vector machine (SVM) for data analysis, and obtains an accuracy of 58.3{\%} eventually.This paper provides a feasible scheme for the study of EEG in the field of emotion analysis.},
	author = {Nie, Yu and Wu, Yang and Yang, Zhong-Yao and Sun, Guangzhi and Yang, Yongjian and Hong, Xuanyi},
	c1 = {College of Software, Jilin University, Jilin, China; College of Software, Jilin University, Jilin, China; College of Software, Jilin University, Jilin, China; College of Software, Jilin University, Jilin, China; College of Software, Jilin University, Jilin, China; College of Software, Jilin University, Jilin, China},
	date = {2017-01-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.2991/amcce-17.2017.111},
	keywords = {Emotion Recognition; Affective Computing; Support Vector Machines; Speech Emotion; Color Psychology},
	la = {en},
	title = {Emotional Evaluation Based on SVM},
	url = {https://doi.org/10.2991/amcce-17.2017.111},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.2991/amcce-17.2017.111}}

@article{yvartdelestage_2016,
	abstract = {Even though the emotional state is increasingly taken into account in scientific studies aimed at determining user experience of user acceptance, there are still only a few normalized tools.In this article, we decided to focus on mood determination as we consider this affective state to be more pervasive and more understandable by the person who is experiencing it.Thus, we propose a prototypical tool called SYM (Spot Your Mood) as a new tool in user mood determination to be used in many different situations.},
	author = {Yvart, Willy and Delestage, Charles-Alexandre and Leleu-Merviel, Sylvie},
	c1 = {(UVHC) TCTS (UMONS) Rue Michel Rondet, 59135 Wallers, FRANCE; DeVisu (UVHC) Rue Michel Rondet, 59135 Wallers, FRANCE; DeVisu (UVHC) Rue Michel Rondet, 59135 Wallers, FRANCE},
	date = {2016-03-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3384/ecp10304},
	isbn = {1650-3686},
	journal = {Link{\"o}ping electronic conference proceedings},
	keywords = {Emotion Recognition; Affective Computing; Emotion Regulation; Speech Emotion; Emotions},
	la = {en},
	publisher = {Link{\"o}ping University Electronic Press},
	title = {SYM: Toward a New Tool in User's Mood Determination},
	url = {https://doi.org/10.3384/ecp10304},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.3384/ecp10304}}

@article{yuezhong_2021,
	abstract = {Affective computing systems can decode cortical activities to facilitate emotional human-computer interaction. However, personalities exist in neurophysiological responses among different users of the brain-computer interface leads to a difficulty for designing a generic emotion recognizer that is adaptable to a novel individual. It thus brings an obstacle to achieve cross-subject emotion recognition (ER). To tackle this issue, in this study we propose a novel feature selection method, manifold feature fusion and dynamical feature selection (MF-DFS), under transfer learning principle to determine generalizable features that are stably sensitive to emotional variations. The MF-DFS framework takes the advantages of local geometrical information feature selection, domain adaptation based manifold learning, and dynamical feature selection to enhance the accuracy of the ER system. Based on three public databases, DEAP, MAHNOB-HCI and SEED, the performance of the MF-DFS is validated according to the leave-one-subject-out paradigm under two types of electroencephalography features. By defining three emotional classes of each affective dimension, the accuracy of the MF-DFS-based ER classifier is achieved at 0.50-0.48 (DEAP) and 0.46-0.50 (MAHNOBHCI) for arousal and valence emotional dimensions, respectively. For the SEED database, it achieves 0.40 for the valence dimension. The corresponding accuracy is significantly superior to several classical feature selection methods on multiple machine learning models.},
	author = {Yue, Hua and Zhong, Xiaolong and Zhang, Bingxue and Yin, Zhong and Zhang, Jianhua},
	c1 = {Engineering Research Center of Optical Instrument and System, Ministry of Education, Shanghai Key Lab of Modern Optical System, University of Shanghai for Science and Technology, Shanghai 200093, China; Engineering Research Center of Optical Instrument and System, Ministry of Education, Shanghai Key Lab of Modern Optical System, University of Shanghai for Science and Technology, Shanghai 200093, China; School of Optical-Electrical and Computer Engineering, University of Shanghai for Science and Technology, Shanghai 200093, China; Engineering Research Center of Optical Instrument and System, Ministry of Education, Shanghai Key Lab of Modern Optical System, University of Shanghai for Science and Technology, Shanghai 200093, China; OsloMet Artificial Intelligence Lab, Department of Computer Science, Oslo Metropolitan University, N-0130 Oslo, Norway},
	date = {2021-10-23},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3390/brainsci11111392},
	isbn = {2076-3425},
	journal = {Brain sciences},
	keywords = {Affective Computing; Emotion Recognition; Feature Extraction; Human-Computer Interaction; Aspect-based Sentiment Analysis},
	la = {en},
	number = {11},
	pages = {1392--1392},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Manifold Feature Fusion with Dynamical Feature Selection for Cross-Subject Emotion Recognition},
	url = {https://doi.org/10.3390/brainsci11111392},
	volume = {11},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.3390/brainsci11111392}}

@article{nini_2021,
	abstract = {The brain-computer interface (BCI) interprets the physiological information of the human brain in the process of consciousness activity. It builds a direct information transmission channel between the brain and the outside world. As the most common non-invasive BCI modality, electroencephalogram (EEG) plays an important role in the emotion recognition of BCI; however, due to the individual variability and non-stationary of EEG signals, the construction of EEG-based emotion classifiers for different subjects, different sessions, and different devices is an important research direction. Domain adaptation utilizes data or knowledge from more than one domain and focuses on transferring knowledge from the source domain (SD) to the target domain (TD), in which the EEG data may be collected from different subjects, sessions, or devices. In this study, a new domain adaptation sparse representation classifier (DASRC) is proposed to address the cross-domain EEG-based emotion classification. To reduce the differences in domain distribution, the local information preserved criterion is exploited to project the samples from SD and TD into a shared subspace. A common domain-invariant dictionary is learned in the projection subspace so that an inherent connection can be built between SD and TD. In addition, both principal component analysis (PCA) and Fisher criteria are exploited to promote the recognition ability of the learned dictionary. Besides, an optimization method is proposed to alternatively update the subspace and dictionary learning. The comparison of CSFDDL shows the feasibility and competitive performance for cross-subject and cross-dataset EEG-based emotion classification problems.},
	author = {Ni, Tongguang and Ni, Yuyao and Xue, Jing and Wang, Suhong},
	c1 = {School of Computer Science and Artificial Intelligence, Changzhou University, China; School of Electrical Engineering, Xi'an Jiaotong University, China; Department of Nephrology, Affiliated Wuxi People's Hospital of Nanjing Medical University, China; Department of Clinical Psychology, The Third Affiliated Hospital of Soochow University, China},
	date = {2021-07-29},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3389/fpsyg.2021.721266},
	isbn = {1664-1078},
	journal = {Frontiers in psychology},
	keywords = {Emotion Recognition; Brain-Computer Interfaces; EEG Analysis; Deep Learning for EEG; Affective Computing},
	la = {en},
	publisher = {Frontiers Media},
	title = {A Domain Adaptation Sparse Representation Classifier for Cross-Domain Electroencephalogram-Based Emotion Classification},
	url = {https://doi.org/10.3389/fpsyg.2021.721266},
	volume = {12},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.3389/fpsyg.2021.721266}}

@article{akbulut_2022,
	abstract = {Emotion recognition has attracted more interest by being applied in many application areas from different domains such as medical diagnosis, e-commerce, and robotics. This research quantifies the stimulated short-term effect of emotions on the autonomic nervous system and sympathetic activity. The primary purpose of this study is to investigate the responses of 21 adults by attaching a wearable system to measure physiological data such as an electrocardiogram and electrodermal activity in a controlled environment. Cardiovascular effects were evaluated with heart rate variability indices that included HR, HRV triangular-index, rMSSD (ms), pNN5O ({\%}); frequency analysis of the very low frequency (VLF: 0-0,04 Hz), low frequency (LF: 0,04-0,15 Hz), and high frequency (HF: 0,15-0,4 Hz) components; nonlinear analysis. The sympathetic activity was evaluated with time-varying and time-invariant spectral analysis results of the EDA. The participants who experience calmness had a 4,8{\%} lower heart rate (75,06$\pm$16,76 and 78,72$\pm$16,52) observed compared to happiness. Negative valance with high-arousal emotions like anger was invariably responded to with a peak in skin conductance level. Besides, negative valance with low-arousal emotions like sadness was allied with a drop in conductance level. Anger, in addition to being the most well-known emotion, elicited coherent time-varying spectral responses.},
	author = {Akbulut, Fatma},
	c1 = {ISTANBUL KULTUR UNIVERSITY},
	date = {2022-06-28},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:08:24 +0100},
	doi = {10.55071/ticaretfbd.1125431},
	isbn = {1305-7820},
	journal = {{\.I}stanbul ticaret {\"u}niversitesi fen bilimleri dergisi},
	keywords = {Emotion Regulation; Emotion Recognition; Heart Rate Variability; Affective Computing; Speech Emotion},
	la = {en},
	number = {41},
	pages = {156--169},
	title = {EVALUATING THE EFFECTS OF THE AUTONOMIC NERVOUS SYSTEM AND SYMPATHETIC ACTIVITY ON EMOTIONAL STATES},
	url = {https://doi.org/10.55071/ticaretfbd.1125431},
	volume = {21},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.55071/ticaretfbd.1125431}}

@article{hosseinifiroozabadi_2023,
	abstract = {The accurate detection of emotions has significant implications in healthcare, psychology, and human-computer interaction. Integrating personality information into emotion recognition can enhance its utility in various applications. The present study introduces a novel deep learning approach to emotion recognition, which utilizes electroencephalography (EEG) signals and the Big Five personality traits. The study recruited 60 participants and recorded their EEG data while they viewed unique sequence stimuli designed to effectively capture the dynamic nature of human emotions and personality traits. A pre-trained convolutional neural network (CNN) was used to extract emotion-related features from the raw EEG data. Additionally, a long short-term memory (LSTM) network was used to extract features related to the Big Five personality traits. The network was able to accurately predict personality traits from EEG data. The extracted features were subsequently used in a novel network to predict emotional states within the arousal and valence dimensions. The experimental results showed that the proposed classifier outperformed common classifiers, with a high accuracy of 93.97{\%}. The findings suggest that incorporating personality traits as features in the designed network, for emotion recognition, leads to higher accuracy, highlighting the significance of examining these traits in the analysis of emotions.},
	author = {Hosseini, Mohammad and Firoozabadi, Seyed and Badie, Kambiz and Azadfallah, Parviz},
	c1 = {Department of Biomedical Engineering, Science and Research Branche, Islamic Azad University, Tehran 14778-93855, Iran; Department of Medical Physics, Faculty of Medicine, Tarbiat Modares University, Tehran 14117-13116, Iran; Content \& E-Services Research Group, IT Research Faculty, ICT Research Institute, Tehran 14399-55471, Iran; Department of Psychology, Faculty of Humanities, Tarbiat Modares University, Tehran 14117-13116, Iran},
	date = {2023-06-14},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/brainsci13060947},
	isbn = {2076-3425},
	journal = {Brain sciences},
	keywords = {Emotion Recognition; Deep Learning for EEG; Affective Computing; EEG Analysis; Speech Emotion},
	la = {en},
	number = {6},
	pages = {947--947},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Personality-Based Emotion Recognition Using EEG Signals with a CNN-LSTM Network},
	url = {https://doi.org/10.3390/brainsci13060947},
	volume = {13},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.3390/brainsci13060947}}

@article{kadirialku_2022,
	abstract = {Understanding of the perception of emotions or affective states in humans is important to develop emotion-aware systems that work in realistic scenarios. In this paper, the perception of emotions in naturalistic human interaction (audio-visual data) is studied using perceptual evaluation. For this purpose, a naturalistic audio-visual emotion database collected from TV broadcasts such as soap-operas and movies, called the IIIT-H Audio-Visual Emotion (IIIT-H AVE) database, is used. The database consists of audio-alone, video-alone, and audio-visual data in English. Using data of all three modes, perceptual tests are conducted for four basic emotions (angry, happy, neutral, and sad) based on category labeling and for two dimensions, namely arousal (active or passive) and valence (positive or negative), based on dimensional labeling. The results indicated that the participants' perception of emotions was remarkably different between the audio-alone, video-alone, and audio-video data. This finding emphasizes the importance of emotion-specific features compared to commonly used features in the development of emotion-aware systems.},
	author = {Kadiri, Sudarsana and Alku, Paavo},
	c1 = {Department of Signal Processing and Acoustics, Aalto University, Otakaari 3, FI-00076 Espoo, Finland; Department of Signal Processing and Acoustics, Aalto University, Otakaari 3, FI-00076 Espoo, Finland},
	date = {2022-06-29},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/s22134931},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Emotion Recognition; Affective Computing; Sensory Expectations; Speech Emotion; Audio Event Detection},
	la = {en},
	number = {13},
	pages = {4931--4931},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Subjective Evaluation of Basic Emotions from Audio--Visual Data},
	url = {https://doi.org/10.3390/s22134931},
	volume = {22},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.3390/s22134931}}

@article{oatestriantafyllopoulos_2019,
	abstract = {In an era where large speech corpora annotated for emotion are hard to come by, and especially ones where emotion is expressed freely instead of being acted, the importance of using free online sources for collecting such data cannot be overstated.Most of those sources, however, contain encoded audio due to storage and bandwidth constraints, often in very low bitrates.In addition, with the increased industry interest on voice-based applications, it is inevitable that speech emotion recognition (SER) algorithms will soon find their way into production environments, where the audio might be encoded in a different bitrate than the one available during training.Our contribution is threefold.First, we show that encoded audio still contains enough relevant information for robust SER.Next, we investigate the effects of mismatched encoding conditions in the training and test set both for traditional machine learning algorithms built on hand-crafted features and modern end-toend methods.Finally, we investigate the robustness of those algorithms in the multi-condition scenario, where the training set is augmented with encoded audio, but still differs from the training set.Our results indicate that end-to-end methods are more robust even in the more challenging scenario of mismatched conditions.},
	author = {Oates, C. and Triantafyllopoulos, Andreas and Steiner, Ingmar and Schuller, Bj{\"o}rn},
	c1 = {audEERING GmbH, Gilching, Germany; audEERING GmbH, Gilching, Germany; audEERING GmbH, Gilching, Germany; GLAM -Group on Language, Audio \& Music, Imperial College London, UK; ZD.B Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany; audEERING GmbH, Gilching, Germany},
	date = {2019-09-15},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.21437/interspeech.2019-1658},
	keywords = {Emotion Recognition; Audio-Visual Speech Recognition; Speech Emotion; Emotional Responses; Affective Computing},
	la = {en},
	title = {Robust Speech Emotion Recognition Under Different Encoding Conditions},
	url = {https://doi.org/10.21437/interspeech.2019-1658},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.21437/interspeech.2019-1658}}

@article{kshirsagarfalk_2022,
	abstract = {To date, several methods have been explored for the challenging task of cross-language speech emotion recognition, including the bag-of-words (BoW) methodology for feature processing, domain adaptation for feature distribution "normalization", and data augmentation to make machine learning algorithms more robust across testing conditions. Their combined use, however, has yet to be explored. In this paper, we aim to fill this gap and compare the benefits achieved by combining different domain adaptation strategies with the BoW method, as well as with data augmentation. Moreover, while domain adaptation strategies, such as the correlation alignment (CORAL) method, require knowledge of the test data language, we propose a variant that we term N-CORAL, in which test languages (in our case, Chinese) are mapped to a common distribution in an unsupervised manner. Experiments with German, French, and Hungarian language datasets were performed, and the proposed N-CORAL method, combined with BoW and data augmentation, was shown to achieve the best arousal and valence prediction accuracy, highlighting the usefulness of the proposed method for "in the wild" speech emotion recognition. In fact, N-CORAL combined with BoW was shown to provide robustness across languages, whereas data augmentation provided additional robustness against cross-corpus nuance factors.},
	author = {Kshirsagar, Shruti and Falk, Tiago},
	c1 = {Institut National de la Recherche Scientifique, University of Quebec, Montr{\'e}al, QC H3C 5J9, Canada; Institut National de la Recherche Scientifique, University of Quebec, Montr{\'e}al, QC H3C 5J9, Canada},
	date = {2022-08-26},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/s22176445},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Emotion Recognition; Speech Emotion; Affective Computing; End-to-End Speech Recognition; Statistical Language Modeling},
	la = {en},
	number = {17},
	pages = {6445--6445},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Cross-Language Speech Emotion Recognition Using Bag-of-Word Representations, Domain Adaptation, and Data Augmentation},
	url = {https://doi.org/10.3390/s22176445},
	volume = {22},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.3390/s22176445}}

@article{mohammadhussain_2023,
	abstract = {In recent years, human-computer interaction (HCI) systems have become increasingly popular. Some of these systems demand particular approaches for discriminating actual emotions through the use of better multimodal methods. In this work, a deep canonical correlation analysis (DCCA) based multimodal emotion recognition method is presented through the fusion of electroencephalography (EEG) and facial video clips. A two-stage framework is implemented, where the first stage extracts relevant features for emotion recognition using a single modality, while the second stage merges the highly correlated features from the two modalities and performs classification. Convolutional neural network (CNN) based Resnet50 and 1D-CNN (1-Dimensional CNN) have been utilized to extract features from facial video clips and EEG modalities, respectively. A DCCA-based approach was used to fuse highly correlated features, and three basic human emotion categories (happy, neutral, and sad) were classified using the SoftMax classifier. The proposed approach was investigated based on the publicly available datasets called MAHNOB-HCI and DEAP. Experimental results revealed an average accuracy of 93.86{\%} and 91.54{\%} on the MAHNOB-HCI and DEAP datasets, respectively. The competitiveness of the proposed framework and the justification for exclusivity in achieving this accuracy were evaluated by comparison with existing work.},
	author = {Mohammad, Farah and Hussain, Muhammad and Aboalsamh, Hatim},
	c1 = {Department of Computer Science, College of Computer Science and Information, King Saud University, Riyadh 11451, Saudi Arabia; Department of Computer Science, College of Computer Science and Information, King Saud University, Riyadh 11451, Saudi Arabia; Department of Computer Science, College of Computer Science and Information, King Saud University, Riyadh 11451, Saudi Arabia},
	date = {2023-03-04},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3390/diagnostics13050977},
	isbn = {2075-4418},
	journal = {Diagnostics},
	keywords = {Emotion Recognition; Affective Computing; Head Gesture Recognition; Human-Computer Interaction; Multimodal Data},
	la = {en},
	number = {5},
	pages = {977--977},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {A Bimodal Emotion Recognition Approach through the Fusion of Electroencephalography and Facial Sequences},
	url = {https://doi.org/10.3390/diagnostics13050977},
	volume = {13},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.3390/diagnostics13050977}}

@article{haratiantimotijevic_2018,
	abstract = {In this paper, a new algorithm is proposed for recognition of user experience through emotion detection using physiological signals, for application in human-machine interaction. The algorithm recognizes user's emotion quality and intensity in a two dimensional emotion space continuously. The continuous recognition of the user's emotion during human-machine interaction will enable the machine to adapt its activity based on the user's emotion in a real-time manner, thus improving user experience. The emotion model underlying the proposed algorithm is one of the most recent emotion models, which models emotion's intensity and quality in a continuous two-dimensional space of valance and arousal axes. Using only two physiological signals, which are correlated to the valance and arousal axes of the emotion space, is among the contributions of this paper. Prediction of emotion through physiological signals has the advantage of elimination of social masking and making the prediction more reliable. The key advantage of the proposed algorithm over other algorithms presented to date is the use of the least number of modalities (only two physiological signals) to predict the quality and intensity of emotion continuously in time, and using the most recent widely accepted emotion model.},
	author = {Haratian, Roya and Timotijevic, Tijana},
	c1 = {Faculty of Science and Technology, Bournemouth University, Poole, UK; School of Electronic Eng. and Computer Science, Queen Mary University of London, London, UK},
	date = {2018-09-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/icfsp.2018.8552058},
	keywords = {Emotion Recognition; Affective Computing; Human-Computer Interaction; Physiological Signals; Emotions},
	la = {en},
	title = {On-body Sensing and Signal Analysis for User Experience Recognition in Human-Machine Interaction},
	url = {https://doi.org/10.1109/icfsp.2018.8552058},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/icfsp.2018.8552058}}

@article{guowu_2018,
	abstract = {Affective computing has become a very important research area in human-machine interaction. However, affects are subjective, subtle, and uncertain. So, it is very difficult to obtain a large number of labeled training samples, compared with the number of possible features we could extract. Thus, dimensionality reduction is critical in affective computing. This paper presents our preliminary study on dimensionality reduction for affect classification. Five popular dimensionality reduction approaches are introduced and compared. Experiments on the DEAP dataset showed that no approach can universally outperform others, and performing classification using the raw features directly may not always be a bad choice.},
	author = {Guo, Chenfeng and Wu, Dongrui},
	c1 = {School of Printing and Packaging, Wuhan University, Wuhan, China; School of Automation, Huazhong University of Science and Technology, Wuhan, China},
	date = {2018-05-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/aciiasia.2018.8470329},
	keywords = {Affective Computing; Affective Design; Emotion Recognition; Feature Extraction; Aspect-based Sentiment Analysis},
	la = {en},
	title = {Feature Dimensionality Reduction for Video Affect Classification: A Comparative Study},
	url = {https://doi.org/10.1109/aciiasia.2018.8470329},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/aciiasia.2018.8470329}}

@article{kurbalijaivanovic_2015,
	abstract = {The electroencephalogram (EEG) is a powerful method for investigation of different cognitive processes. Recently, EEG analysis became very popular and important, where classification of these signals stands out as one of the mostly used methodologies. Emotion recognition is one of the most challenging tasks in EEG analysis since not much is known about representation of different emotion in EEG signals. In addition, inducing of desired emotion is by itself difficult, since various individuals react differently to external stimuli (audio, video, etc.) In this paper, we will examine the similarities in emotion perception of different individuals on the basis of audio stimuli. Since some of the participants in the experiment did not understand the language of the stimuli, we will also investigate the impact of language understanding on emotion perception. This study presents some preliminary results of more complex experiments in the area of affective computing that are planned.},
	author = {Kurbalija, Vladimir and Ivanovi{\'c}, Mirjana and Radovanovi{\'c}, Milo{\v s} and Geler, Zoltan and Mitrovi{\'c}, Darko and Dai, Weihui and Zhang, Weidong},
	c1 = {Department of Mathematics and Informatics, Faculty of Science, University of Novi Sad, Trg Dositeja Obradovi{\'c}a 4, 21000 Novi Sad, Serbia, +381 21 4852877; Department of Mathematics and Informatics, Faculty of Science, University of Novi Sad, Trg Dositeja Obradovi{\'c}a 4, 21000 Novi Sad, Serbia, +381 21 4852855; Department of Mathematics and Informatics, Faculty of Science, University of Novi Sad, Trg Dositeja Obradovi{\'c}a 4, 21000 Novi Sad, Serbia, +381 21 4852875; Department of Media Studies, Faculty of Philosophy, University of Novi Sad, Dr Zorana Đinđi{\'c}a 2, 21000 Novi Sad, Serbia, +381 21 4853918; Department of Mathematics and Informatics, Faculty of Science, University of Novi Sad, Trg Dositeja Obradovi{\'c}a 4, 21000 Novi Sad, Serbia, +381 21 4852875; School of Management, Fudan University, Shanghai 200433, China; School of Software, Fudan University, Shanghai 200433, China},
	date = {2015-09-02},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1145/2801081.2801093},
	keywords = {Emotion Recognition; Speech Emotion; Audio Event Detection; Environmental Sound Recognition; Affective Computing},
	la = {en},
	title = {Cultural Differences and Similarities in Emotion Recognition},
	url = {https://doi.org/10.1145/2801081.2801093},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1145/2801081.2801093}}

@article{mcintosh_2021,
	author = {Mcintosh, Tyler},
	c1 = {University of Greenwich London, UK},
	date = {2021-07-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.14236/ewic/eva2021.49},
	keywords = {Emotion Recognition; Music Information Retrieval; Melody Extraction; Affective Computing; Audio Event Detection},
	la = {en},
	title = {Exploring the Relationship Between Music and Emotions with Machine Learning},
	url = {https://doi.org/10.14236/ewic/eva2021.49},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.14236/ewic/eva2021.49}}

@article{abayomiolugbara_2021,
	abstract = {Emotion is a complex state of human mind influenced by body physiological changes and interdependent external events thus making an automatic recognition of emotional state a challenging task. A number of recognition methods have been applied in recent years to recognize human emotion. The motivation for this study is therefore to discover a combination of emotion features and recognition method that will produce the best result in building an efficient emotion recognizer in an affective system. We introduced a shifted tanh normalization scheme to realize the inverse Fisher transformation applied to the DEAP physiological dataset and consequently performed series of experiments using the Radial Basis Function Artificial Neural Networks (RBFANN). In our experiments, we have compared the performances of digital image based feature extraction techniques such as the Histogram of Oriented Gradient (HOG), Local Binary Pattern (LBP) and the Histogram of Images (HIM). These feature extraction techniques were utilized to extract discriminatory features from the multimodal DEAP dataset of physiological signals. Experimental results obtained indicate that the best recognition accuracy was achieved with the EEG modality data using the HIM features extraction technique and classification done along the dominance emotion dimension. The result is very remarkable when compared with existing results in the literature including deep learning studies that have utilized the DEAP corpus and also applicable to diverse fields of engineering studies.},
	author = {Abayomi, Abdultaofeek and Olugbara, Oludayo and Heukelman, Delene},
	c1 = {Durban University of Technology; Durban University of Technology, Durban, South Africa.; Durban University of Technology, Durban, South Africa.},
	date = {2021-08-31},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:38 +0100},
	doi = {10.30880/ijie.2021.13.06.001},
	isbn = {2229-838X},
	journal = {International journal of integrated engineering/International Journal of Integrated Engineering},
	keywords = {Emotion Recognition; Affective Computing; Deep Learning for EEG; Feature Extraction; Signal Processing},
	la = {en},
	number = {6},
	publisher = {Penerbit UTHM},
	title = {Recognition of Human Emotion using Radial Basis Function Neural Networks with Inverse Fisher Transformed Physiological Signals},
	url = {https://doi.org/10.30880/ijie.2021.13.06.001},
	volume = {13},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.30880/ijie.2021.13.06.001}}

@article{duanzhong_2023,
	abstract = {Most of the existing emotion elicitation databases use the film clips as stimuli and do not take into account the age and gender differences of participants. Considering the short videos have the advantages of short in time, easy to understand and strong emotional appeal, we choose them to construct a standardized database of Chinese emotional short videos by the joint analysis of age and gender differences. Two experiments are performed to establish and validate our database. In the Experiment 1, we selected 240 stimuli from 2700 short videos and analyzed the subjective evaluation results of 360 participants with different ages and genders. As a result, a total of 54 short videos with three categories of emotions were picked out for 6 groups of participants, including the male and female respectively aged in 20-24, 25-29 and 30-34. In the Experiment 2, we recorded the EEG signals and subjective experience scores of 81 participants while watching different video stimuli. Both the results of EEG emotion recognition and subjective evaluation indicate that our database of 54 short videos can achieve better emotion elicitation effects compared with film clips. Furthermore, the targeted delivery of specific short videos has also been verified to be effective, helping the researchers choose appropriate emotional elicitation stimuli for different participants and promoting the study of individual differences in emotion responses.},
	author = {Duan, Danting and Zhong, Wei and Ran, Shuang and Ye, Long and Zhang, Qin},
	c1 = {Key Laboratory of Media Audio \& Video, Communication University of China, Beijing, China; State Key Laboratory of Media Convergence and Communication, Communication University of China, Beijing, China; Key Laboratory of Media Audio \& Video, Communication University of China, Beijing, China; State Key Laboratory of Media Convergence and Communication, Communication University of China, Beijing, China; State Key Laboratory of Media Convergence and Communication, Communication University of China, Beijing, China},
	date = {2023-03-30},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1371/journal.pone.0283573},
	isbn = {1932-6203},
	journal = {PloS one},
	keywords = {Emotion Recognition; Affective Computing; Emotion Regulation},
	la = {en},
	number = {3},
	pages = {e0283573--e0283573},
	publisher = {Public Library of Science},
	title = {A standardized database of Chinese emotional short videos based on age and gender differences},
	url = {https://doi.org/10.1371/journal.pone.0283573},
	volume = {18},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1371/journal.pone.0283573}}

@article{zawadzkawiercinski_2021,
	abstract = {Data reusability is an important feature of current research, just in every field of science. Modern research in Affective Computing, often rely on datasets containing experiments-originated data such as biosignals, video clips, or images. Moreover, conducting experiments with a vast number of participants to build datasets for Affective Computing research is time-consuming and expensive. Therefore, it is extremely important to provide solutions allowing one to (re)use data from a variety of sources, which usually demands data integration. This paper presents the Graph Representation Integrating Signals for Emotion Recognition and Analysis (GRISERA) framework, which provides a persistent model for storing integrated signals and methods for its creation. To the best of our knowledge, this is the first approach in Affective Computing field that addresses the problem of integrating data from multiple experiments, storing it in a consistent way, and providing query patterns for data retrieval. The proposed framework is based on the standardized graph model, which is known to be highly suitable for signal processing purposes. The validation proved that data from the well-known AMIGOS dataset can be stored in the GRISERA framework and later retrieved for training deep learning models. Furthermore, the second case study proved that it is possible to integrate signals from multiple sources (AMIGOS, ASCERTAIN, and DEAP) into GRISERA and retrieve them for further statistical analysis.},
	author = {Zawadzka, Teresa and Wierci{\'n}ski, Tomasz and Meller, Grzegorz and Rock, Mateusz and Zwierzycki, Robert and Wr{\'o}bel, Micha{\l}},
	c1 = {Faculty of Electronics, Telecommunications and Informatics, Gda{\'n}sk University of Technology, 80-233 Gda{\'n}sk, Poland; Faculty of Electronics, Telecommunications and Informatics, Gda{\'n}sk University of Technology, 80-233 Gda{\'n}sk, Poland; Faculty of Electronics, Telecommunications and Informatics, Gda{\'n}sk University of Technology, 80-233 Gda{\'n}sk, Poland; Faculty of Electronics, Telecommunications and Informatics, Gda{\'n}sk University of Technology, 80-233 Gda{\'n}sk, Poland; Faculty of Electronics, Telecommunications and Informatics, Gda{\'n}sk University of Technology, 80-233 Gda{\'n}sk, Poland; Faculty of Electronics, Telecommunications and Informatics, Gda{\'n}sk University of Technology, 80-233 Gda{\'n}sk, Poland},
	date = {2021-06-11},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3390/s21124035},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Affective Computing; Emotion Recognition},
	la = {en},
	number = {12},
	pages = {4035--4035},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Graph Representation Integrating Signals for Emotion Recognition and Analysis},
	url = {https://doi.org/10.3390/s21124035},
	volume = {21},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.3390/s21124035}}

@article{liujia_2023,
	abstract = {Emotion recognition using multi-modal physiological signals is an emerging field in affective computing that significantly improves performance compared to unimodal approaches. The combination of Electroencephalogram(EEG) and Galvanic Skin Response(GSR) signals are particularly effective for objective and complementary emotion recognition. However, the high cost and inconvenience of EEG signal acquisition severely hinder the popularity of multi-modal emotion recognition in real-world scenarios, while GSR signals are easier to obtain. To address this challenge, we propose EmotionKD, a framework for cross-modal knowledge distillation that simultaneously models the heterogeneity and interactivity of GSR and EEG signals under a unified framework. By using knowledge distillation, fully fused multi-modal features can be transferred to an unimodal GSR model to improve performance. Additionally, an adaptive feedback mechanism is proposed to enable the multi-modal model to dynamically adjust according to the performance of the unimodal model during knowledge distillation, which guides the unimodal model to enhance its performance in emotion recognition. Our experiment results demonstrate that the proposed model achieves state-of-the-art performance on two public datasets. Furthermore, our approach has the potential to reduce reliance on multi-modal data with lower sacrificed performance, making emotion recognition more applicable and feasible. The source code is available at https://github.com/YuchengLiu-Alex/EmotionKD},
	author = {Liu, Yucheng and Jia, Ziyang and Wang, H.},
	c1 = {Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Tsinghua-Berkeley Shenzhen Institute, Shenzhen , China},
	date = {2023-10-26},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1145/3581783.3612277},
	keywords = {Emotion Recognition; Deep Learning for EEG; Affective Computing; Speech Emotion; EEG Analysis},
	la = {en},
	title = {EmotionKD: A Cross-Modal Knowledge Distillation Framework for Emotion Recognition Based on Physiological Signals},
	url = {https://doi.org/10.1145/3581783.3612277},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1145/3581783.3612277}}

@article{smithcross_2022,
	abstract = {The ability to exchange affective cues with others plays a key role in our ability to create and maintain meaningful social relationships. We express our emotions through a variety of socially salient cues, including facial expressions, the voice, and body movement. While significant advances have been made in our understanding of verbal and facial communication, to date, understanding of the role played by human body movement in our social interactions remains incomplete. To this end, here we describe the creation and validation of a new set of emotionally expressive whole-body dance movement stimuli, named the Motion Capture Norming (McNorm) Library, which was designed to reconcile a number of limitations associated with previous movement stimuli. This library comprises a series of point-light representations of a dancer's movements, which were performed to communicate to observers neutrality, happiness, sadness, anger, and fear. Based on results from two validation experiments, participants could reliably discriminate the intended emotion expressed in the clips in this stimulus set, with accuracy rates up to 60{\%} (chance = 20{\%}). We further explored the impact of dance experience and trait empathy on emotion recognition and found that neither significantly impacted emotion discrimination. As all materials for presenting and analysing this movement library are openly available, we hope this resource will aid other researchers in further exploration of affective communication expressed by human bodily movement.},
	author = {Smith, Rebecca and Cross, Emily},
	c1 = {Institute of Neuroscience and Psychology, University of Glasgow, Glasgow, Scotland; Institute of Neuroscience and Psychology, University of Glasgow, Glasgow, Scotland},
	date = {2022-04-06},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1007/s00426-022-01669-9},
	isbn = {0340-0727},
	journal = {Psychological research},
	keywords = {Affective Computing; Emotion Recognition; Emotional Responses; Speech Emotion; Interpersonal Synchrony},
	la = {en},
	number = {2},
	pages = {484--508},
	publisher = {Springer Science+Business Media},
	title = {The McNorm library: creating and validating a new library of emotionally expressive whole body dance movements},
	url = {https://doi.org/10.1007/s00426-022-01669-9},
	volume = {87},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1007/s00426-022-01669-9}}

@article{yangli_2022,
	abstract = {Recent studies have shown that the recognition and monitoring of different valence emotions can effectively avoid the occurrence of human errors due to the decline in cognitive ability. The quality of features directly affects emotion recognition results, so this manuscript explores the effective electroencephalography (EEG) features for the recognition of different valence emotions. First, 110 EEG features were extracted from the time domain, frequency domain, time-frequency domain, spatial domain, and brain network, including all the current mainly used features. Then, the classification performance, computing time, and important electrodes of each feature were systematically compared and analyzed on the self-built dataset involving 40 subjects and the public dataset DEAP. The experimental results show that the first-order difference, second-order difference, high-frequency power, and high-frequency differential entropy features perform better in the recognition of different valence emotions. Also, the time-domain features, especially the first-order difference features and second-order difference features, have less computing time, so they are suitable for real-time emotion recognition applications. Besides, the features extracted from the frontal, temporal, and occipital lobes are more effective than others for the recognition of different valence emotions. Especially, when the number of electrodes is reduced by 3/4, the classification accuracy of using features from 16 electrodes located in these brain regions is 91.8{\%}, which is only about 2{\%} lower than that of using all electrodes. The study results can provide an important reference for feature extraction and selection in emotion recognition based on EEG.},
	author = {Yang, Kai and Li, Tong and Zeng, Ying and Lu, Runnan and Zhang, Rongkai and Gao, Yuanlong and Yan, Bin},
	c1 = {Henan Key Laboratory of Imaging and Intelligent Processing, People's Liberation Army (PLA), Strategy Support Force Information Engineering University, Zhengzhou, China; Henan Key Laboratory of Imaging and Intelligent Processing, People's Liberation Army (PLA), Strategy Support Force Information Engineering University, Zhengzhou, China; Henan Key Laboratory of Imaging and Intelligent Processing, People's Liberation Army (PLA), Strategy Support Force Information Engineering University, Zhengzhou, China; Henan Key Laboratory of Imaging and Intelligent Processing, People's Liberation Army (PLA), Strategy Support Force Information Engineering University, Zhengzhou, China; Henan Key Laboratory of Imaging and Intelligent Processing, People's Liberation Army (PLA), Strategy Support Force Information Engineering University, Zhengzhou, China; Henan Key Laboratory of Imaging and Intelligent Processing, People's Liberation Army (PLA), Strategy Support Force Information Engineering University, Zhengzhou, China; Henan Key Laboratory of Imaging and Intelligent Processing, People's Liberation Army (PLA), Strategy Support Force Information Engineering University, Zhengzhou, China},
	date = {2022-10-17},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3389/fnins.2022.1010951},
	isbn = {1662-453X},
	journal = {Frontiers in neuroscience},
	keywords = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Epilepsy Detection; Affective Computing},
	la = {en},
	publisher = {Frontiers Media},
	title = {Exploration of effective electroencephalography features for the recognition of different valence emotions},
	url = {https://doi.org/10.3389/fnins.2022.1010951},
	volume = {16},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.3389/fnins.2022.1010951}}

@article{bhanumathijayadevappa_2022,
	abstract = {Emotion recognition is very important for the humans in order to enhance the self-awareness and react correctly to the actions around them. Based on the complication and series of emotions, EEG-enabled emotion recognition is still a difficult issue. Hence, an effective human recognition approach is designed using the proposed feedback artificial shuffled shepherd optimization- (FASSO-) based deep maxout network (DMN) for recognizing emotions using EEG signals. The proposed technique incorporates feedback artificial tree (FAT) algorithm and shuffled shepherd optimization algorithm (SSOA). Here, median filter is used for preprocessing to remove the noise present in the EEG signals. The features, like DWT, spectral flatness, logarithmic band power, fluctuation index, spectral decrease, spectral roll-off, and relative energy, are extracted to perform further processing. Based on the data augmented results, emotion recognition can be accomplished using the DMN, where the training process of the DMN is performed using the proposed FASSO method. Furthermore, the experimental results and performance analysis of the proposed algorithm provide efficient performance with respect to accuracy, specificity, and sensitivity with the maximal values of 0.889, 0.89, and 0.886, respectively.},
	author = {Bhanumathi, K. and Jayadevappa, D. and Tunga, Satish},
	c1 = {Department of Electronics and Instrumentation Engineering, JSS Academy of Technical Education, Bengaluru, VTU, India; Department of Electronics and Instrumentation Engineering, JSS Academy of Technical Education, Bengaluru, VTU, India; Department of Electronics \& Telecommunication Engineering, Ramaiah Institute of Technology, Bengaluru, India},
	date = {2022-01-21},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1155/2022/3749413},
	isbn = {1687-6415},
	journal = {International journal of telemedicine and applications},
	keywords = {Emotion Recognition; Deep Learning for EEG; Affective Computing; Speech Emotion; Signal Decomposition},
	la = {en},
	pages = {1--14},
	publisher = {Hindawi Publishing Corporation},
	title = {Feedback Artificial Shuffled Shepherd Optimization-Based Deep Maxout Network for Human Emotion Recognition Using EEG Signals},
	url = {https://doi.org/10.1155/2022/3749413},
	volume = {2022},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1155/2022/3749413}}

@article{kimhwang_2022,
	abstract = {We report a deep learning-based emotion recognition method using EEG data collected while applying cosmetic creams. Four creams with different textures were randomly applied, and they were divided into two classes, ``like (positive)''and ``dislike (negative)'', according to the preference score given by the subject. We extracted frequency features using well-known frequency bands, i.e., alpha, beta and low and high gamma bands, and then we created a matrix including frequency and spatial information of the EEG data. We developed seven CNN-based models: (1) inception-like CNN with four-band merged input, (2) stacked CNN with four-band merged input, (3) stacked CNN with four-band parallel input, and stacked CNN with single-band input of (4) alpha, (5) beta, (6) low gamma, and (7) high gamma. The models were evaluated by the Leave-One-Subject-Out Cross-Validation method. In like/dislike two-class classification, the average accuracies of all subjects were 73.2{\%}, 75.4{\%}, 73.9{\%}, 68.8{\%}, 68.0{\%}, 70.7{\%}, and 69.7{\%}, respectively. We found that the classification performance is higher when using multi-band features than when using single-band feature. This is the first study to apply a CNN-based deep learning method based on EEG data to evaluate preference for cosmetic creams.},
	author = {Kim, Jieun and Hwang, Dong‐Uk and Son, E. and Oh, Sang and Kim, Whansun and Kim, Youngkyung and Kwon, Gusang},
	c1 = {AIRISS AI Team, Yuseong-gu, Deajeon, South Korea; Division of Fundamental Research on Public Agenda, National Institute for Mathematical Sciences, Daejeon, South Korea; Division of Fundamental Research on Public Agenda, National Institute for Mathematical Sciences, Daejeon, South Korea; Division of Fundamental Research on Public Agenda, National Institute for Mathematical Sciences, Daejeon, South Korea; Division of Fundamental Research on Public Agenda, National Institute for Mathematical Sciences, Daejeon, South Korea; Division of Fundamental Research on Public Agenda, National Institute for Mathematical Sciences, Daejeon, South Korea; AMOREPACIFIC R\&D Center, Yongin-si, Gyeonggi-do, South Korea; AMOREPACIFIC R\&D Center, Yongin-si, Gyeonggi-do, South Korea},
	date = {2022-11-10},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1371/journal.pone.0274203},
	isbn = {1932-6203},
	journal = {PloS one},
	keywords = {Emotion Recognition; Affective Computing; Deep Learning; Color Psychology; Emotions},
	la = {en},
	number = {11},
	pages = {e0274203--e0274203},
	publisher = {Public Library of Science},
	title = {Emotion recognition while applying cosmetic cream using deep learning from EEG data; cross-subject analysis},
	url = {https://doi.org/10.1371/journal.pone.0274203},
	volume = {17},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1371/journal.pone.0274203}}

@article{zhangzhang_2023,
	abstract = {Electroencephalography (EEG)-based emotion recognition technologies can effectively help robots to perceive human behavior, which have attracted extensive attention in human--machine interaction (HMI). Due to the complexity of EEG data, current researchers tend to extract different types of hand-crafted features and connect all frequency bands for further study. However, this may result in the loss of some discriminative information of frequency band combinations and make the classification models unable to obtain the best results. In order to recognize emotions accurately, this paper designs a novel EEG-based emotion recognition framework using complementary information of frequency bands. First, after the features of the preprocessed EEG data are extracted, the combinations of all the adjacent frequency bands in different scales are obtained through permutation and reorganization. Subsequently, the improved classification method, homogeneous-collaboration-representation-based classification, is used to obtain the classification results of each combination. Finally, the circular multi-grained ensemble learning method is put forward to re-exact the characteristics of each result and merge the machine learning methods and simple majority voting for the decision fusion. In the experiment, the classification accuracies of our framework in arousal and valence on the DEAP database are 95.09{\%} and 94.38{\%} respectively, and that in the four classification problems on the SEED IV database is 96.37{\%}.},
	author = {Zhang, Zhipeng and Zhang, Liyi},
	c1 = {School of Information and Management, Wuhan University, No. 16, Luojiashan Road, Wuchang District, Wuhan 430072, China; School of Information and Management, Wuhan University, No. 16, Luojiashan Road, Wuchang District, Wuhan 430072, China},
	date = {2023-02-02},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3390/app13031954},
	isbn = {2076-3417},
	journal = {Applied sciences},
	keywords = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Head Gesture Recognition},
	la = {en},
	number = {3},
	pages = {1954--1954},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {A Two-Step Framework to Recognize Emotion Using the Combinations of Adjacent Frequency Bands of EEG},
	url = {https://doi.org/10.3390/app13031954},
	volume = {13},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.3390/app13031954}}

@article{da-costaabreubezerra_2017,
	abstract = {Emotion-based analysis has raised a lot of interest, particularly in areas such as forensics, medicine, music, psychology, and human-machine interface. Following this trend, the use of facial analysis (either automatic or human-based) is the most common subject to be investigated once this type of data can easily be collected and is well accepted in the literature as a metric for inference of emotional states. Despite this popularity, due to several constraints found in real-world scenarios (e.g. lightning, complex backgrounds, facial hair and so on), automatically obtaining affective information from face accurately is a very challenging accomplishment. This work presents a framework which aims to analyse emotional experiences through spontaneous facial expressions. The method consists of a new four-dimensional model, called FAMOS, to describe emotional experiences in terms of appraisal, facial expressions, mood, and subjective experiences using a semi-automatic facial expression analyser as ground truth for describing the facial actions. In addition, we present an experiment using a new protocol proposed to obtain spontaneous emotional reactions. The results have suggested that the initial emotional state described by the participants of the experiment was different from that described after the exposure to the eliciting stimulus, thus showing that the used stimuli were capable of inducing the expected emotional states in most individuals. Moreover, our results pointed out that spontaneous facial reactions to emotions are very different from those in prototypic expressions, especially in terms of expressiveness.},
	author = {Da Costa‐Abreu, M{\'a}rjory and Bezerra, Giuliana},
	c1 = {DIMAp/UFRN, Natal, RN, 59078-970, Brazil; DIMAp/UFRN, Natal, RN, 59078-970, Brazil},
	date = {2017-12-28},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1007/s10044-017-0675-y},
	isbn = {1433-7541},
	journal = {Pattern analysis and applications/Pattern analysis \& applications},
	keywords = {Emotion Recognition; Affective Computing; Face Perception; Facial Expression; Emotional Expressions},
	la = {en},
	number = {2},
	pages = {683--701},
	publisher = {Springer Science+Business Media},
	title = {FAMOS: a framework for investigating the use of face features to identify spontaneous emotions},
	url = {https://doi.org/10.1007/s10044-017-0675-y},
	volume = {22},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1007/s10044-017-0675-y}}

@article{baradaranfarzan_2023,
	abstract = {Emotions are an inextricably linked component of human life. Automatic emotion recognition can be widely used in brain--computer interfaces. This study presents a new model for automatic emotion recognition from electroencephalography signals based on a combination of deep learning and fuzzy networks, which can recognize two different emotions: positive, and negative. To accomplish this, a standard database based on musical stimulation using EEG signals was compiled. Then, to deal with the phenomenon of overfitting, generative adversarial networks were used to augment the data. The generative adversarial network output is fed into the proposed model, which is based on improved deep convolutional networks with type-2 fuzzy activation functions. Finally, in two separate class, two positive and two negative emotions were classified. In the classification of the two classes, the proposed model achieved an accuracy of more than 98{\%}. In addition, when compared to previous studies, the proposed model performed well and can be used in future brain--computer interface applications.},
	author = {Baradaran, Farzad and Farzan, Ali and Daneshvar, Sabalan and Sheykhivand, Sobhan},
	c1 = {Department of Computer Engineering, Shabestar Branch, Islamic Azad University, Shabestar 5381637181, Iran; Department of Computer Engineering, Shabestar Branch, Islamic Azad University, Shabestar 5381637181, Iran; College of Engineering, Design and Physical Sciences, Brunel University London, Uxbridge UB8 3PH, UK; Department of Biomedical Engineering, University of Bonab, Bonab 5551395133, Iran},
	date = {2023-05-12},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/electronics12102216},
	isbn = {2079-9292},
	journal = {Electronics},
	keywords = {Emotion Recognition; Affective Computing; Deep Learning for EEG; EEG Analysis; Speech Emotion},
	la = {en},
	number = {10},
	pages = {2216--2216},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Automatic Emotion Recognition from EEG Signals Using a Combination of Type-2 Fuzzy and Deep Convolutional Networks},
	url = {https://doi.org/10.3390/electronics12102216},
	volume = {12},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.3390/electronics12102216}}

@article{bawasharma_2021,
	abstract = {Every individual's perception of multimedia content varies based on their interpretation.Therefore, it is quite challenging to predict likability of any multimedia just based on its content.This paper presents a novel system for analysis of facial expressions of subject against the multimedia content to be evaluated.First, we developed a dataset by recording facial expressions of subjects under uncontrolled environment.These subjects are volunteers recruited to watch the videos of different genre, and provide their feedback in terms of likability.Subject responses are divided into three categories: Like, Neutral and Dislike.A novel multimodal system is developed using the developed dataset.The model learns feature representation from data based on the three provided categories.The proposed system contains ensemble of time distributed convolutional neural network, 3D convolutional neural network, and long short term memory networks.All the modalities in proposed architecture are evaluated independently as well as in distinct combinations.The paper also provides detailed insight into learning behavior of the proposed system.},
	author = {Bawa, Vivek and Sharma, Shailza and Usman, Mohammed and Gupta, Abhimat and Kumar, Vinay},
	c1 = {Visual Artificial Intelligence Lab, Oxford Brookes University, Oxford, United Kingdom.; Department of Electronics \& Communication Engineering, Thapar Institute of Engineering \& Technology, Patiala, India.; Department of Electrical Engineering, King Khalid University, Abha, 61411, Saudi Arabia.; Department of Computer Science, Thapar Institute of Engineering \& Technology, Patiala, India.; Department of Electronics \& Communication Engineering, Thapar Institute of Engineering \& Technology, Patiala, India.},
	date = {2021-01-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/access.2021.3102042},
	isbn = {2169-3536},
	journal = {IEEE access},
	keywords = {Emotion Recognition; Visual Attention; Affective Computing; Video Object Segmentation; Facial Expression},
	la = {en},
	pages = {110421--110434},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {An Automatic Multimedia Likability Prediction System Based on Facial Expression of Observer},
	url = {https://doi.org/10.1109/access.2021.3102042},
	volume = {9},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/access.2021.3102042}}

@article{goshvarpourgoshvarpour_2023,
	abstract = {Electroencephalogram (EEG) connectivity patterns can reflect neural correlates of emotion. However, the necessity of evaluating bulky data for multi-channel measurements increases the computational cost of the EEG network. To date, several approaches have been presented to pick the optimal cerebral channels, mainly depending on available data. Consequently, the risk of low data stability and reliability has increased by reducing the number of channels. Alternatively, this study suggests an electrode combination approach in which the brain is divided into six areas. After extracting EEG frequency bands, an innovative Granger causality-based measure was introduced to quantify brain connectivity patterns. The feature was subsequently subjected to a classification module to recognize valence-arousal dimensional emotions. A Database for Emotion Analysis Using Physiological Signals (DEAP) was used as a benchmark database to evaluate the scheme. The experimental results revealed a maximum accuracy of 89.55{\%}. Additionally, EEG-based connectivity in the beta-frequency band was able to effectively classify dimensional emotions. In sum, combined EEG electrodes can efficiently replicate 32-channel EEG information.},
	author = {Goshvarpour, Atefeh and Goshvarpour, Ateke},
	c1 = {Department of Biomedical Engineering, Faculty of Electrical Engineering, Sahand University of Technology, Tabriz 51335-1996, Iran;; Department of Biomedical Engineering, Imam Reza International University, Mashhad 91388-3186, Iran},
	date = {2023-05-04},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/brainsci13050759},
	isbn = {2076-3425},
	journal = {Brain sciences},
	keywords = {Emotion Recognition; EEG Analysis; Deep Learning for EEG; Affective Computing; Epilepsy Detection},
	la = {en},
	number = {5},
	pages = {759--759},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Emotion Recognition Using a Novel Granger Causality Quantifier and Combined Electrodes of EEG},
	url = {https://doi.org/10.3390/brainsci13050759},
	volume = {13},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.3390/brainsci13050759}}

@article{ayesharevalilloherraez_2017,
	abstract = {Many approaches to recognising emotions from metrical data such as EEG signals rely on identifying a very small number of classes and to train a classifier. The interpretation of these classes varies from a single emotion such as stress {$[$}24{$]$} to features of emotional model such as valence-arousal {$[$}4{$]$}. There are two major issues here. First classification approach limits the analysis of the data within the selected classes and is also highly dependent on training data/cycles, all of which limits generalisation. Second issue is that it does not explore the inter-relationships between the data collected missing out on any correlations that could tell us interesting facts beyond emotional recognition. This second issue would be of particular interest to psychologists and medical professions. In this paper, we investigate the use of Self-Organizing Maps (SOM) in identifying clusters from EEG signals that could then be translated into classes. We start by training varying sizes of SOM with the EEG data provided in a public dataset (DEAP). The produced graphs showing Neighbour Distance, Sample Hits, Weight Position are analysed holistically to identify patterns in the structure. Following that, we have considered the ground-truth label provided in DEAP, in order to identify correlations between the label and the clustering produced by the SOM. The results show the potential of SOM for class discovery in this particular context. We conclude with a discussion on the implications of this work and the difficulties in evaluating the outcome.},
	author = {Ayesh, Aladdin and Arevalillo‐Herr{\'a}ez, Miguel and Arnau-Gonz{\'a}lez, Pablo},
	c1 = {Faculty of Technology, De Montfort University, UK; Departament dInform{\`a}tica, Universitat de Val{\'e}ncia; University of the West of Scotland Scotland, United Kingdom},
	date = {2017-07-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/icci-cc.2017.8109736},
	keywords = {Emotion Recognition; Affective Computing; EEG Analysis; Deep Learning for EEG; Speech Emotion},
	la = {en},
	title = {Class discovery from semi-structured EEG data for affective computing and personalisation},
	url = {https://doi.org/10.1109/icci-cc.2017.8109736},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/icci-cc.2017.8109736}}

@article{zongxiong_2023,
	abstract = {In recent years, artificial intelligence (AI) technology has promoted the development of electroencephalogram (EEG) emotion recognition. However, existing methods often overlook the computational cost of EEG emotion recognition, and there is still room for improvement in the accuracy of EEG emotion recognition. In this study, we propose a novel EEG emotion recognition algorithm called FCAN--XGBoost, which is a fusion of two algorithms, FCAN and XGBoost. The FCAN module is a feature attention network (FANet) that we have proposed for the first time, which processes the differential entropy (DE) and power spectral density (PSD) features extracted from the four frequency bands of the EEG signal and performs feature fusion and deep feature extraction. Finally, the deep features are fed into the eXtreme Gradient Boosting (XGBoost) algorithm to classify the four emotions. We evaluated the proposed method on the DEAP and DREAMER datasets and achieved a four-category emotion recognition accuracy of 95.26{\%} and 94.05{\%}, respectively. Additionally, our proposed method reduces the computational cost of EEG emotion recognition by at least 75.45{\%} for computation time and 67.51{\%} for memory occupation. The performance of FCAN--XGBoost outperforms the state-of-the-art four-category model and reduces computational costs without losing classification performance compared with other models.},
	author = {Zong, Jing and Xiong, Xin and Zhou, Jingtao and Ji, Ying and Zhou, Deyi and Zhang, Qi},
	c1 = {Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming 650500, China;; Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming 650500, China;; Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming 650500, China;; Graduate School, Kunming Medical University, Kunming 650500, China;; Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming 650500, China;; Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming 650500, China;},
	date = {2023-06-17},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3390/s23125680},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Speech Emotion},
	la = {en},
	number = {12},
	pages = {5680--5680},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {FCAN--XGBoost: A Novel Hybrid Model for EEG Emotion Recognition},
	url = {https://doi.org/10.3390/s23125680},
	volume = {23},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.3390/s23125680}}

@article{lammersbente_2019,
	abstract = {Others'movements inform us about their current activities as well as their intentions and emotions. Research on the distinct mechanisms underlying action recognition and emotion inferences has been limited due to a lack of suitable comparative stimulus material. Problematic confounds can derive from low-level physical features, (e.g. luminance), as well as from higher-level psychological features (e.g. stimulus difficulty). Here we present a standardized stimulus dataset, which allows to address both action and emotion recognition with identical stimuli. The stimulus set consists of 792 computer animations with a neutral avatar based on full body motion capture protocols. Motion capture was performed on 22 human volunteers, instructed to perform six everyday activities (mopping, sweeping, painting with a roller, painting with a brush, wiping, sanding) in three different moods (angry, happy, sad). Five-second clips of each motion protocol were rendered into AVI-files using two virtual camera perspectives for each clip. In contrast to video stimuli, the computer animations allowed to standardize the physical appearance of the avatarand to control lighting and coloring conditions, thus reducing the stimulus variation to mere movement. To control for low level optical features of the stimuli, we developed and applied a set of MATLAB routines extracting basic physical features of the stimuli, including average background-foreground proportion and frame-by-frame pixel change dynamics. This information was used to identify outliers and to homogenize the stimuli across action and emotion categories. This led to a smaller stimulus subset (n = 83 animations within the 792 clip database) which only contained two different actions (mopping, sweeping) and two different moods (angry, happy). To further homogenize this stimulus subset with regard to psychological criteria we conducted an online observer study (N = 112 participants) to assess the recognition rates for actions and moods, which led to a final sub-selection of 32 clips (8 per category) within the database. The ACASS database and its subsets provide unique opportunities for research applications in social psychology, social neuroscience and applied clinical studies on communication disorders. All 792 AVI-files, selected subsets, MATLAB code, annotations and motion capture data (FBX-files) are available online.},
	author = {Lammers, Sebastian and Bente, Gary and Tepest, Ralf and Jording, Mathis and Roth, Daniel and Vogeley, Kai},
	c1 = {Cognitive Neuroscience (INM-3), Institute of Neuroscience and Medicine, Research Center J{\"u}lich, J{\"u}lich, Germany; Department of Psychiatry, Faculty of Medicine and University Hospital Cologne, University of Cologne, Cologne, Germany; Department of Communication, Michigan State University, East Lansing, MI, United States; Department of Psychiatry, Faculty of Medicine and University Hospital Cologne, University of Cologne, Cologne, Germany; Cognitive Neuroscience (INM-3), Institute of Neuroscience and Medicine, Research Center J{\"u}lich, J{\"u}lich, Germany; Human-Computer Interaction, Institute for Computer Science, University of W{\"u}rzburg, W{\"u}rzburg, Germany; Cognitive Neuroscience (INM-3), Institute of Neuroscience and Medicine, Research Center J{\"u}lich, J{\"u}lich, Germany; Department of Psychiatry, Faculty of Medicine and University Hospital Cologne, University of Cologne, Cologne, Germany},
	date = {2019-09-27},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3389/frobt.2019.00094},
	isbn = {2296-9144},
	journal = {Frontiers in robotics and AI},
	keywords = {Action Observation; Emotion Recognition; Affective Computing; Human-Computer Interaction},
	la = {en},
	publisher = {Frontiers Media},
	title = {Introducing ACASS: An Annotated Character Animation Stimulus Set for Controlled (e)Motion Perception Studies},
	url = {https://doi.org/10.3389/frobt.2019.00094},
	volume = {6},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.3389/frobt.2019.00094}}

@article{liangyin_2022,
	abstract = {Electroencephalography (EEG) based emotion recognition enables machines to perceive users' affective states, which has attracted increasing attention. However, most of the current emotion recognition methods neglect the structural information among different brain regions, which can lead to the incorrect learning of high-level EEG feature representation. To mitigate possible performance degradation, we propose a novel nuclear norm regularized deep neural network framework (NRDNN) that can capture the structural information among different brain regions in EEG decoding. The proposed NRDNN first utilizes deep neural networks to learn high-level feature representations of multiple brain regions, respectively. Then, a set of weights indicating the contributions of each brain region can be automatically learned using a region-attention layer. Subsequently, the weighted feature representations of multiple brain regions are stacked into a feature matrix, and the nuclear norm regularization is adopted to learn the structural information within the feature matrix. The proposed NRDNN method can learn the high-level representations of EEG signals within multiple brain regions, and the contributions of them can be automatically adjusted by assigning a set of weights. Besides, the structural information among multiple brain regions can be captured in the learning procedure. Finally, the proposed NRDNN can perform in an efficient end-to-end manner. We conducted extensive experiments on publicly available emotion EEG dataset to evaluate the effectiveness of the proposed NRDNN. Experimental results demonstrated that the proposed NRDNN can achieve state-of-the-art performance by leveraging the structural information.},
	author = {Liang, Shuang and Yin, Mingbo and Huang, Yiyun and Dai, Xiubin and Wang, Qiong},
	c1 = {Smart Health Big Data Analysis and Location Services Engineering Lab of Jiangsu Province, Nanjing University of Posts and Telecommunications, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; School of Computer Science and Technology, Nanjing Tech University, Nanjing, China; School of Computer Science and Technology, Nanjing Tech University, Nanjing, China; Smart Health Big Data Analysis and Location Services Engineering Lab of Jiangsu Province, Nanjing University of Posts and Telecommunications, Nanjing, China; Guangdong Provincial Key Laboratory of Computer Vision and Virtual Reality Technology, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China},
	date = {2022-06-29},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3389/fpsyg.2022.924793},
	isbn = {1664-1078},
	journal = {Frontiers in psychology},
	keywords = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Neural Ensemble Physiology},
	la = {en},
	publisher = {Frontiers Media},
	title = {Nuclear Norm Regularized Deep Neural Network for EEG-Based Emotion Recognition},
	url = {https://doi.org/10.3389/fpsyg.2022.924793},
	volume = {13},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.3389/fpsyg.2022.924793}}

@article{melogratch_2023,
	abstract = {Virtual humans and social robots frequently generate behaviors that human observers naturally see as expressing emotion. In this review article, we highlight that these expressions can have important benefits for human--machine interaction. We first summarize the psychological findings on how emotional expressions achieve important social functions in human relationships and highlight that artificial emotional expressions can serve analogous functions in human--machine interaction. We then review computational methods for determining what expressions make sense to generate within the context of interaction and how to realize those expressions across multiple modalities, such as facial expressions, voice, language, and touch. The use of synthetic expressions raises a number of ethical concerns, and we conclude with a discussion of principles to achieve the benefits of machine emotion in ethical ways.},
	author = {de Melo, Celso and Gratch, Jonathan and Marsella, Stacy and P{\'e}lachaud, Catherine},
	c1 = {DEVCOM U.S. Army Research Laboratory (ARL), Adelphi, MD, USA; USC Institute for Creative Technologies, Los Angeles, CA, USA; Department of Computer Science and the Department of Psychology, Northeastern University, Boston, MA, USA; Centre National de Recherche Scientifique-Institut de Syst\&{\#}x00E8;mes Intelligents et de Robotique (CNRS-ISIR), Sorbonne University, Paris, France},
	date = {2023-10-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/jproc.2023.3261137},
	isbn = {0018-9219},
	journal = {Proceedings of the IEEE},
	keywords = {Emotional Expressions; Emotion Recognition; Affective Computing; Human Perception of Robots; Emotion Perception},
	la = {en},
	number = {10},
	pages = {1382--1397},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {Social Functions of Machine Emotional Expressions},
	url = {https://doi.org/10.1109/jproc.2023.3261137},
	volume = {111},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/jproc.2023.3261137}}

@article{serrano-mamolararevalilloherraez_2021,
	abstract = {Previous research has proven the strong influence of emotions on student engagement and motivation. Therefore, emotion recognition is becoming very relevant in educational scenarios, but there is no standard method for predicting students'affects. However, physiological signals have been widely used in educational contexts. Some physiological signals have shown a high accuracy in detecting emotions because they reflect spontaneous affect-related information, which is fresh and does not require additional control or interpretation. Most proposed works use measuring equipment for which applicability in real-world scenarios is limited because of its high cost and intrusiveness. To tackle this problem, in this work, we analyse the feasibility of developing low-cost and nonintrusive devices to obtain a high detection accuracy from easy-to-capture signals. By using both inter-subject and intra-subject models, we present an experimental study that aims to explore the potential application of Hidden Markov Models (HMM) to predict the concentration state from 4 commonly used physiological signals, namely heart rate, breath rate, skin conductance and skin temperature. We also study the effect of combining these four signals and analyse their potential use in an educational context in terms of intrusiveness, cost and accuracy. The results show that a high accuracy can be achieved with three of the signals when using HMM-based intra-subject models. However, inter-subject models, which are meant to obtain subject-independent approaches for affect detection, fail at the same task.},
	author = {Serrano-Mamolar, Ana and Arevalillo‐Herr{\'a}ez, Miguel and Chicote-Huete, Guillermo and Boticario, Jes{\'u}s},
	c1 = {aDeNu Research Group, Artificial Intelligence Department, Universidad Nacional de Educaci{\'o}n a Distancia, 28040 Madrid, Spain; Departament d'Inform{\`a}tica, Universitat de Val{\`e}ncia, 46100 Burjassot, Valencia, Spain; Departament d'Inform{\`a}tica, Universitat de Val{\`e}ncia, 46100 Burjassot, Valencia, Spain; aDeNu Research Group, Artificial Intelligence Department, Universidad Nacional de Educaci{\'o}n a Distancia, 28040 Madrid, Spain},
	date = {2021-03-04},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3390/s21051777},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Emotion Recognition; Emotion Regulation; Affective Computing; Physiological Signals; Speech Emotion},
	la = {en},
	number = {5},
	pages = {1777--1777},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {An Intra-Subject Approach Based on the Application of HMM to Predict Concentration in Educational Contexts from Nonintrusive Physiological Signals in Real-World Situations},
	url = {https://doi.org/10.3390/s21051777},
	volume = {21},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.3390/s21051777}}

@article{mahfoudimeyer_2023,
	abstract = {Many areas in computer science are facing the need to analyze, quantify and reproduce movements expressing emotions.This paper presents a systematic review of the intelligible factors involved in the expression of emotions in human movement and posture.We have gathered the works that have studied and tried to identify these factors by sweeping many disciplinary fields such as psychology, biomechanics, choreography, robotics and computer vision.These researches have each used their own definitions, units and emotions, which prevents a global and coherent vision.We propose a meta-analysis approach that cross-references and aggregates these researches in order to have a unified list of expressive factors quantified for each emotion.A calculation method is then proposed for each of the expressive factors and we extract them from an emotionally annotated animation dataset: Emilya.The comparison between the results of the meta-analysis and the Emilya analysis reveals high correlation rates, which validates the relevance of the quantified values obtained by both methodologies.The analysis of the results raises interesting perspectives for future research in affective computing.},
	author = {Mahfoudi, Mehdi-Antoine and Meyer, Alexandre and Gaudin, Thibaut and Buendia, Axel and Bouakaz, Sa{\"\i}da},
	c1 = {Laboratoire d'InfoRmatique en Image et Syst{\`e}mes d'information; SPIR.OPS; Simulation, Analyse et Animation pour la R{\'e}alit{\'e}Augment{\'e}e; Universit{\'e}Claude Bernard Lyon 1; Laboratoire d'InfoRmatique en Image et Syst{\`e}mes d'information; Simulation, Analyse et Animation pour la R{\'e}alit{\'e}Augment{\'e}e; Universit{\'e}Claude Bernard Lyon 1; SPIR.OPS; CEDRIC - Interactivit{\'e}pour Lire et Jouer; SPIR.OPS; Laboratoire d'InfoRmatique en Image et Syst{\`e}mes d'information; Simulation, Analyse et Animation pour la R{\'e}alit{\'e}Augment{\'e}e; Universit{\'e}Claude Bernard Lyon 1},
	date = {2023-10-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/taffc.2022.3226252},
	isbn = {1949-3045},
	journal = {IEEE transactions on affective computing},
	keywords = {Emotion Recognition; Emotions; Affective Computing; Pose Estimation; Facial Expression},
	la = {en},
	number = {4},
	pages = {2697--2721},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {Emotion Expression in Human Body Posture and Movement: A Survey on Intelligible Motion Factors, Quantification and Validation},
	url = {https://doi.org/10.1109/taffc.2022.3226252},
	volume = {14},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/taffc.2022.3226252}}

@article{dahersaad_2022,
	abstract = {Empathy plays a crucial role in human life, and the evolution of technology is affecting the way humans interact with machines. The area of affective computing is attracting considerable interest within the human--computer interaction community. However, the area of empathic interactions has not been explored in depth. This systematic review explores the latest advances in empathic interactions and behaviour. We provide key insights into the exploration, design, implementation, and evaluation of empathic interactions. Data were collected from the CHI conference between 2011 and 2021 to provide an overview of all studies covering empathic and empathetic interactions. Two authors screened and extracted data from a total of 59 articles relevant to this review. The features extracted cover interaction modalities, context understanding, usage fields, goals, and evaluation. The results reported here can be used as a foundation for the future research and development of empathic systems and interfaces and as a starting point for the gaps found.},
	author = {Daher, Karl and Saad, Dahlia and Mugellini, Elena and Lalanne, Denis and Khaled, Omar},
	c1 = {HumanTech Institute, HEIA-FR, University of Applied Sciences Western Switzerland HES-SO, 2800 Del{\'e}mont, Switzerland; HumanTech Institute, HEIA-FR, University of Applied Sciences Western Switzerland HES-SO, 2800 Del{\'e}mont, Switzerland; HumanTech Institute, HEIA-FR, University of Applied Sciences Western Switzerland HES-SO, 2800 Del{\'e}mont, Switzerland; Human-IST, University of Fribourg, 1700 Fribourg, Switzerland; HumanTech Institute, HEIA-FR, University of Applied Sciences Western Switzerland HES-SO, 2800 Del{\'e}mont, Switzerland},
	date = {2022-04-15},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/s22083046},
	isbn = {1424-8220},
	journal = {Sensors},
	keywords = {Affective Computing; Human-Computer Interaction; Emotional Design; Emotion Recognition; Interaction Design},
	la = {en},
	number = {8},
	pages = {3046--3046},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Empathic and Empathetic Systematic Review to Standardize the Development of Reliable and Sustainable Empathic Systems},
	url = {https://doi.org/10.3390/s22083046},
	volume = {22},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.3390/s22083046}}

@article{zhangmin_2021,
	abstract = {Inspired by the neuroscience research results that the human brain can produce dynamic responses to different emotions, a new electroencephalogram (EEG)-based human emotion classification model was proposed, named R2G-ST-BiLSTM, which uses a hierarchical neural network model to learn more discriminative spatiotemporal EEG features from local to global brain regions. First, the bidirectional long- and short-term memory (BiLSTM) network is used to obtain the internal spatial relationship of EEG signals on different channels within and between regions of the brain. Considering the different effects of various cerebral regions on emotions, the regional attention mechanism is introduced in the R2G-ST-BiLSTM model to determine the weight of different brain regions, which could enhance or weaken the contribution of each brain area to emotion recognition. Then a hierarchical BiLSTM network is again used to learn the spatiotemporal EEG features from regional to global brain areas, which are then input into an emotion classifier. Especially, we introduce a domain discriminator to work together with the classifier to reduce the domain offset between the training and testing data. Finally, we make experiments on the EEG data of the DEAP and SEED datasets to test and compare the performance of the models. It is proven that our method achieves higher accuracy than those of the state-of-the-art methods. Our method provides a good way to develop affective brain-computer interface applications.},
	author = {Zhang, Pengwei and Min, Chongdan and Zhang, Kangjia and Xue, Wen and Chen, Jingxia},
	c1 = {School of Electronic Information and Artificial Intelligence, Shaanxi University of Science and Technology, Xi'an, China; School of Electronic Information and Artificial Intelligence, Shaanxi University of Science and Technology, Xi'an, China; School of Electronic Information and Artificial Intelligence, Shaanxi University of Science and Technology, Xi'an, China; School of Electronic Information and Artificial Intelligence, Shaanxi University of Science and Technology, Xi'an, China; School of Electronic Information and Artificial Intelligence, Shaanxi University of Science and Technology, Xi'an, China},
	date = {2021-12-02},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.3389/fnins.2021.738167},
	isbn = {1662-453X},
	journal = {Frontiers in neuroscience},
	keywords = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Epilepsy Detection},
	la = {en},
	publisher = {Frontiers Media},
	title = {Hierarchical Spatiotemporal Electroencephalogram Feature Learning and Emotion Recognition With Attention-Based Antagonism Neural Network},
	url = {https://doi.org/10.3389/fnins.2021.738167},
	volume = {15},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.3389/fnins.2021.738167}}

@article{gonzalezfernandez_2021,
	abstract = {In this research, neuroscience techniques are applied to the field of marketing in the analysis of advertisements that include the COVID-19 pandemic in their stories. A study of emotion and memory in these audiovisual productions is carried out as two fundamental factors for the knowledge of consumer habits and decision making. By means of facial recognition biosensor systems (AFFDEX) and various tests, six informative and narrative, emotional and rational advertisements are presented to the subjects of the experiment to detect which emotions predominate; how they affect variables such as neuroticism, psychoticism or extroversion, among others; or what is remembered about the different works, brands and advertisers. Outstanding results are obtained in both emotional and cognitive analysis. Thus, in the field of public health, it is found that messages referring to COVID-19 included in advertisements are remembered more than other narratives or even the brands, products or services themselves. Likewise, joy is the predominant emotion, and its significance in such varied advertising stories stands out. Finally, it is clear that neuroscience research applied to marketing requires new methods and integrated applications to obtain satisfactory results in the advertising field.},
	author = {Gonz{\'a}lez, Miguel and Fern{\'a}ndez, Mario and Mart{\'\i}n, Dolores},
	c1 = {Faculty of Communication Sciences, Rey Juan Carlos University, 28942 Fuenlabrada, Spain;; Faculty of Communication Sciences, Rey Juan Carlos University, 28942 Fuenlabrada, Spain;; Faculty of Law and Social Sciences, Rey Juan Carlos University, 28032 Vic{\'a}lvaro, Spain},
	date = {2021-08-18},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.3390/ijerph18168721},
	isbn = {1660-4601},
	journal = {International journal of environmental research and public health/International journal of environmental research and public health},
	keywords = {Emotion Recognition; Affective Computing; Speech Emotion},
	la = {en},
	number = {16},
	pages = {8721--8721},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {Analysis of Emotion and Recall in COVID-19 Advertisements: A Neuroscientific Study},
	url = {https://doi.org/10.3390/ijerph18168721},
	volume = {18},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.3390/ijerph18168721}}

@article{lykartsiskotti_2019,
	abstract = {In this paper we aim to predict dialogue success and user satisfaction as well as emotion on a turn level.To achieve this, we investigate the use of spectrogram representations, extracted from audio files, in combination with several types of convolutional neural networks.The experiments were performed on the Let's Go V2 database, comprising 5065 audio files and having labels for subjective and objective dialogue turn success, as well as the emotional state of the user.Results show that by using only audio, it is possible to predict turn success with very high accuracy for all three labels (90{\%}).The best performing input representation were 1s long mel-spectrograms in combination with a CNN with a bottleneck architecture.The resulting system has the potential to be used real-time.Our results significantly surpass the state of the art for dialogue success prediction based only on audio.},
	author = {Lykartsis, Athanasios and Kotti, Margarita},
	c1 = {Audio Communication Group TU Berlin Germany; Speech Technology Group Toshiba Research Cambridge United Kingdom},
	date = {2019-01-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.18653/v1/w19-5939},
	keywords = {Affective Computing; Emotion Recognition; Spoken Dialogue Systems; Speech Emotion; User Simulation},
	la = {en},
	title = {Prediction of User Emotion and Dialogue Success Using Audio Spectrograms and Convolutional Neural Networks},
	url = {https://doi.org/10.18653/v1/w19-5939},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.18653/v1/w19-5939}}

@article{dudeng_2023,
	abstract = {Emotional information plays an important role in various multimedia applications. Movies, as a widely available form of multimedia content, can induce multiple positive emotions and stimulate people's pursuit of a better life. Different from negative emotions, positive emotions are highly correlated and difficult to distinguish in the emotional space. Since different positive emotions are often induced simultaneously by movies, traditional single-target or multi-class methods are not suitable for the classification of movie-induced positive emotions. In this paper, we propose <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">TransEEG</i> , a model for multi-label positive emotion classification from a viewer's brain activities when watching emotional movies. The key features of <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">TransEEG</i> include (1) explicitly modeling the spatial correlation and temporal dependencies of multi-channel EEG signals using the Transformer structure based model, which effectively addresses long-distance dependencies, (2) exploiting the label-label correlations to guide the discriminative EEG representation learning, for that we design an Inter-Emotion Mask for guiding the Multi-Head Attention to learn the inter-emotion correlations, and (3) constructing an attention score vector from the representation-label correlation matrix to refine emotion-relevant EEG features. To evaluate the ability of our model for multi-label positive emotion classification, we demonstrate our model on a state-of-the-art positive emotion database CPED. Extensive experimental results show that our proposed method achieves superior performance over the competitive approaches.},
	author = {Du, Xiaobing and Deng, Xiaoming and Qin, Haoran and Shu, Yezhi and Liu, Fang and Zhao, Guozhen and Lai, Yu‐Kun and Ma, Chenyue and Liu, Yong-Jin and Wang, Hongan},
	c1 = {Beijing Key Laboratory of Human Computer Interactions, Institute of Software, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Beijing Key Laboratory of Human Computer Interactions, Institute of Software, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Beijing Key Laboratory of Human Computer Interactions, Institute of Software, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Department of Computer Science and Technology, BNRist, MOE-Key Laboratory of Pervasive Computing, Tsinghua University, Beijing, China; Department of Computer Science and Technology, BNRist, MOE-Key Laboratory of Pervasive Computing, Tsinghua University, Beijing, China; CAS Key Laboratory of Behavioral Science, Institute of Psychology, Beijing, China; Department of Psychology, University of Chinese Academy of Sciences, Beijing, China; School of Computer Science and Informatics, Cardiff University, Cardiff, Wales, U.K.; Beijing Key Laboratory of Human Computer Interactions, International Joint Laboratory of Artificial Intelligence and Emotional Interaction, Beijing, China; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Department of Computer Science and Technology, BNRist, MOE-Key Laboratory of Pervasive Computing, Tsinghua University, Beijing, China; Beijing Key Laboratory of Human Computer Interactions, International Joint Laboratory of Artificial Intelligence and Emotional Interaction, Beijing, China; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China},
	date = {2023-10-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1109/taffc.2022.3221554},
	isbn = {1949-3045},
	journal = {IEEE transactions on affective computing},
	keywords = {Emotion Recognition; Speech Emotion; Affective Computing; EEG Analysis; Deep Learning for EEG},
	la = {en},
	number = {4},
	pages = {2925--2938},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {MMPosE: Movie-Induced Multi-Label Positive Emotion Classification Through EEG Signals},
	url = {https://doi.org/10.1109/taffc.2022.3221554},
	volume = {14},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/taffc.2022.3221554}}

@article{valderramasarmiento_2023,
	author = {Valderrama, El{\'\i}as and Sarmiento, Auxiliadora and Dur{\'a}n-D{\'\i}az, Iv{\'a}n and Becerra, Juan and Garc{\'\i}a, Irene},
	c1 = {Departamento de Teor{\'\i}a de la Se{\~n}al y Comunicaciones, Escuela Superior de Ingenier{\'\i}a, Universidad de Sevilla, 41092 Seville, Spain, --- Select a Country ---; Departamento de Teor{\'\i}a de la Se{\~n}al y Comunicaciones, Escuela Superior de Ingenier{\'\i}a, Universidad de Sevilla, 41092 Seville, Spain, --- Select a Country ---; Departamento de Teor{\'\i}a de la Se{\~n}al y Comunicaciones, Escuela Superior de Ingenier{\'\i}a, Universidad de Sevilla, 41092 Seville, Spain, --- Select a Country ---; Departamento de Teor{\'\i}a de la Se{\~n}al y Comunicaciones, Escuela Superior de Ingenier{\'\i}a, Universidad de Sevilla, 41092 Seville, Spain, --- Select a Country ---; Departamento de Teor{\'\i}a de la Se{\~n}al y Comunicaciones, Escuela Superior de Ingenier{\'\i}a, Universidad de Sevilla, 41092 Seville, Spain, --- Select a Country ---},
	date = {2023-01-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.5220/0011656600003417},
	keywords = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Speech Emotion},
	la = {en},
	title = {Experimental Setup and Protocol for Creating an EEG-signal Database for Emotion Analysis Using Virtual Reality Scenarios},
	url = {https://doi.org/10.5220/0011656600003417},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.5220/0011656600003417}}

@article{ahmadkhan_2023,
	abstract = {{$<$}p{$>$}Physiological Signals are the most reliable form of signals for emotion recognition, as they cannot be controlled deliberately by the subject. Existing review papers on emotion recognition based on physiological signals surveyed only the regular steps involved in the workflow of emotion recognition such as preprocessing, feature extraction, and classification. While these are important steps, such steps are required for any signal processing application. Emotion recognition poses its own set of challenges that are very important to address for a robust system. Thus, to bridge the gap in the existing literature, in this paper, we review the effect of inter-subject data variance on emotion recognition, important data annotation techniques for emotion recognition and their comparison, data preprocessing techniques for each physiological signal, data splitting techniques for improving the generalization of emotion recognition models and different multimodal fusion techniques and their comparison. Finally we discuss key challenges and future directions in this field.{$<$}/p{$>$}},
	author = {Ahmad, Zeeshan and Khan, Naimul},
	c1 = {Department of Electrical, Computer and Biomedical Engineering, Toronto Metropolitan University, Toronto, ON M5B 2K3, Canada; Department of Electrical, Computer and Biomedical Engineering, Toronto Metropolitan University, Toronto, ON M5B 2K3, Canada},
	date = {2023-05-03},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:51 +0100},
	doi = {10.32920/22734278.v1},
	keywords = {Emotion Recognition; Physiological Signals; Affective Computing; Speech Emotion},
	la = {en},
	title = {A Survey on Physiological Signal-Based Emotion Recognition},
	url = {https://doi.org/10.32920/22734278.v1},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.32920/22734278.v1}}

@article{saravanangovindarajan_2018,
	abstract = {Internet, e-mail and other social networks like Myspace, Facebook, twitter, LinkedIn are the indispensable components in today's world. These social networking makes the human to addict into the digital world. Digital world has become the integral part of our society. Addiction to the digital world slowly develops the negative symptoms in the area of physical, physiological, emotional and psychological. The most affected of all is the change in Emotional behaviour of the Humans. Emotions plays an important role in our day today life. The existing research work, based on subjective self-reports shows prolonged use of Digital Media induce negative emotions for Humans. There are several techniques are used to extract the human emotions from brain such as Electroencephalography (EEG), functional Magnetic Resonance Imaging (fMRI), or Positron Emission Tomography (PET). Many of the researchers are extensively used to extract the brain waves using EEG. The negative emotions are controlled by human through meditation. In this paper, the Mind Wave device has been used to extract the EEG signal using different range of age people during they use the Digital Medias and after they perform mediation. The proposed method identify the stress level of the human while they are using social media with meditation and without meditation. It evidently proved that the meditation reduces the stress level of human.},
	author = {Saravanan, S. and Govindarajan, Suganya},
	c1 = {Department of Computer Science and Engineering, SRM Institute of Science and Technology, Chennai, India.; Department of EDP, SRM Institute of Medical Sciences, Chennai, India.},
	date = {2018-12-09},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.14419/ijet.v7i4.36.24539},
	isbn = {2227-524X},
	journal = {International journal of engineering \& technology},
	keywords = {Emotion Recognition; EEG Analysis; Affective Computing; Speech Emotion},
	la = {en},
	number = {4.36},
	pages = {817--817},
	title = {Mental health analysis on digital world with meditation using EEG},
	url = {https://doi.org/10.14419/ijet.v7i4.36.24539},
	volume = {7},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.14419/ijet.v7i4.36.24539}}

@article{oliveirakhanshan_2023,
	abstract = {Human Digital Twins (HDTs) are specifically designed to represent humans virtually. Despite their recency, HDTs are considered to be powerful tools for enhancing personalized healthcare, early disease detection, improving patient outcomes, and optimizing lifestyle. Building HDTs is challenging especially due to the complexity of modeling human behavior. To unravel such complexity, in this paper we focus on specific aspects of human behavior and emotions (valence and arousal) to enable a more informed HTD behavior modeling. We assessed the feasibility and performance of our data collection infrastructure with N=112 participants. During a science festival, our participants wore a smartwatch to self-report their emotions while simultaneously their physiological data were collected through smartwatch sensors. We explored predictive modeling possibilities once with all the collected data and then with only the sensor-related data. The former performed best with 25 features, modeled with the Quadratic Discriminant Analysis classifier, resulting in 72.4{\%} accuracy, however, with only 4 features derived from the self-reports, the K-Nearest Neighbors Classifier was able to achieve an accuracy of 71.6{\%}. The latter used 17 sensor-related features and the Quadratic Discriminant Analysis classifier estimator reaching a 51.3{\%} accuracy.},
	author = {de Oliveira, Catarina and Khanshan, Alireza and Van Gorp, Pieter},
	c1 = {Industrial Design Department of the Eindhoven University of Technology, Netherlands; Industrial Design Department of the Eindhoven University of Technology, Netherlands; Industrial Engineering Department of the Eindhoven University of Technology, Netherlands},
	date = {2023-07-05},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1145/3594806.3596535},
	keywords = {Affective Computing; Emotion Recognition; Emotion Dynamics; Personality Data},
	la = {en},
	title = {Exploring the Feasibility of Data-Driven Emotion Modeling for Human Digital Twins},
	url = {https://doi.org/10.1145/3594806.3596535},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1145/3594806.3596535}}

@article{gongjia_2023,
	abstract = {Emotion recognition based on electroencephalography (EEG) has attracted significant attention and achieved considerable advances in the fields of affective computing and human-computer interaction. However, most existing studies ignore the coupling and complementarity of complex spatiotemporal patterns in EEG signals. Moreover, how to exploit and fuse crucial discriminative aspects in high redundancy and low signal-to-noise ratio EEG signals remains a great challenge for emotion recognition. In this paper, we propose a novel attention-based spatial-temporal dual-stream fusion network, named ASTDF-Net, for EEG-based emotion recognition. Specifically, ASTDF-Net comprises three main stages: first, the collaborative embedding module is designed to learn a joint latent subspace to capture the coupling of complicated spatiotemporal information in EEG signals. Second, stacked parallel spatial and temporal attention streams are employed to extract the most essential discriminative features and filter out redundant task-irrelevant factors. Finally, the hybrid attention-based feature fusion module is proposed to integrate significant features discovered from the dual-stream structure to take full advantage of the complementarity of the diverse characteristics. Extensive experiments on two publicly available emotion recognition datasets indicate that our proposed approach consistently outperforms state-of-the-art methods.},
	author = {Gong, Peiliang and Jia, Ziyang and Wang, Pengpai and Zhou, Yanqing and Zhang, Daoqiang},
	c1 = {College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics \& Key Laboratory of Brain-Machine Intelligence Technology, Ministry of Education, Nanjing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics \& Key Laboratory of Brain-Machine Intelligence Technology, Ministry of Education, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics \& Key Laboratory of Brain-Machine Intelligence Technology, Ministry of Education, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics \& Key Laboratory of Brain-Machine Intelligence Technology, Ministry of Education, Nanjing, China},
	date = {2023-10-26},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1145/3581783.3612208},
	keywords = {Emotion Recognition; Affective Computing; Deep Learning for EEG; EEG Analysis; Emotion Regulation},
	la = {en},
	title = {ASTDF-Net: Attention-Based Spatial-Temporal Dual-Stream Fusion Network for EEG-Based Emotion Recognition},
	url = {https://doi.org/10.1145/3581783.3612208},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1145/3581783.3612208}}

@article{khotacooper_2022,
	abstract = {Non-Linguistic Utterances (NLUs), produced for popular media, computers, robots, and public spaces, can quickly and wordlessly convey emotional characteristics of a message. They have been studied in terms of their ability to convey affect in robot communication. The objective of this research is to develop a model that correctly infers the emotional Valence and Arousal of an NLU. On a Likert scale, 17 subjects evaluated the relative Valence and Arousal of 560 sounds collected from popular movies, TV shows, and video games, including NLUs and other character utterances. Three audio feature sets were used to extract features including spectral energy, spectral spread, zero-crossing rate (ZCR), Mel Frequency Cepstral Coefficients (MFCCs), and audio chroma, as well as pitch, jitter, formant, shimmer, loudness, and Harmonics-to-Noise Ratio, among others. After feature reduction by Factor Analysis, the best-performing models inferred average Valence with a Mean Absolute Error (MAE) of 0.107 and Arousal with MAE of 0.097 on audio samples removed from the training stages. These results suggest the model infers Valence and Arousal of most NLUs to less than the difference between successive rating points on the 7-point Likert scale (0.14). This inference system is applicable to the development of novel NLUs to augment robot-human communication or to the design of sounds for other systems, machines, and settings.},
	author = {Khota, Ahmed and Cooper, Eric and Yu, Yanfang and Kovacs, Mate},
	c1 = {Ritsumeikan University, Japan,; Ritsumeikan University, Japan,; Ritsumeikan University, Japan,},
	date = {2022-09-01},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.5821/conference-9788419184849.52},
	keywords = {Affective Computing; Speech Emotion; Emotion Recognition; Auditory Processing; Speech Perception},
	la = {en},
	title = {Modelling emotional valence and arousal of non-linguistic utterances for sound design support},
	url = {https://doi.org/10.5821/conference-9788419184849.52},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.5821/conference-9788419184849.52}}

@article{jiangliu_2023,
	abstract = {Emotion recognition from physiological signals is a topic of widespread interest, and researchers continue to develop novel techniques for perceiving emotions. However, the emergence of deep learning has highlighted the need for high-quality emotional datasets to accurately decode human emotions. In this study, we present a novel multimodal emotion dataset that incorporates electroencephalography (EEG) and eye movement signals to systematically explore human emotions. Seven basic emotions (happy, sad, fear, disgust, surprise, anger, and neutral) are elicited by a large number of 80 videos and fully investigated with continuous labels that indicate the intensity of the corresponding emotions. Additionally, we propose a novel Multimodal Adaptive Emotion Transformer (MAET), that can flexibly process both unimodal and multimodal inputs. Adversarial training is utilized in MAET to mitigate subject discrepancy, which enhances domain generalization. Our extensive experiments, encompassing both subject-dependent and cross-subject conditions, demonstrate MAET's superior performance in handling various inputs. The filtering of data for high emotional evocation using continuous labels proved to be effective in the experiments. Furthermore, the complementary properties between EEG and eye movements are observed. Our code is available at https://github.com/935963004/MAET.},
	author = {Jiang, Wei-Bang and Liu, X. and Zheng, Wei‐Long and Lu, Bao‐Liang},
	c1 = {Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China},
	date = {2023-10-26},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:02 +0100},
	doi = {10.1145/3581783.3613797},
	keywords = {Emotion Recognition; Affective Computing; Speech Emotion; Deep Learning for EEG; Multimodal Data},
	la = {en},
	title = {Multimodal Adaptive Emotion Transformer with Flexible Modality Inputs on A Novel Dataset with Continuous Labels},
	url = {https://doi.org/10.1145/3581783.3613797},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1145/3581783.3613797}}

@article{ruli_2023,
	abstract = {Affective and psychological perception are pivotal in human-machine interaction and essential domains within artificial intelligence. Existing physiological signal-based affective and psychological datasets primarily rely on contact-based sensors, potentially introducing extraneous affectives during the measurement process. Consequently, creating accurate non-contact affective and psychological perception datasets is crucial for overcoming these limitations and advancing affective intelligence. In this paper, we introduce the Remote Multimodal Affective and Psychological (ReMAP) dataset, for the first time, apply head micro-tremor (HMT) signals for affective and psychological perception. ReMAP features 68 participants and comprises two sub-datasets. The stimuli videos utilized for affective perception undergo rigorous screening to ensure the efficacy and universality of affective elicitation. Additionally, we propose a novel remote affective and psychological perception framework, leveraging multimodal complementarity and interrelationships to enhance affective and psychological perception capabilities. Extensive experiments demonstrate HMT as a "small yet powerful" physiological signal in psychological perception. Our method outperforms existing state-of-the-art approaches in remote affective recognition and psychological perception. The ReMAP dataset is publicly accessible at https://remap-dataset.github.io/ReMAP.},
	author = {Ru, Yiwei and Li, Peipei and Sun, Muyi and Wang, Yunlong and Zhang, Kunbo and Li, Qi and He, Z. and Sun, Zhenan},
	c1 = {Beijing University of Posts and Telecommunications \& Institute of Automation, Chinese Academy of Sciences, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Beijing University of Post and Telecommunication, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China},
	date = {2023-10-26},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1145/3581783.3611754},
	keywords = {Affective Computing; Emotion Recognition; Head Gesture Recognition; Multimodal Data; Human-Computer Interaction},
	la = {en},
	title = {Sensing Micro-Motion Human Patterns using Multimodal mmRadar and Video Signal for Affective and Psychological Intelligence},
	url = {https://doi.org/10.1145/3581783.3611754},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1145/3581783.3611754}}

@article{ruli_2023,
	abstract = {Affective and psychological perception are pivotal in human-machine interaction and essential domains within artificial intelligence. Existing physiological signal-based affective and psychological datasets primarily rely on contact-based sensors, potentially introducing extraneous affectives during the measurement process. Consequently, creating accurate non-contact affective and psychological perception datasets is crucial for overcoming these limitations and advancing affective intelligence. In this paper, we introduce the Remote Multimodal Affective and Psychological (ReMAP) dataset, for the first time, apply head micro-tremor (HMT) signals for affective and psychological perception. ReMAP features 68 participants and comprises two sub-datasets. The stimuli videos utilized for affective perception undergo rigorous screening to ensure the efficacy and universality of affective elicitation. Additionally, we propose a novel remote affective and psychological perception framework, leveraging multimodal complementarity and interrelationships to enhance affective and psychological perception capabilities. Extensive experiments demonstrate HMT as a "small yet powerful" physiological signal in psychological perception. Our method outperforms existing state-of-the-art approaches in remote affective recognition and psychological perception. The ReMAP dataset is publicly accessible at https://remap-dataset.github.io/ReMAP.},
	author = {Ru, Yiwei and Li, Peipei and Sun, Muyi and Wang, Yunlong and Zhang, Kunbo and Li, Qi and He, Z. and Sun, Zhenan},
	c1 = {Beijing University of Posts and Telecommunications \& Institute of Automation, Chinese Academy of Sciences, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Beijing University of Post and Telecommunication, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China},
	date = {2023-10-26},
	date-added = {2024-05-13 15:05:26 +0100},
	date-modified = {2024-05-13 16:07:03 +0100},
	doi = {10.1145/3581783.3611754},
	keywords = {Affective Computing; Emotion Recognition; Head Gesture Recognition; Multimodal Data; Human-Computer Interaction},
	la = {en},
	title = {Sensing Micro-Motion Human Patterns using Multimodal mmRadar and Video Signal for Affective and Psychological Intelligence},
	url = {https://doi.org/10.1145/3581783.3611754},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1145/3581783.3611754}}
