[
  {
    "objectID": "data/template.html",
    "href": "data/template.html",
    "title": "template",
    "section": "",
    "text": "We code the studies in ascii format into files titled authoryearxx.R stored in data/ folder.\n\n\n\nInitial list, to be modified\nIssues that need to be resolved can be marked after hashtag (#).\nIf there are multiple studies/samples in one article, another data can be created and study_id can be adjusted by adding a postfix (_a,_b,_c).\n\n\nstudy_id &lt;- 'authorlastnameyeartwoletterfromthetitle'\nemotion_type &lt;- c('dimensional','discrete','music-specific','other')\nemotion_list &lt;- c('valence','arousal','happy','sad','anger','fear','tender','other')\nemotion_locus &lt;- c('perceived','induced','not specified')\nstimulus_type &lt;- c('classical','pop','genre','mixed')\nstimulus_duration &lt;- 20\nstimulus_N &lt;- 24\nparticipant_N &lt;- 35\nparticipant_expertise &lt;- c('non-musician','musician','not specified')\nparticipant_origin &lt;- c('Country','not-specified')\nparticipant_sampling &lt;- c('convenience','not-specified')\nparticipant_task &lt;- c('rating','forced choice','ranking','other')\nfeature_list &lt;- c('attack rate','pitch height','mode')\nfeature_validation &lt;- c('annotation','none') # this was done in another study\nfeature_source &lt;- c('librosa','essentia','MIR toolbox','marsyas','custom','not specified')\nmodel_type &lt;- c('correlation','regression','commonality analysis','classification')\nmodel_measure &lt;- c('rsquared','cor','rmse','classification','other')\nmodel_complexity_parameters &lt;- c(numeric, 'not defined')\nmodel_rate_emotion_list &lt;- c(0.00,0.00)\nmodel_validation &lt;- c('none','cross-validation')\nmeta_comments &lt;- 'template'\nmeta_encoder &lt;- 'Initials of the name'\nmeta_date &lt;- '3/5/2024'"
  },
  {
    "objectID": "data/template.html#fields",
    "href": "data/template.html#fields",
    "title": "template",
    "section": "",
    "text": "Initial list, to be modified\nIssues that need to be resolved can be marked after hashtag (#).\nIf there are multiple studies/samples in one article, another data can be created and study_id can be adjusted by adding a postfix (_a,_b,_c).\n\n\nstudy_id &lt;- 'authorlastnameyeartwoletterfromthetitle'\nemotion_type &lt;- c('dimensional','discrete','music-specific','other')\nemotion_list &lt;- c('valence','arousal','happy','sad','anger','fear','tender','other')\nemotion_locus &lt;- c('perceived','induced','not specified')\nstimulus_type &lt;- c('classical','pop','genre','mixed')\nstimulus_duration &lt;- 20\nstimulus_N &lt;- 24\nparticipant_N &lt;- 35\nparticipant_expertise &lt;- c('non-musician','musician','not specified')\nparticipant_origin &lt;- c('Country','not-specified')\nparticipant_sampling &lt;- c('convenience','not-specified')\nparticipant_task &lt;- c('rating','forced choice','ranking','other')\nfeature_list &lt;- c('attack rate','pitch height','mode')\nfeature_validation &lt;- c('annotation','none') # this was done in another study\nfeature_source &lt;- c('librosa','essentia','MIR toolbox','marsyas','custom','not specified')\nmodel_type &lt;- c('correlation','regression','commonality analysis','classification')\nmodel_measure &lt;- c('rsquared','cor','rmse','classification','other')\nmodel_complexity_parameters &lt;- c(numeric, 'not defined')\nmodel_rate_emotion_list &lt;- c(0.00,0.00)\nmodel_validation &lt;- c('none','cross-validation')\nmeta_comments &lt;- 'template'\nmeta_encoder &lt;- 'Initials of the name'\nmeta_date &lt;- '3/5/2024'"
  },
  {
    "objectID": "studies/search_syntax.html",
    "href": "studies/search_syntax.html",
    "title": "Search Syntax",
    "section": "",
    "text": "Search Syntax\n\n\n\n\n\n\n\n\n\nDatabase\nDate of search\nResults\nSearch syntax\n\n\n\n\nPsychINFO\nX May 20XX\nXXX\n(emotion OR mood* OR affect) AND (music OR audio OR recognition OR percei* OR recogn*) AND (music)\n\n\nScopus\nX May 20XX\nXXX\n\n\n\nWeb of Science\nX May 20XX\nXXX\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "preregistration/preregistration.html#authors-te-ca-ms-to-be-decided",
    "href": "preregistration/preregistration.html#authors-te-ca-ms-to-be-decided",
    "title": "metaMER",
    "section": "Authors: TE + CA + MS ? (to be decided)",
    "text": "Authors: TE + CA + MS ? (to be decided)"
  },
  {
    "objectID": "preregistration/preregistration.html#brief-rationale",
    "href": "preregistration/preregistration.html#brief-rationale",
    "title": "metaMER",
    "section": "Brief rationale",
    "text": "Brief rationale\nTo assess what degree models are able to predict emotions in music, what models, what emotions, what genres, etc."
  },
  {
    "objectID": "preregistration/preregistration.html#research-objectives",
    "href": "preregistration/preregistration.html#research-objectives",
    "title": "metaMER",
    "section": "Research objectives",
    "text": "Research objectives\n\nHow good are MER models?\nWhat are the systematic biases of the models?\nHow are models dealing with genres and cultures?\nHow generalisible are the models\nWEIRD\nTheory orientations\n\nFINER: feasible, interesting, novel, ethical, relevant [@browner2022de]"
  },
  {
    "objectID": "preregistration/preregistration.html#hypotheses",
    "href": "preregistration/preregistration.html#hypotheses",
    "title": "metaMER",
    "section": "Hypotheses",
    "text": "Hypotheses\nWe expect that the"
  },
  {
    "objectID": "preregistration/preregistration.html#eligibility-criteria",
    "href": "preregistration/preregistration.html#eligibility-criteria",
    "title": "metaMER",
    "section": "Eligibility criteria",
    "text": "Eligibility criteria\nEnglish, peer-reviewed journal articles indexed in X and Y, published between 20xx-20xx"
  },
  {
    "objectID": "preregistration/preregistration.html#database-sources",
    "href": "preregistration/preregistration.html#database-sources",
    "title": "metaMER",
    "section": "Database sources",
    "text": "Database sources\n(PubMed, Web of Science, Scopus)"
  },
  {
    "objectID": "preregistration/preregistration.html#search-strategy",
    "href": "preregistration/preregistration.html#search-strategy",
    "title": "metaMER",
    "section": "Search strategy",
    "text": "Search strategy\nMusic emotion recognition"
  },
  {
    "objectID": "preregistration/preregistration.html#analysis-plan",
    "href": "preregistration/preregistration.html#analysis-plan",
    "title": "metaMER",
    "section": "Analysis plan",
    "text": "Analysis plan\nFixed effects model Tests of small study/publication bias"
  },
  {
    "objectID": "manuscript/manuscript.html",
    "href": "manuscript/manuscript.html",
    "title": "Music Emotion Recognition…",
    "section": "",
    "text": "blah blah (Anderson & Schutz, 2022)\n\n\nblah blah"
  },
  {
    "objectID": "manuscript/manuscript.html#aims",
    "href": "manuscript/manuscript.html#aims",
    "title": "Music Emotion Recognition…",
    "section": "",
    "text": "blah blah"
  },
  {
    "objectID": "analysis/analysis.html",
    "href": "analysis/analysis.html",
    "title": "analysis",
    "section": "",
    "text": "analysis\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "README",
    "section": "",
    "text": "How well we can predict emotions in music? What is the evidence in the published literature for explaining what emotions the listeners can perceive in music when the source consists of audio examples. To what degree the results are dependent on the actual models, emotions, musical/acoustic features, or musical materials or participants?\nTo obtain answers to these questions, we have set out to record and analyse the current state of the art from the literature using a meta-analysis paradigm. We focus on Music Emotion Recognition and hence the acronym metaMER.\n\n\nWe define the aims and methods in preregistration plan.\n\n\n\nSearch databases and criteria are documented in studies/search_syntax.qmd.\n\n\n\nData coding and extraction is described in data template data/template.qmd.\n\n\n\nData analysis is covered in analysis/analysis.qmd document.\n\n\n\nThe study report is available manuscript/manuscript.qmd document."
  },
  {
    "objectID": "index.html#plan",
    "href": "index.html#plan",
    "title": "README",
    "section": "",
    "text": "We define the aims and methods in preregistration plan."
  },
  {
    "objectID": "index.html#study-search-and-selection",
    "href": "index.html#study-search-and-selection",
    "title": "README",
    "section": "",
    "text": "Search databases and criteria are documented in studies/search_syntax.qmd."
  },
  {
    "objectID": "index.html#data-extraction-and-coding",
    "href": "index.html#data-extraction-and-coding",
    "title": "README",
    "section": "",
    "text": "Data coding and extraction is described in data template data/template.qmd."
  },
  {
    "objectID": "index.html#analysis",
    "href": "index.html#analysis",
    "title": "README",
    "section": "",
    "text": "Data analysis is covered in analysis/analysis.qmd document."
  },
  {
    "objectID": "index.html#manuscript",
    "href": "index.html#manuscript",
    "title": "README",
    "section": "",
    "text": "The study report is available manuscript/manuscript.qmd document."
  },
  {
    "objectID": "studies/library_parser.html",
    "href": "studies/library_parser.html",
    "title": "Library Parser",
    "section": "",
    "text": "Status: in progress.\nTODO:\n- Separate scripts and functions into separate .qmd files/directories.\n- Update model_rate_emotion_values nomenclature process for classification papers.\n- Replace rbind with custom bind function throughout"
  },
  {
    "objectID": "studies/library_parser.html#preparing-data",
    "href": "studies/library_parser.html#preparing-data",
    "title": "Library Parser",
    "section": "Preparing data",
    "text": "Preparing data\nThe follow code extracts information from the .bib library to format it as a data.frame for further processing. The code makes use of the stringr package and uses regular expressions to extract relevant parameters. First we read in the .bib file and use some string manipulations to retrieve the citation keys.\n\nlibrary(stringr)\nlibrary(knitr, include.only = 'kable')\n\n\nbib_file &lt;- read.delim('bib/extractions.bib',\n           sep = '@', header = F)\n\nWarning in scan(file = file, what = what, sep = sep, quote = quote, dec = dec,\n: EOF within quoted string\n\n# get citekeys from bibtex file:\ncitekeys &lt;- unique(bib_file$V2)\n# improve formatting\ncitekeys &lt;- str_remove(citekeys, '\\\\{')\ncitekeys &lt;- str_remove(citekeys, ',')\ncitekeys &lt;- str_remove(citekeys, '%%.*$')\ncitekeys &lt;- str_remove(citekeys, 'Article')\ncitekeys[citekeys ==''] &lt;- NA\ncitekeys &lt;- na.omit(citekeys)\n\nR reads the .bib file as a two column data.frame, with the citation key appearing in the second column and the remaining metadata appearing in the first column. When the citation key appears in the second column, the corresponding row in the first column is blank. Because of this quirk, we can index metadata matching each citation key by keeping track of blank rows in the first column. We’ll append each to a new entry of a list. The name of each list entry is the citation key; the corresponding value is the remaining unprocessed metadata.\n\n# find where new entries begin:\nnew_entries = which(bib_file$V2 != '')\n\n# loop across unique indices for each entry\nmeta_list = list()\n# loop across unique indices for each entry\nmeta_list = list()\nfor(this_entry in 1:(length(new_entries)-1))\n{\n  # get unique citekey\n  this_cite_key &lt;- citekeys[this_entry]\n  # capture lines following citekey\n  corresponding_lines &lt;- bib_file[new_entries[this_entry]:new_entries[this_entry+1]-1,]$V1\n  # store matching lines as data frame\n  corresponding_lines &lt;- data.frame(corresponding_lines)\n  # assign lines distinct name\n  names(corresponding_lines) &lt;- this_cite_key\n  # add to a list for further processing\n  meta_list &lt;- append(meta_list, corresponding_lines)\n}"
  },
  {
    "objectID": "studies/library_parser.html#extracting-relevant-.bib-fields",
    "href": "studies/library_parser.html#extracting-relevant-.bib-fields",
    "title": "Library Parser",
    "section": "Extracting Relevant .bib Fields",
    "text": "Extracting Relevant .bib Fields\nNot every bibtex field is equally useful for analysis. To facilitate data manipulation, we can save the names of the target fields separately in a .txt file, and use a regular expression to create a new column each time R finds one of the target fields in a string containing the bibtex metadata.\n\n# read in target bibtex fields\nsearch_fields &lt;- field_names &lt;- readLines('bibtex_fields.txt')\n\nWarning in readLines(\"bibtex_fields.txt\"): incomplete final line found on\n'bibtex_fields.txt'\n\n# match casing in bibtex file\nfield_names &lt;- toupper(field_names)\n# add a pattern allowing us to find text between two adjacent bibtex fields\nrep_pattern &lt;- paste0(field_names[1:length(field_names)-1], '\\\\s*(.*?)\\\\s')\n# apply this same pattern to all but the last of the field names                  \nfield_names[1:length(field_names)-1] &lt;- rep_pattern\n# collapse all the new field names into a single string for string manipulation with stringr\nfield_names[length(field_names)] &lt;- paste0(field_names[length(field_names)], '.*')\nfield_names &lt;- paste0(field_names, collapse = '')"
  },
  {
    "objectID": "studies/library_parser.html#prepare-dataframe",
    "href": "studies/library_parser.html#prepare-dataframe",
    "title": "Library Parser",
    "section": "Prepare dataframe",
    "text": "Prepare dataframe\nNow we can convert our list into a data frame with the target bibtex fields. For the last field MODEL_VALIDATION, we will apply a different regex pattern which matches all characters following the field name (?&lt;=MODEL_VALIDATION).* .\n\n# create new column containing information between two adjacent target fields for all entries in list\nmeta_df &lt;- lapply(meta_list, function(x) str_match(paste0(x, collapse = ' '), field_names))\nmeta_df &lt;- lapply(meta_list, function(x) str_match(paste0(x, collapse = ' '), field_names))\n\n# collapse list entries into rows\nmeta_df &lt;- do.call('rbind', meta_df)\n# format as a data.frame\nmeta_df &lt;- data.frame(meta_df)\n# match text after final column name\nmeta_df[,ncol(meta_df)+1] &lt;- sapply(meta_df[,1], function(x) str_match(paste0(x, collapse = ' '), '(?&lt;=FINAL_NOTES).*'))\n# replace first column with citationkeys\nmeta_df[,1] &lt;- names(meta_list)\nnames(meta_df) &lt;- c('citekey', search_fields)\nnames(meta_df) &lt;- trimws(names(meta_df))"
  },
  {
    "objectID": "studies/library_parser.html#formatting",
    "href": "studies/library_parser.html#formatting",
    "title": "Library Parser",
    "section": "Formatting",
    "text": "Formatting\nFinally, we’ll perform some formatting to remove unwanted characters left over following the conversion (in progress)\n\n## remove bibtext field formatting\n# remove curly braces\nmeta_df &lt;- apply(meta_df, 2, function(x) str_remove_all(x, '\\\\{')) \nmeta_df &lt;- apply(meta_df, 2, function(x) str_remove_all(x, '\\\\},'))\n# remove first '=' (from bibtex field )\nmeta_df &lt;- apply(meta_df, 2, function(x) str_remove(x, '='))\n# remove double-commas\n#meta_df &lt;- apply(meta_df, 2, function(x) str_remove_all(x, ',,'))\n# remove comments\nmeta_df &lt;- apply(meta_df, 2, function(x) str_remove_all(x, '%%.*'))\n#meta_df &lt;- apply(meta_df, 2, function(x) str_remove_all(x, ' , '))\n# remove extra characters in final column\nmeta_df[, ncol(meta_df)] = str_remove_all(meta_df[, ncol(meta_df)], '\\\\}')\nmeta_df &lt;- as.data.frame(meta_df)\n\nExample: Evaluate model_rate_emotion_values as R code\n\neval(parse(text = meta_df$model_rate_emotion_values[22]))\n\n                             r2_arousal explained_variance_arousal r2_valence\nsvr                              0.7249                     0.7556     0.6119\nsbr                              0.7381                     0.7395     0.6296\nvbr                              0.7108                     0.7415     0.7108\nr2_valence                       0.6328                     0.6328     0.6328\nexplained_variance_valence       0.6340                     0.6340     0.6340\nr2_resonance                     0.5554                     0.5554     0.5554\nexplained_variance_resonance     0.5630                     0.5630     0.5630\n                             explained_variance_valence r2_resonance\nsvr                                              0.6142       0.5374\nsbr                                              0.6376       0.5456\nvbr                                              0.7415       0.7108\nr2_valence                                       0.6328       0.6328\nexplained_variance_valence                       0.6340       0.6340\nr2_resonance                                     0.5554       0.5554\nexplained_variance_resonance                     0.5630       0.5630\n                             explained_variance_resonance\nsvr                                                0.5496\nsbr                                                0.5558\nvbr                                                0.7415\nr2_valence                                         0.6328\nexplained_variance_valence                         0.6340\nr2_resonance                                       0.5554\nexplained_variance_resonance                       0.5630"
  },
  {
    "objectID": "studies/library_parser.html#track-excluded-studies-during-extraction",
    "href": "studies/library_parser.html#track-excluded-studies-during-extraction",
    "title": "Library Parser",
    "section": "Track Excluded Studies (During Extraction)",
    "text": "Track Excluded Studies (During Extraction)\n\nmeta_df[which(str_detect(meta_df$final_notes, '!EXCL!')),] |&gt; dplyr::tibble()\n\n# A tibble: 7 × 27\n  citekey        paradigm notes_ca notes_te emotions emotion_locus stimulus_type\n  &lt;chr&gt;          &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;         &lt;chr&gt;        \n1 \"feng2024ex\"   \" class… \" inclu… \" inclu… \"    \"   \"    \"        \"    \"       \n2 \"nag2022on\"    \" class… \" inclu… \" inclu… \" happy… \"    \"        \"    \"       \n3 \"malheiro2018… \" regre… \" inclu… \" inclu… \"    \"   \"    \"        \"    \"       \n4 \"medina2020em\" \" class… \" inclu… \" inclu… \" valen… \" perceived … \" MediaEval …\n5 \"panwar2019ar\" \" regre… \" inclu… \" inclu… \"    \"   \"    \"        \"    \"       \n6 \"vempala2024p… \" regre… \" inclu… \" inclu… \" valen… \" perceived … \" classical …\n7 \"xia2022st\"    \" regre… \" inclu… \" inclu… \"   \"    \"    \"        \"   \"        \n# ℹ 20 more variables: stimulus_duration &lt;chr&gt;, stimulus_duration_unit &lt;chr&gt;,\n#   stimulus_n &lt;chr&gt;, feature_n &lt;chr&gt;, participant_n &lt;chr&gt;,\n#   participant_expertise &lt;chr&gt;, participant_origin &lt;chr&gt;,\n#   participant_sampling &lt;chr&gt;, participant_task &lt;chr&gt;,\n#   feature_categories &lt;chr&gt;, feature_source &lt;chr&gt;,\n#   feature_reduction_method &lt;chr&gt;, model_category &lt;chr&gt;, model_detail &lt;chr&gt;,\n#   model_measure &lt;chr&gt;, model_complexity_parameters &lt;chr&gt;, …\n\n\n\nmeta_df[-which(str_detect(meta_df$final_notes, '!EXCL!')),] -&gt; included_studies\n\nincluded_studies |&gt; dplyr::tibble()\n\n# A tibble: 35 × 27\n   citekey       paradigm notes_ca notes_te emotions emotion_locus stimulus_type\n   &lt;chr&gt;         &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;         &lt;chr&gt;        \n 1 agarwal2021an \" class… \" inclu… \" inclu… \" discr… \" not specif… \" Hindi musi…\n 2 alvarez2023ri \" class… \" inclu… \" inclu… \" discr… \" not specif… \" Spotify  \" \n 3 bai2017mu     \" class… \" inclu… \" inclu… \" valen… \" perceived … \" MediaEval …\n 4 bhuvanakumar… \" class… \" inclu… \" inclu… \" quadr… \" not specif… \" pop piano …\n 5 dufour2021us  \" class… \" inclu… \" inclu… \"  c1 (… \" not specif… \" pop, disco…\n 6 hizlisoy2021… \" class… \" inclu… \" inclu… \" valen… \" perceived … \" Turkish tr…\n 7 nguyen2017an  \" class… \" inclu… \" inclu… \" 288 e… \" perceived … \" AMG  \"     \n 8 panda2020no   \" class… \" inclu… \" inclu… \"    \"   \"    \"        \" AllMusic  \"\n 9 sorussa2020em \" class… \" inclu… \" inclu… \" valen… \" perceived … \" DEAM  \"    \n10 yang2021an    \" class… \" inclu… \" inclu… \" happy… \" perceived … \" MediaEval …\n# ℹ 25 more rows\n# ℹ 20 more variables: stimulus_duration &lt;chr&gt;, stimulus_duration_unit &lt;chr&gt;,\n#   stimulus_n &lt;chr&gt;, feature_n &lt;chr&gt;, participant_n &lt;chr&gt;,\n#   participant_expertise &lt;chr&gt;, participant_origin &lt;chr&gt;,\n#   participant_sampling &lt;chr&gt;, participant_task &lt;chr&gt;,\n#   feature_categories &lt;chr&gt;, feature_source &lt;chr&gt;,\n#   feature_reduction_method &lt;chr&gt;, model_category &lt;chr&gt;, model_detail &lt;chr&gt;, …"
  },
  {
    "objectID": "studies/library_parser.html#data-frame-expansion",
    "href": "studies/library_parser.html#data-frame-expansion",
    "title": "Library Parser",
    "section": "Data Frame Expansion",
    "text": "Data Frame Expansion\nNext we want to copy the number of rows for each of the bibtex cells requiring special nesting"
  },
  {
    "objectID": "studies/library_parser.html#sanity-check",
    "href": "studies/library_parser.html#sanity-check",
    "title": "Library Parser",
    "section": "Sanity check",
    "text": "Sanity check"
  },
  {
    "objectID": "studies/library_parser.html#print-output-of-all-studies",
    "href": "studies/library_parser.html#print-output-of-all-studies",
    "title": "Library Parser",
    "section": "Print output of all studies:",
    "text": "Print output of all studies:\n\n# low-level function to extract relevant values from named array\nmodel_result_2_df &lt;- function(x) {\n  # evaluate the expression\n  x &lt;- rlang::eval_tidy(rlang::parse_expr(x))\n  # get name of fitted models from column names\n  model_names &lt;- colnames(x)\n  # print(model_names)\n  # split across model name for summary statistic (e.g., mean, sd, etc.)\n  model_statistic &lt;- str_split(model_names, '\\\\.')\n  model_statistic &lt;- unlist(lapply(model_statistic, function(x) x[2]))\n  # get values\n  model_values &lt;- as.numeric(x)\n  # names(model_values) &lt;- 'score'\n  print(model_values)\n  # get additional attributes (feature.data.exp)\n  model_attributes &lt;- rownames(x)\n  data.frame(model_attributes)\n  # get additional model details\n  model_attributes &lt;- data.frame(do.call('rbind', str_split(model_attributes, '\\\\.')))\n  names(model_attributes) &lt;- c('library_id', 'model_id', 'feature_id', 'data_id', 'experiment_id')\n  print(names(model_attributes))\n  # split across '_' to \n  model_measures &lt;- data.frame(do.call('rbind', stringr::str_split(colnames(x), '_')))\n  print(model_measures)\n  names(model_measures) &lt;- c('dimension', 'measure')\n  model_measures_split &lt;- stringr::str_split(model_measures[,'measure'], '\\\\.')\n  model_measures[,'measure'] &lt;- unlist(lapply(model_measures_split, \n                                              function(x) x[1]))\n  return(data.frame(\n             model_attributes, \n             model_measures, \n             values = model_values,\n             statistic = model_statistic))\n}\n\n# high level function to apply model_result_2_df to multiple studies\nget_study_summaries &lt;- function(df) {\n  do.call(rbind,\n          lapply(df$model_rate_emotion_values, \n                 FUN = function(x) {\n                   study_id &lt;- unique(df$citekey[which(df$model_rate_emotion_values == x)])\n                   model_results &lt;- model_result_2_df(x)\n                   return(cbind(study_id, model_results))\n                   }\n                 )\n          )\n}"
  }
]