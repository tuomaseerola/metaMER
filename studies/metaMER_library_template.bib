@Article{agarwal2021an,
  AUTHOR = {Agarwal, Gaurav and Om, Hari},
  JOURNAL = {IET Signal Processing},
  NOTE = {Cited by: 23},
  NUMBER = {2},
  PAGES = {98 – 121},
  TITLE = {An efficient supervised framework for music mood recognition using autoencoder-based optimised support vector regression model},
  TYPE = {Article},
  VOLUME = {15},
  YEAR = {2021},
  NOTES.CA = {include, classification},
  DOI = {10.1049/sil2.12015},
  ABSTRACT = {Music is the art of ‘language of emotions’. Recently, music mood recognition is an emerging task. An efficient supervised framework for music mood recognition using autoencoder-based optimised support vector regression (SVR) model is developed for the music emotion recognition. Our main intention is to increase the accuracy of emotion classification of music by considering text-dependent and non-text-dependent features. For the high level feature representation, stacked autoencoder is used with two hidden layers. Modified K-Medoid-based brain storm optimisation-based support vector regression (SVR_KMBSO) model is utilised for the emotion classification. Using the K-Medoid-based brain storm algorithm, the optimal parameters of the SVR are selected. The proposed framework utilises ISMIR2012 dataset and NJU_V1 dataset for English and for Hindi; online songs are also gathered and used for the music mood recognition. All the three datasets include songs based on four emotions like happy, angry, relax and sad. The experimental results are evaluated and compared with the existing classifiers including SVR, deep belief network (DBN) and Recurrent neural network (RNN). The proposed method SVR_KMBSO achieved high accuracy using three different datasets. © 2021 The Authors. IET Signal Processing published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.},
  KEYWORDS = {Character recognition; Emotion Recognition; Learning systems; Recurrent neural networks; Regression analysis; Storms; Auto encoders; Emotion classification; Emotion recognition; Feature representation; Hidden layers; High-level features; K-medoid; Music emotions; Support vector regression models; Support vector regressions; Music},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85116929889&doi = 10.1049%2fsil2.12015&partnerID = 40&md5 = 4e078ee8b1c740d21d4b7d8dbf7f6cd8},
  NOTES.TE = {include, classification},
  PARADIGM = {classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{alvarez2023ri,
  AUTHOR = {Álvarez, P. and de Quirós, J. García and Baldassarri, S.},
  JOURNAL = {International Journal of Interactive Multimedia and Artificial Intelligence},
  NOTE = {Cited by: 4; All Open Access, Gold Open Access, Green Open Access},
  NUMBER = {2},
  PAGES = {168 – 181},
  TITLE = {RIADA: A Machine-Learning Based Infrastructure for Recognising the Emotions of Spotify Songs},
  TYPE = {Article},
  VOLUME = {8},
  YEAR = {2023},
  NOTES.CA = {include, classification task},
  DOI = {10.9781/ijimai.2022.04.002},
  ABSTRACT = {The music emotions can help to improve the personalization of services and contents offered by music streaming providers. Many research works based on the use of machine learning techniques have addressed the problem of recognising the music emotions during the last years. Nevertheless, the results obtained are only applied on small-size music repositories and do not consider what the users feel when they listen to the songs. These issues prevent the existing proposals to be integrated into the personalization mechanisms of the online music providers. In this paper, we present the RIADA infrastructure which is composed by a set of systems able to annotate emotionally the catalog of songs offered by Spotify based on the users’ perception. RIADA works with the Spotify playlist miner and data services to build emotion recognition models that can solve the open challenges previously mentioned. Machine learning algorithms, music information retrieval techniques, architectures for parallelization of applications and cloud computing have been combined to develop a complex result of engineering able to integrate the music emotions into the Spotify-based applications. © 2023, Universidad Internacional de la Rioja. All rights reserved.},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85147947463&doi = 10.9781%2fijimai.2022.04.002&partnerID = 40&md5 = 8f61f829c998052047f5da67adcf48c9},
  NOTES.TE = {include, classification, spotify},
  AUTHOR_KEYWORDS = {Affective Annotation; Cloud Computing; Emotion Recognition; Machine Learning; Music; Spotify},
  PARADIGM = {classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{bai2017mu,
  AUTHOR = {Bai, Junjie and Luo, Kan and Peng, Jun and Shi, Jinliang and Wu, Ying and Feng, Lixiao and Li, Jianqing and Wang, Yingxu},
  JOURNAL = {International Journal of Cognitive Informatics and Natural Intelligence},
  NOTE = {Cited by: 6},
  NUMBER = {4},
  PAGES = {80 – 92},
  TITLE = {Music emotions recognition by machine learning with cognitive classification methodologies},
  TYPE = {Article},
  VOLUME = {11},
  YEAR = {2017},
  NOTES.CA = {include, classification},
  DOI = {10.4018/IJCINI.2017100105},
  ABSTRACT = {Music emotions recognition (MER) is a challenging field of studies addressed in multiple disciplines such as musicology, cognitive science, physiology, psychology, arts and affective computing. In this article, music emotions are classified into four types known as those of pleasing, angry, sad and relaxing. MER is formulated as a classification problem in cognitive computing where 548 dimensions of music features are extracted and modeled. A set of classifications and machine learning algorithms are explored and comparatively studied for MER, which includes Support Vector Machine (SVM), k-Nearest Neighbors (KNN), Neuro-Fuzzy Networks Classification (NFNC), Fuzzy KNN (FKNN), Bayes classifier and Linear Discriminant Analysis (LDA). Experimental results show that the SVM, FKNN and LDA algorithms are the most effective methodologies that obtain more than 80% accuracy for MER. © 2017 IGI Global.},
  KEYWORDS = {Artificial intelligence; Arts computing; Barium compounds; Discriminant analysis; Feature extraction; Fuzzy neural networks; Image retrieval; Learning systems; Nearest neighbor search; Pattern recognition; Psychophysiology; Support vector machines; Affective Computing; Classification methodologies; Emotion classification; K nearest neighbor (KNN); Linear discriminant analysis; Multiple disciplines; Music emotions; Neuro-fuzzy network; Learning algorithms},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85038434665&doi = 10.4018%2fIJCINI.2017100105&partnerID = 40&md5 = a2e1f1d69ab6ceb6f0cee7f2eb26985a},
  NOTES.TE = {include, classification, quality issues?},
  AUTHOR_KEYWORDS = {Emotion Classification; Feature Extraction; Machine Learning; Music Emotion Recognition; Pattern Recognition},
  PARADIGM = {classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{bhuvanakumar2023em,
  AUTHOR = {Bhuvana Kumar, V. and Kathiravan, M.},
  JOURNAL = {Frontiers in Computer Science},
  NOTE = {Cited by: 0; All Open Access, Gold Open Access},
  TITLE = {Emotion recognition from MIDI musical file using Enhanced Residual Gated Recurrent Unit architecture},
  TYPE = {Article},
  VOLUME = {5},
  YEAR = {2023},
  NOTES.CA = {include, quadrant classification},
  DOI = {10.3389/fcomp.2023.1305413},
  ABSTRACT = {The complex synthesis of emotions seen in music is meticulously composed using a wide range of aural components. Given the expanding soundscape and abundance of online music resources, creating a music recommendation system is significant. The area of music file emotion recognition is particularly fascinating. The RGRU (Enhanced Residual Gated Recurrent Unit), a complex architecture, is used in our study to look at MIDI (Musical Instrument and Digital Interface) compositions for detecting emotions. This involves extracting diverse features from the MIDI dataset, encompassing harmony, rhythm, dynamics, and statistical attributes. These extracted features subsequently serve as input to our emotion recognition model for emotion detection. We use an improved RGRU version to identify emotions and the Adaptive Red Fox Algorithm (ARFA) to optimize the RGRU hyperparameters. Our suggested model offers a sophisticated classification framework that effectively divides emotional content into four separate quadrants: positive-high, positive-low, negative-high, and negative-low. The Python programming environment is used to implement our suggested approach. We use the EMOPIA dataset to compare its performance to the traditional approach and assess its effectiveness experimentally. The trial results show better performance compared to traditional methods, with higher accuracy, recall, F-measure, and precision. Copyright © 2023 Kumar and Kathiravan.},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85181443074&doi = 10.3389%2ffcomp.2023.1305413&partnerID = 40&md5 = 977f4cd7e7b4a9a110b29a8f48b0e632},
  NOTES.TE = {include, midi, classification},
  AUTHOR_KEYWORDS = {adaptive Red Fox algorithm; EMOPIA; emotion recognition; Enhanced Residual Gated Recurrent Unit; Musical Instrument Digital Interface},
  PARADIGM = {classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{dufour2021us,
  AUTHOR = {Dufour, Isabelle and Tzanetakis, George},
  JOURNAL = {IEEE TRANSACTIONS ON AFFECTIVE COMPUTING},
  MONTH = {JUL-SEP},
  NUMBER = {3},
  PAGES = {666-681},
  TITLE = {Using Circular Models to Improve Music Emotion Recognition},
  VOLUME = {12},
  YEAR = {2021},
  NOTES.CA = {include, classification},
  DOI = {10.1109/TAFFC.2018.2885744},
  SOURCE = {web_of_science},
  NOTES.TE = {include, classification},
  BDSK = {https://doi.org/10.1109/TAFFC.2018.2885744},
  ISSN = {1949-3045},
  ORCID = {Tzanetakis, George/0000-0002-6844-7912},
  RESEARCHERID = {Tzanetakis, George/I-6593-2013},
  PARADIGM = {classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{feng2024ex,
  AUTHOR = {Feng, Qiyue},
  JOURNAL = {Applied Mathematics and Nonlinear Sciences},
  NOTE = {Cited by: 0},
  NUMBER = {1},
  TITLE = {Exploration of the Integration of Traditional Music Cultural Elements in Music Informatization Teaching},
  TYPE = {Article},
  VOLUME = {9},
  YEAR = {2024},
  NOTES.CA = {include, classification task},
  DOI = {10.2478/amns-2024-0973},
  ABSTRACT = {With the development of information technology, music teaching methods are getting more affluent and more prosperous. This paper proposes a model for emotion classification and assessment that integrates traditional music culture elements with information technology in music teaching. The research first combines TextCNN and BiLSTM algorithms to establish the emotion classification model of conventional music. Then it combines PYIN and DTW algorithms to establish the evaluation model of traditional music, which completes the auxiliary efficacy of music informationized teaching. In the emotion classification test of the model, the classification accuracy and F1 value of emotions of different music samples are 82.98% and 75.22%, respectively. The model’s recognition accuracy of the four voices is 86.76%, and the overall effective scoring percentage is 81.98% under different playing abnormalities. This study has had an impact on traditional music evaluation. The model in this paper can be used to classify and evaluate emotions in conventional music, providing more intelligent and high-quality technical services for integrating traditional music into music teaching. © 2023 Qiyue Feng, published by Sciendo.},
  KEYWORDS = {Quality control; BiLSTM; Classification models; DTW; Emotion classification; Informatization; Music evaluation; PYIN; Teaching methods; TextCNN; Traditional music; Classification (of information)},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85192244090&doi = 10.2478%2famns-2024-0973&partnerID = 40&md5 = f61a9cbc265402b97c5600f8b5f1d679},
  NOTES.TE = {include, classification},
  AUTHOR_KEYWORDS = {BiLSTM; DTW; Music evaluation; PYIN; TextCNN; Traditional music},
  PARADIGM = {classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{he2022al,
  AUTHOR = {He, Jiao},
  JOURNAL = {Computational Intelligence and Neuroscience},
  NOTE = {Cited by: 5; All Open Access, Gold Open Access, Green Open Access},
  TITLE = {Algorithm Composition and Emotion Recognition Based on Machine Learning},
  TYPE = {Article},
  VOLUME = {2022},
  YEAR = {2022},
  NOTES.CA = {unsure, appears task is not comparable, classification},
  DOI = {10.1155/2022/1092383},
  ABSTRACT = {This paper proposes a new algorithm composition network from the perspective of machine learning, based on an in-depth study of related literature. At the same time, this paper examines the characteristics of music and develops a model for recognising musical emotions. Using the model's information entropy of pitch and intensity to extract the main melody track, note features are extracted from bar features. Finally, the cosine of the vector included angle is used to judge the similarity between feature vectors of several adjacent sections, allowing the music to be divided into several independent segments. The emotional model of music is used to analyze each segment's emotion. By quantifying music features, this paper classifies and quantifies music emotion based on the mapping relationship between music features and emotion. Music emotion can be accurately identified by the model. The model's emotion recognition accuracy is up to 93.78 percent, and the algorithm's recall rate is up to 96.3 percent, according to simulation results. The recognition method used in this paper has a higher recognition ability than other methods, and the emotion recognition result is more reliable. This paper can not only meet the composer's auxiliary creative needs, but it can also help intelligent music services. © 2022 Jiao He.},
  KEYWORDS = {Algorithms; Emotions; Machine Learning; Music; Recognition, Psychology; Learning algorithms; Machine learning; Music; Speech recognition; Emotion recognition; Emotional models; Features vector; In-depth study; Information entropy; Machine-learning; Model informations; Music emotions; Musical emotion; On-machines; algorithm; emotion; machine learning; music; psychology; Emotion Recognition},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85132146675&doi = 10.1155%2f2022%2f1092383&partnerID = 40&md5 = c8b96f3849ce73b57928a3d94771a96c},
  NOTES.TE = {exclude, non-comparable output values, generation},
  PARADIGM = {classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{hizlisoy2021mu,
  AUTHOR = {Hizlisoy, Serhat and Yildirim, Serdar and Tufekci, Zekeriya},
  JOURNAL = {Engineering Science and Technology, an International Journal},
  NOTE = {Cited by: 85; All Open Access, Gold Open Access},
  NUMBER = {3},
  PAGES = {760 – 767},
  TITLE = {Music emotion recognition using convolutional long short term memory deep neural networks},
  TYPE = {Article},
  VOLUME = {24},
  YEAR = {2021},
  NOTES.CA = {include, classification},
  DOI = {10.1016/j.jestch.2020.10.009},
  ABSTRACT = {In this paper, we propose an approach for music emotion recognition based on convolutional long short term memory deep neural network (CLDNN) architecture. In addition, we construct a new Turkish emotional music database composed of 124 Turkish traditional music excerpts with a duration of 30 s each and the performance of the proposed approach is evaluated on the constructed database. We utilize features obtained by feeding convolutional neural network (CNN) layers with log-mel filterbank energies and mel frequency cepstral coefficients (MFCCs) in addition to standard acoustic features. Classification results show that the best performance is obtained when the new feature set is combined with the standard features using the long short term memory (LSTM) + deep neural network (DNN) classi fier. The overall accuracy of 99.19% is obtained using the proposed system with 10 fold cross-validation. Specifically, 6.45 points improvement is achieved. Additionally, the results also show that the LSTM + DNN classifier yields 1.61, 1.61 and 3.23 points improvements in music emotion recognition accuracies compared to k-nearest neighbor (k-NN), support vector machine (SVM), and Random Forest classifiers, respectively. © 2020 Karabuk University},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85096857185&doi = 10.1016%2fj.jestch.2020.10.009&partnerID = 40&md5 = a6d9948932df1065662495509f77bfe5},
  NOTES.TE = {include, classification},
  AUTHOR_KEYWORDS = {Convolutional long short term memory deep neural networks; Music emotion recognition; Turkish emotional music database},
  PARADIGM = {classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{huang2023th,
  AUTHOR = {Huang, Linna},
  JOURNAL = {International Journal of Networking and Virtual Organisations},
  NOTE = {Cited by: 0},
  NUMBER = {2-4},
  PAGES = {445 – 460},
  TITLE = {The application and research of double-layer music emotion classification model based on random forest algorithm in digital music},
  TYPE = {Article},
  VOLUME = {28},
  YEAR = {2023},
  NOTES.CA = {unsure, cannot find full text, classification},
  DOI = {10.1504/IJNVO.2023.133878},
  ABSTRACT = {It is urgent to solve the problem of music emotion classification. The stochastic forest algorithm is easy to operate and performs better than other single-layer classification models. Aiming at the problems of feature extraction and classification in conventional music emotion classification methods, music features are divided into long-term features and short-term features, and a two-layer music emotion classification model integrating a random forest (RF) algorithm is designed. The experimental results showed that the SVM model using the Gaussian radial basis kernel function had the highest classification accuracy of 90.78% in training the SVM model. The overall classification accuracy of the two-layer music emotion classification model was 98.92%, the recall rate was 97.63%, and its indicators in different emotion categories were the highest, with an average F1 value of 0.919. To sum up, the two-layer music emotion classification model based on the RF algorithm proposed in the research has excellent recognition and classification capabilities. Copyright © 2023 Inderscience Enterprises Ltd.},
  KEYWORDS = {Classification (of information); Emotion Recognition; Stochastic systems; Support vector machines; Classification models; Double layer models; Emotional classification; Music characteristic; Music emotion classifications; Random forest algorithm; Random forests; SVM; Two-layer; Stochastic models},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85174308407&doi = 10.1504%2fIJNVO.2023.133878&partnerID = 40&md5 = 9a511d481ab7c25caee2871f387596ba},
  NOTES.TE = {exclude, no access},
  AUTHOR_KEYWORDS = {double layer model; emotional classification; music characteristics; random forest; RF; SVM},
  PARADIGM = {classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{na2022mu,
  AUTHOR = {Na, Wang and Yong, Fang},
  JOURNAL = {Scientific Programming},
  NOTE = {Cited by: 5; All Open Access, Gold Open Access},
  TITLE = {Music Recognition and Classification Algorithm considering Audio Emotion},
  TYPE = {Article},
  VOLUME = {2022},
  YEAR = {2022},
  NOTES.CA = {unsure, may be worth including but may not fit with other paradigms, classification},
  DOI = {10.1155/2022/3138851},
  ABSTRACT = {At present, the existing music classification and recognition algorithms have the problem of low accuracy. Therefore, this paper proposes a music recognition and classification algorithm considering the characteristics of audio emotion. Firstly, the emotional features of music are extracted from the feedforward neural network and parameterized with the mean square deviation. Gradient descent learning algorithm is used to train audio emotion features. The neural network models of input layer, output layer, and hidden layer are established to realize the classification and recognition of music emotion. Experimental results show that the algorithm has good effect on music emotion classification. The data stream driven by the algorithm is higher than 55 MBbs, the anti-attack ability is 91%, the data integrity is 83%, the average accuracy is 85%, and it has good effectiveness and feasibility. © 2022 Wang Na and Fang Yong.},
  KEYWORDS = {Audio acoustics; Gradient methods; Multilayer neural networks; Classification algorithm; Classification and recognition; Emotion feature; Gradient descent learning algorithm; Mean-square deviation; Music classification; Music recognition; Neural network model; Parameterized; Recognition algorithm; Music},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85124103186&doi = 10.1155%2f2022%2f3138851&partnerID = 40&md5 = b3cf6939f5b991ba48c037de5319363b},
  NOTES.TE = {include, although some quality problems},
  PARADIGM = {classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{nag2022on,
  AUTHOR = {Nag, Sayan and Basu, Medha and Sanyal, Shankha and Banerjee, Archi and Ghosh, Dipak},
  JOURNAL = {Physica A: Statistical Mechanics and its Applications},
  NOTE = {Cited by: 19},
  TITLE = {On the application of deep learning and multifractal techniques to classify emotions and instruments using Indian Classical Music},
  TYPE = {Article},
  VOLUME = {597},
  YEAR = {2022},
  NOTES.CA = {include, classification},
  DOI = {10.1016/j.physa.2022.127261},
  ABSTRACT = {Music is often considered as the language of emotions. The way it stimulates the emotional appraisal across people from different communities, culture and demographics has long been known and hence categorizing on the basis of emotions is indeed an intriguing basic research area. Indian Classical Music (ICM) is famous for its ambiguous nature, i.e. its ability to evoke a number of mixed emotions through only a single musical narration, and hence classifying evoked emotions from ICM becomes a more challenging task. With the rapid advancements in the field of Deep Learning, this Music Emotion Recognition (MER) task is becoming more and more relevant and robust, hence can be applied to one of the most challenging test case i.e. classifying emotions elicited from ICM. In this paper we present a new dataset called JUMusEmoDB which presently has 1600 audio clips (approximately 30 s each) where 400 clips each correspond to happy, sad, calm and anxiety emotional scales. The initial annotations and emotional classification of the database was done based on an emotional rating test (5-point Likert scale) performed by 100 participants. The clips have been taken from different conventional ‘raga’ renditions played in two Indian stringed instruments – sitar and sarod by eminent maestros of ICM and digitized in 44.1 kHz frequency. The ragas, which are unique to ICM, are described as musical structures capable of inducing different moods or emotions. For supervised classification purposes, we have used Convolutional Neural Network (CNN) based architectures (resnet50, mobilenet v2.0, squeezenet v1.0 and a proposed ODE-Net) on corresponding music spectrograms of the 6400 sub-clips (where every clip was segmented into 4 sub-clips) which contain both time as well as frequency domain information. Along with emotion classification, instrument classification based response was also attempted on the same dataset using the CNN based architectures. In this context, a nonlinear technique, Multifractal Detrended Fluctuation Analysis (MFDFA) was also applied on the musical clips to classify them on the basis of complexity values extracted from the method. The initial classification accuracy obtained from the applied methods are quite inspiring and have been corroborated with ANOVA results to determine the statistical significance. This type of CNN based classification algorithm using a rich corpus of Indian Classical Music is unique even in the global perspective and can be replicated in other modalities of music also. The link to this newly developed dataset has been provided in the dataset description section of the paper. This dataset is still under development and we plan to include more data containing other emotional as well as instrumental entities into consideration. © 2022 Elsevier B.V.},
  KEYWORDS = {Classification (of information); Convolutional neural networks; Deep learning; Frequency domain analysis; Music; Network architecture; Convolutional neural network; Emotion; Emotion recognition; Indian classical music; Learning techniques; Multifractal detrended fluctuation analysis; Multifractal technique; Music emotions; Network-based architectures; Research areas; Fractals},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85127732290&doi = 10.1016%2fj.physa.2022.127261&partnerID = 40&md5 = 634cc77ee93e9ac834ca0dad8b815119},
  NOTES.TE = {include, basic emotion classification},
  AUTHOR_KEYWORDS = {Classification; CNN; Emotions; Indian Classical Music; Instruments; MFDFA},
  PARADIGM = {classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{nguyen2017an,
  AUTHOR = {Nguyen, Van Loi and Kim, Donglim and Ho, Van Phi and Lim, Younghwan},
  JOURNAL = {International Journal of Electrical and Computer Engineering},
  NOTE = {Cited by: 14},
  NUMBER = {3},
  PAGES = {1246 – 1254},
  TITLE = {A new recognition method for visualizing music emotion},
  TYPE = {Article},
  VOLUME = {7},
  YEAR = {2017},
  NOTES.CA = {include, classification},
  DOI = {10.11591/ijece.v7i3.pp1246-1254},
  ABSTRACT = {This paper proposes an emotion detection method using a combination of dimensional approach and categorical approach. Thayer's model is divided into discrete emotion sections based on the level of arousal and valence. The main objective of the method is to increase the number of detected emotions which is used for emotion visualization. To evaluate the suggested method, we conducted various experiments with supervised learning and feature selection strategies. We collected 300 music clips with emotions annotated by music experts. Two feature sets are employed to create two training models for arousal and valence dimensions of Thayer's model. Finally, 36 music emotions are detected by proposed method. The results showed that the suggested algorithm achieved the highest accuracy when using RandomForest classifier with 70% and 57.3% for arousal and valence, respectively. These rates are better than previous studies. © 2017 Institute of Advanced Engineering and Science.},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85021156960&doi = 10.11591%2fijece.v7i3.pp1246-1254&partnerID = 40&md5 = c3ac2cfa0f3a859b3232ebd8875ba59b},
  NOTES.TE = {include},
  AUTHOR_KEYWORDS = {Feature extraction; Music emotion recognition algorithm; Music information retrieval; Music mood detection},
  PARADIGM = {classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{panda2020no,
  AUTHOR = {Panda, Renato and Malheiro, Ricardo and Paiva, Rui Pedro},
  JOURNAL = {IEEE Transactions on Affective Computing},
  NOTE = {Cited by: 83; All Open Access, Green Open Access},
  NUMBER = {4},
  PAGES = {614 – 626},
  TITLE = {Novel Audio Features for Music Emotion Recognition},
  TYPE = {Article},
  VOLUME = {11},
  YEAR = {2020},
  NOTES.CA = {include, quadrant classification},
  DOI = {10.1109/TAFFC.2018.2820691},
  ABSTRACT = {This work advances the music emotion recognition state-of-the-art by proposing novel emotionally-relevant audio features. We reviewed the existing audio features implemented in well-known frameworks and their relationships with the eight commonly defined musical concepts. This knowledge helped uncover musical concepts lacking computational extractors, to which we propose algorithms - namely related with musical texture and expressive techniques. To evaluate our work, we created a public dataset of 900 audio clips, with subjective annotations following Russell's emotion quadrants. The existent audio features (baseline) and the proposed features (novel) were tested using 20 repetitions of 10-fold cross-validation. Adding the proposed features improved the F1-score to 76.4 percent (by 9 percent), when compared to a similar number of baseline-only features. Moreover, analysing the features relevance and results uncovered interesting relations, namely the weight of specific features and musical concepts to each emotion quadrant, and warrant promising new directions for future research in the field of music emotion recognition, interactive media, and novel music interfaces. © 2010-2012 IEEE.},
  KEYWORDS = {Feature extraction; Speech recognition; Textures; 10-fold cross-validation; Affective Computing; Audio database; Emotion recognition; Interactive media; Music information retrieval; Music interfaces; Musical concepts; Audio acoustics},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85044777756&doi = 10.1109%2fTAFFC.2018.2820691&partnerID = 40&md5 = 6ff59839d539ee19db6d2a75a843d7d6},
  NOTES.TE = {include, classification},
  AUTHOR_KEYWORDS = {Affective computing; audio databases; emotion recognition; feature extraction; music information retrieval},
  PARADIGM = {classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{priscillajoy2023mu,
  AUTHOR = {Priscilla Joy, R. and Roshni Thanka, M. and Sangeetha, Dr. and Malar Dhas, Julia Punitha and Edwin, E. Bijolin and Ebenezer},
  JOURNAL = {International Journal of Intelligent Systems and Applications in Engineering},
  NOTE = {Cited by: 1},
  NUMBER = {2},
  PAGES = {904 – 911},
  TITLE = {Music Mood Based Recognition System Based on Machine Learning and Deep Learning},
  TYPE = {Article},
  VOLUME = {11},
  YEAR = {2023},
  NOTES.CA = {Unsure, classification, possible quality issues},
  ABSTRACT = {There are extensive studies about music’s impact on human’s emotional state. Humans detect a wide range of emotions from various genres of music, and music plays an integral role in personality development and the treatment of ailments. Music has tremendous effects on human moods and thoughts. Consequently, it impacts cognitive and biological health, and the concept of well-being through music is acquiring traction. In the treatment of depression, music therapy gets witnessed as an addendum to psychoanalysis. Music can enhance intellectual and physical work, study, sports, relaxation, relieve fatigue, and music therapy, among other things. People often get confused while searching for music according to their interests and mood. Individuals usually listen to a particular genre or performer when they are in a certain mood. Music has the ability to control mood, specifically to boost energy, and reduce anxiety. Listening to the correct song at the opportune timing, may help with mental health. As a result, human mood changes and music have an interdependent affinity. In this paper, we aim to develop an application that can understand facial features (Mood and Emotions) and recommend music accordingly using Machine Learning and Deep Learning as tools and algorithms. © 2023, Ismail Saritas. All rights reserved.},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85165961875&partnerID = 40&md5 = a64b5b04b8094d3750a5f90a23c6df02},
  NOTES.TE = {exclude, no outcome measures, no features identified},
  AUTHOR_KEYWORDS = {Deep Learning; Facial Recognition; Machine Learning; Mood; Music; OpenCV},
  PARADIGM = {classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{sorussa2020em,
  AUTHOR = {Sorussa, Kanawat and Choksuriwong, Anant and Karnjanadecha, Montri},
  JOURNAL = {ECTI Transactions on Computer and Information Technology},
  NOTE = {Cited by: 4; All Open Access, Gold Open Access},
  NUMBER = {1},
  PAGES = {53 – 66},
  TITLE = {Emotion classi cation system for digital music with a cascaded technique},
  TYPE = {Article},
  VOLUME = {14},
  YEAR = {2020},
  NOTES.CA = {include, classification},
  DOI = {10.37936/ecti-cit.2020141.205317},
  ABSTRACT = {Music selection is diffcult without effcient organization based on metadata or tags, and one effective tag scheme is based on the emotion expressed by the music. However, manual annotation is labor intensive and unstable because the perception of music emotion varies from person to person. This paper presents an emotion classi cation system for digital music with a resolution of eight emotional classes. Russell’s emotion model was adopted as common ground for emotional annotation. The music information retrieval (MIR) toolbox was employed to extract acoustic features from audio les. The classi cation system utilized a supervised machine learning technique to recognize acoustic features and create predictive models. Four predictive models were proposed and compared. The models were composed by crossmatching two types of neural networks, the Levenberg-Marquardt (LM) and resilient backpropagation (Rprop), with two types of structures: a traditional multiclass model and the cascaded structure of a binary-class model. The performance of each model was evaluated via the MediaEval Database for Emotional Analysis (DEAM) benchmark. The best result was achieved by the model trained with the cascaded Rprop neural network (accuracy of 89.5%). In addition, correlation coeffcient analysis showed that timbre features were the most impactful for prediction. Our work offers an opportunity for a competitive advantage in music classi cation because only a few music providers currently tag music with emotional terms. © 2020, ECTI Association Sirindhon International Institute of Technology. All rights reserved.},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85084006024&doi = 10.37936%2fecti-cit.2020141.205317&partnerID = 40&md5 = 508dd81e9a9120b0c6e87d44e90a2d5a},
  NOTES.TE = {include, classification},
  AUTHOR_KEYWORDS = {Articial neural networks; Classi cation algorithms; Emotion recognition; Music information retrieval},
  PARADIGM = {classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{tian2023mu,
  AUTHOR = {Tian, Yuan},
  JOURNAL = {PeerJ Computer Science},
  NOTE = {Cited by: 0; All Open Access, Gold Open Access, Green Open Access},
  TITLE = {Music emotion representation based on non-negative matrix factorization algorithm and user label information},
  TYPE = {Article},
  VOLUME = {9},
  YEAR = {2023},
  NOTES.CA = {unsure, little detail about predictors, classification},
  DOI = {10.7717/peerj-cs.1590},
  ABSTRACT = {Music emotion representation learning forms the foundation of user emotion recognition, addressing the challenges posed by the vast volume of digital music data and the scarcity of emotion annotation data. This article introduces a novel music emotion representation model, leveraging the nonnegative matrix factorization algorithm (NMF) to derive emotional embeddings of music by utilizing user-generated listening lists and emotional labels. This approach facilitates emotion recognition by positioning music within the emotional space. Furthermore, a dedicated music emotion recognition algorithm is formulated, alongside the proposal of a user emotion recognition model, which employs similarity-weighted calculations to obtain user emotion representations. Experimental findings demonstrate the method's convergence after a mere 400 iterations, yielding a remarkable 47.62% increase in F1 value across all emotion classes. In practical testing scenarios, the comprehensive accuracy rate of user emotion recognition attains an impressive 52.7%, effectively discerning emotions within seven emotion categories and accurately identifying users' emotional states. © Copyright 2023 Tian},
  KEYWORDS = {Matrix algebra; Non-negative matrix factorization; Social networking (online); Speech recognition; Depression; Emotion recognition; Emotion representation; Emotional label; Label information; Music emotions; Non-negative matrix factorization algorithms; Social media; User emotions; User labels; Emotion Recognition},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85173055873&doi = 10.7717%2fpeerj-cs.1590&partnerID = 40&md5 = ab056c686a7baf460d0ef7d1868a6763},
  NOTES.TE = {unsure, classification, but serious doubts about the quality of data},
  AUTHOR_KEYWORDS = {Depression; Emotional labels; Music; NMF; Social media},
  PARADIGM = {classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{tian2023mua,
  AUTHOR = {Tian, Rui and Yin, Ruheng and Gan, Feng},
  JOURNAL = {Data Technologies and Applications},
  NOTE = {Cited by: 1},
  NUMBER = {5},
  PAGES = {719 – 733},
  TITLE = {Music sentiment classification based on an optimized CNN-RF-QPSO model},
  TYPE = {Article},
  VOLUME = {57},
  YEAR = {2023},
  NOTES.CA = {include, classification},
  DOI = {10.1108/DTA-07-2022-0267},
  ABSTRACT = {Purpose: Music sentiment analysis helps to promote the diversification of music information retrieval methods. Traditional music emotion classification tasks suffer from high manual workload and low classification accuracy caused by difficulty in feature extraction and inaccurate manual determination of hyperparameter. In this paper, the authors propose an optimized convolution neural network-random forest (CNN-RF) model for music sentiment classification which is capable of optimizing the manually selected hyperparameters to improve the accuracy of music sentiment classification and reduce labor costs and human classification errors. Design/methodology/approach: A CNN-RF music sentiment classification model is designed based on quantum particle swarm optimization (QPSO). First, the audio data are transformed into a Mel spectrogram, and feature extraction is conducted by a CNN. Second, the music features extracted are processed by RF algorithm to complete a preliminary emotion classification. Finally, to select the suitable hyperparameters for a CNN, the QPSO algorithm is adopted to extract the best hyperparameters and obtain the final classification results. Findings: The model has gone through experimental validations and achieved a classification accuracy of 97 per cent for different sentiment categories with shortened training time. The proposed method with QPSO achieved 1.2 and 1.6 per cent higher accuracy than that with particle swarm optimization and genetic algorithm, respectively. The proposed model had great potential for music sentiment classification. Originality/value: The dual contribution of this work comprises the proposed model which integrated two deep learning models and the introduction of a QPSO into model optimization. With these two innovations, the efficiency and accuracy of music emotion recognition and classification have been significantly improved. © 2023, Emerald Publishing Limited.},
  KEYWORDS = {Classification (of information); Deep learning; Emotion Recognition; Extraction; Feature extraction; Music; Particle swarm optimization (PSO); Wages; Classification accuracy; Convolution neural network; Emotion classification; Features extraction; Hyper-parameter; Particle swarm optimization models; Quanta particle swarm optimizations; Random forests; RF; Sentiment classification; Genetic algorithms},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85150013677&doi = 10.1108%2fDTA-07-2022-0267&partnerID = 40&md5 = b0a35894329ea82150a07e0b87d4e6c6},
  NOTES.TE = {exclude, sentiment analysis, cannot access article},
  AUTHOR_KEYWORDS = {Classification; CNN; Music; QPSO; RF},
  PARADIGM = {classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{tiple2022mu,
  AUTHOR = {Tiple, Bhavana and Patwardhan, Manasi},
  JOURNAL = {Multimedia Tools and Applications},
  NOTE = {Cited by: 5},
  NUMBER = {6},
  PAGES = {8853 – 8870},
  TITLE = {Multi-label emotion recognition from Indian classical music using gradient descent SNN model},
  TYPE = {Article},
  VOLUME = {81},
  YEAR = {2022},
  NOTES.CA = {include, relevant task, though reports overall classification performance,},
  DOI = {10.1007/s11042-022-11975-4},
  ABSTRACT = {Music enthusiasts are growing exponentially and based on this, many songs are being introduced to the market and stored in signal music libraries. Due to this development emotion recognition model from music contents has received increasing attention in today’s world. Of these technologies, a novel Music Emotion Recognition (MER) system is introduced to meet the ever-increasing demand for easy and efficient access to music information. Even though this system was well-developed it lacks in maintaining accuracy of the system and finds difficulty in predicting multi-label emotion type. To address these shortcomings, in this research article, a novel MER system is developed by inter-linking the pre-processing, feature extraction and classification steps. Initially, pre-processing step is employed to convert larger audio files into smaller audio frames. Afterwards, music related temporal, spectral and energy features are extracted for those pre-processed frames which are subjected to the proposed gradient descent based Spiking Neural Network (SNN) classifier. While learning SNN, it is important to determine the optimal weight values for reducing the training error so that gradient descent optimization approach is adopted. To prove the effectiveness of proposed research, proposed model is compared with conventional classification algorithms. The proposed methodology was experimentally tested using various evaluation metrics and it achieves 94.55% accuracy. Hence the proposed methodology attains a good accuracy measure and outperforms well than other algorithms. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
  KEYWORDS = {Audio acoustics; Convolutional neural networks; Music; Optimization; Speech recognition; Convolutional neural network; Emotion recognition; Gradient-descent; Multi-labels; Music emotions; Neural-networks; Short-term Fourier transform; Spectral; Spiking neural network; Temporal; Gradient methods},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85124341623&doi = 10.1007%2fs11042-022-11975-4&partnerID = 40&md5 = 94edd585a680d988f8b434e1fe74cf27},
  NOTES.TE = {exclude, i dont think this actually has emotion data, just tonic prediction},
  AUTHOR_KEYWORDS = {Convolutional neural network; Gradient descent; Short Term Fourier Transform; Spectral; Spiking neural network; Temporal},
  PARADIGM = {classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{wang2021mu,
  AUTHOR = {Wang, Yu},
  JOURNAL = {Computational Intelligence and Neuroscience},
  NOTE = {Cited by: 3; All Open Access, Gold Open Access, Green Open Access},
  TITLE = {Music Composition and Emotion Recognition Using Big Data Technology and Neural Network Algorithm},
  TYPE = {Article},
  VOLUME = {2021},
  YEAR = {2021},
  NOTES.CA = {unsure, classification, only overall accuracies reported},
  DOI = {10.1155/2021/5398922},
  ABSTRACT = {To implement a mature music composition model for Chinese users, this paper analyzes the music composition and emotion recognition of composition content through big data technology and Neural Network (NN) algorithm. First, through a brief analysis of the current music composition style, a new Music Composition Neural Network (MCNN) structure is proposed, which adjusts the probability distribution of the Long Short-Term Memory (LSTM) generation network by constructing a reasonable Reward function. Meanwhile, the rules of music theory are used to restrict the generation of music style and realize the intelligent generation of specific style music. Afterward, the generated music composition signal is analyzed from the time-frequency domain, frequency domain, nonlinearity, and time domain. Finally, the emotion feature recognition and extraction of music composition content are realized. Experiments show that: When the iteration times of the function increase, the number of weight parameter adjustments and learning ability will increase, and thus the accuracy of the model for music composition can be greatly improved. Meanwhile, when the iteration times increases, the loss function will decrease slowly. Moreover, the music composition generated through the proposed model includes the following four aspects: Sadness, joy, loneliness, and relaxation. The research results can promote music composition intellectualization and impacts traditional music composition mode. © 2021 Yu Wang.},
  KEYWORDS = {Algorithms; Big Data; Emotions; Music; Neural Networks, Computer; Technology; Big data; Frequency domain analysis; Iterative methods; Long short-term memory; Music; Probability distributions; Time domain analysis; Composition content; Composition modeling; Data technologies; Emotion recognition; Iteration time; Music composition; Music emotions; Neural networks algorithms; Paper analysis; Technology network; algorithm; emotion; music; technology; Speech recognition},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85122382786&doi = 10.1155%2f2021%2f5398922&partnerID = 40&md5 = 8526c913cb2f0b1c8962a1b7da8afb30},
  NOTES.TE = {exclude, quality control, lack of suitable outcome measures},
  PARADIGM = {classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{wang2021re,
  AUTHOR = {Wang, Daliang and Guo, Xiaowen},
  JOURNAL = {Complexity},
  NOTE = {Cited by: 7; All Open Access, Gold Open Access},
  TITLE = {Research on Intelligent Recognition and Classification Algorithm of Music Emotion in Complex System of Music Performance},
  TYPE = {Article},
  VOLUME = {2021},
  YEAR = {2021},
  NOTES.CA = {unsure, classification, unusual categories},
  DOI = {10.1155/2021/4251827},
  ABSTRACT = {In the complex system of music performance, there are differences in the expression of music emotions by listeners, so it is of great significance to study the classification of different emotions under different audio signals. In this paper, the research of human emotional intelligence recognition and classification algorithm in the complex system of music performance is proposed. Through the recognition of SVM, KNN, ANN, and ID3 classifiers, the accuracy of a single classifier is compared, and then the four classifiers are combined to compare the classification accuracy of audio signals before and after preprocessing. The results show that the accuracy of SVM and ANN fusion is the highest. Finally, recall and F1 are comprehensively compared in the fusion algorithm, and the fusion classification effect of SVM and ANN is better than that of the algorithm model. © 2021 Daliang Wang and Xiaowen Guo.},
  KEYWORDS = {Audio acoustics; Emotional intelligence; Support vector machines; Algorithm model; Classification accuracy; Classification algorithm; Fusion algorithms; Fusion classification; Intelligent recognition; Music emotions; Music performance; Computer music},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85109435136&doi = 10.1155%2f2021%2f4251827&partnerID = 40&md5 = b1866e5ba2f0b3b1ca5047cf2e22c97c},
  NOTES.TE = {exclude, quality control, lack of stimulus details},
  PARADIGM = {classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{wang2022mu,
  AUTHOR = {Wang, Chen and Zhao, Yu},
  JOURNAL = {Wireless Communications and Mobile Computing},
  NOTE = {Cited by: 1; All Open Access, Gold Open Access},
  TITLE = {Music Emotion Recognition Based on Bilayer Feature Extraction},
  TYPE = {Article},
  VOLUME = {2022},
  YEAR = {2022},
  NOTES.CA = {include, classification task},
  DOI = {10.1155/2022/7832548},
  ABSTRACT = {Music is a kind of art which is to express the thoughts and emotion and reflect the reality life by organized sounds; every piece of music expresses emotions through lyrics and melodies. Human emotions are rich and colorful, and there are also differences in music. It is unreasonable for a song to correspond to only one emotional feature. According to the results of existing algorithms, some music has obviously wrong category labels. To solve the above problems, we established a two-layer feature extraction model based on a spectrogram from shallow to deep (from primary feature extraction to deep feature extraction), which can not only extract the most basic musical features but also dig out the deep emotional features. And further classify the features with the improved CRNN neural network, and get the final music emotion category. Through a large number of comparative experiments, it is proven that our model is suitable for a music classification task. © 2022 Chen Wang and Yu Zhao.},
  KEYWORDS = {Arts computing; Classification (of information); Emotion Recognition; Extraction; Feature extraction; Bi-layer; Emotion recognition; Express emotions; Extraction modeling; Features extraction; Human emotion; Model-based OPC; Music emotions; Spectrograms; Two-layer; Music},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85134532351&doi = 10.1155%2f2022%2f7832548&partnerID = 40&md5 = 52ee0d7f530c4b616baf2965109291f8},
  NOTES.TE = {unsure, no N given of musical excertpts, other missing information},
  PARADIGM = {classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{wang2022mua,
  AUTHOR = {Wang, Jingjing and Huang, Ru},
  JOURNAL = {Huadong Ligong Daxue Xuebao/Journal of East China University of Science and Technology},
  NOTE = {Cited by: 2},
  NUMBER = {3},
  PAGES = {373 – 380},
  TITLE = {Music Emotion Recognition Based on the Broad and Deep Learning Network; [基于宽深学习网络的音乐情感识别]},
  TYPE = {Article},
  VOLUME = {48},
  YEAR = {2022},
  NOTES.CA = {unsure, cannot access full text, classification},
  DOI = {10.14135/j.cnki.1006-3080.20210225007},
  ABSTRACT = {With the development of artificial intelligence and digital audio technology, music information retrieval (MIR) has gradually become a research hotspot. Meanwhile, music emotion recognition (MER) is becoming an important research direction, due to its great research value for video soundtracks. Although some researchers combine Mel Frequency Cepstral coefficient (MFCC) and Residual Phase (RP) to extract music emotional features and improve classification accuracy, the training models in traditional deep learning takes longer time. In order to improve the efficiency of feature mining of music emotional features, MFCC and RP are weighted and combined in this work to extract music emotion features so that the mining efficiency of music emotion features can be effectively improved. At the same time, in order to improve the classification accuracy of music emotion and shorten the training time of the model, by integrating the Long Short-Term Memory (LSTM) and the Broad Learning System (BLS), a new wide and deep learning network (LSTM-BLS) is further built to train music emotion recognition and classification by using LSTM as the feature mapping node of BLS. The network structure of this model makes full use of the ability of BLS to quickly process complex data. Its advantages are simple structure and short model training time, thereby improving recognition efficiency, and LSTM has excellent performance in extracting time series features from time series data. The time sequence relationship of music can be extracted so that the emotional characteristics of the music can be preserved to the greatest extent. Finally, the experimental results on the emotion dataset show that the proposed algorithm can achieve higher recognition accuracy than other complex networks and provide new feasible ideas for the music emotion recognition. © 2022 East China University of Science and Technology. All rights reserved.},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85167579262&doi = 10.14135%2fj.cnki.1006-3080.20210225007&partnerID = 40&md5 = dd8ef11135681629c977177fd3d05597},
  NOTES.TE = {exclude, in chinese only},
  AUTHOR_KEYWORDS = {broad learning; deep learning; long short-term memory; music emotion recognition; residual phase},
  PARADIGM = {classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{yang2021an,
  AUTHOR = {Yang, Jing},
  JOURNAL = {Frontiers in Psychology},
  NOTE = {Cited by: 14; All Open Access, Gold Open Access, Green Open Access},
  TITLE = {A Novel Music Emotion Recognition Model Using Neural Network Technology},
  TYPE = {Article},
  VOLUME = {12},
  YEAR = {2021},
  NOTES.CA = {include, classification},
  DOI = {10.3389/fpsyg.2021.760060},
  ABSTRACT = {Music plays an extremely important role in people’s production and life. The amount of music is growing rapidly. At the same time, the demand for music organization, classification, and retrieval is also increasing. Paying more attention to the emotional expression of creators and the psychological characteristics of music are also indispensable personalized needs of users. The existing music emotion recognition (MER) methods have the following two challenges. First, the emotional color conveyed by the first music is constantly changing with the playback of the music, and it is difficult to accurately express the ups and downs of music emotion based on the analysis of the entire music. Second, it is difficult to analyze music emotions based on the pitch, length, and intensity of the notes, which can hardly reflect the soul and connotation of music. In this paper, an improved back propagation (BP) algorithm neural network is used to analyze music data. Because the traditional BP network tends to fall into local solutions, the selection of initial weights and thresholds directly affects the training effect. This paper introduces artificial bee colony (ABC) algorithm to improve the structure of BP neural network. The output value of the ABC algorithm is used as the weight and threshold of the BP neural network. The ABC algorithm is responsible for adjusting the weights and thresholds, and feeds back the optimal weights and thresholds to the BP neural network system. BP neural network with ABC algorithm can improve the global search ability of the BP network, while reducing the probability of the BP network falling into the local optimal solution, and the convergence speed is faster. Through experiments on public music data sets, the experimental results show that compared with other comparative models, the MER method used in this paper has better recognition effect and faster recognition speed. © Copyright © 2021 Yang.},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85116855247&doi = 10.3389%2ffpsyg.2021.760060&partnerID = 40&md5 = d8babd4e6050f14375d2b4632dfdb587},
  NOTES.TE = {include, classification},
  AUTHOR_KEYWORDS = {ABC algorithm; BP neural network; emotion recognition; MediaEval Emotion in Music data set; music},
  PARADIGM = {classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{yeh2014po,
  AUTHOR = {Yeh, Chia-Hung and Tseng, Wen-Yu and Chen, Chia-Yen and Lin, Yu-Dun and Tsai, Yi-Ren and Bi, Hsuan-I and Lin, Yu-Ching and Lin, Ho-Yi},
  JOURNAL = {Multimedia Tools and Applications},
  NOTE = {Cited by: 11},
  NUMBER = {3},
  PAGES = {2103 – 2128},
  TITLE = {Popular music representation: chorus detection & emotion recognition},
  TYPE = {Article},
  VOLUME = {73},
  YEAR = {2014},
  NOTES.CA = {include, classification},
  DOI = {10.1007/s11042-013-1687-2},
  ABSTRACT = {This paper proposes a popular music representation strategy based on the song’s emotion. First, a piece of popular music is decomposed into chorus and verse segments through the proposed chorus detection algorithm. Three descriptive features: intensity, frequency band and rhythm regularity are extracted from the structured segments for emotion detection. A hierarchical Adaboost classifier is employed to recognize the emotion of a piece of popular music. The general emotion of the music is classified according to Thayer’s model into four emotions: happy, angry, depressed and relaxed. Experiments conducted on a 350-popular-music database show the average recall and precision of our proposed chorus detection are approximately 95 % and 84 %, respectively; and the average precision rate of emotion detection is 92 %. Additional tests are performed on songs with cover versions in different lyrics and languages, and the resultant precision rate is 90 %. The proposes approaches have been tested and proven by the professional online music company, KKBOX Inc. and show promising performance for effectively and efficiently identifying the emotions of a variety of popular music. © 2013, Springer Science+Business Media New York.},
  KEYWORDS = {Multimedia systems; Chorus; Emotion; MFCCs; Popular music; Rhythm; Verse; Adaptive boosting},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-84912027522&doi = 10.1007%2fs11042-013-1687-2&partnerID = 40&md5 = 6ce0fe7a0b38a6e2037403e140bd0a60},
  NOTES.TE = {include, or unclear, report classification broadly/vaguely},
  AUTHOR_KEYWORDS = {Adaboost; Chorus; Emotion; MFCCs; Popular music; Rhythm; Verse},
  PARADIGM = {classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{zhang2017fe,
  AUTHOR = {Zhang, Jiang Long and Huang, Xiang Lin and Yang, Li Fang and Xu, Ye and Sun, Shu Tao},
  JOURNAL = {Multimedia Systems},
  NOTE = {Cited by: 10},
  NUMBER = {2},
  PAGES = {251 – 264},
  TITLE = {Feature selection and feature learning in arousal dimension of music emotion by using shrinkage methods},
  TYPE = {Article},
  VOLUME = {23},
  YEAR = {2017},
  NOTES.CA = {include, classification},
  DOI = {10.1007/s00530-015-0489-y},
  ABSTRACT = {Music emotion recognition is an important topic in music information retrieval area. A lot of acoustic features are used to train a music classification or regression emotion model. However, these existing features may not be efficient for classification or regression task. Furthermore, most works do not explain why these features do work for classification. In our work, eight features are extracted to represent the arousal dimension of music emotion, and various commonly used statistical learning methods such as Logistic Regression, and tree-based methods are applied to interpret important features. Then the shrinkage methods are applied to feature selection and classification in music emotion recognition for the first time. Our tests show that the proposed approaches are efficient for feature selection just as entropy-based filter methods, and better than wrapper methods. The shrinkage methods can produce more continuous and low variance model than wrapper methods. Then, we discover that the most useful features are low specific loudness sensation coefficients (low-SONE), root mean square and loudness-flux. Moreover, the shrinkage methods apply in logistic regression perform better for classification than most of other methods. We get an average accuracy rate of 83.8 %. © 2015, Springer-Verlag Berlin Heidelberg.},
  KEYWORDS = {Feature extraction; Regression analysis; Shrinkage; Speech recognition; Feature selection and classification; Features learning; Features selection; Music classification; Music information retrieval; Shrinkage methods; Statistical learning; Statistical learning methods; Classification (of information)},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-84944937222&doi = 10.1007%2fs00530-015-0489-y&partnerID = 40&md5 = f12764992ee29a4976644aacf6bbb43d},
  NOTES.TE = {include},
  AUTHOR_KEYWORDS = {Features learning; Features selection; Music arousal dimension classification; Shrinkage method; Statistical learning},
  PARADIGM = {classification},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{alakus2020em,
  AUTHOR = {Alaku{\c s}, Talha and T{\"u}rko{\u g}lu, İ̇brahim},
  JOURNAL = {Electronics letters},
  MONTH = {},
  NUMBER = {25},
  PAGES = {1364--1367},
  PUBLISHER = {Institution of Engineering and Technology},
  TITLE = {Emotion recognition with deep learning using GAMEEMO data set},
  VOLUME = {56},
  YEAR = {2020},
  NOTES.CA = {exclude, eeg},
  DOI = {10.1049/el.2020.2460},
  ABSTRACT = {Electronics LettersVolume 56, Issue 25 p. 1364-1367 Special Issue: Current Trends in Cognitive Science and Brain Computing Research and ApplicationsFree Access Emotion recognition with deep learning using GAMEEMO data set T. B. Alakus, Corresponding Author T. B. Alakus talhaburakalakus{\char64}klu.edu.tr orcid.org/0000-0003-3136-3341 Kirklareli University, Faculty of Engineering, Department of Software Engineering, Kirklareli, TurkeySearch for more papers by this authorI. Turkoglu, I. Turkoglu Firat University, Faculty of Technology, Elazig, TurkeySearch for more papers by this author T. B. Alakus, TurkeySearch for more papers by this author First published: 22 October 2020 https://doi.org/10.1049/el.2020.2460Citations: 10AboutSectionsPDF ToolsRequest permissionExport citationAdd to favoritesTrack citation ShareShare Give accessShare full text accessShare full-text accessPlease review our Terms and Conditions of Use and check box below to share full-text version of article.I have read and accept the Wiley Online Library Terms and Conditions of UseShareable LinkUse the link below to share a full-text version of this article with your friends and colleagues. Learn more.Copy URL Share a linkShare onFacebookTwitterLinkedInRedditWechat Abstract Emotion recognition is actively used in brain--computer interface, health care, security, e-commerce, education and entertainment applications to increase and control human--machine interaction. Therefore, emotions affect people's lives and decision-making mechanisms throughout their lives. However, the fact that emotions vary from person to person, being an abstract concept and being dependent on internal and external factors makes the studies in this field difficult. In recent years, studies based on electroencephalography (EEG) signals, which perform emotion analysis in a more robust and reliable way, have gained momentum. In this article, emotion analysis based on EEG signals was performed to predict positive and negative emotions. The study consists of four parts. In the first part, EEG signals were obtained from the GAMEEMO data set. In the second stage, the spectral entropy values of the EEG signals of all channels were calculated and these values were classified by the bidirectional long-short term memory architecture in the third stage. In the last stage, the performance of the deep-learning architecture was evaluated with accuracy, sensitivity, specificity and receiver operating characteristic (ROC) curve. With the proposed method, an accuracy of 76.91{\%} and a ROC value of 90{\%} were obtained. Introduction Emotion can be defined as the voluntary or involuntary reaction of people against an external stimulus while performing actions such as talking, thinking, communicating, learning, making decisions etc. Since all these and similar actions are carried out through emotions, emotions have a great impact on daily life. While negative emotions affect people both physically and psychologically, positive emotions make people more successful in society and bring better living conditions {$[$}1, 2 {$]$}. There are many different emotion analysis studies in order to comprehend the nature and behaviour of emotions. However, the fact that the concept of emotion is abstract and does not have an objective result makes it difficult to analyse emotions {$[$}3, 4 {$]$}. In addition, a large number of methods to collect and process emotion data makes the analysis process more difficult and time-consuming. For these reasons, a computer-based system is needed {$[$}5 {$]$}. Emotions can be obtained through physical and non-physical methods. Examples of these include voice signals, body language, facial expressions and physical activities. Since these methods are easy to apply, data can be obtained quickly and easily. However, during the data collection phase, emotions can be manipulated intentionally or unintentionally by the subjects. While the voice signals are collected, subjects can imitate their voices and similarly hide their facial expressions {$[$}6 {$]$}. Therefore, the fact that the data obtained by these methods are both incomplete and untrustworthy caused the need for a more reliable system and increased the importance of physiological signals such as electroencephalography (EEG) {$[$}7 {$]$}. EEG signals are the most widely used method in this area because of their ease of use, low cost and portable models {$[$}8 {$]$}. There are two types of emotional patterns in the literature, discrete and dimensional. There are eight basic emotions (anger, joy, trust, fear, surprise, sadness, disgust and anticipation) in the discrete emotion model {$[$}9 {$]$}. In the dimensional model, emotions are expressed not by their names but according to their positions in the arousal--valence plane {$[$}10 {$]$}. In this plane, emotions are divided into four main areas. Valence axis refers to the x -axis, and this axis indicates whether the emotion is negative or positive. Y -axis expresses arousal and emotions are ordered from low to high according to the degree of activity. The arousal--valence plane is given in Fig. 1. The plane is divided into four different zones, as can be seen in Fig. 1. While there are high arousal positive valence emotions in the first zone, there are high valence negative emotions in the second zone. In the third and fourth zones, there are emotions of negative valence-low arousal and positive valence-low arousal, respectively. In this model, emotions are named according to their location in the coordinate plane rather than their names. For example, the emotion of happiness is expressed as high arousal positive valence. Similar inferences can be made for other types of emotions. In this study, dimensional emotion model was used and emotions were evaluated as positive-valence and negative-valence. Fig. 1Open in figure viewerPowerPoint Example of arousal--valence dimensional emotion model The study consists of four stages. In the first step, EEG signals were collected from the GAMEEMO data set. In the second step, the spectral entropy values of each EEG signal were calculated. Then these values were classified with the bidirectional long-short term memory (BiLSTM) deep-learning model and the prediction process was carried out. In the last stage, the performance of the BiLSTM model was measured with different evaluation metrics. The main contributions of the study can be summarised as follows: To the best of our knowledge, the GAMEEMO data set was analysed for the first time in this study with the BiLSTM deep-learning model. With this study, it was observed that EEG signals obtained from a portable device can also be used for emotion analysis. The rest of the work is organized as follows: studies conducted with EEG signals are mentioned in the related works section. In the data and methods section, general information about the data set, the spectral entropy and BiLSTM model used in this study are given. In the application results section, the performance of the BiLSTM model was examined and the results were discussed. In the conclusion section, the study was examined and explanations were made based on possible future applications. Related works In this section, emotion analysis studies performed with EEG signals are examined. The authors in {$[$}11 {$]$} used the LSTM model for emotion prediction and classified the EEG signals for it. The DEAP data set was used in the study and a classification process was performed for valence, arousal and liking classes. The signals have not been preprocessed and classified directly in the LSTM architecture. The study was validated with four-fold cross-validation and the performance of the LSTM model was evaluated with the accuracy metric only. At the end of the study, the accuracy of 0.8565 for arousal, 0.8545 for valence and 0.8799 for liking was obtained. Authors in {$[$}12 {$]$} carried out emotion analysis using deep learning. The DEAP data set was used in the study and the feature extraction was carried out before classification. In the feature extraction phase, the signals were transformed with empirical model decomposition and variational model decomposition and the power spectral density, and the first difference values of intrinsic mode functions were collected from these converted signals. Then the signals were classified with both support vector machines (SVMs) and deep neural network. After the classification process, accuracies of 0.6125 for arousal and 0.6250 for valence were reached. The authors in {$[$}13 {$]$} performed emotion analysis with principal component analysis and deep learning. As in other studies, the DEAP data set was used in this study. The signals were first transformed into five different bands with fast Fourier transform and power spectral values were obtained from each band. Then the values were normalised and classified with deep-learning network. Valence emotions were predicted with 0.5342 and arousal emotions with 0.5205 accuracies with the proposed method. Data and methods In this study, EEG signals belonging to the GAMEEMO {$[$}14 {$]$} data set are used. The data set contains EEG signals of 28 people. Unlike conventional EEG collecting devices, the data were obtained with a portable EEG device (Emotiv EPOC + 14-Channel Wireless EEG Headset). The EEG device used has 14 channels in total as AF3, AF4, F3, F4, F7, F8, FC5, FC6, O1, O2, P7, P8, T7 and T8. The sampling rate of the obtained signals is 128 Hz. The data set contains raw and preprocessed signals. Since noise-free data were used in this study, pre-processed data were considered. In order to obtain emotions, the subjects played four computer games and each subject played games for 5 min. There are a total of 1568 (4 ×14 ×28) EEG data in the data set. The number 4 refers to the stimuli used. This value is 4 because 4 games were played in the data set. The number 14 indicates the number of EEG channels, while number 28 refers to the subjects. Sample length of each EEG data is 38, 252. More technical and detailed information about the data set can be obtained from {$[$}14 {$]$}. Researchers who want to use GAMEEMO data can access the data from the link provided (https://data.mendeley.com/datasets/b3pn4kwpmn/3 ). In the study, feature extraction was carried out and spectral entropy values of EEG signals were calculated. Spectral entropy measures how sharp the spectrum of a signal is {$[$}15 {$]$}. A signal with a sharp spectrum, such as the sum of sinusoids, has low spectral entropy. In contrast, a flat spectrum signal such as white noise has high spectral entropy. Spectral entropy treats the normalised power distribution in the frequency domain of the signal as a probability distribution and calculates the Shannon entropy. Shannon entropy in this context is the spectral entropy of the signal. Spectral entropy is effectively used in fault detection and diagnosis {$[$}16, 17 {$]$}, speech recognition {$[$}18 {$]$} and biomedical signal processing {$[$}19 {$]$}. In this study, the spectral entropy value of each EEG signal was calculated and these values were used to classify with BiLSTM. In this study, recurrent neural network was used instead of traditional CNN architectures because of their success in time series applications {$[$}20-22 {$]$}. For this reason, a recurrent neural network model --bidirectional LSTM, was used in the proposed study. Bidirectional LSTMs are an extension of the LSTM model and have been proposed to improve model performance in classification problems. In the BiLSTM architecture, input values train two LSTMs instead of one. Therefore, information flows both from the past to the future and from the future to the past. In traditional LSTM architectures, information from the future is evaluated and preserved, while in BLSTM architecture, information from both the past and the future is preserved and valued. Owing to this advantage, BiLSTM is more successful than LSTM {$[$}23 {$]$}. Thus, BiLSTM was considered in the study. The graphical abstract of the study is given in Fig. 2. Fig. 2Open in figure viewerPowerPoint Flow chart of the study Application results In this study, the EEG signals of the GAMEEMO data set were classified and positive--negative emotions were predicted. BiLSTM was used for the classification process and the performance of the deep-learning model was measured with accuracy, specificity and receiver operating characteristic (ROC) values. The parameters of the developed BiLSTM model can be summarised as follows: EEG data, whose spectral entropy values were calculated, were used in the input layer. Then the 128-unit BiLSTM layer was designed. ReLU function was used as an activation function. Then, the data were transformed into a one-dimensional vector by the flattening process. Later, the batch normalisation was performed and the data were normalised. Dropout was used to prevent overfitting problem and its degree was set to 0.25. Finally, a fully connected layer was designed and the number of neurons was determined as 512. In the classification layer, the sigmoid function has been used and the binary classification process has been made. Stochastic gradient descent was applied as an optimiser with default values. The loss of the model was calculated by binary cross-entropy. The epoch value was chosen to be 250. To validate the model, the train-test split approach was used and 80{\%} of the data was used for training and 20{\%} for testing. All of these parameters were determined by trial and error approach and the parameters giving the best result were used in the study. Table 1 shows the classification results of the BiLSTM model. Table 1. Classification results of positive and negative emotions Accuracy, {\%} Sensitivity, {\%} Specificity, {\%} ROC 76.91 76.93 76.89 0.90 As seen in Table 1, positive and negative emotions were classified with an accuracy rate of 0.7691 with the proposed BiLSTM model. In addition, the sensitivity value was 0.7693 and the specificity value was 0.7689. ROC value was measured as 0.90 for both classes. The graph of the ROC is given in Fig. 3. The area under the curve (AUC) score is used effectively in biomedical studies and is expressed as a better analysis {$[$}24 {$]$}. In order for the classification process to be considered good, the AUC score must be > 0.8 {$[$}24 {$]$}. Furthermore, the AUC score between 0.9 and 1.0 indicates that the classification is excellent {$[$}24 {$]$}. In this study, the AUC score was calculated as 0.9, indicating that the proposed method is effective and successful. Fig. 3Open in figure viewerPowerPoint ROC curve of positive and negative emotions (class 0 refers to negative emotions, class 1 refers to positive emotions) These results were also compared with the machine-learning algorithm results used in the original article. The comparison results are given in Table 2. Since there is only one study in the literature with this data set, only the results in the original article could be examined. According to the results given in Table 2, it is seen that the proposed deep-learning method was better than the machine-learning algorithms used in the original study. While 73 and 66{\%} accuracy values were achieved for SVM and KNN, respectively, this rate increased to ∼77{\%} with the BiLSTM model. According to these results, it can be inferred that the deep-learning method is at least as successful and even better as existing machine-learning methods. Table 2. Comparison of classification results Reference SVM (accuracy) KNN (accuracy) BiLSTM (accuracy) {$[$}14 {$]$} 73{\%} 66{\%} ---the proposed method ------76.93{\%} Conclusion In this study, positive and negative emotions were analysed using the EEG data of the GAMEEMO data set. In the first part of the study, pre-processed data were obtained from the data set. Then, spectral entropy values were collected from the data of each EEG channel and these values were used in the BiLSTM model. In the final phase, the classification process was made with BiLSTM and the performance of the deep-learning model was measured with accuracy, specificity and ROC values. With the proposed method, 76.91{\%} accuracy, 76.93{\%} sensitivity, 76.89{\%} specificity and 90{\%} ROC values were achieved. In addition, the proposed method was compared with the machine-learning algorithms used in the original article and it was observed that the proposed method was at least as successful as them. In the future, this data set will be examined in more detail and comparisons will be made using different deep-learning algorithms and signal processing methods. Emotions will be examined with both binary-class classification and multi-class classification. Emotions are of great importance in human life. In daily life, we use our emotions intentionally or unintentionally. Therefore, emotion analysis studies are important for understanding emotions and determining their behaviour. References 1Naji M. Firoozabadi M. Azadfallah P.: 'Emotion classification during music listening from forehead biosignals ', Signal. Image. Video. Process., 2015, 9, pp. 1365--1375, doi: 10.1007/s11760-013-0591-6 2Alakus T.B. Turkoglu I.: 'EEG based emotion analysis systems ', TBV J. Comput. Sci. Eng., 2018, 11, (1 ), pp. 26--39 3Michalopoulos K. Bourbakis N.: 'Application of multiscale entropy on EEG signals for emotion detection '. IEEE EMBS Int. Conf. Information Technology Applications in Biomedicine, Orlando, FL, USA, February 2017, pp. 341--344, doi: 10.1109/BHI.2017.7897275 4Alakus T.B. Turkoglu I.: 'Feature selection with sequential forward selection algorithm from emotion estimation based on EEG signals ', Sakarya Univ. J. Sci., 2019, 23, (6 ), pp. 1096--1105, doi: 10.16984/saufenbilder.501799 5Turnip A. Simbolon A.I. Amri M.F. et al.: 'Backpropagation neural networks training for EEG-SSVEP classification of emotion recognition ', Internetwork. Indonesia J., 2017, pp. 53--57 6Alakus T.B. Turkoglu I.: 'Determination of effective EEG channels for discrimination of positive and negative emotions with wavelet decomposition and support vector machines ', Int. J. Inform. Technol., 12, (3 ), pp. 229--237 7Yan J. Chen S. Deng S.: 'A EEG-based emotion recognition model with rhythm and time characteristics ', Brain. Inform., 6, pp. 1--8, doi: 10.1186/s40708-019-0100-y 8Pan J. Li Y. Wang J.: 'An EEG-based brain-computer interface for emotion recognition '. Int. Joint Conf. Neural Networks, Vancouver, BC, Canada, July 2016, pp. 2063--2067, doi: 10.1109/IJCNN.2016.7727453 9Mason W.A. Capitanio J.P.: 'Basic emotions: a reconstruction ', Emot. Rev., 2012, 4, pp. 238--244, doi: 10.1177/1754073912439763 10Russel A.: 'Core affect and psychological construction of emotion ', Psychol. Rev., 2003, 110, pp. 145--150 11Alhagry S. Fahmy A.A. El-Khoribi R.A.: 'Emotion recognition based on EEG using LSTM recurrent neural network ', Int. J. Adv. Comput. Sci. Appl., 8, (10 ), pp. 355--358, doi: 10.14569/IJACSA.2017.081046 12Pandey P. Seeja K.R.: 'Subject independent emotion recognition from EEG using VMD and deep learning ', J. King Saud Univ. --Comput. Inform. Sci., pp. 53--58, doi: 10.1016/j.jksuci.2019.11.003 13Jirayucharoensak S. Pan-Ngum S. Israsena P.: 'EEG-based emotion recognition using deep learning network with principal component based covariate shift adaptation ', Sci. World J., 2014, pp. 1--10, 627892, doi: 10.1155/2014/627892 14Alakus T.B. Gonen M. Turkoglu I.: 'Database for an emotion recognition system based on EEG signals and various computer games --GAMEEMO ', Biomed. Signal Proc. Control, 2020, 60, pp. 1--12, doi: 10.1016/j.bspc.2020.101951 15Vanluchene A.L.G. Vereecke H. Thas O. et al.: 'Spectral entropy as an electroencephalographic measure of anesthetic drug effect: a comparison with bispectral index and processed midlatency auditory evoked response ', Anesthesiology, 2004, 101, pp. 34--42 16Pan Y.N. Chen J. Li X.L.: 'Spectral entropy: a complementary index for rolling element bearing performance degradation assessment ', Proc. Inst. Mech. Eng. C, J. Mech. Eng. Sci., 2009, 223, (5 ), pp. 1223--1231, doi: 10.1243/09544062JMES1224 17Sharma V. Parey A.: 'A review of gear fault diagnosis using various condition indicators ', Procedia Eng., 2016, 144, pp. 256--263, doi: 10.1016/j.proeng.2016.05.131 18Majstorovic N. Andric M. Mikluc D.: 'Entropy-based algorithm for speech recognition in noisy environment '. Telecommunications Forum, Belgrade, Serbia, November 2011, pp. 667--670, doi: 10.1109/TELFOR.2011.6143635 19Vakkuri A. Yli-Hankala A. Talja P. et al.: 'Time-frequency balanced spectral entropy as a measure of anesthetic drug effect in central nervous system during sevoflurane, propofol, and thiopental anesthesia ', Acta Anaesthesiol. Scand., 48, (2 ), pp. 145--153, doi: 10.1111/j.0001-5172.2004.00323.x 20Hewamalage H. Bergmeir C. Bandara K.: 'Recurrent neural networks for time series forecasting: current status and future directions ', Int. J. Forecast., pp. 1--40, doi: 10.1016/j.ijforecast.2020.06.008 21Guo T. Xu Z. Yao X. et al.: 'Robust online time series prediction with recurrent neural networks '. Int. Conf. Data Science and Advanced Analytics, Montreal, QC, October 2016, pp. 816--825, doi: 10.1109/DSAA.2016.92 22Connor J.T. Martin R.D. Atlas L.E.: 'Recurrent neural networks and robust time series prediction ', IEEE Trans. Neural Netw., 1994, 5, pp. 240--254, doi: 10.1109/72.279188 23Graves A. Fernandez S. Schmidhuber J.: 'Bidirectional LSTM networks for improved phoneme classification and recognition '. Int. Conf. Artificial Neural Networks: Formal Models and Their Applications, Warsaw, Poland, September 2005, doi: 10.1007/11550907{\_}126 24Mandrekar J.N.: 'Receiver operating characteristic curve in diagnostic test assessment ', J. Thorac. Oncol., 2010, (9 ), pp. 1315--1316, doi: 10.1097/JTO.0b013e3181ec173d Citing Literature Volume56, Issue25December 2020Pages 1364-1367 FiguresReferencesRelatedInformation},
  KEYWORDS = {Emotion Recognition; Deep Learning for EEG; EEG Analysis; Affective Computing; Speech Emotion},
  SOURCE = {open_alex},
  URL = {https://doi.org/10.1049/el.2020.2460},
  NOTES.TE = {exclude, no audio features/prediction},
  BDSK = {https://doi.org/10.1049/el.2020.2460},
  ISBN = {0013-5194},
  DATE = {2024-05-13 15:05:25 +0100},
  DATE.1 = {2024-05-13 15:05:25 +0100},
  LA = {en},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{aljanaki2017de,
  AUTHOR = {Aljanaki, Anna and Yang, Yi-Hsuan and Soleymani, Mohammad},
  JOURNAL = {PLOS ONE},
  MONTH = {MAR 10},
  NUMBER = {3},
  TITLE = {Developing a benchmark for emotional analysis of music},
  VOLUME = {12},
  YEAR = {2017},
  NOTES.CA = {exclude, benchmark dataset},
  DOI = {10.1371/journal.pone.0173392},
  SOURCE = {web_of_science},
  NOTES.TE = {include},
  BDSK = {https://doi.org/10.1371/journal.pone.0173392},
  ISSN = {1932-6203},
  ARTICLE = {e73392},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{andrade2017as,
  AUTHOR = {Andrade, Paulo E. and Vanzella, Patricia and Andrade, Olga V. C. A. and Schellenberg, E. Glenn},
  JOURNAL = {PSYCHOLOGY OF MUSIC},
  MONTH = {SEP},
  NUMBER = {5},
  PAGES = {752-760},
  TITLE = {Associating emotions with Wagner's music: A developmental perspective},
  VOLUME = {45},
  YEAR = {2017},
  NOTES.CA = {exclude, no relevant task},
  DOI = {10.1177/0305735616678056},
  SOURCE = {web_of_science},
  NOTES.TE = {exclude, no audio prediction},
  BDSK = {https://doi.org/10.1177/0305735616678056},
  ISSN = {0305-7356},
  EISSN = {1741-3087},
  ORCID = {Vanzella, Patricia M/0000-0002-7709-1495 Schellenberg, Glenn/0000-0003-3681-6020},
  RESEARCHERID = {Vanzella, Patricia M/K-6184-2016},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{bai2016di,
  AUTHOR = {Bai, Junjie and Feng, Lixiao and Peng, Jun and Shi, Jinliang and Luo, Kan and Li, Zuojin and Liao, Lu and Wang, Yingxu},
  JOURNAL = {INTERNATIONAL JOURNAL OF COGNITIVE INFORMATICS AND NATURAL INTELLIGENCE},
  MONTH = {OCT-DEC},
  NUMBER = {4},
  PAGES = {74-89},
  TITLE = {Dimensional Music Emotion Recognition by Machine Learning},
  VOLUME = {10},
  YEAR = {2016},
  NOTES.CA = {include},
  DOI = {10.4018/IJCINI.2016100104},
  SOURCE = {web_of_science},
  NOTES.TE = {include},
  BDSK = {https://doi.org/10.4018/IJCINI.2016100104},
  ISSN = {1557-3958},
  EISSN = {1557-3966},
  ORCID = {Wang, Yingxu/0000-0003-0445-3632 Peng, Jun/0000-0001-6800-0064 luo, kan/0000-0003-2317-6714},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{bao2023ge,
  AUTHOR = {Bao, Chunhui and Sun, Qianru},
  JOURNAL = {IEEE Transactions on Multimedia},
  NOTE = {Cited by: 2; All Open Access, Green Open Access},
  PAGES = {3602 – 3614},
  TITLE = {Generating Music With Emotions},
  TYPE = {Article},
  VOLUME = {25},
  YEAR = {2023},
  NOTES.CA = {exclude, although task relevant, may not be directly comparable with other studies},
  DOI = {10.1109/TMM.2022.3163543},
  ABSTRACT = {We focus on the music generation conditional on human emotions, specifically the positive and negative emotions. There is no existing large-scale music datasets with the annotation of human emotion labels. It is thus not intuitive how to generate music conditioned on emotion labels. In this paper, we propose an annotation-free method to build a new dataset where each sample is a triplet of lyric, melody and emotion label (without requiring any labours). Specifically, we first train the automated emotion recognition model using the BERT (pre-trained on GoEmotions dataset) on Edmonds Dance dataset. We use it to automatically 'label' the music with the emotion labels recognized from the lyrics. We then train the encoder-decoder based model to generate emotional music on that dataset, and call our overall method as Emotional Lyric and Melody Generator (ELMG). The framework of ELMG is consisted of three modules: 1) an encoder-decoder model trained end-to-end to generate lyric and melody; 2) a music emotion classifier trained on labeled data (our proposed dataset); and 3) a modified beam search algorithm that guides the music generation process by incorporating the music emotion classifier. We conduct objective and subjective evaluations on the generated music pieces, and our results show that ELMG is capable of generating tuneful lyric and melody with specified human emotions. © 1999-2012 IEEE.},
  KEYWORDS = {Classification (of information); Decoding; Deep learning; Generative adversarial networks; Large dataset; Music; Signal encoding; Speech recognition; Beam search; Conditional music generation; Deep learning; Emotion recognition; Generator; Human emotion; Natural languages; Seq2seq; Social networking (online); Transformer; Emotion Recognition},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85127520415&doi = 10.1109%2fTMM.2022.3163543&partnerID = 40&md5 = 61bc5f8c7a6ea21475248daeb75a7415},
  NOTES.TE = {exclude, no valid task},
  AUTHOR_KEYWORDS = {beam search; Conditional music generation; Seq2Seq; transformer},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{battcock2021in,
  AUTHOR = {Battcock, Aimee and Schutz, Michael},
  JOURNAL = {JOURNAL OF NEW MUSIC RESEARCH},
  MONTH = {OCT 20},
  NUMBER = {5},
  PAGES = {447-468},
  TITLE = {Individualized interpretation: Exploring structural and interpretive effects on evaluations of emotional content in Bach's Well Tempered Clavier},
  VOLUME = {50},
  YEAR = {2021},
  NOTES.CA = {include},
  DOI = {10.1080/09298215.2021.1979050},
  SOURCE = {web_of_science},
  NOTES.TE = {include, commonality analysis},
  BDSK = {https://doi.org/10.1080/09298215.2021.1979050},
  ISSN = {0929-8215},
  EISSN = {1744-5027},
  EARLYACCESSDATE = {JAN 2022},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{beveridge2018po,
  AUTHOR = {Beveridge, Scott and Knox, Don},
  JOURNAL = {PSYCHOLOGY OF MUSIC},
  MONTH = {MAY},
  NUMBER = {3},
  PAGES = {411-423},
  TITLE = {Popular music and the role of vocal melody in perceived emotion},
  VOLUME = {46},
  YEAR = {2018},
  NOTES.CA = {include},
  DOI = {10.1177/0305735617713834},
  SOURCE = {web_of_science},
  NOTES.TE = {include, R},
  BDSK = {https://doi.org/10.1177/0305735617713834},
  ISSN = {0305-7356},
  EISSN = {1741-3087},
  ORCID = {Knox, Don/0000-0003-1303-1183},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{cao2023th,
  AUTHOR = {Cao, Yujing and Park, Jinwan},
  JOURNAL = {IEEE ACCESS},
  PAGES = {141192-141204},
  TITLE = {The Analysis of Music Emotion and Visualization Fusing Long Short-Term Memory Networks Under the Internet of Things},
  VOLUME = {11},
  YEAR = {2023},
  NOTES.CA = {include, single-modality reported},
  DOI = {10.1109/ACCESS.2023.3341926},
  SOURCE = {web_of_science},
  NOTES.TE = {include, some quality issues},
  BDSK = {https://doi.org/10.1109/ACCESS.2023.3341926},
  ISSN = {2169-3536},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{chen2017co,
  AUTHOR = {Chen, Yu-An and Wang, Ju-Chiang and Yang, Yi-Hsuan and Chen, Homer H.},
  JOURNAL = {IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING},
  MONTH = {JUL},
  NUMBER = {7},
  PAGES = {1409-1420},
  TITLE = {Component Tying for Mixture Model Adaptation in Personalization of Music Emotion Recognition},
  VOLUME = {25},
  YEAR = {2017},
  NOTES.CA = {include},
  DOI = {10.1109/TASLP.2017.2693565},
  SOURCE = {web_of_science},
  NOTES.TE = {include},
  BDSK = {https://doi.org/10.1109/TASLP.2017.2693565},
  ISSN = {2329-9290},
  EISSN = {2329-9304},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{chin2014mu,
  AUTHOR = {Chin, Yu-Hao and Lin, Chang-Hong and Siahaan, Ernestasia and Wang, Jia-Ching},
  JOURNAL = {The Scientific World Journal},
  NOTE = {Cited by: 4; All Open Access, Gold Open Access, Green Open Access},
  TITLE = {Music emotion detection using hierarchical sparse kernel machines},
  TYPE = {Article},
  VOLUME = {2014},
  YEAR = {2014},
  NOTES.CA = {exclude, insufficient information},
  DOI = {10.1155/2014/270378},
  ABSTRACT = {For music emotion detection, this paper presents a music emotion verification system based on hierarchical sparse kernel machines. With the proposed system, we intend to verify if a music clip possesses happiness emotion or not. There are two levels in the hierarchical sparse kernel machines. In the first level, a set of acoustical features are extracted, and principle component analysis (PCA) is implemented to reduce the dimension. The acoustical features are utilized to generate the first-level decision vector, which is a vector with each element being a significant value of an emotion. The significant values of eight main emotional classes are utilized in this paper. To calculate the significant value of an emotion, we construct its 2-class SVM with calm emotion as the global (non-target) side of the SVM. The probability distributions of the adopted acoustical features are calculated and the probability product kernel is applied in the first-level SVMs to obtain first-level decision vector feature. In the second level of the hierarchical system, we merely construct a 2-class relevance vector machine (RVM) with happiness as the target side and other emotions as the background side of the RVM. The first-level decision vector is used as the feature with conventional radial basis function kernel. The happiness verification threshold is built on the probability value. In the experimental results, the detection error tradeoff (DET) curve shows that the proposed system has a good performance on verifying if a music clip reveals happiness emotion. © 2014 Yu-Hao Chin et al.},
  KEYWORDS = {Algorithms; Auditory Perception; Biomimetics; Emotions; Humans; Music; Pattern Recognition, Automated; Sound Spectrography; Support Vector Machines; algorithm; article; emotion; happiness; kernel method; music; principal component analysis; probability; relevance vector machine; support vector machine; algorithm; automated pattern recognition; biomimetics; emotion; hearing; human; physiology; procedures; sound detection; support vector machine},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-84896904854&doi = 10.1155%2f2014%2f270378&partnerID = 40&md5 = 48fe84315e318b35b7b402c797cb9a66},
  NOTES.TE = {exclude, no valence and arousal prediction},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{chin2018pr,
  AUTHOR = {Chin, Yu-Hao and Wang, Jia-Ching and Wang, Ju-Chiang and Yang, Yi-Hsuan},
  JOURNAL = {IEEE TRANSACTIONS ON AFFECTIVE COMPUTING},
  MONTH = {OCT-DEC},
  NUMBER = {4},
  PAGES = {541-549},
  TITLE = {Predicting the Probability Density Function of Music Emotion Using Emotion Space Mapping},
  VOLUME = {9},
  YEAR = {2018},
  NOTES.CA = {include},
  DOI = {10.1109/TAFFC.2016.2628794},
  SOURCE = {web_of_science},
  NOTES.TE = {include, R2},
  BDSK = {https://doi.org/10.1109/TAFFC.2016.2628794},
  ISSN = {1949-3045},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{coutinho2017sh,
  AUTHOR = {Coutinho, Eduardo and Schuller, Bjorn},
  JOURNAL = {PLOS ONE},
  MONTH = {JUN 28},
  NUMBER = {6},
  TITLE = {Shared acoustic codes underlie emotional communication in music and speech-Evidence from deep transfer learning},
  VOLUME = {12},
  YEAR = {2017},
  NOTES.CA = {include, can report intradomain accuracies},
  DOI = {10.1371/journal.pone.0179289},
  SOURCE = {web_of_science},
  NOTES.TE = {include},
  BDSK = {https://doi.org/10.1371/journal.pone.0179289},
  ISSN = {1932-6203},
  ARTICLE = {e0179289},
  ORCID = {Coutinho, Eduardo/0000-0001-5234-1497 Schuller, Bj{\"o}rn Wolfgang/0000-0002-6478-8699},
  RESEARCHERID = {Coutinho, Eduardo/K-1391-2019 Schuller, Bj{\"o}rn Wolfgang/D-3241-2011},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{cunningham2021su,
  AUTHOR = {Cunningham, Stuart and Ridley, Harrison and Weinel, Jonathan and Picking, Richard},
  JOURNAL = {PERSONAL AND UBIQUITOUS COMPUTING},
  MONTH = {AUG},
  NUMBER = {4},
  PAGES = {637-650},
  TITLE = {Supervised machine learning for audio emotion recognition Enhancing film sound design using audio features, regression models and artificial neural networks},
  VOLUME = {25},
  YEAR = {2021},
  NOTES.CA = {include},
  DOI = {10.1007/s00779-020-01389-0},
  SOURCE = {web_of_science},
  NOTES.TE = {exclude, IADS no music},
  BDSK = {https://doi.org/10.1007/s00779-020-01389-0},
  ISSN = {1617-4909},
  EISSN = {1617-4917},
  ORCID = {Cunningham, Stuart/0000-0002-5348-7700 Weinel, Jonathan/0000-0001-5347-3897},
  RESEARCHERID = {Cunningham, Stuart/HSH-5303-2023},
  EARLYACCESSDATE = {APR 2020},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{deng2015em,
  AUTHOR = {Deng, James J. and Leung, Clement H. C. and Milani, Alfredo and Chen, Li},
  JOURNAL = {ACM TRANSACTIONS ON INTERACTIVE INTELLIGENT SYSTEMS},
  MONTH = {MAR},
  NUMBER = {1},
  TITLE = {Emotional States Associated with Music: Classification, Prediction of Changes, and Consideration in Recommendation},
  VOLUME = {5},
  YEAR = {2015},
  NOTES.CA = {include},
  DOI = {10.1145/2723575},
  SOURCE = {web_of_science},
  NOTES.TE = {include},
  BDSK = {https://doi.org/10.1145/2723575},
  ISSN = {2160-6455},
  EISSN = {2160-6463},
  ORCID = {Chen, Li/0000-0002-5842-838X MILANI, Alfredo/0000-0003-4534-1805},
  COMMENT = {discussed},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{deng2024an,
  AUTHOR = {Deng, Ya and Lin, Na},
  JOURNAL = {Computer-Aided Design and Applications},
  NOTE = {Cited by: 0; All Open Access, Bronze Open Access},
  NUMBER = {S23},
  PAGES = {19 – 34},
  TITLE = {Analysis and Expression of Music Emotion Based on CAD and Deep Reinforcement Learning Algorithm},
  TYPE = {Article},
  VOLUME = {21},
  YEAR = {2024},
  NOTES.CA = {unsure, insufficient detail, quality issues},
  DOI = {10.14733/cadaps.2024.S23.19-34},
  ABSTRACT = {This article seeks to introduce a fresh perspective and approach to music emotion analysis by proposing a novel algorithm that integrates Deep Reinforcement Learning (DRL). The algorithm leverages the capabilities of deep neural networks to discern emotional characteristics within music autonomously. By utilizing the reinforcement learning framework, the decision-making process of the model is refined, enabling more precise identification of subtle emotional nuances in music. Experimental findings reveal that this algorithm significantly outperforms traditional methods in music emotion analysis, demonstrating a notable enhancement in detection accuracy. The key advantage of this algorithm lies in its ability to circumvent the intricacies and uncertainties associated with manual feature extraction. Furthermore, it exhibits superior adaptability to intricate music emotion analysis tasks, effectively elevating the accuracy and efficacy of classification. Following extensive training iterations, the model demonstrates a remarkable capacity to swiftly accommodate new data distributions and emotional expression patterns. In conclusion, the DRL-based music emotion analysis algorithm presented in this article contributes innovative research concepts and methodologies to the field and holds substantial theoretical and practical significance. © 2024 U-turn Press LLC.},
  KEYWORDS = {Computer aided design; Decision making; Deep neural networks; Emotion Recognition; Learning algorithms; Music; Decision-making process; Deep reinforcement learning; Detection accuracy; Emotion analysis; Learning frameworks; Music emotion analyse; Music emotions; Novel algorithm; Reinforcement learning algorithms; Reinforcement learnings; Reinforcement learning},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85188622045&doi = 10.14733%2fcadaps.2024.S23.19-34&partnerID = 40&md5 = 8655f1e08bc6af376ed60d18654e2591},
  NOTES.TE = {exclude, no audio features, quality issues},
  AUTHOR_KEYWORDS = {CAD; Deep Reinforcement Learning; Music Emotion Analysis},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{drossos2015ev,
  AUTHOR = {Drossos, Konstantinos and Floros, Andreas and Kermanidis, Katia-Lida},
  JOURNAL = {JOURNAL OF THE AUDIO ENGINEERING SOCIETY},
  MONTH = {MAR},
  NUMBER = {3},
  PAGES = {139-153},
  TITLE = {Evaluating the Impact of Sound Events' Rhythm Characteristics to Listener's Valence},
  VOLUME = {63},
  YEAR = {2015},
  NOTES.CA = {exclude, sound events},
  DOI = {10.17743/jaes.2015.0010},
  SOURCE = {web_of_science},
  NOTES.TE = {exclude, no music},
  BDSK = {https://doi.org/10.17743/jaes.2015.0010},
  ISSN = {1549-4950},
  ORCID = {Drosos, Konstantinos/0000-0002-3605-7127},
  RESEARCHERID = {Kermanidis, Katia/AAM-2025-2021 Floros, Andreas AF/F-1478-2019 Drossos, Konstantinos/I-8305-2015},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{du2023va,
  AUTHOR = {Du, Ruoyu and Zhu, Shujin and Ni, Huangjing and Mao, Tianyi and Li, Jiajia and Wei, Ran},
  JOURNAL = {MULTIMEDIA TOOLS AND APPLICATIONS},
  MONTH = {APR},
  NUMBER = {10},
  PAGES = {15439-15456},
  TITLE = {Valence-arousal classification of emotion evoked by Chinese ancient-style music using 1D-CNN-BiLSTM model on EEG signals for college students},
  VOLUME = {82},
  YEAR = {2023},
  NOTES.CA = {exclude, eeg},
  DOI = {10.1007/s11042-022-14011-7},
  SOURCE = {web_of_science},
  NOTES.TE = {exclude, EEG prediction},
  BDSK = {https://doi.org/10.1007/s11042-022-14011-7},
  ISSN = {1380-7501},
  EISSN = {1573-7721},
  EARLYACCESSDATE = {OCT 2022},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{duan2022co,
  AUTHOR = {Duan, Ying},
  JOURNAL = {Scientific Programming},
  NOTE = {Cited by: 0; All Open Access, Gold Open Access},
  TITLE = {Construction of Vocal Timbre Evaluation System Based on Classification Algorithm},
  TYPE = {Article},
  VOLUME = {2022},
  YEAR = {2022},
  NOTES.CA = {exclude, no relevant task},
  DOI = {10.1155/2022/6893128},
  ABSTRACT = {With the continuous development of communication technology, computer technology, and network technology, a large amount of information such as images, videos, and audios has grown exponentially, and people have started to be exposed to massive multimedia contents, which can easily and quickly access the increasingly rich music resources, so new technologies are urgently needed for their effective management, and automatic classification of audio signals has become the focus of engineering and academic attention. Currently, music retrieval can be achieved by selecting song titles and singer names, but as people's living standards continue to improve, the spiritual realm is also enriched. People want to be able to select music with different types of emotional expressions with their emotions. It mainly includes the basic principles of audio classification, the analysis and extraction of music emotion features, and the selection of the best classifier. Two classification algorithms, hybrid Gaussian model and AdaBoost, are used to classify music emotions, and the two classifiers are combined. In this paper, we propose the Discrete Harmonic Transform (DHT), a sparse transform based on harmonic frequencies. This paper derives and proves the formula of Discrete Harmonic Transform and further analyzes the harmonic structure of musical tone signal and the accuracy of harmonic structure. Since the timbre of musical instruments depends on the harmonic structure, and similar instruments have similar harmonic structures, the discrete harmonic transform coefficients can be defined as objective indicators corresponding to the timbre of musical instruments, and thus the concept of timbre expression spectrum is proposed, and a specific construction algorithm is given in this paper. In the application of musical instrument recognition, the 53-dimensional combined features of LPCC, MFCC, and timbre expression spectrum are selected, and a nonlinear support vector machine is used as the classifier. The classification recognition rate is improved by reducing the number of feature dimensions. © 2022 Ying Duan.},
  KEYWORDS = {Adaptive boosting; Audio acoustics; Classification (of information); Emotion Recognition; Harmonic analysis; Multimedia systems; Musical instruments; Support vector machines; Classification algorithm; Communicationtechnology; Computer technology; Continuous development; Discrete harmonic transforms; Harmonic structures; Large amounts; Music emotions; Network technologies; Spectra's; Music},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85132509543&doi = 10.1155%2f2022%2f6893128&partnerID = 40&md5 = 86a4d695f04445072861b069f30d2dab},
  NOTES.TE = {exclude, instrument classification},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{eyben2015em,
  AUTHOR = {Eyben, Florian and Salomao, Glaucia L. and Sundberg, Johan and Scherer, Klaus R. and Schuller, Bjoern W.},
  JOURNAL = {EURASIP JOURNAL ON AUDIO SPEECH AND MUSIC PROCESSING},
  MONTH = {JUN 30},
  TITLE = {Emotion in the singing voice-a deeper look at acoustic features in the light of automatic classification},
  YEAR = {2015},
  NOTES.CA = {include},
  DOI = {10.1186/s13636-015-0057-6},
  SOURCE = {web_of_science},
  NOTES.TE = {exclude, singing voice only},
  BDSK = {https://doi.org/10.1186/s13636-015-0057-6},
  ISSN = {1687-4722},
  ARTICLE = {19},
  ORCID = {Schuller, Bj{\"o}rn Wolfgang/0000-0002-6478-8699},
  RESEARCHERID = {Schuller, Bj{\"o}rn Wolfgang/D-3241-2011},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{fabio2019ad,
  AUTHOR = {Fabio, Rosa Angela and Iannizzotto, Giancarlo and Nucita, Andrea and Caprì, Tindara},
  JOURNAL = {Cogent Engineering},
  NOTE = {Cited by: 7; All Open Access, Gold Open Access, Green Open Access},
  NUMBER = {1},
  TITLE = {Adult listening behaviour, music preferences and emotions in the mobile context. Does mobile context affect elicited emotions?},
  TYPE = {Article},
  VOLUME = {6},
  YEAR = {2019},
  NOTES.CA = {exclude, no relevant task},
  DOI = {10.1080/23311916.2019.1597666},
  ABSTRACT = {After the introduction of mobile computing devices, the way people listen to music has changed considerably. Although there is a broad scientific consensus on the fact that people show music preferences and make music choices based on their feelings and emotions, the sources of such preferences and choices are still debated. The main aim of this study is to understand whether listening in ecological (mobile) contexts differs from listening in non-mobile contexts in terms of the elicited emotive response. A total of 328 participants listen to 100 classical music tracks, available through an ad-hoc mobile application for mobile devices. The participants were asked to report their self-evaluation of each of the tracks, according to the Pleasure-Arousal-Dominance model and filled out a questionnaire about their listening behaviour. Our findings show that the same factors that affect music listening in non-mobile contexts also affect it in a mobile context. © 2019, © 2019 The Author(s). This open access article is distributed under a Creative Commons Attribution (CC-BY) 4.0 license.},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85064259930&doi = 10.1080%2f23311916.2019.1597666&partnerID = 40&md5 = 66927d75f900041fc51c968baa4ac66c},
  NOTES.TE = {exclude, no audio prediction},
  AUTHOR_KEYWORDS = {emotion recognition; mobile context; music; music listening in ecological contexts},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{gingras2014be,
  AUTHOR = {Gingras, Bruno and Marin, Manuela M. and Fitch, W. Tecumseh},
  JOURNAL = {Quarterly Journal of Experimental Psychology},
  NOTE = {Cited by: 32},
  NUMBER = {7},
  PAGES = {1428 – 1446},
  TITLE = {Beyond intensity: Spectral features effectively predict music-induced subjective arousal},
  TYPE = {Article},
  VOLUME = {67},
  YEAR = {2014},
  NOTES.CA = {include},
  DOI = {10.1080/17470218.2013.863954},
  ABSTRACT = {Emotions in music are conveyed by a variety of acoustic cues. Notably, the positive association between sound intensity and arousal has particular biological relevance. However, although amplitude normalization is a common procedure used to control for intensity in music psychology research, direct comparisons between emotional ratings of original and amplitude-normalized musical excerpts are lacking.In this study, 30 nonmusicians retrospectively rated the subjective arousal and pleasantness induced by 84 six-second classical music excerpts, and an additional 30 nonmusicians rated the same excerpts normalized for amplitude. Following the cue-redundancy and Brunswik lens models of acoustic communication, we hypothesized that arousal and pleasantness ratings would be similar for both versions of the excerpts, and that arousal could be predicted effectively by other acoustic cues besides intensity.Although the difference in mean arousal and pleasantness ratings between original and amplitude-normalized excerpts correlated significantly with the amplitude adjustment, ratings for both sets of excerpts were highly correlated and shared a similar range of values, thus validating the use of amplitude normalization in music emotion research. Two acoustic parameters, spectral flux and spectral entropy, accounted for 65% of the variance in arousal ratings for both sets, indicating that spectral features can effectively predict arousal. Additionally, we confirmed that amplitude-normalized excerpts were adequately matched for loudness. Overall, the results corroborate our hypotheses and support the cue-redundancy and Brunswik lens models. © 2013 The Experimental Psychology Society.},
  KEYWORDS = {Acoustic Stimulation; Acoustics; Adult; Arousal; Emotions; Female; Humans; Linear Models; Loudness Perception; Male; Music; Predictive Value of Tests; Questionnaires; Recognition (Psychology); Young Adult; acoustics; adult; arousal; auditory stimulation; emotion; female; hearing; human; male; music; physiology; predictive value; psychology; questionnaire; recognition; statistical model; young adult},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-84902376903&doi = 10.1080%2f17470218.2013.863954&partnerID = 40&md5 = 021d634d0f679aaff17d6770894daad5},
  NOTES.TE = {include, has R2},
  AUTHOR_KEYWORDS = {Arousal; Brunswik lens model; Emotion; Intensity; Music},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{grekow2018au,
  AUTHOR = {Grekow, Jacek},
  JOURNAL = {JOURNAL OF INFORMATION AND TELECOMMUNICATION},
  MONTH = {JUL 3},
  NUMBER = {3},
  PAGES = {322-333},
  TITLE = {Audio features dedicated to the detection and tracking of arousal and valence in musical compositions},
  VOLUME = {2},
  YEAR = {2018},
  NOTES.CA = {include},
  DOI = {10.1080/24751839.2018.1463749},
  SOURCE = {web_of_science},
  NOTES.TE = {include},
  BDSK = {https://doi.org/10.1080/24751839.2018.1463749},
  ISSN = {2475-1839},
  EISSN = {2475-1847},
  RESEARCHERID = {Grekow, Jacek/M-9500-2015},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{grekow2018mu,
  AUTHOR = {Grekow, Jacek},
  JOURNAL = {JOURNAL OF INTELLIGENT INFORMATION SYSTEMS},
  MONTH = {OCT},
  NUMBER = {2},
  PAGES = {415-437},
  TITLE = {Musical performance analysis in terms of emotions it evokes},
  VOLUME = {51},
  YEAR = {2018},
  NOTES.CA = {exclude, no relevant task},
  DOI = {10.1007/s10844-018-0510-y},
  SOURCE = {web_of_science},
  NOTES.TE = {exclude, prediction of similarity based on valence and arousal},
  BDSK = {https://doi.org/10.1007/s10844-018-0510-y},
  ISSN = {0925-9902},
  EISSN = {1573-7675},
  ORCID = {Grekow, Jacek/0000-0003-2094-0107},
  RESEARCHERID = {Grekow, Jacek/M-9500-2015},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{grekow2021mu,
  AUTHOR = {Grekow, Jacek},
  JOURNAL = {JOURNAL OF INTELLIGENT INFORMATION SYSTEMS},
  MONTH = {DEC},
  NUMBER = {3},
  PAGES = {531-546},
  TITLE = {Music emotion recognition using recurrent neural networks and pretrained models},
  VOLUME = {57},
  YEAR = {2021},
  NOTES.CA = {include},
  DOI = {10.1007/s10844-021-00658-5},
  SOURCE = {web_of_science},
  NOTES.TE = {include},
  BDSK = {https://doi.org/10.1007/s10844-021-00658-5},
  ISSN = {0925-9902},
  EISSN = {1573-7675},
  ORCID = {Grekow, Jacek/0000-0003-2094-0107},
  RESEARCHERID = {Grekow, Jacek/M-9500-2015},
  EARLYACCESSDATE = {AUG 2021},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{griffiths2021am,
  AUTHOR = {Griffiths, Darryl and Cunningham, Stuart and Weinel, Jonathan and Picking, Richard},
  JOURNAL = {JOURNAL OF NEW MUSIC RESEARCH},
  MONTH = {AUG 8},
  NUMBER = {4},
  PAGES = {355-372},
  TITLE = {A multi-genre model for music emotion recognition using linear regressors},
  VOLUME = {50},
  YEAR = {2021},
  NOTES.CA = {include},
  DOI = {10.1080/09298215.2021.1977336},
  SOURCE = {web_of_science},
  NOTES.TE = {include, linear regression},
  BDSK = {https://doi.org/10.1080/09298215.2021.1977336},
  ISSN = {0929-8215},
  EISSN = {1744-5027},
  ORCID = {Cunningham, Stuart/0000-0002-5348-7700 Weinel, Jonathan/0000-0001-5347-3897},
  RESEARCHERID = {Cunningham, Stuart/HSH-5303-2023},
  EARLYACCESSDATE = {SEP 2021},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{hakanpaeae2019em,
  AUTHOR = {Hakanpää, Tua and Waaramaa, T. and Laukkanen, Anne-Maria},
  JOURNAL = {Journal of Voice},
  NOTE = {Cited by: 11; All Open Access, Green Open Access},
  NUMBER = {4},
  PAGES = {501 – 509},
  TITLE = {Emotion Recognition From Singing Voices Using Contemporary Commercial Music and Classical Styles},
  TYPE = {Article},
  VOLUME = {33},
  YEAR = {2019},
  NOTES.CA = {exclude, no relevant task},
  DOI = {10.1016/j.jvoice.2018.01.012},
  ABSTRACT = {Objectives: This study examines the recognition of emotion in contemporary commercial music (CCM) and classical styles of singing. This information may be useful in improving the training of interpretation in singing. Study design: This is an experimental comparative study. Methods: Thirteen singers (11 female, 2 male) with a minimum of 3 years' professional-level singing studies (in CCM or classical technique or both) participated. They sang at three pitches (females: a, e1, a1, males: one octave lower) expressing anger, sadness, joy, tenderness, and a neutral state. Twenty-nine listeners listened to 312 short (0.63- to 4.8-second) voice samples, 135 of which were sung using a classical singing technique and 165 of which were sung in a CCM style. The listeners were asked which emotion they heard. Activity and valence were derived from the chosen emotions. Results: The percentage of correct recognitions out of all the answers in the listening test (N = 9048) was 30.2%. The recognition percentage for the CCM-style singing technique was higher (34.5%) than for the classical-style technique (24.5%). Valence and activation were better perceived than the emotions themselves, and activity was better recognized than valence. A higher pitch was more likely to be perceived as joy or anger, and a lower pitch as sorrow. Both valence and activation were better recognized in the female CCM samples than in the other samples. Conclusions: There are statistically significant differences in the recognition of emotions between classical and CCM styles of singing. Furthermore, in the singing voice, pitch affects the perception of emotions, and valence and activity are more easily recognized than emotions. © 2018 The Voice Foundation},
  KEYWORDS = {Adult; Auditory Perception; Cues; Emotions; Female; Humans; Male; Pitch Perception; Recognition, Psychology; Singing; Voice Quality; Young Adult; adult; anger; article; comparative study; female; human; major clinical study; male; music; perception; pitch; sadness; singing; sorrow; study design; voice; association; emotion; hearing; pitch perception; young adult},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85042298581&doi = 10.1016%2fj.jvoice.2018.01.012&partnerID = 40&md5 = 2d3215e2fbb4734dceec5b1f5cdbdf16},
  NOTES.TE = {exclude, singing only},
  AUTHOR_KEYWORDS = {Emotion expression; Perception; Singing style; Song genre; Voice quality},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{hao2022re,
  AUTHOR = {Hao, Wang and Yuanchen, Liu and Meng, Zhao and Jingwen, Qiu},
  JOURNAL = {Journal of Modern Information},
  NOTE = {Cited by: 1},
  NUMBER = {11},
  PAGES = {61 – 75},
  TITLE = {Research on Multi-task Music Emotion Recognition Based on Multi-modal Features; [基于多模态特征的音乐情感多任务识别研究]},
  TYPE = {Article},
  VOLUME = {42},
  YEAR = {2022},
  NOTES.CA = {unsure, can't find full text},
  DOI = {10.3969/j.issn.1008-0821.2022.11.006},
  ABSTRACT = {[Purpose/ Significance] Emotion is one of the common ways to organize and retrieve resources on online music platforms. Exploration and research on emotion classification of song lists and songs using feature fusion can optimize management and utilization of music resources, better meeting the demand of internet users for music culture life. [Meth⁃ od/ Process] In this paper, Hevner's music emotion model was introduced to build an emotion lexicon, which was used with names and introduction of song lists to classify emotions of large-grained lists. Multimodal features of lyrics and audios were fused to identify emotions of small-grained songs through pre-trained model's semantic representation and audio signal processing. [Result/ Conclusion] Introduction of emotion lexicon effectively improves the accuracy of song list emotion clas⁃ sification, and manual preprocessing can help algorithms learn features better. Lyrics and audios both contain rich emotion information and the multimodal fusion model performs best in emotion recognition of songs. © 2022 Editorial Board of Journal of Modern Information. All rights reserved.},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85174213057&doi = 10.3969%2fj.issn.1008-0821.2022.11.006&partnerID = 40&md5 = 866512162b3b137dba6e25cf334ecd62},
  NOTES.TE = {exclude, chinese only},
  AUTHOR_KEYWORDS = {emotion classification of song lists; Mel spectro⁃ gram; multimodal fusion; music emotion classification; NetEase cloud music},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{he2020di,
  AUTHOR = {He, Jing-Xian and Zhou, Li and Liu, Zhen-Tao and Hu, Xin-Yue},
  JOURNAL = {Journal of Advanced Computational Intelligence and Intelligent Informatics},
  NOTE = {Cited by: 4; All Open Access, Gold Open Access},
  NUMBER = {7},
  PAGES = {872 – 881},
  TITLE = {Digital empirical research of influencing factors of musical emotion classification based on pleasure-arousal musical emotion fuzzy model},
  TYPE = {Article},
  VOLUME = {24},
  YEAR = {2020},
  NOTES.CA = {exclude, no relevant analysis},
  DOI = {10.20965/JACIII.2020.P0872},
  ABSTRACT = {In recent years, with the further breakthrough of artificial intelligence theory and technology, as well as the further expansion of the Internet scale, the recognition of human emotions and the necessity for satisfying human psychological needs in future artificial intelligence technology development tendencies have been highlighted, in addition to physical task accomplishment. Musical emotion classification is an important research topic in artificial intelligence. The key premise of realizing music emotion classification is to construct a musical emotion model that conforms to the characteristics of music emotion recognition. Currently, three types of music emotion classification models are available: discrete category, continuous dimensional, and music emotion-specific models. The pleasure-arousal music emotion fuzzy model, which includes a wide range of emotions compared with other models, is selected as the emotional classification system in this study to investigate the influencing factor for musical emotion classification. Two representative emotional attributes, i.e., speed and strength, are used as variables. Based on test experiments involving music and non-music majors combined with questionnaire results, the relationship between music properties and emotional changes under the pleasure-arousal model is revealed quantitatively. © 2020 Fuji Technology Press. All rights reserved.},
  KEYWORDS = {Classification (of information); Artificial intelligence technologies; Emotional change; Emotional classification; Empirical research; Music emotion classifications; Psychological needs; Research topics; Task accomplishment; Artificial intelligence},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85098774308&doi = 10.20965%2fJACIII.2020.P0872&partnerID = 40&md5 = a20634ec00d9b4bcc6e9980a4b708007},
  NOTES.TE = {exclude, no outcome measures},
  AUTHOR_KEYWORDS = {Affective computing; Classification; Fuzzy model; Music emotion; Pleasure-arousal emotion space},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{hendry2019pr,
  AUTHOR = {Hendry and Chen, Rung-Ching},
  JOURNAL = {ICIC Express Letters},
  NOTE = {Cited by: 2},
  NUMBER = {3},
  PAGES = {255 – 262},
  TITLE = {Predicting business category with multi-label classification from user-item review and business data based on K-means},
  TYPE = {Article},
  VOLUME = {13},
  YEAR = {2019},
  NOTES.CA = {exclude, no relevant task},
  DOI = {10.24507/icicel.13.03.255},
  ABSTRACT = {Currently, many recommendation systems propose the breakthrough of traditional single recommendation. Many items usually belong to more than one label at a time, for example, genres of music, categories of the products and emotions. One data point could be labeled more than one tag which is a problem for many classification algorithms. Clustering analysis is a primary task of data mining, which works by dividing the dataset into the partitions based on the distance of data points. Clustering is an unsupervised learning model, which is suitable to learn multi-label classification problem. The technique is commonly used in machine learning, pattern recognition, and many others. K-means is one of the simple and widely used clustering algorithms. In this paper, we propose the collaboration between business and user-item reviews to predict the multi-label classification. We implement the combination of k-means between business and user-items review. We found that the value of k equal to three will have the best multi-label classification results for business categories and business rating. © 2019, ICIC International. All rights reserved.},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85062332918&doi = 10.24507%2ficicel.13.03.255&partnerID = 40&md5 = 6cc587c4901a00bacb697bc0f8ad4151},
  NOTES.TE = {exclude, not relevant},
  AUTHOR_KEYWORDS = {K-means; Multi-label classification; User-item reviews},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{hofbauer2023em,
  AUTHOR = {Hofbauer, Lena M. and Rodriguez, Francisca S.},
  JOURNAL = {INTERNATIONAL JOURNAL OF PSYCHOLOGY},
  MONTH = {OCT},
  NUMBER = {5},
  PAGES = {465-475},
  TITLE = {Emotional valence perception in music and subjective arousal: Experimental validation of stimuli},
  VOLUME = {58},
  YEAR = {2023},
  NOTES.CA = {exclude, no relevant task},
  DOI = {10.1002/ijop.12922},
  SOURCE = {web_of_science},
  NOTES.TE = {exclude, no audio prediction},
  BDSK = {https://doi.org/10.1002/ijop.12922},
  ISSN = {0020-7594},
  EISSN = {1464-066X},
  ORCID = {Rodriguez, Francisca/0000-0003-2919-5510 Hofbauer, Lena/0000-0003-1789-711X},
  RESEARCHERID = {Rodriguez, Francisca/AAD-6803-2019 Hofbauer, Lena/AAT-3664-2021},
  EARLYACCESSDATE = {MAY 2023},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{hong2017an,
  AUTHOR = {Hong, Yu and Chau, Chuck-Jee and Horner, Andrew},
  JOURNAL = {AES: Journal of the Audio Engineering Society},
  NOTE = {Cited by: 14; All Open Access, Bronze Open Access},
  NUMBER = {4},
  PAGES = {304 – 320},
  TITLE = {An analysis of low-arousal piano music ratings to uncover what makes calm and sad music so difficult to distinguish in music emotion recognition},
  TYPE = {Article},
  VOLUME = {65},
  YEAR = {2017},
  NOTES.CA = {exclude, no relevant task, no modeling},
  DOI = {10.17743/jaes.2017.0001},
  ABSTRACT = {Music emotion recognition and recommendation systems often use a simplified 4-quadrant model with categories such as Happy, Sad, Angry, and Calm. Previous research has shown that both listeners and automated systems often have difficulty distinguishing low-arousal categories such as Calm and Sad. This paper seeks to explore what makes the categories Calm and Sad so difficult to distinguish. We used 300 low-arousal excerpts from the classical piano repertoire to determine the coverage of the categories Calm and Sad in the low-arousal space, their overlap, and their balance to one another. Our results show that Calm was 40% bigger in terms of coverage than Sad, but that on average Sad excerpts were significantly more negative in mood than Calm excerpts were positive. Calm and Sad overlapped in nearly 20% of the excerpts, meaning 20% of the excerpts were about equally Calm and Sad. Calm and Sad covered about 92% of the low-arousal space, where 8% of the space were holes that were not-at-all Calm or Sad. The largest holes were for excerpts considered Mysterious and Doubtful, but there were smaller holes among positive excerpts as well. Due to the holes in the coverage, the overlaps, and imbalances the Calm-Sad model adds about 6% more errors when compared to asking users directly whether the mood of the music is positive or negative. Nevertheless, the Calm-Sad model is still useful and appropriate for applications in music emotion recognition and recommendation such as when a simple and intuitive interface is preferred or when categorization is more important than precise differentiation.},
  KEYWORDS = {Automation; Automated systems; Intuitive interfaces; Music emotions; Piano music; Speech recognition},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85019993531&doi = 10.17743%2fjaes.2017.0001&partnerID = 40&md5 = e66e2e892e26fd861e70e06d45bf06b9},
  NOTES.TE = {exclude, no audio prediction},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{hu2017cr,
  AUTHOR = {Hu, Xiao and Yang, Yi-Hsuan},
  JOURNAL = {IEEE TRANSACTIONS ON AFFECTIVE COMPUTING},
  MONTH = {APR-JUN},
  NUMBER = {2},
  PAGES = {228-240},
  TITLE = {Cross-Dataset and Cross-Cultural Music Mood Prediction: A Case on Western and Chinese Pop Songs},
  VOLUME = {8},
  YEAR = {2017},
  NOTES.CA = {include},
  DOI = {10.1109/TAFFC.2016.2523503},
  SOURCE = {web_of_science},
  NOTES.TE = {include},
  BDSK = {https://doi.org/10.1109/TAFFC.2016.2523503},
  ISSN = {1949-3045},
  ORCID = {Hu, Xiao/0000-0003-3994-0385},
  RESEARCHERID = {Hu, Xiao/A-7645-2013 Hu, Xiao/AAD-8405-2020},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{hu2022de,
  AUTHOR = {Hu, Xiao and Li, Fanjie and Liu, Ruilun},
  JOURNAL = {APPLIED SCIENCES-BASEL},
  MONTH = {SEP},
  NUMBER = {18},
  TITLE = {Detecting Music-Induced Emotion Based on Acoustic Analysis and Physiological Sensing: A Multimodal Approach},
  VOLUME = {12},
  YEAR = {2022},
  NOTES.CA = {include , single-modality reported},
  DOI = {10.3390/app12189354},
  SOURCE = {web_of_science},
  NOTES.TE = {include, includes audio only classification},
  BDSK = {https://doi.org/10.3390/app12189354},
  ARTICLE = {9354},
  EISSN = {2076-3417},
  ORCID = {Hu, Xiao/0000-0003-3994-0385 Li, Fanjie/0000-0001-7016-6354},
  RESEARCHERID = {Hu, Xiao/AAD-8405-2020},
  COMMENT = {includes audio-only model},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{huang2021re,
  AUTHOR = {Huang, Chun and Shen, Diao},
  JOURNAL = {Scientific Programming},
  NOTE = {Cited by: 3; All Open Access, Gold Open Access},
  TITLE = {Research on Music Emotion Intelligent Recognition and Classification Algorithm in Music Performance System},
  TYPE = {Article},
  VOLUME = {2021},
  YEAR = {2021},
  NOTES.CA = {exclude, no relevant task},
  DOI = {10.1155/2021/7886570},
  ABSTRACT = {The music performance system works by identifying the emotional elements of music to control the lighting changes. However, if there is a recognition error, a good stage effect will not be able to create. Therefore, this paper proposes an intelligent music emotion recognition and classification algorithm in the music performance system. The first part of the algorithm is to analyze the emotional features of music, including acoustic features, melody features, and audio features. Then, the three kinds of features are combined together to form a feature vector set. In the latter part of the algorithm, it divides the feature vector set into training samples and test samples. The training samples are trained by using recognition and classification model based on the neural network. And then, the testing samples are input into the trained model, which is aiming to realize the intelligent recognition and classification of music emotion. The result shows that the kappa coefficient k values calculated by the proposed algorithm are greater than 0.75, which indicates that the recognition and classification results are consistent with the actual results, and the accuracy of recognition and classification is high. So, the research purpose is achieved. © 2021 Chun Huang and Diao Shen.},
  KEYWORDS = {Audio acoustics; Sampling; Speech recognition; Classification algorithm; Features vector; Intelligent classification; Intelligent recognition; Music emotions; Music performance; Performance system; Recognition algorithm; Recognition error; Training sample; Music},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85119913107&doi = 10.1155%2f2021%2f7886570&partnerID = 40&md5 = 04d9fb372c6e843a9daf134c1e0885f8},
  NOTES.TE = {exclude, quality issues, features, data, stimuli},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{juremi2017in,
  AUTHOR = {Juremi, Nor Rashidah Md and Zulkifley, Mohd Asyraf and Hussain, Aini and Zaki, Wan Mimi Diyana Wan},
  JOURNAL = {Journal of Theoretical and Applied Information Technology},
  NOTE = {Cited by: 8},
  NUMBER = {2},
  PAGES = {259 – 264},
  TITLE = {Inter-rater reliability of actual tagged emotion categories validation using Cohen’s Kappa coefficient},
  TYPE = {Article},
  VOLUME = {95},
  YEAR = {2017},
  NOTES.CA = {exclude, no relevant task},
  ABSTRACT = {It is necessary to find the human inter-rater agreement in emotion recognition research especially when handling with publicly available database. This paper discusses the Cohen’s Kappa coefficient technique to verify the actual tagged emotion categories for hybrid emotion model using music video as stimulus. This method has been done by finding the degree of inter-rater reliability between the five selected raters. As the results, the values of Cohen’s Kappa coefficients are over 0.87 for four actual tagged emotion categories which are happy, relaxed, sad and angry. These values demonstrate that the degree of inter-rater agreement are excellent. The actual tagged emotion categories are selected based on the division of average value of arousal-valence rating. © 2005 - 2017 JATIT & LLS. All rights reserved.},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85011659640&partnerID = 40&md5 = b3c6804a58c78ab147e87dc60d3f986a},
  NOTES.TE = {exclude, no audio prediction},
  AUTHOR_KEYWORDS = {Cohen’s Kappa coefficient; Emotion recognition},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{koh2023me,
  AUTHOR = {Koh, En Yan and Cheuk, Kin Wai and Heung, Kwan Yee and Agres, Kat R. and Herremans, Dorien},
  JOURNAL = {SENSORS},
  MONTH = {JAN},
  NUMBER = {1},
  TITLE = {MERP: A Music Dataset with Emotion Ratings and Raters' Profile Information},
  VOLUME = {23},
  YEAR = {2023},
  NOTES.CA = {include},
  DOI = {10.3390/s23010382},
  SOURCE = {web_of_science},
  NOTES.TE = {include},
  BDSK = {https://doi.org/10.3390/s23010382},
  ARTICLE = {382},
  EISSN = {1424-8220},
  ORCID = {Herremans, Dorien/0000-0001-8607-1640 Cheuk, Kin Wai/0000-0003-3213-8242 Agres, Kathleen/0000-0001-7260-2447},
  RESEARCHERID = {Herremans, Dorien/G-9599-2018 Cheuk, Kin Wai/HZJ-8015-2023},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{li2024im,
  AUTHOR = {Li, Jiajia and Soradi-Zeid, Samaneh and Yousefpour, Amin and Pan, Daohua},
  JOURNAL = {Applied Soft Computing},
  NOTE = {Cited by: 1},
  TITLE = {Improved differential evolution algorithm based convolutional neural network for emotional analysis of music data},
  TYPE = {Article},
  VOLUME = {153},
  YEAR = {2024},
  NOTES.CA = {exclude, no relevant task},
  DOI = {10.1016/j.asoc.2024.111262},
  ABSTRACT = {Evolutionary computation is derived from the simulation of natural selection and genetic processes in biological evolution. This approach provides a method for optimizing the structure and parameters of neural networks. When combined with neural networks, forming what's termed as evolutionary computation based neural networks, it offers a systematic approach to optimize neural network models in diverse applications. In this study, we introduce a method that employs differential evolution algorithms to optimize parameters of convolutional neural network (CNN) for music emotion recognition tasks. This method optimizes the initial weights of the CNN, aiming to achieve near-global optimal solutions and expedite network convergence. Comparative experiments indicate that the proposed approach effectively identifies optimal parameters and structures for CNN, suggesting potential advancements in automated music emotion recognition. © 2024 Elsevier B.V.},
  KEYWORDS = {Bioinformatics; Convolution; Convolutional neural networks; Emotion Recognition; Speech recognition; Structural optimization; Convolutional neural network; Differential Evolution; Differential evolution algorithms; Emotion recognition; Emotional analysis; Improved differential evolutions; Music data; Music emotions; Natural selection process; Neural-networks; Evolutionary algorithms},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85183833934&doi = 10.1016%2fj.asoc.2024.111262&partnerID = 40&md5 = 0c127bfaac9efc8e0dd2135c3ef8af5d},
  NOTES.TE = {include, classification},
  AUTHOR_KEYWORDS = {CNN; Differential evolution; Emotional analysis; Evolutionary computation; Music data},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{li2024re,
  AUTHOR = {Li, Lin},
  JOURNAL = {Applied Mathematics and Nonlinear Sciences},
  NOTE = {Cited by: 0; All Open Access, Gold Open Access},
  NUMBER = {1},
  TITLE = {Research on the Comparative Development of Modern Popular Music and Traditional Music Culture in Colleges and Universities in the Age of Artificial Intelligence},
  TYPE = {Article},
  VOLUME = {9},
  YEAR = {2024},
  NOTES.CA = {exclude, quality issues, no relevant task},
  DOI = {10.2478/amns.2023.2.01359},
  ABSTRACT = {In this paper, the forward neural network multi-feature fusion algorithm is used to extract the emotional features of music culture on artificial intelligence technology, considering the diversity and intermittency of the emotional features of the study, which needs to be parameterized. In the forward neural network architecture, the activation value obtained by using the nonlinear activation function is used, and the results obtained are passed to the next layer of data to realize layer-by-layer forward computation, which leads to the back-propagation activation function. The music culture emotion classification model is constructed based on the propagation mode of the forward neural network to determine the emotion recognition process. The research object is selected, the research process is determined, and in order to ensure the true validity of the research, it is necessary to test the reliability and validity of the research design scheme and to develop an empirical analysis of the comparison between popular music and traditional music culture. The results show that on the model, especially in the recognition of sacred, sad, passionate emotion type of music classification accuracy reached more than 88.2%. This paper’s model can improve the classification accuracy of music emotion to a certain extent. In the ontological knowledge analysis of popular music and traditional music culture, all three editions of textbooks show that general knowledge of music is predominant and has a large proportion, appreciation knowledge and extended knowledge are also considerable, and music knowledge is the least and has a small proportion. This study demonstrates the synergistic development of traditional culture and modern popular music, which is of great significance to the development of music education in colleges and universities. © 2023 Lin Li, published by Sciendo.},
  KEYWORDS = {Backpropagation; Chemical activation; Classification (of information); Emotion Recognition; Multilayer neural networks; Music; Reliability analysis; Activation functions; Classification models; Forward neural network; Fusion algorithms; Multi-feature fusion; Multi-feature fusion algorithm; Music culture; Neural-networks; Sentiment classification; Sentiment classification model; Network architecture},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85180349364&doi = 10.2478%2famns.2023.2.01359&partnerID = 40&md5 = 04f5a74d5769a6a8d15be347ef78bee2},
  NOTES.TE = {exclude, no outcome measures},
  AUTHOR_KEYWORDS = {Activation function; Forward neural network; Multi-feature fusion algorithm; Music culture; Sentiment classification model},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{livingstone2018th,
  AUTHOR = {Livingstone, Steven and Russo, Frank},
  JOURNAL = {PloS one},
  MONTH = {},
  NUMBER = {5},
  PAGES = {e0196391--e0196391},
  PUBLISHER = {Public Library of Science},
  TITLE = {The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English},
  VOLUME = {13},
  YEAR = {2018},
  NOTES.CA = {exclude, no relevant task},
  DOI = {10.1371/journal.pone.0196391},
  ABSTRACT = {The RAVDESS is a validated multimodal database of emotional speech and song. The database is gender balanced consisting of 24 professional actors, vocalizing lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, and fearful emotions. Each expression is produced at two levels of emotional intensity, with an additional neutral expression. All conditions are available in face-and-voice, face-only, and voice-only formats. The set of 7356 recordings were each rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity and test-retest intrarater reliability were reported. Corrected accuracy and composite "goodness" measures are presented to assist researchers in the selection of stimuli. All recordings are made freely available under a Creative Commons license and can be downloaded at https://doi.org/10.5281/zenodo.1188976.},
  KEYWORDS = {Speech Emotion; Emotion Recognition; Facial Expression; Affective Computing; Speech Perception},
  SOURCE = {open_alex},
  URL = {https://doi.org/10.1371/journal.pone.0196391},
  NOTES.TE = {exclude, no music content},
  BDSK = {https://doi.org/10.1371/journal.pone.0196391},
  ISBN = {1932-6203},
  DATE = {2024-05-13 15:05:25 +0100},
  DATE.1 = {2024-05-13 15:05:25 +0100},
  LA = {en},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{malheiro2018em,
  AUTHOR = {Malheiro, Ricardo and Panda, Renato and Gomes, Paulo and Paiva, Rui Pedro},
  JOURNAL = {IEEE TRANSACTIONS ON AFFECTIVE COMPUTING},
  MONTH = {APR-JUN},
  NUMBER = {2},
  PAGES = {240-254},
  TITLE = {Emotionally-Relevant Features for Classification and Regression of Music Lyrics},
  VOLUME = {9},
  YEAR = {2018},
  NOTES.CA = {include},
  DOI = {10.1109/TAFFC.2016.2598569},
  SOURCE = {web_of_science},
  NOTES.TE = {include},
  BDSK = {https://doi.org/10.1109/TAFFC.2016.2598569},
  ISSN = {1949-3045},
  ORCID = {Malheiro, Ricardo/0000-0002-3010-2732 Panda, Renato/0000-0003-2539-5590 Paiva, Rui Pedro/0000-0003-3215-3960},
  RESEARCHERID = {Malheiro, Ricardo/L-9369-2017 Panda, Renato/AAK-7581-2020 Paiva, Rui Pedro/D-9602-2018},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{marimpis2020am,
  AUTHOR = {Marimpis, Avraam D. and Dimitriadis, Stavros I. and Goebel, Rainer},
  JOURNAL = {IEEE ACCESS},
  PAGES = {170928-170938},
  TITLE = {A Multiplex Connectivity Map of Valence-Arousal Emotional Model},
  VOLUME = {8},
  YEAR = {2020},
  NOTES.CA = {exclude, eeg},
  DOI = {10.1109/ACCESS.2020.3025370},
  SOURCE = {web_of_science},
  NOTES.TE = {exclude, no sufficient data reported},
  BDSK = {https://doi.org/10.1109/ACCESS.2020.3025370},
  ISSN = {2169-3536},
  ORCID = {DIMITRIADIS, STAVROS I/0000-0002-0000-5392 Goebel, Rainer/0000-0003-1780-2467 Marimpis, Avraam/0000-0003-1551-9940},
  RESEARCHERID = {DIMITRIADIS, STAVROS I/AAN-8992-2020 Goebel, Rainer/JXX-5301-2024},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{markov2014mu,
  AUTHOR = {Markov, Konstantin and Matsui, Tomoko},
  JOURNAL = {IEEE Access},
  NOTE = {Cited by: 73; All Open Access, Gold Open Access},
  PAGES = {688 – 697},
  TITLE = {Music genre and emotion recognition using Gaussian processes},
  TYPE = {Article},
  VOLUME = {2},
  YEAR = {2014},
  NOTES.CA = {include},
  DOI = {10.1109/ACCESS.2014.2333095},
  ABSTRACT = {Gaussian Processes (GPs) are Bayesian nonparametric models that are becoming more and more popular for their superior capabilities to capture highly nonlinear data relationships in various tasks, such as dimensionality reduction, time series analysis, novelty detection, as well as classical regression and classification tasks. In this paper, we investigate the feasibility and applicability of GP models for music genre classification and music emotion estimation. These are two of the main tasks in the music information retrieval (MIR) field. So far, the support vector machine (SVM) has been the dominant model used in MIR systems. Like SVM, GP models are based on kernel functions and Gram matrices; but, in contrast, they produce truly probabilistic outputs with an explicit degree of prediction uncertainty. In addition, there exist algorithms for GP hyperparameter learning - something the SVM framework lacks. In this paper, we built two systems, one for music genre classification and another for music emotion estimation using both SVM and GP models, and compared their performances on two databases of similar size. In all cases, the music audio signal was processed in the same way, and the effects of different feature extraction methods and their various combinations were also investigated. The evaluation experiments clearly showed that in both music genre classification and music emotion estimation tasks the GP performed consistently better than the SVM. The GP achieved a 13.6% relative genre classification error reduction and up to an 11% absolute increase of the coefficient of determination in the emotion estimation task. © 2014 IEEE.},
  KEYWORDS = {Classification (of information); Feature extraction; Gaussian distribution; Gaussian noise (electronic); Image retrieval; Support vector machines; Time series analysis; Coefficient of determination; Dimensionality reduction; Feature extraction methods; Gaussian Processes; Music emotions; Music genre classification; Music information retrieval; Prediction uncertainty; Audio acoustics},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-84923317754&doi = 10.1109%2fACCESS.2014.2333095&partnerID = 40&md5 = 91033029d14599965aafb73fa7175273},
  NOTES.TE = {include, mediaeval, R2 metrics},
  AUTHOR_KEYWORDS = {Gaussian processes; Music emotion estimation; Music genre classification},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{mcintosh2021ex,
  AUTHOR = {Mcintosh, Tyler},
  MONTH = {},
  TITLE = {Exploring the Relationship Between Music and Emotions with Machine Learning},
  YEAR = {2021},
  NOTES.CA = {exclude, poster},
  DOI = {10.14236/ewic/eva2021.49},
  KEYWORDS = {Emotion Recognition; Music Information Retrieval; Melody Extraction; Affective Computing; Audio Event Detection},
  SOURCE = {open_alex},
  URL = {https://doi.org/10.14236/ewic/eva2021.49},
  NOTES.TE = {exclude},
  BDSK = {https://doi.org/10.14236/ewic/eva2021.49},
  DATE = {2024-05-13 15:05:26 +0100},
  DATE.1 = {2024-05-13 15:05:26 +0100},
  LA = {en},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{medina2020em,
  AUTHOR = {Medina, Yesid Ospitia and Beltran, Jose Ramon and Baldassarri, Sandra},
  JOURNAL = {PERSONAL AND UBIQUITOUS COMPUTING},
  MONTH = {2020 APR 15},
  TITLE = {Emotional classification of music using neural networks with the MediaEval dataset},
  YEAR = {2020},
  NOTES.CA = {include},
  DOI = {10.1007/s00779-020-01393-4},
  SOURCE = {web_of_science},
  NOTES.TE = {include, classification},
  BDSK = {https://doi.org/10.1007/s00779-020-01393-4},
  ISSN = {1617-4909},
  EISSN = {1617-4917},
  ORCID = {Ospitia, Yesid/0000-0002-5494-2787 Beltran, Jose Ramon/0000-0002-7500-4650 Baldassarri, Sandra/0000-0002-9315-6391},
  RESEARCHERID = {Ospitia, Yesid/AAD-6729-2021 Beltran, Jose Ramon/K-7693-2015 Baldassarri, Sandra/L-6033-2014},
  EARLYACCESSDATE = {APR 2020},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{ntalampiras2017at,
  AUTHOR = {Ntalampiras, Stavros},
  JOURNAL = {Journal of the Acoustical Society of America},
  NOTE = {Cited by: 23; All Open Access, Green Open Access},
  NUMBER = {3},
  PAGES = {1694 – 1701},
  TITLE = {A transfer learning framework for predicting the emotional content of generalized sound events},
  TYPE = {Article},
  VOLUME = {141},
  YEAR = {2017},
  NOTES.CA = {exclude, does not appear music features analyzed separately (in absence of sound features)},
  DOI = {10.1121/1.4977749},
  ABSTRACT = {Predicting the emotions evoked by generalized sound events is a relatively recent research domain which still needs attention. In this work a framework aiming to reveal potential similarities existing during the perception of emotions evoked by sound events and songs is presented. To this end the following are proposed: (a) the usage of temporal modulation features, (b) a transfer learning module based on an echo state network, and (c) a k-medoids clustering algorithm predicting valence and arousal measurements associated with generalized sound events. The effectiveness of the proposed solution is demonstrated after a thoroughly designed experimental phase employing both sound and music data. The results demonstrate the importance of transfer learning in the specific field and encourage further research on approaches which manage the problem in a synergistic way. © 2017 Acoustical Society of America.},
  KEYWORDS = {Acoustic Stimulation; Adult; Algorithms; Auditory Perception; Cues; Emotions; Female; Humans; Male; Models, Theoretical; Music; Pattern Recognition, Physiological; Sound; Time Factors; Transfer (Psychology); Young Adult; Behavioral research; Forecasting; Echo state networks; K-medoids clustering; Music data; Recent researches; Sound events; Temporal modulations; Transfer learning; adult; algorithm; association; auditory stimulation; emotion; female; hearing; human; male; music; pattern recognition; sound; theoretical model; time factor; transfer of learning; young adult; Clustering algorithms},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85015211289&doi = 10.1121%2f1.4977749&partnerID = 40&md5 = 744823c3bb1062a093db56fb31511f25},
  NOTES.TE = {exclude, no music},
  COMMENT = {discussed},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{orjesek2022en,
  AUTHOR = {Orjesek, Richard and Jarina, Roman and Chmulik, Michal},
  JOURNAL = {MULTIMEDIA TOOLS AND APPLICATIONS},
  MONTH = {FEB},
  NUMBER = {4},
  PAGES = {5017-5031},
  TITLE = {End-to-end music emotion variation detection using iteratively reconstructed deep features},
  VOLUME = {81},
  YEAR = {2022},
  NOTES.CA = {include},
  DOI = {10.1007/s11042-021-11584-7},
  SOURCE = {web_of_science},
  NOTES.TE = {include, both R2 and classification included},
  BDSK = {https://doi.org/10.1007/s11042-021-11584-7},
  ISSN = {1380-7501},
  EISSN = {1573-7721},
  ORCID = {Chmulik, Michal/0000-0002-0513-5129 Jarina, Roman/0000-0002-0478-5808},
  RESEARCHERID = {Chmulik, Michal/IQW-1183-2023 Jarina, Roman/E-2541-2018},
  EARLYACCESSDATE = {JAN 2022},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{pandeya2024gl,
  AUTHOR = {Pandeya, Yagya Raj and Lee, Joonwhoan},
  JOURNAL = {Multimedia Tools and Applications},
  NOTE = {Cited by: 0},
  TITLE = {GlocalEmoNet: An optimized neural network for music emotion classification and segmentation using timbre and chroma features},
  TYPE = {Article},
  YEAR = {2024},
  NOTES.CA = {exclude, insufficient detail for meta-analysis},
  DOI = {10.1007/s11042-024-18246-4},
  ABSTRACT = {Music is a powerful language capable of eliciting a variety of emotions in individuals. Understanding and recognizing these emotions is pivotal for applications ranging from personalized music recommendations and music therapy to automatic music composition and affective computing. Presently, deep learning for music emotion recognition is gaining popularity, primarily relying on timbre features to capture local spatial information. However, there is an untapped potential in incorporating other pertinent audio features and global correlations in the feature space to capture the repetitive temporal information of music for emotion classification. This study introduces GlocalEmoNet as a method to capture both local and global correlations in music, utilizing timbre and Chroma audio features for tasks related to emotion classification and segmentation. The neural network underwent training and testing on approximately six thousand music audio samples, encompassing six music-emotion categories. The utilization of a genetic algorithm is employed for optimizing the hyperparameters of the proposed neural networks, aiming to attain optimal performance, efficiency, and generalization. The best classifier demonstrated superior performance, surpassing previously published results by a significant margin of approximately 14%. The optimal classifier achieved an accuracy score of 81.66%, an f1-score of 0.812, and an area under the curve score of 0.956. The evaluation of classification and segmentation outcomes also involved the use of visual representations. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.},
  KEYWORDS = {Audio acoustics; Classification (of information); Computer music; Deep learning; Emotion Recognition; Music; Audio features; Chroma features; Emotion classification; Global correlation; Glocalemonet; Music emotion classifications; Music emotions; Music recommendation; Neural-networks; Segmentation; Genetic algorithms},
  PUBLICATION_STAGE = {Article in press},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85185132967&doi = 10.1007%2fs11042-024-18246-4&partnerID = 40&md5 = 96770150ba0f11e7611abf80bffde28a},
  NOTES.TE = {include, classification},
  AUTHOR_KEYWORDS = {Classification; Genetic algorithm; GlocalEmoNet; Music emotion; Segmentation},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{panwar2019ar,
  AUTHOR = {Panwar, Sharaj and Rad, Paul and Choo, Kim-Kwang Raymond and Roopaei, Mehdi},
  JOURNAL = {JOURNAL OF SUPERCOMPUTING},
  MONTH = {JUN},
  NUMBER = {6, SI},
  PAGES = {2986-3009},
  TITLE = {Are you emotional or depressed? Learning about your emotional state from your music using machine learning},
  VOLUME = {75},
  YEAR = {2019},
  NOTES.CA = {include, authors add detail about VA accuracy},
  DOI = {10.1007/s11227-018-2499-y},
  SOURCE = {web_of_science},
  NOTES.TE = {include, DEAM},
  BDSK = {https://doi.org/10.1007/s11227-018-2499-y},
  ISSN = {0920-8542},
  EISSN = {1573-0484},
  ORCID = {Choo, Kim-Kwang Raymond/0000-0001-9208-5336},
  RESEARCHERID = {Choo, Kim-Kwang Raymond/A-3634-2009 najafirad, peyman/ACB-9554-2022},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{saizclar2022pr,
  AUTHOR = {Saiz-Clar, Elena and Angel Serrano, Miguel and Manuel Reales, Jose},
  JOURNAL = {PSYCHOLOGY OF MUSIC},
  MONTH = {JUL},
  NUMBER = {4},
  PAGES = {1107-1120},
  TITLE = {Predicting emotions in music using the onset curve},
  VOLUME = {50},
  YEAR = {2022},
  NOTES.CA = {exclude, no modeling task},
  DOI = {10.1177/03057356211031658},
  SOURCE = {web_of_science},
  NOTES.TE = {include, R2},
  BDSK = {https://doi.org/10.1177/03057356211031658},
  ISSN = {0305-7356},
  EISSN = {1741-3087},
  ORCID = {Serrano, Miguel-Angel/0000-0002-6574-4532 Serrano, Miguel-Angel/0000-0002-6574-4532 Saiz-Clar, Elena/0000-0002-8714-2305},
  RESEARCHERID = {Serrano, Miguel-Angel/ABE-7279-2021 Serrano, Miguel-Angel/AAZ-4393-2020},
  EARLYACCESSDATE = {AUG 2021},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{sanmillancastillo2022an,
  AUTHOR = {San Millan-Castillo, Roberto and Martino, Luca and Morgado, Eduardo and Llorente, Fernando},
  JOURNAL = {IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING},
  PAGES = {2460-2474},
  TITLE = {An Exhaustive Variable Selection Study for Linear Models of Soundscape Emotions: Rankings and Gibbs Analysis},
  VOLUME = {30},
  YEAR = {2022},
  NOTES.CA = {exclude, no music},
  DOI = {10.1109/TASLP.2022.3192664},
  SOURCE = {web_of_science},
  NOTES.TE = {include, R2 albeit mostly focussed on ranking features},
  BDSK = {https://doi.org/10.1109/TASLP.2022.3192664},
  ISSN = {2329-9290},
  EISSN = {2329-9304},
  ORCID = {Llorente Fernandez, Fernando/0000-0003-4436-5709 Morgado, Eduardo/0000-0001-9243-409X Martino, Luca/0000-0002-7611-6558 San Millan-Castillo, Roberto/0000-0001-5821-9767},
  RESEARCHERID = {Morgado, Eduardo/G-2185-2016},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{shen2024re,
  AUTHOR = {Shen, Dan and Zhao, Wenjia},
  JOURNAL = {Computer-Aided Design and Applications},
  NOTE = {Cited by: 0; All Open Access, Bronze Open Access},
  NUMBER = {S1},
  PAGES = {204 – 217},
  TITLE = {Research on the Construction of Music Animation CAD System Based on Music Form and Emotion Recognition},
  TYPE = {Article},
  VOLUME = {21},
  YEAR = {2024},
  NOTES.CA = {unsure, insufficient detail, quality issues},
  DOI = {10.14733/cadaps.2024.S1.204-217},
  ABSTRACT = {Music, as a unique artistic form of people’s emotional expression, focuses on a rendering of the soul of the music audience and a re-experience of the virtual reality world. This experience is all-round, not just limited to hearing. In the process of music visualization, it is difficult to mechanically transform it into vision by a single rule. How to enhance the musical form and emotional characteristics of visual music works through multi-channel mapping mode is the research purpose of this paper. This article proposes a music based emotion recognition model feature, and a musical animation CAD (Computer Aided Design) system is constructed. The system extracts some basic musical features from MIDI (Musical Instrument Digital Interface) files, and then extracts the musical features of the music. Thus, each music segment is sliced into emotional music visualization programs. And by summarizing the obtained segment features as a whole, design an emotional feature program that can reflect the musical form. Through deep learning algorithms, it visualizes and matches the improved segment nodes, improving the expressive form of music emotional animation. © 2024, CAD Solutions, LLC. All rights reserved.},
  KEYWORDS = {Animation; Audio acoustics; Audition; Computer aided design; Computer aided instruction; Deep learning; Emotion Recognition; Learning algorithms; Learning systems; Speech recognition; Virtual reality; Visualization; Computer aided design systems; Computer-aided design; Deep learning; Emotion recognition; Emotional expressions; Emotional recognition; Form recognition; Music animation; Music visualization; Musical features; Music},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85169049920&doi = 10.14733%2fcadaps.2024.S1.204-217&partnerID = 40&md5 = 34fec38348b00e63cecb493ef6e85bad},
  NOTES.TE = {exclude, no valence/arousal nor clear outcome},
  AUTHOR_KEYWORDS = {Computer Aided Design; Deep Learning; Emotional Recognition; Music Animation; Musical Features; Visualization},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{tang2023ap,
  AUTHOR = {Tang, Zhangcheng},
  JOURNAL = {International Journal of Advanced Computer Science and Applications},
  NOTE = {Cited by: 0; All Open Access, Gold Open Access},
  NUMBER = {11},
  PAGES = {1055 – 1062},
  TITLE = {Application Model Construction of Emotional Expression and Propagation Path of Deep Learning in National Vocal Music},
  TYPE = {Article},
  VOLUME = {14},
  YEAR = {2023},
  NOTES.CA = {exclude, uses image features},
  DOI = {10.14569/IJACSA.2023.01411107},
  ABSTRACT = {Emotional expression is important in Chinese national vocal music art. The emotional expression in national vocal music is based on the art of national vocal music, with distinct characteristics and requirements. The ultimate goal is to spread the expression of various emotions in the national vocal music art. Promoting the spread of national vocal music singing art using modern media is an urgent requirement for the inheritance and development of national vocal music singing art. With the rapid development of science and technology, integrating deep learning and traditional music has become the general trend. It has been gradually applied to melody recognition, intelligent composition, virtual performance, and other aspects of traditional music and has achieved good results, but also hidden behind a series of ideas and technical and ethical issues. In this paper, the application of deep learning has been discussed and prospected. The recognition rate of emotional expression in national vocal music is 92 %. In terms of communication, combined with the deep learning algorithm, this paper analyzes the characteristics and requirements of emotional expression in the art of national vocal music singing and puts forward a new method of promoting the development of the art of national vocal music singing, hoping to attract more attention and enhance the social awareness of the application field, to promote the steady development of Chinese traditional music in the information age. © (2023), (Science and Information Organization). All Rights Reserved.},
  KEYWORDS = {Deep learning; Emotion Recognition; Learning algorithms; Learning systems; Application models; Deep learning; Dissemination; Emotion; Emotional expressions; Innovation; Model construction; National vocal music; Propagation paths; Vocal music; Music},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85179180244&doi = 10.14569%2fIJACSA.2023.01411107&partnerID = 40&md5 = ff6c976bdcf3b3aeaaf465465cd1f940},
  NOTES.TE = {unsure, does not contain relevant features, lack of details on stimuli},
  AUTHOR_KEYWORDS = {Deep learning; dissemination; emotion; innovation; national vocal music},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{vempala2024pr,
  AUTHOR = {Vempala, Naresh and Russo, Frank and Lab, Smart},
  MONTH = {},
  TITLE = {Predicting Emotion from Music Audio Features Using Neural Networks},
  YEAR = {2024},
  NOTES.CA = {include},
  DOI = {10.32920/25413184},
  ABSTRACT = {{$<$}p{$ > $}We describe our implementation of two neural networks: a static feedforward network, and an Elman network, for predicting mean valence/arousal ratings of participants for musical excerpts based on audio features. Thirteen audio features were extracted from 12 classical music excerpts (3 from each emotion quadrant). Valence/arousal ratings were collected from 45 participants for the static network, and 9 participants for the Elman network. For the Elman network, each excerpt was temporally segmented into four, sequential chunks of equal duration. Networks were trained on eight of the 12 excerpts and tested on the remaining four. The static network predicted values that closely matched mean participant ratings of valence and arousal. The Elman network did a good job of predicting the arousal trend but not the valence trend. Our study indicates that neural networks can be trained to identify statistical consistencies across audio features to predict valence/arousal values.{$<$}/p{$ > $}},
  KEYWORDS = {Audio Event Detection; Melody Extraction; Emotion Recognition; Affective Computing; Speech Emotion},
  SOURCE = {open_alex},
  URL = {https://doi.org/10.32920/25413184},
  NOTES.TE = {include, classification of quadrant},
  BDSK = {https://doi.org/10.32920/25413184},
  DATE = {2024-05-13 15:05:26 +0100},
  DATE.1 = {2024-05-13 15:05:26 +0100},
  LA = {en},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{wang2015mo,
  AUTHOR = {Wang, Ju-Chiang and Yang, Yi-Hsuan and Wang, Hsin-Min and Jeng, Shyh-Kang},
  JOURNAL = {IEEE TRANSACTIONS ON AFFECTIVE COMPUTING},
  MONTH = {JAN-MAR},
  NUMBER = {1},
  PAGES = {56-68},
  TITLE = {Modeling the Affective Content of Music with a Gaussian Mixture Model},
  VOLUME = {6},
  YEAR = {2015},
  NOTES.CA = {exclude, no relevant task},
  DOI = {10.1109/TAFFC.2015.2397457},
  SOURCE = {web_of_science},
  NOTES.TE = {include, not sure of the outcome measures (no R2)},
  BDSK = {https://doi.org/10.1109/TAFFC.2015.2397457},
  ISSN = {1949-3045},
  ORCID = {Wang, Hsin-Min/0000-0003-3599-5071},
  RESEARCHERID = {Wang, Hsin-Min/ABA-8747-2020},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Incollection{wang2016af,
  AUTHOR = {Wang, Ju-Chiang and Yang, Yi-Hsuan and Wang, Hsin-Min},
  BOOKTITLE = {EMOTIONS AND PERSONALITY IN PERSONALIZED SERVICES: MODELS, EVALUATION AND APPLICATIONS},
  EDITOR = {Tkalcic, M and DeCarolis, B and DeGemmis, M and Odic, A and Kosir, A},
  PAGES = {227-261},
  SERIES = {Human-Computer Interaction Series},
  TITLE = {Affective Music Information Retrieval},
  YEAR = {2016},
  NOTES.CA = {exclude, chapter},
  DOI = {10.1007/978-3-319-31413-6\_12},
  SOURCE = {web_of_science},
  NOTES.TE = {include, preprint, R2},
  BDSK = {https://doi.org/10.1007/978-3-319-31413-6%5C_12},
  ISBN = {978-3-319-31413-6; 978-3-319-31411-2},
  ISSN = {1571-5035},
  EISSN = {2524-4477},
  ORCID = {Wang, Hsin-Min/0000-0003-3599-5071},
  RESEARCHERID = {Wang, Hsin-Min/ABA-8747-2020},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{wang2021ac,
  AUTHOR = {Wang, Xin and Wei, Yujia and Heng, Lena and McAdams, Stephen},
  JOURNAL = {FRONTIERS IN PSYCHOLOGY},
  MONTH = {SEP 29},
  TITLE = {A Cross-Cultural Analysis of the Influence of Timbre on Affect Perception in Western Classical Music and Chinese Music Traditions},
  VOLUME = {12},
  YEAR = {2021},
  NOTES.CA = {include, acoustic model present in paper},
  DOI = {10.3389/fpsyg.2021.732865},
  SOURCE = {web_of_science},
  NOTES.TE = {include},
  BDSK = {https://doi.org/10.3389/fpsyg.2021.732865},
  ISSN = {1664-1078},
  ARTICLE = {732865},
  ORCID = {McAdams, Stephen/0000-0002-6744-9035 Heng, Lena/0000-0002-4395-2576},
  RESEARCHERID = {McAdams, Stephen/GQB-0225-2022 Wei, Yujia/IAQ-8917-2023},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{wang2022co,
  AUTHOR = {Wang, Xin and Wang, Li and Xie, Lingyun},
  JOURNAL = {Applied Sciences (Switzerland)},
  NOTE = {Cited by: 7; All Open Access, Gold Open Access},
  NUMBER = {12},
  TITLE = {Comparison and Analysis of Acoustic Features of Western and Chinese Classical Music Emotion Recognition Based on V‐A Model},
  TYPE = {Article},
  VOLUME = {12},
  YEAR = {2022},
  NOTES.CA = {include, though reports negative r^2 values in model comparison in table 2},
  DOI = {10.3390/app12125787},
  ABSTRACT = {Music emotion recognition is increasingly becoming important in scientific research and practical applications. Due to the differences in musical characteristics between Western and Chinese classical music, it is necessary to investigate the distinctions in music emotional feature sets to improve the accuracy of cross‐cultural emotion recognition models. Therefore, a comparative study on emotion recognition in Chinese and Western classical music was conducted. Using the V‐A model as an emotional perception model, approximately 1000 pieces of Western and Chinese classical excerpts in total were selected, and approximately 20‐dimension feature sets for different emotional dimensions of different datasets were finally extracted. We considered different kinds of al-gorithms at each step of the training process, from pre‐processing to feature selection and regression model selection. The results reveal that the combination of MaxAbsScaler pre‐processing and the wrapper method using the recursive feature elimination algorithm based on extremely randomized trees is the optimal algorithm. The harmonic change detection function is a culturally universal fea-ture, whereas spectral flux is a culturally specific feature for Chinese classical music. It is also found that pitch features are more significant for Western classical music, whereas loudness and rhythm features are more significant for Chinese classical music. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85132193259&doi = 10.3390%2fapp12125787&partnerID = 40&md5 = 96f19b725a52bb52a769612e02f0a74c},
  NOTES.TE = {include},
  AUTHOR_KEYWORDS = {classical music; extreme random tree; feature selection; music emotion recognition; V‐A model},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{wang2022cr,
  AUTHOR = {Wang, Xin and Wei, Yujia and Yang, Dasheng},
  JOURNAL = {COGNITIVE COMPUTATION AND SYSTEMS},
  MONTH = {JUN},
  NUMBER = {2, SI},
  PAGES = {116-129},
  TITLE = {Cross-cultural analysis of the correlation between musical elements and emotion},
  VOLUME = {4},
  YEAR = {2022},
  NOTES.CA = {include},
  DOI = {10.1049/ccs2.12032},
  SOURCE = {web_of_science},
  NOTES.TE = {include, regression},
  BDSK = {https://doi.org/10.1049/ccs2.12032},
  EISSN = {2517-7567},
  RESEARCHERID = {Wei, Yujia/IAQ-8917-2023},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{wang2024em,
  AUTHOR = {Wang, Lu},
  JOURNAL = {Computer-Aided Design and Applications},
  NOTE = {Cited by: 0; All Open Access, Bronze Open Access},
  NUMBER = {s8},
  PAGES = {44 – 55},
  TITLE = {Embedded Systems for Analyzing Digital Art Aesthetics in Piano Performances using Emotional Recognition},
  TYPE = {Article},
  VOLUME = {21},
  YEAR = {2024},
  NOTES.CA = {exclude, insufficient detail},
  DOI = {10.14733/cadaps.2024.S8.44-55},
  ABSTRACT = {Aesthetics is an innate ability. It is a meaningful study to make computers perceive "beauty", discover "beauty" and generate "beauty". With the deepening of intelligent optimization algorithm research, artificial intelligence technology "aesthetic" has penetrated into photos, paintings, web pages, ICONS, men and other aspects. However, there are very few studies on the evaluation of piano performance aesthetics. The study of piano performance aesthetics has certain research significance. First of all, limited by personal time and energy, people cannot select high-quality piano repertoire quickly. Secondly, limited by personal aesthetic consciousness and aesthetic ability, people cannot improve the aesthetic quality of piano music just like professional piano players. In the face of such problems, the aesthetic quality evaluation and improvement technology with artificial intelligence as the core provides economically feasible solutions for people to obtain high-quality tracks. Meanwhile, this technology promotes the development of simulated human aesthetic and thinking technology in the field of artificial intelligence. Since the key to aesthetics lies in the perception and classification of piano music score, timbre, audio and emotion, the emotion recognition of piano performance is crucial for the research of artificial intelligence "aesthetics". Piano performance emotion recognition is realized by using the computer to analyze performance characteristics and according to the mapping relationship between performance characteristics and emotion. The study of automatic emotion recognition of piano performance is of great significance to improve the human-computer emotional interaction ability of computer. Based on the above analysis, the main work and innovations of this paper are as follows:This paper first with MIDI music file as a research sample, follow the research method of classical music theory, combined with music psychology, cognitive psychology, music aesthetics and other related research results, the characteristics of the piano performance of a comprehensive and detailed description, and established a set of suitable for computer understanding and expression of the piano performance characteristics system. In the process of feature extraction of piano performance features, high-level features such as rhythm, speed and melody are mathematically defined. In this paper, we realize the computer recognition of the piano playing emotion by using the BP neural network. Finally, the research in this paper can realize the emotional classification of piano performance from the perspective of artificial intelligence, which can use the above research content to quickly and automatically select high-quality piano performance tracks, saving a lot of time for manual screening. © 2024 U-turn Press LLC.},
  KEYWORDS = {Behavioral research; Embedded systems; Emotion Recognition; Human computer interaction; Musical instruments; Neural networks; Quality control; Speech recognition; Websites; Aesthetic qualities; BP neural networks; Digital art; Embedded-system; Emotional recognition; Esthetic research; High quality; Performance; Performance characteristics; Piano playing; Music},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85171447451&doi = 10.14733%2fcadaps.2024.S8.44-55&partnerID = 40&md5 = a5abac1875bbab01840e5162b2f2ad88},
  NOTES.TE = {exclude, no outcome measures},
  AUTHOR_KEYWORDS = {Aesthetic Research; BP neural network; Digital Art; Embedded Systems; Emotional Recognition; Piano Playing},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{xia2022st,
  AUTHOR = {Xia, Yu and Xu, Fumei},
  JOURNAL = {Mathematical Problems in Engineering},
  NOTE = {Cited by: 6; All Open Access, Gold Open Access},
  TITLE = {Study on Music Emotion Recognition Based on the Machine Learning Model Clustering Algorithm},
  TYPE = {Article},
  VOLUME = {2022},
  YEAR = {2022},
  NOTES.CA = {include},
  DOI = {10.1155/2022/9256586},
  ABSTRACT = {In recent years, the explosive growth of online music resources makes it difficult to retrieve and manage music information. To efficiently retrieve and classify music information has become a hot research topic. Thayer's two-dimensional emotion plane is selected as the basis for establishing the music emotion database. Music is divided into five categories, the concept of continuous emotion perception is introduced, and music emotion is regarded as a point on a two-dimensional emotional plane, together with the two sentiment variables to determine its location. The artificial labeling method is used to determine the position range of the five types of emotions on the emotional plane, and the regression method is used to obtain the relationship between the VA value and the music features so that the music emotion classification problem is transformed into a regression problem. A regression-based music emotion classification system is designed and implemented, which mainly includes a training part and a testing part. In the training part, three algorithms, namely, polynomial regression, support vector regression, and k-plane piecewise regression, are used to obtain the regression model. In the test part, the input music data is regressed and predicted to obtain its VA value and then classified, and the system performance is considered by classification accuracy. Results show that the combined method of support vector regression and k-plane piecewise regression improves the accuracy by 3 to 4 percentage points compared to using one algorithm alone; compared with the traditional classification method based on a support vector machine, the accuracy improves by 6 percentage points. Music emotion is classified by algorithms such as support vector machine classification, K-neighborhood classification, fuzzy neural network classification, fuzzy K-neighborhood classification, Bayesian classification, and Fisher linear discrimination, among which the support vector machine, fuzzy K-neighborhood, and the accuracy rate of music emotion classification realized by Fisher linear discriminant algorithm are more than 80%; a new algorithm "mixed classifier"is proposed, and the music emotion recognition rate based on this algorithm reaches 84.9%. © 2022 Yu Xia and Fumei Xu.},
  KEYWORDS = {Behavioral research; Classification (of information); Clustering algorithms; Fuzzy neural networks; Information management; Regression analysis; Speech recognition; Support vector machines; Vectors; Classifieds; Emotion recognition; K neighborhoods; Music emotion classifications; Music emotions; Music information; Percentage points; Piecewise regression; Support vector regressions; Two-dimensional; Emotion Recognition},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85140847927&doi = 10.1155%2f2022%2f9256586&partnerID = 40&md5 = 95604b41d31354a5954af89be5f47193},
  NOTES.TE = {include, although some quality issues},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{xiang2022co,
  AUTHOR = {Xiang, Yuehua},
  JOURNAL = {Mathematical Problems in Engineering},
  NOTE = {Cited by: 7; All Open Access, Gold Open Access},
  TITLE = {Computer Analysis and Automatic Recognition Technology of Music Emotion},
  TYPE = {Article},
  VOLUME = {2022},
  YEAR = {2022},
  NOTES.CA = {exclude, quality issues},
  DOI = {10.1155/2022/3145785},
  ABSTRACT = {With the rapid development of the related computer industry, the use of computer-related technologies has become more and more frequent. The music industry is no exception. The research and analysis of music emotions has been a problem since ancient times. Due to the diversification of music emotions, people with different music in the same piece of music will have different feelings. The research topic of this article is to make a comprehensive analysis of the computer's automatic identification technology, combined with the powerful subcapacity of the computer, so that the research on music emotion can be developed rapidly. The article analyzes the technical research of the automatic recognition and analysis of music emotion in the computer, and conducts a comprehensive analysis of the music emotion through the research of the computer-related automatic recognition technology. This paper focuses on the computer automatic recognition model of music emotion, and successfully realizes the design and simulation of the automatic recognition system based on the MATLAB platform. An automatic identification model using BP neural network algorithm is proposed. By comparing it with the statistical classification algorithm, the experimental results verify the effectiveness of the designed BP network in music emotion recognition. © 2022 Yuehua Xiang.},
  KEYWORDS = {Automation; Music; Neural networks; Simulation platform; Automatic identification; Automatic recognition; Comprehensive analysis; Computer analysis; Computer industry; Identification technology; Music emotions; Music industry; Research and analysis; Research topics; MATLAB},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85128135662&doi = 10.1155%2f2022%2f3145785&partnerID = 40&md5 = 3ee19512d1d7e1d34ae7e823a78310b3},
  NOTES.TE = {exclude},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{xie2020mu,
  AUTHOR = {Xie, Baijun and Kim, Jonathan C. and Park, Chung Hyuk},
  JOURNAL = {APPLIED SCIENCES-BASEL},
  MONTH = {FEB},
  NUMBER = {3},
  TITLE = {Musical Emotion Recognition with Spectral Feature Extraction Based on a Sinusoidal Model with Model-Based and Deep-Learning Approaches},
  VOLUME = {10},
  YEAR = {2020},
  NOTES.CA = {include},
  DOI = {10.3390/app10030902},
  SOURCE = {web_of_science},
  NOTES.TE = {include},
  BDSK = {https://doi.org/10.3390/app10030902},
  ARTICLE = {902},
  EISSN = {2076-3417},
  ORCID = {Park, Chung Hyuk/0000-0003-0742-6541 Xie, Baijun/0000-0001-5080-198X},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{xing2015em,
  AUTHOR = {Xing, Baixi and Zhang, Kejun and Sun, Shouqian and Zhang, Lekai and Gao, Zenggui and Wang, Jiaxi and Chen, Shi},
  JOURNAL = {Neurocomputing},
  NOTE = {Cited by: 28},
  PAGES = {619 – 627},
  TITLE = {Emotion-driven Chinese folk music-image retrieval based on DE-SVM},
  TYPE = {Article},
  VOLUME = {148},
  YEAR = {2015},
  NOTES.CA = {exclude, only reports det and equal error rate},
  DOI = {10.1016/j.neucom.2014.08.007},
  ABSTRACT = {In this study, we attempt to explore cross-media retrieval between music and image data based on the emotional correlation. Emotion feature analytic could be the bridge of cross-media retrieval, since emotion represents the user's perspective and effectively meets the user's retrieval need. Currently, there is little research about the emotion correlation of different multimedia data (e.g. image or music). We propose a promising model based on Differential Evolutionary-Support Vector Machine (DE-SVM) to build up the emotion-driven cross-media retrieval system between Chinese folk image and Chinese folk music. In this work, we first build up the Chinese Folk Music Library and Chinese Folk Image Library.Second, we compare Back Propagation(BP), Linear Regression(LR) and Differential Evolutionary-Support Vector Machine (DE-SVM), and find that DE-SVM has the best performance. Then we conduct DE-SVM to build the optimal model for music/image emotion recognition. Finally, an Emotion-driven Chinese Folk Music-Image Exploring System based on DE-SVM is developed and experiment results show our method is effective in terms of retrieval performance. © 2014 Elsevier B.V.},
  KEYWORDS = {Cross-media information retrieval; Differential evolutionary algorithm; Emotion recognition; Music emotions; accuracy; Article; back propagation; calculation; Chinese; classification algorithm; cultural period; Differential Evolutionary Support Vector Machine algorithm; emotion; image retrieval; library; linear system; mathematical analysis; mathematical model; music; nonlinear system; support vector machine; Support vector machines},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-84908032200&doi = 10.1016%2fj.neucom.2014.08.007&partnerID = 40&md5 = 62cf1f46b87868a59e2d0da9852809db},
  NOTES.TE = {unsure, classification error measure MSE},
  AUTHOR_KEYWORDS = {Back propagation; Cross-media information retrieval; Differential Evolutionary algorithm; Image emotion recognition; Music emotion recognition; Support vector machine},
  COMMENT = {discussed, classification, unique},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{xu2021us,
  AUTHOR = {Xu, Liang and Sun, Zaoyi and Wen, Xin and Huang, Zhengxi and Chao, Chi-ju and Xu, Liuchang},
  JOURNAL = {PEERJ COMPUTER SCIENCE},
  MONTH = {NOV 15},
  TITLE = {Using machine learning analysis to interpret the relationship between music emotion and lyric features},
  VOLUME = {7},
  YEAR = {2021},
  NOTES.CA = {include},
  DOI = {10.7717/peerj-cs.785},
  SOURCE = {web_of_science},
  NOTES.TE = {include, audio prediction and R2 included},
  BDSK = {https://doi.org/10.7717/peerj-cs.785},
  ARTICLE = {e785},
  EISSN = {2376-5992},
  ORCID = {Xu, Liang/0000-0003-3889-927X Sun, Zaoyi/0000-0002-4551-3606},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{yang2022mu,
  AUTHOR = {Yang, Chaode and Li, Qingxun},
  JOURNAL = {Computer-Aided Design and Applications},
  NOTE = {Cited by: 5; All Open Access, Bronze Open Access},
  NUMBER = {S6},
  PAGES = {80 – 90},
  TITLE = {Music Emotion Feature Recognition based on Internet of Things and Computer-Aided Technology},
  TYPE = {Article},
  VOLUME = {19},
  YEAR = {2022},
  NOTES.CA = {exclude, no relevant task, quality issues},
  DOI = {10.14733/cadaps.2022.S6.80-90},
  ABSTRACT = {As an important part of human life, music can convey emotion and regulate the emotions of listeners. Emotion is one of the essential features of music, and the relationship between music and emotion has become the subject of many academic studies. At present, with the rapid development of information technology and artificial intelligence, music emotion recognition has made rapid progress and become one of the important research directions in the field of digital music. Aiming at the problem of poor classification effect of musical emotion caused by the monotony of Support Vector Machine (SVM) projection space, this paper proposes an optimized SVM model for music feature emotion recognition. The new method can not only improve the accuracy of music emotion classification, but also improve the running speed and interpretability of the model. At the end, the practicality and reliability of the new approach are verified by public classification data sets and real music emotion data sets. This paper proposes an optimized SVM model for music feature emotion recognition. The new method can not only improve the accuracy of music emotion classification, but also improve the running speed and interpretability of the model. Finally, the practicality and reliability of the new approach are verified by both the public classification data sets and real music emotion data sets. © 2022 CAD Solutions, LLC,.},
  KEYWORDS = {Classification (of information); Internet of things; Music; Speech recognition; Vector spaces; Computer aided technologies; Computer-aided technologies; Data set; Emotion recognition; Music emotion classifications; Music emotions; Music feature emotion recognition; Optimization algorithms; Support vector machine models; Support vectors machine; Support vector machines},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85122323526&doi = 10.14733%2fcadaps.2022.S6.80-90&partnerID = 40&md5 = d2f6a750386672cd2f4f123548cbd50b},
  NOTES.TE = {exclude, quality issues, lack of outcome measures},
  AUTHOR_KEYWORDS = {Computer aided technology; Music feature emotion recognition; Optimization algorithm; SVM},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{yang2023ex,
  AUTHOR = {Yang, Simin and Reed, Courtney N. and Chew, Elaine and Barthet, Mathieu},
  JOURNAL = {IEEE TRANSACTIONS ON AFFECTIVE COMPUTING},
  MONTH = {APR-JUN},
  NUMBER = {2},
  PAGES = {1442-1460},
  TITLE = {Examining Emotion Perception Agreement in Live Music Performance},
  VOLUME = {14},
  YEAR = {2023},
  NOTES.CA = {exclude, no relevant task},
  DOI = {10.1109/TAFFC.2021.3093787},
  SOURCE = {web_of_science},
  NOTES.TE = {include, not sure, maybe the final metrics missing},
  BDSK = {https://doi.org/10.1109/TAFFC.2021.3093787},
  ISSN = {1949-3045},
  ORCID = {Barthet, Mathieu/0000-0002-9869-1668 Reed, Courtney Nicole/0000-0003-0893-9277 Chew, Elaine/0000-0002-8342-1024},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{zhang2016br,
  AUTHOR = {Zhang, JiangLong and Huang, XiangLin and Yang, Lifang and Nie, Liqiang},
  JOURNAL = {NEUROCOMPUTING},
  MONTH = {OCT 5},
  NUMBER = {SI},
  PAGES = {333-341},
  TITLE = {Bridge the semantic gap between pop music acoustic feature and emotion: Build an interpretable model},
  VOLUME = {208},
  YEAR = {2016},
  NOTES.CA = {include},
  DOI = {10.1016/j.neucom.2016.01.099},
  SOURCE = {web_of_science},
  NOTES.TE = {include, classification},
  BDSK = {https://doi.org/10.1016/j.neucom.2016.01.099},
  ISSN = {0925-2312},
  EISSN = {1872-8286},
  ORCID = {Zhang, Jianglong/0000-0002-9079-2499},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{zhang2016rea,
  AUTHOR = {Zhang, Yang and Liu, Heng},
  JOURNAL = {RISTI - Revista Iberica de Sistemas e Tecnologias de Informacao},
  NOTE = {Cited by: 0},
  NUMBER = {E13},
  PAGES = {321 – 331},
  TITLE = {Research on music production based on computer analysis and recognition technology},
  TYPE = {Article},
  VOLUME = {2016},
  YEAR = {2016},
  NOTES.CA = {unsure, cannot access full text},
  ABSTRACT = {Based on the researches and improvement of the main melody recognition technology of music, this paper constructs the music feature space model, and the characteristic parameters of the model feature space are marked, furthermore on this basis, the music emotion computer automatic recognition model is studied, and design and simulation of automatic recognition system are achieved based on the MATLAB platform. This paper also proposes an automatic recognition model by using BP neural network algorithm. Experimental results show that the proposed BP neural network is effective in music emotion recognition after comparing it with the statistical classification algorithm. © AISTI 2016.},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85016046810&partnerID = 40&md5 = 7c6f1412441ed987b75e88238a9e53ce},
  NOTES.TE = {exclude, no proper outcome measures},
  AUTHOR_KEYWORDS = {Computer analysis; Computer recognition technology; Emotion analysis; Music production},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {exclude}
}


@Article{zhang2019us,
  AUTHOR = {Zhang, Le-kai and Sun, Shou-qian and Xing, Bai-xi and Luo, Rui-ming and Zhang, Ke-jun},
  JOURNAL = {FRONTIERS OF INFORMATION TECHNOLOGY \& ELECTRONIC ENGINEERING},
  MONTH = {JUL},
  NUMBER = {7},
  PAGES = {964-974},
  TITLE = {Using psychophysiological measures to recognize personal music emotional experience},
  VOLUME = {20},
  YEAR = {2019},
  NOTES.CA = {include, single-modality reported},
  DOI = {10.1631/FITEE.1800101},
  SOURCE = {web_of_science},
  NOTES.TE = {include, use a subset of the results},
  BDSK = {https://doi.org/10.1631/FITEE.1800101},
  ISSN = {2095-9184},
  EISSN = {2095-9230},
  RESEARCHERID = {zhang, ke/AAH-8217-2019},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{zhang2023mo,
  AUTHOR = {Zhang, Meixian and Zhu, Yonghua and Zhang, Wenjun and Zhu, Yunwen and Feng, Tianyu},
  JOURNAL = {MULTIMEDIA TOOLS AND APPLICATIONS},
  MONTH = {FEB},
  NUMBER = {5},
  PAGES = {7319-7341},
  TITLE = {Modularized composite attention network for continuous music emotion recognition},
  VOLUME = {82},
  YEAR = {2023},
  NOTES.CA = {include},
  DOI = {10.1007/s11042-022-13577-6},
  SOURCE = {web_of_science},
  NOTES.TE = {include, R2},
  BDSK = {https://doi.org/10.1007/s11042-022-13577-6},
  ISSN = {1380-7501},
  EISSN = {1573-7721},
  ORCID = {Zhang, Meixian/0000-0002-6696-2814},
  RESEARCHERID = {Zhu, Yonghua/HSI-1360-2023 Feng, Tian-Yu/AAX-3892-2020},
  EARLYACCESSDATE = {AUG 2022},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}


@Article{zhang2024ap,
  AUTHOR = {Zhang, Yao and Cai, Delin and Zhang, Dongmei},
  JOURNAL = {Environment and Social Psychology},
  NOTE = {Cited by: 0; All Open Access, Gold Open Access},
  NUMBER = {4},
  TITLE = {Application and algorithm optimization of music emotion recognition in piano performance evaluation},
  TYPE = {Article},
  VOLUME = {9},
  YEAR = {2024},
  NOTES.CA = {include},
  DOI = {10.54517/esp.v9i4.2344},
  ABSTRACT = {In the current research, we integrate distinct learning modalities—Curriculum Learning (CL) and Reinforcement Learning (RL)—in an attempt to develop and optimize Music Emotion Recognition (MER) in piano performance. Classical approaches have never been successful when applied in the field of determining the degree of emotion in the music of the piano, owing to the substantial complexity required. Addressing this particular issue is the primary motivation for the present endeavour. In an approach that’s comparable to how human beings acquire information, it trains the RL agent CL in phases; such an approach improves the student’s learning model in understanding the diverse emotions expressed by musical compositions. A higher rating of performance can be achieved after learning the model to recognize emotions more effectively and precisely. A set of piano melodies with emotional content notes has been included in the EMOPIA repository for use when conducting the process of evaluation. In order to benchmark the proposed approach with different models, parameters including R2, Mean Absolute Error (MAE), and Root Mean Square Error (RMSE) were deployed. Studies indicate that the recommended approach accurately recognizes the emotions expressed by piano-playing music. In challenging tasks like MER, the significance of implementing the CL paradigm with the RL has been emphasized using the outcomes mentioned earlier. © 2024 by author(s).},
  PUBLICATION_STAGE = {Final},
  SOURCE = {scopus},
  URL = {https://www.scopus.com/inward/record.uri?eid = 2-s2.0-85184876855&doi = 10.54517%2fesp.v9i4.2344&partnerID = 40&md5 = f2f5f142bc51490d199e335a63b73647},
  NOTES.TE = {include, classification of quadrants},
  AUTHOR_KEYWORDS = {Curriculum Learning; Machine Learning; MBE; Music Emotion Recognition; piano music; Reinforcement Learning; RMSE},
  PARADIGM = {regression},
  EMOTIONS = { },
  EMOTION_LOCUS = { },
  STIMULUS_TYPE = { },
  STIMULUS_DURATION = { },
  STIMULUS_DURATION_UNIT = { },
  STIMULUS_N = { },
  FEATURE_N = { },
  PARTICIPANT_N = { },
  PARTICIPANT_EXPERTISE = { },
  PARTICIPANT_ORIGIN = { },
  PARTICIPANT_SAMPLING = { },
  PARTICIPANT_TASK = { },
  FEATURE_CATEGORIES = { },
  FEATURE_SOURCE = { },
  FEATURE_REDUCTION_METHOD = { },
  MODEL_CATEGORY = { },
  MODEL_DETAIL = { },
  MODEL_MEASURE = { },
  MODEL_COMPLEXITY_PARAMETERS = { },
  MODEL_RATE_EMOTION_NAMES = { },
  MODEL_RATE_EMOTION_VALUES = { },
  MODEL_VALIDATION = { },
  FINAL_DECISION = {include}
}
